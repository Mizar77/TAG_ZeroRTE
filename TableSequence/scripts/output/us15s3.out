/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_3', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:14, 14.56s/it]Extractor Predicting: 2it [00:15,  6.71s/it]Extractor Predicting: 3it [00:16,  3.93s/it]Extractor Predicting: 4it [00:17,  2.63s/it]Extractor Predicting: 5it [00:17,  1.90s/it]Extractor Predicting: 6it [00:18,  1.49s/it]Extractor Predicting: 7it [00:18,  1.21s/it]Extractor Predicting: 8it [00:19,  1.03s/it]Extractor Predicting: 9it [00:20,  1.07it/s]Extractor Predicting: 10it [00:20,  1.22it/s]Extractor Predicting: 11it [00:21,  1.31it/s]Extractor Predicting: 12it [00:22,  1.39it/s]Extractor Predicting: 13it [00:22,  1.43it/s]Extractor Predicting: 14it [00:23,  1.48it/s]Extractor Predicting: 15it [00:24,  1.47it/s]Extractor Predicting: 16it [00:24,  1.51it/s]Extractor Predicting: 17it [00:25,  1.53it/s]Extractor Predicting: 18it [00:26,  1.54it/s]Extractor Predicting: 19it [00:26,  1.52it/s]Extractor Predicting: 20it [00:27,  1.54it/s]Extractor Predicting: 21it [00:27,  1.54it/s]Extractor Predicting: 22it [00:28,  1.53it/s]Extractor Predicting: 23it [00:29,  1.55it/s]Extractor Predicting: 24it [00:29,  1.52it/s]Extractor Predicting: 25it [00:30,  1.54it/s]Extractor Predicting: 26it [00:31,  1.55it/s]Extractor Predicting: 27it [00:31,  1.57it/s]Extractor Predicting: 28it [00:32,  1.55it/s]Extractor Predicting: 29it [00:33,  1.53it/s]Extractor Predicting: 30it [00:33,  1.57it/s]Extractor Predicting: 31it [00:34,  1.57it/s]Extractor Predicting: 32it [00:35,  1.57it/s]Extractor Predicting: 33it [00:35,  1.61it/s]Extractor Predicting: 34it [00:36,  1.60it/s]Extractor Predicting: 35it [00:36,  1.57it/s]Extractor Predicting: 36it [00:37,  1.54it/s]Extractor Predicting: 37it [00:38,  1.53it/s]Extractor Predicting: 38it [00:38,  1.53it/s]Extractor Predicting: 39it [00:39,  1.51it/s]Extractor Predicting: 40it [00:40,  1.52it/s]Extractor Predicting: 41it [00:40,  1.50it/s]Extractor Predicting: 42it [00:41,  1.51it/s]Extractor Predicting: 43it [00:42,  1.51it/s]Extractor Predicting: 44it [00:42,  1.49it/s]Extractor Predicting: 45it [00:43,  1.50it/s]Extractor Predicting: 46it [00:44,  1.53it/s]Extractor Predicting: 47it [00:44,  1.51it/s]Extractor Predicting: 48it [00:45,  1.50it/s]Extractor Predicting: 49it [00:46,  1.52it/s]Extractor Predicting: 50it [00:46,  1.51it/s]Extractor Predicting: 51it [00:47,  1.51it/s]Extractor Predicting: 52it [00:48,  1.54it/s]Extractor Predicting: 53it [00:48,  1.53it/s]Extractor Predicting: 54it [00:49,  1.51it/s]Extractor Predicting: 55it [00:50,  1.49it/s]Extractor Predicting: 56it [00:50,  1.51it/s]Extractor Predicting: 57it [00:51,  1.50it/s]Extractor Predicting: 58it [00:52,  1.51it/s]Extractor Predicting: 59it [00:52,  1.49it/s]Extractor Predicting: 60it [00:53,  1.49it/s]Extractor Predicting: 61it [00:54,  1.49it/s]Extractor Predicting: 62it [00:54,  1.53it/s]Extractor Predicting: 63it [00:55,  1.54it/s]Extractor Predicting: 64it [00:56,  1.53it/s]Extractor Predicting: 65it [00:56,  1.55it/s]Extractor Predicting: 66it [00:57,  1.54it/s]Extractor Predicting: 67it [00:58,  1.56it/s]Extractor Predicting: 68it [00:58,  1.57it/s]Extractor Predicting: 69it [00:59,  1.60it/s]Extractor Predicting: 70it [00:59,  1.61it/s]Extractor Predicting: 71it [01:00,  1.61it/s]Extractor Predicting: 72it [01:01,  1.58it/s]Extractor Predicting: 73it [01:01,  1.58it/s]Extractor Predicting: 74it [01:02,  1.47it/s]Extractor Predicting: 75it [01:03,  1.46it/s]Extractor Predicting: 76it [01:03,  1.46it/s]Extractor Predicting: 77it [01:04,  1.51it/s]Extractor Predicting: 78it [01:05,  1.56it/s]Extractor Predicting: 79it [01:05,  1.56it/s]Extractor Predicting: 80it [01:06,  1.57it/s]Extractor Predicting: 81it [01:07,  1.57it/s]Extractor Predicting: 82it [01:07,  1.59it/s]Extractor Predicting: 83it [01:08,  1.58it/s]Extractor Predicting: 84it [01:08,  1.55it/s]Extractor Predicting: 85it [01:09,  1.59it/s]Extractor Predicting: 86it [01:10,  1.59it/s]Extractor Predicting: 87it [01:10,  1.61it/s]Extractor Predicting: 88it [01:11,  1.60it/s]Extractor Predicting: 89it [01:12,  1.59it/s]Extractor Predicting: 90it [01:12,  1.59it/s]Extractor Predicting: 91it [01:13,  1.58it/s]Extractor Predicting: 92it [01:14,  1.53it/s]Extractor Predicting: 93it [01:14,  1.54it/s]Extractor Predicting: 94it [01:15,  1.55it/s]Extractor Predicting: 95it [01:15,  1.53it/s]Extractor Predicting: 96it [01:16,  1.52it/s]Extractor Predicting: 97it [01:17,  1.54it/s]Extractor Predicting: 98it [01:17,  1.57it/s]Extractor Predicting: 99it [01:18,  1.56it/s]Extractor Predicting: 100it [01:19,  1.53it/s]Extractor Predicting: 101it [01:19,  1.55it/s]Extractor Predicting: 102it [01:20,  1.54it/s]Extractor Predicting: 103it [01:21,  1.52it/s]Extractor Predicting: 104it [01:21,  1.53it/s]Extractor Predicting: 105it [01:22,  1.53it/s]Extractor Predicting: 106it [01:23,  1.52it/s]Extractor Predicting: 107it [01:23,  1.55it/s]Extractor Predicting: 108it [01:24,  1.56it/s]Extractor Predicting: 109it [01:25,  1.50it/s]Extractor Predicting: 110it [01:25,  1.50it/s]Extractor Predicting: 111it [01:26,  1.50it/s]Extractor Predicting: 112it [01:27,  1.51it/s]Extractor Predicting: 113it [01:27,  1.52it/s]Extractor Predicting: 114it [01:28,  1.52it/s]Extractor Predicting: 115it [01:29,  1.52it/s]Extractor Predicting: 116it [01:29,  1.55it/s]Extractor Predicting: 117it [01:30,  1.55it/s]Extractor Predicting: 118it [01:30,  1.56it/s]Extractor Predicting: 119it [01:31,  1.59it/s]Extractor Predicting: 120it [01:32,  1.63it/s]Extractor Predicting: 121it [01:32,  1.63it/s]Extractor Predicting: 122it [01:33,  1.63it/s]Extractor Predicting: 123it [01:34,  1.60it/s]Extractor Predicting: 124it [01:34,  1.60it/s]Extractor Predicting: 125it [01:35,  1.60it/s]Extractor Predicting: 126it [01:35,  1.59it/s]Extractor Predicting: 127it [01:36,  1.59it/s]Extractor Predicting: 128it [01:37,  1.55it/s]Extractor Predicting: 129it [01:37,  1.59it/s]Extractor Predicting: 130it [01:38,  1.56it/s]Extractor Predicting: 131it [01:39,  1.56it/s]Extractor Predicting: 132it [01:39,  1.57it/s]Extractor Predicting: 133it [01:40,  1.55it/s]Extractor Predicting: 134it [01:41,  1.54it/s]Extractor Predicting: 135it [01:41,  1.57it/s]Extractor Predicting: 136it [01:42,  1.58it/s]Extractor Predicting: 137it [01:42,  1.56it/s]Extractor Predicting: 138it [01:43,  1.55it/s]Extractor Predicting: 139it [01:44,  1.43it/s]Extractor Predicting: 140it [01:45,  1.46it/s]Extractor Predicting: 141it [01:45,  1.49it/s]Extractor Predicting: 142it [01:46,  1.51it/s]Extractor Predicting: 143it [01:47,  1.53it/s]Extractor Predicting: 144it [01:47,  1.55it/s]Extractor Predicting: 145it [01:48,  1.33it/s]Extractor Predicting: 145it [01:48,  1.33it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5563218390804597,
  "recall": 0.13872169676124965,
  "score": 0.22206928194540032,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:20,  1.49it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:21,  1.52it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:23,  1.61it/s]Extractor Predicting: 38it [00:24,  1.60it/s]Extractor Predicting: 39it [00:25,  1.60it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:26,  1.60it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.56it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:31,  1.57it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:32,  1.56it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:34,  1.58it/s]Extractor Predicting: 54it [00:34,  1.59it/s]Extractor Predicting: 55it [00:35,  1.55it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:36,  1.58it/s]Extractor Predicting: 58it [00:37,  1.62it/s]Extractor Predicting: 59it [00:37,  1.61it/s]Extractor Predicting: 60it [00:38,  1.69it/s]Extractor Predicting: 61it [00:38,  1.70it/s]Extractor Predicting: 62it [00:39,  1.72it/s]Extractor Predicting: 63it [00:40,  1.71it/s]Extractor Predicting: 64it [00:40,  1.74it/s]Extractor Predicting: 65it [00:41,  1.74it/s]Extractor Predicting: 66it [00:41,  1.74it/s]Extractor Predicting: 67it [00:42,  1.76it/s]Extractor Predicting: 68it [00:42,  1.80it/s]Extractor Predicting: 69it [00:43,  1.83it/s]Extractor Predicting: 70it [00:43,  1.81it/s]Extractor Predicting: 71it [00:44,  1.80it/s]Extractor Predicting: 72it [00:45,  1.81it/s]Extractor Predicting: 73it [00:45,  1.81it/s]Extractor Predicting: 74it [00:46,  1.79it/s]Extractor Predicting: 75it [00:46,  1.76it/s]Extractor Predicting: 76it [00:47,  1.79it/s]Extractor Predicting: 77it [00:47,  1.81it/s]Extractor Predicting: 78it [00:48,  1.80it/s]Extractor Predicting: 79it [00:48,  1.81it/s]Extractor Predicting: 80it [00:49,  1.82it/s]Extractor Predicting: 81it [00:50,  1.80it/s]Extractor Predicting: 82it [00:50,  1.78it/s]Extractor Predicting: 83it [00:51,  1.75it/s]Extractor Predicting: 84it [00:51,  1.74it/s]Extractor Predicting: 85it [00:52,  1.75it/s]Extractor Predicting: 86it [00:52,  1.71it/s]Extractor Predicting: 87it [00:53,  1.66it/s]Extractor Predicting: 88it [00:54,  1.63it/s]Extractor Predicting: 89it [00:54,  1.59it/s]Extractor Predicting: 90it [00:55,  1.54it/s]Extractor Predicting: 91it [00:56,  1.54it/s]Extractor Predicting: 92it [00:56,  1.52it/s]Extractor Predicting: 93it [00:57,  1.56it/s]Extractor Predicting: 94it [00:58,  1.54it/s]Extractor Predicting: 95it [00:58,  1.54it/s]Extractor Predicting: 96it [00:59,  1.53it/s]Extractor Predicting: 97it [01:00,  1.53it/s]Extractor Predicting: 98it [01:00,  1.52it/s]Extractor Predicting: 99it [01:01,  1.53it/s]Extractor Predicting: 100it [01:02,  1.52it/s]Extractor Predicting: 101it [01:02,  1.53it/s]Extractor Predicting: 102it [01:03,  1.51it/s]Extractor Predicting: 103it [01:04,  1.54it/s]Extractor Predicting: 104it [01:04,  1.50it/s]Extractor Predicting: 105it [01:05,  1.51it/s]Extractor Predicting: 106it [01:06,  1.48it/s]Extractor Predicting: 107it [01:06,  1.47it/s]Extractor Predicting: 108it [01:07,  1.48it/s]Extractor Predicting: 109it [01:08,  1.50it/s]Extractor Predicting: 110it [01:08,  1.50it/s]Extractor Predicting: 111it [01:09,  1.49it/s]Extractor Predicting: 112it [01:10,  1.45it/s]Extractor Predicting: 113it [01:10,  1.47it/s]Extractor Predicting: 114it [01:11,  1.51it/s]Extractor Predicting: 115it [01:12,  1.50it/s]Extractor Predicting: 116it [01:12,  1.52it/s]Extractor Predicting: 117it [01:13,  1.55it/s]Extractor Predicting: 118it [01:14,  1.50it/s]Extractor Predicting: 119it [01:14,  1.46it/s]Extractor Predicting: 120it [01:15,  1.48it/s]Extractor Predicting: 121it [01:16,  1.47it/s]Extractor Predicting: 122it [01:16,  1.49it/s]Extractor Predicting: 123it [01:17,  1.50it/s]Extractor Predicting: 124it [01:18,  1.50it/s]Extractor Predicting: 125it [01:18,  1.48it/s]Extractor Predicting: 126it [01:19,  1.45it/s]Extractor Predicting: 127it [01:20,  1.46it/s]Extractor Predicting: 128it [01:21,  1.43it/s]Extractor Predicting: 129it [01:21,  1.44it/s]Extractor Predicting: 130it [01:22,  1.44it/s]Extractor Predicting: 131it [01:23,  1.33it/s]Extractor Predicting: 132it [01:23,  1.38it/s]Extractor Predicting: 133it [01:24,  1.42it/s]Extractor Predicting: 134it [01:25,  1.46it/s]Extractor Predicting: 135it [01:25,  1.46it/s]Extractor Predicting: 136it [01:26,  1.44it/s]Extractor Predicting: 137it [01:27,  1.45it/s]Extractor Predicting: 138it [01:28,  1.45it/s]Extractor Predicting: 139it [01:28,  1.45it/s]Extractor Predicting: 140it [01:29,  1.47it/s]Extractor Predicting: 141it [01:30,  1.47it/s]Extractor Predicting: 142it [01:30,  1.48it/s]Extractor Predicting: 143it [01:31,  1.45it/s]Extractor Predicting: 144it [01:32,  1.45it/s]Extractor Predicting: 145it [01:32,  1.47it/s]Extractor Predicting: 146it [01:33,  1.48it/s]Extractor Predicting: 147it [01:34,  1.51it/s]Extractor Predicting: 148it [01:34,  1.53it/s]Extractor Predicting: 149it [01:35,  1.50it/s]Extractor Predicting: 150it [01:36,  1.50it/s]Extractor Predicting: 151it [01:36,  1.53it/s]Extractor Predicting: 152it [01:37,  1.55it/s]Extractor Predicting: 153it [01:37,  1.56it/s]Extractor Predicting: 154it [01:38,  1.58it/s]Extractor Predicting: 155it [01:39,  1.59it/s]Extractor Predicting: 156it [01:39,  1.61it/s]Extractor Predicting: 157it [01:40,  1.59it/s]Extractor Predicting: 158it [01:41,  1.61it/s]Extractor Predicting: 159it [01:41,  1.66it/s]Extractor Predicting: 160it [01:42,  1.61it/s]Extractor Predicting: 161it [01:42,  1.58it/s]Extractor Predicting: 162it [01:43,  1.55it/s]Extractor Predicting: 163it [01:44,  1.56it/s]Extractor Predicting: 164it [01:44,  1.54it/s]Extractor Predicting: 165it [01:45,  1.53it/s]Extractor Predicting: 166it [01:46,  1.53it/s]Extractor Predicting: 167it [01:46,  1.54it/s]Extractor Predicting: 168it [01:47,  1.53it/s]Extractor Predicting: 169it [01:48,  1.52it/s]Extractor Predicting: 170it [01:48,  1.55it/s]Extractor Predicting: 171it [01:49,  1.54it/s]Extractor Predicting: 172it [01:50,  1.52it/s]Extractor Predicting: 173it [01:50,  1.54it/s]Extractor Predicting: 174it [01:51,  1.54it/s]Extractor Predicting: 175it [01:52,  1.54it/s]Extractor Predicting: 176it [01:52,  1.53it/s]Extractor Predicting: 177it [01:53,  1.57it/s]Extractor Predicting: 178it [01:53,  1.56it/s]Extractor Predicting: 179it [01:54,  1.59it/s]Extractor Predicting: 180it [01:55,  1.60it/s]Extractor Predicting: 181it [01:55,  1.58it/s]Extractor Predicting: 182it [01:56,  1.63it/s]Extractor Predicting: 183it [01:57,  1.63it/s]Extractor Predicting: 184it [01:57,  1.60it/s]Extractor Predicting: 185it [01:58,  1.63it/s]Extractor Predicting: 186it [01:58,  1.63it/s]Extractor Predicting: 187it [01:59,  1.59it/s]Extractor Predicting: 188it [02:00,  1.63it/s]Extractor Predicting: 189it [02:00,  1.64it/s]Extractor Predicting: 190it [02:01,  1.68it/s]Extractor Predicting: 191it [02:01,  1.61it/s]Extractor Predicting: 192it [02:02,  1.64it/s]Extractor Predicting: 193it [02:03,  1.65it/s]Extractor Predicting: 194it [02:03,  1.68it/s]Extractor Predicting: 195it [02:04,  1.65it/s]Extractor Predicting: 196it [02:04,  1.64it/s]Extractor Predicting: 197it [02:05,  1.66it/s]Extractor Predicting: 198it [02:06,  1.63it/s]Extractor Predicting: 199it [02:06,  1.60it/s]Extractor Predicting: 200it [02:07,  1.61it/s]Extractor Predicting: 201it [02:08,  1.61it/s]Extractor Predicting: 202it [02:08,  1.64it/s]Extractor Predicting: 203it [02:09,  1.67it/s]Extractor Predicting: 204it [02:09,  1.68it/s]Extractor Predicting: 205it [02:10,  1.68it/s]Extractor Predicting: 206it [02:10,  1.73it/s]Extractor Predicting: 207it [02:11,  1.70it/s]Extractor Predicting: 208it [02:12,  1.74it/s]Extractor Predicting: 209it [02:12,  1.72it/s]Extractor Predicting: 210it [02:13,  1.71it/s]Extractor Predicting: 211it [02:13,  1.70it/s]Extractor Predicting: 212it [02:14,  1.74it/s]Extractor Predicting: 213it [02:14,  1.76it/s]Extractor Predicting: 214it [02:15,  1.77it/s]Extractor Predicting: 215it [02:16,  1.81it/s]Extractor Predicting: 216it [02:16,  1.79it/s]Extractor Predicting: 217it [02:17,  1.74it/s]Extractor Predicting: 218it [02:17,  1.72it/s]Extractor Predicting: 219it [02:18,  1.73it/s]Extractor Predicting: 220it [02:18,  1.74it/s]Extractor Predicting: 221it [02:19,  1.75it/s]Extractor Predicting: 222it [02:20,  1.81it/s]Extractor Predicting: 223it [02:20,  1.75it/s]Extractor Predicting: 224it [02:21,  1.78it/s]Extractor Predicting: 225it [02:21,  1.76it/s]Extractor Predicting: 226it [02:22,  1.76it/s]Extractor Predicting: 227it [02:22,  1.79it/s]Extractor Predicting: 228it [02:23,  1.77it/s]Extractor Predicting: 229it [02:24,  1.68it/s]Extractor Predicting: 230it [02:24,  1.63it/s]Extractor Predicting: 231it [02:25,  1.55it/s]Extractor Predicting: 232it [02:26,  1.52it/s]Extractor Predicting: 233it [02:26,  1.52it/s]Extractor Predicting: 234it [02:27,  1.50it/s]Extractor Predicting: 235it [02:28,  1.50it/s]Extractor Predicting: 236it [02:28,  1.50it/s]Extractor Predicting: 237it [02:29,  1.46it/s]Extractor Predicting: 238it [02:30,  1.45it/s]Extractor Predicting: 239it [02:31,  1.45it/s]Extractor Predicting: 240it [02:31,  1.46it/s]Extractor Predicting: 241it [02:32,  1.47it/s]Extractor Predicting: 242it [02:32,  1.50it/s]Extractor Predicting: 243it [02:33,  1.44it/s]Extractor Predicting: 244it [02:34,  1.46it/s]Extractor Predicting: 245it [02:35,  1.47it/s]Extractor Predicting: 246it [02:35,  1.48it/s]Extractor Predicting: 247it [02:36,  1.31it/s]Extractor Predicting: 248it [02:37,  1.35it/s]Extractor Predicting: 249it [02:38,  1.35it/s]Extractor Predicting: 250it [02:38,  1.39it/s]Extractor Predicting: 251it [02:39,  1.41it/s]Extractor Predicting: 252it [02:40,  1.44it/s]Extractor Predicting: 253it [02:40,  1.46it/s]Extractor Predicting: 254it [02:41,  1.45it/s]Extractor Predicting: 255it [02:42,  1.45it/s]Extractor Predicting: 256it [02:42,  1.48it/s]Extractor Predicting: 257it [02:43,  1.53it/s]Extractor Predicting: 258it [02:44,  1.55it/s]Extractor Predicting: 259it [02:44,  1.55it/s]Extractor Predicting: 260it [02:45,  1.58it/s]Extractor Predicting: 261it [02:45,  1.58it/s]Extractor Predicting: 262it [02:46,  1.58it/s]Extractor Predicting: 263it [02:47,  1.59it/s]Extractor Predicting: 264it [02:47,  1.60it/s]Extractor Predicting: 265it [02:48,  1.57it/s]Extractor Predicting: 266it [02:49,  1.59it/s]Extractor Predicting: 267it [02:49,  1.62it/s]Extractor Predicting: 268it [02:50,  1.59it/s]Extractor Predicting: 269it [02:50,  1.60it/s]Extractor Predicting: 270it [02:51,  1.58it/s]Extractor Predicting: 271it [02:52,  1.57it/s]Extractor Predicting: 272it [02:52,  1.56it/s]Extractor Predicting: 273it [02:53,  1.56it/s]Extractor Predicting: 274it [02:54,  1.55it/s]Extractor Predicting: 275it [02:54,  1.58it/s]Extractor Predicting: 276it [02:55,  1.58it/s]Extractor Predicting: 277it [02:56,  1.56it/s]Extractor Predicting: 278it [02:56,  1.53it/s]Extractor Predicting: 279it [02:57,  1.56it/s]Extractor Predicting: 280it [02:58,  1.54it/s]Extractor Predicting: 281it [02:58,  1.57it/s]Extractor Predicting: 282it [02:59,  1.55it/s]Extractor Predicting: 283it [03:00,  1.55it/s]Extractor Predicting: 284it [03:00,  1.57it/s]Extractor Predicting: 285it [03:01,  1.54it/s]Extractor Predicting: 286it [03:01,  1.53it/s]Extractor Predicting: 287it [03:02,  1.56it/s]Extractor Predicting: 288it [03:03,  1.56it/s]Extractor Predicting: 289it [03:03,  1.55it/s]Extractor Predicting: 290it [03:04,  1.55it/s]Extractor Predicting: 291it [03:05,  1.54it/s]Extractor Predicting: 292it [03:05,  1.55it/s]Extractor Predicting: 293it [03:06,  1.54it/s]Extractor Predicting: 294it [03:07,  1.54it/s]Extractor Predicting: 295it [03:07,  1.55it/s]Extractor Predicting: 296it [03:08,  1.55it/s]Extractor Predicting: 297it [03:09,  1.56it/s]Extractor Predicting: 298it [03:09,  1.53it/s]Extractor Predicting: 299it [03:10,  1.57it/s]Extractor Predicting: 300it [03:10,  1.56it/s]Extractor Predicting: 301it [03:11,  1.56it/s]Extractor Predicting: 302it [03:12,  1.55it/s]Extractor Predicting: 303it [03:12,  1.60it/s]Extractor Predicting: 304it [03:13,  1.59it/s]Extractor Predicting: 305it [03:14,  1.62it/s]Extractor Predicting: 306it [03:14,  1.59it/s]Extractor Predicting: 307it [03:15,  1.57it/s]Extractor Predicting: 308it [03:16,  1.56it/s]Extractor Predicting: 309it [03:16,  1.59it/s]Extractor Predicting: 310it [03:17,  1.54it/s]Extractor Predicting: 311it [03:17,  1.53it/s]Extractor Predicting: 312it [03:18,  1.54it/s]Extractor Predicting: 313it [03:19,  1.56it/s]Extractor Predicting: 314it [03:19,  1.56it/s]Extractor Predicting: 315it [03:20,  1.59it/s]Extractor Predicting: 316it [03:21,  1.58it/s]Extractor Predicting: 317it [03:21,  1.56it/s]Extractor Predicting: 318it [03:22,  1.62it/s]Extractor Predicting: 319it [03:22,  1.61it/s]Extractor Predicting: 320it [03:23,  1.64it/s]Extractor Predicting: 321it [03:24,  1.62it/s]Extractor Predicting: 322it [03:24,  1.60it/s]Extractor Predicting: 323it [03:25,  1.58it/s]Extractor Predicting: 324it [03:26,  1.57it/s]Extractor Predicting: 325it [03:26,  1.56it/s]Extractor Predicting: 326it [03:27,  1.56it/s]Extractor Predicting: 327it [03:28,  1.54it/s]Extractor Predicting: 328it [03:28,  1.54it/s]Extractor Predicting: 329it [03:29,  1.49it/s]Extractor Predicting: 330it [03:30,  1.51it/s]Extractor Predicting: 331it [03:30,  1.52it/s]Extractor Predicting: 332it [03:31,  1.56it/s]Extractor Predicting: 333it [03:32,  1.56it/s]Extractor Predicting: 334it [03:32,  1.56it/s]Extractor Predicting: 335it [03:33,  1.55it/s]Extractor Predicting: 336it [03:33,  1.55it/s]Extractor Predicting: 337it [03:34,  1.54it/s]Extractor Predicting: 338it [03:35,  1.55it/s]Extractor Predicting: 339it [03:35,  1.53it/s]Extractor Predicting: 340it [03:36,  1.58it/s]Extractor Predicting: 341it [03:37,  1.55it/s]Extractor Predicting: 342it [03:37,  1.58it/s]Extractor Predicting: 343it [03:38,  1.52it/s]Extractor Predicting: 344it [03:39,  1.54it/s]Extractor Predicting: 345it [03:39,  1.52it/s]Extractor Predicting: 346it [03:40,  1.54it/s]Extractor Predicting: 347it [03:41,  1.55it/s]Extractor Predicting: 348it [03:41,  1.57it/s]Extractor Predicting: 349it [03:42,  1.34it/s]Extractor Predicting: 350it [03:43,  1.37it/s]Extractor Predicting: 351it [03:44,  1.40it/s]Extractor Predicting: 352it [03:44,  1.44it/s]Extractor Predicting: 353it [03:45,  1.45it/s]Extractor Predicting: 354it [03:46,  1.48it/s]Extractor Predicting: 355it [03:46,  1.48it/s]Extractor Predicting: 356it [03:47,  1.53it/s]Extractor Predicting: 357it [03:47,  1.51it/s]Extractor Predicting: 358it [03:48,  1.50it/s]Extractor Predicting: 359it [03:49,  1.49it/s]Extractor Predicting: 360it [03:50,  1.47it/s]Extractor Predicting: 361it [03:50,  1.51it/s]Extractor Predicting: 362it [03:51,  1.54it/s]Extractor Predicting: 363it [03:51,  1.53it/s]Extractor Predicting: 364it [03:52,  1.56it/s]Extractor Predicting: 365it [03:53,  1.56it/s]Extractor Predicting: 366it [03:53,  1.53it/s]Extractor Predicting: 367it [03:54,  1.54it/s]Extractor Predicting: 368it [03:55,  1.51it/s]Extractor Predicting: 369it [03:55,  1.53it/s]Extractor Predicting: 370it [03:56,  1.52it/s]Extractor Predicting: 371it [03:57,  1.54it/s]Extractor Predicting: 372it [03:57,  1.56it/s]Extractor Predicting: 373it [03:58,  1.58it/s]Extractor Predicting: 374it [03:59,  1.58it/s]Extractor Predicting: 375it [03:59,  1.56it/s]Extractor Predicting: 376it [04:00,  1.57it/s]Extractor Predicting: 377it [04:00,  1.58it/s]Extractor Predicting: 378it [04:01,  1.59it/s]Extractor Predicting: 379it [04:02,  1.60it/s]Extractor Predicting: 380it [04:02,  1.59it/s]Extractor Predicting: 381it [04:03,  1.59it/s]Extractor Predicting: 382it [04:04,  1.56it/s]Extractor Predicting: 383it [04:04,  1.59it/s]Extractor Predicting: 384it [04:05,  1.60it/s]Extractor Predicting: 385it [04:05,  1.60it/s]Extractor Predicting: 386it [04:06,  1.62it/s]Extractor Predicting: 387it [04:07,  1.64it/s]Extractor Predicting: 388it [04:07,  1.60it/s]Extractor Predicting: 389it [04:08,  1.60it/s]Extractor Predicting: 390it [04:09,  1.59it/s]Extractor Predicting: 391it [04:09,  1.59it/s]Extractor Predicting: 392it [04:10,  1.59it/s]Extractor Predicting: 393it [04:10,  1.60it/s]Extractor Predicting: 394it [04:11,  1.61it/s]Extractor Predicting: 395it [04:12,  1.56it/s]Extractor Predicting: 396it [04:12,  1.58it/s]Extractor Predicting: 397it [04:13,  1.60it/s]Extractor Predicting: 398it [04:14,  1.57it/s]Extractor Predicting: 399it [04:14,  1.58it/s]Extractor Predicting: 400it [04:15,  1.57it/s]Extractor Predicting: 401it [04:15,  1.59it/s]Extractor Predicting: 402it [04:16,  1.62it/s]Extractor Predicting: 403it [04:17,  1.59it/s]Extractor Predicting: 404it [04:17,  1.61it/s]Extractor Predicting: 405it [04:18,  1.61it/s]Extractor Predicting: 406it [04:19,  1.61it/s]Extractor Predicting: 407it [04:19,  1.60it/s]Extractor Predicting: 408it [04:20,  1.62it/s]Extractor Predicting: 409it [04:20,  1.64it/s]Extractor Predicting: 410it [04:21,  1.63it/s]Extractor Predicting: 411it [04:22,  1.60it/s]Extractor Predicting: 412it [04:22,  1.62it/s]Extractor Predicting: 413it [04:23,  1.62it/s]Extractor Predicting: 414it [04:24,  1.62it/s]Extractor Predicting: 415it [04:24,  1.65it/s]Extractor Predicting: 416it [04:25,  1.65it/s]Extractor Predicting: 417it [04:25,  1.62it/s]Extractor Predicting: 418it [04:26,  1.65it/s]Extractor Predicting: 419it [04:27,  1.63it/s]Extractor Predicting: 420it [04:27,  1.63it/s]Extractor Predicting: 421it [04:28,  1.62it/s]Extractor Predicting: 422it [04:28,  1.61it/s]Extractor Predicting: 423it [04:29,  1.61it/s]Extractor Predicting: 424it [04:30,  1.61it/s]Extractor Predicting: 425it [04:30,  1.81it/s]Extractor Predicting: 425it [04:30,  1.57it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4627450980392157,
  "recall": 0.09267622226585509,
  "score": 0.15442499591035494,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:02,  1.42it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.43it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.73it/s]Extractor Predicting: 7it [00:04,  1.56it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.38461538461538464,
  "recall": 0.01592356687898089,
  "score": 0.03058103975535168,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:17<05:38, 17.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:36<05:29, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:53<04:57, 17.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:07<04:18, 16.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:24<04:09, 16.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:40<03:50, 16.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:57<03:36, 16.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:17<03:31, 17.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:37<03:21, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:55<03:02, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:12<02:40, 17.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:32<02:27, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:46<02:00, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:05<01:46, 17.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:22<01:27, 17.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:40<01:10, 17.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:54<00:50, 16.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:10<00:32, 16.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:26<00:16, 16.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:41<00:00, 15.90s/it]Generating: 100%|██████████| 20/20 [05:41<00:00, 17.07s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : has part .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : location .', 'success_rate': 0.8020833333333334, 'errors': {'', '(\'Central Market\', \'location\', \'\', "A popular location for the city \'s food court was the Central Market , which opened in 1880 , near the city center .")'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8220108695652174, 'errors': {'', '(\'Syriza\', \'member of political party\', \'\', \'Since the 2002 elections , he has been re - elected to Parliament in Greece \\\'s highest - profile - seat , " Syriza " .\')'}}
['Relation : platform . Context : The game is downloadable for Windows Windows and Mac OS X ( both of which have a similar platform ) . Head Entity : games , Tail Entity : Windows .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : position held .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('French Open', 'competition class', '', 'He finished second in the Italian Open in 1973 , and third in the French Open in 1973 .')"}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 156, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 218, 'raw': 320}
{'target': 600, 'success': 245, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 295, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 343, 'raw': 480}
{'target': 600, 'success': 368, 'raw': 512}
{'target': 600, 'success': 393, 'raw': 544}
{'target': 600, 'success': 419, 'raw': 576}
{'target': 600, 'success': 447, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 546, 'raw': 736}
{'target': 600, 'success': 570, 'raw': 768}
{'target': 600, 'success': 595, 'raw': 800}
{'target': 600, 'success': 617, 'raw': 832}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7415865384615384, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : father .', 'success_rate': 0.8111979166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9122023809523809, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8792613636363636, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7955729166666666, 'errors': {'', "('Bishop Nicholas II of York', 'occupation', '', 'In May 1789 , he purchased three acres from Bishop Nicholas II of York to improve and expand the estate ; within 30 days , he took possession of it .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9300595238095238, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9315476190476191, 'errors': {''}}
['Relation : position played on team / speciality . Context : On 31 occasions with the club , he has played a total of 9 games ( including six internationals ) , scoring four goals for the club . Head Entity : Erik Erik Erik , Tail Entity : club .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 14444
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14544, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:01,  1.76s/it]Extractor Estimating: 2it [00:03,  1.48s/it]Extractor Estimating: 3it [00:05,  2.15s/it]Extractor Estimating: 4it [00:06,  1.57s/it]Extractor Estimating: 5it [00:07,  1.25s/it]Extractor Estimating: 6it [00:08,  1.05s/it]Extractor Estimating: 7it [00:08,  1.06it/s]Extractor Estimating: 8it [00:09,  1.16it/s]Extractor Estimating: 9it [00:10,  1.25it/s]Extractor Estimating: 10it [00:10,  1.35it/s]Extractor Estimating: 11it [00:11,  1.46it/s]Extractor Estimating: 12it [00:12,  1.40it/s]Extractor Estimating: 13it [00:12,  1.44it/s]Extractor Estimating: 14it [00:13,  1.48it/s]Extractor Estimating: 15it [00:14,  1.45it/s]Extractor Estimating: 16it [00:14,  1.49it/s]Extractor Estimating: 17it [00:15,  1.54it/s]Extractor Estimating: 18it [00:15,  1.55it/s]Extractor Estimating: 19it [00:16,  1.57it/s]Extractor Estimating: 20it [00:17,  1.51it/s]Extractor Estimating: 21it [00:17,  1.55it/s]Extractor Estimating: 22it [00:18,  1.52it/s]Extractor Estimating: 23it [00:19,  1.52it/s]Extractor Estimating: 24it [00:19,  1.50it/s]Extractor Estimating: 25it [00:21,  1.17it/s]Extractor Estimating: 26it [00:21,  1.24it/s]Extractor Estimating: 27it [00:22,  1.31it/s]Extractor Estimating: 28it [00:23,  1.34it/s]Extractor Estimating: 29it [00:23,  1.39it/s]Extractor Estimating: 30it [00:24,  1.44it/s]Extractor Estimating: 31it [00:25,  1.51it/s]Extractor Estimating: 32it [00:25,  1.49it/s]Extractor Estimating: 33it [00:26,  1.45it/s]Extractor Estimating: 34it [00:27,  1.48it/s]Extractor Estimating: 35it [00:27,  1.54it/s]Extractor Estimating: 36it [00:28,  1.55it/s]Extractor Estimating: 37it [00:29,  1.57it/s]Extractor Estimating: 38it [00:29,  1.54it/s]Extractor Estimating: 39it [00:30,  1.52it/s]Extractor Estimating: 40it [00:31,  1.48it/s]Extractor Estimating: 41it [00:31,  1.45it/s]Extractor Estimating: 42it [00:32,  1.43it/s]Extractor Estimating: 43it [00:33,  1.42it/s]Extractor Estimating: 44it [00:33,  1.41it/s]Extractor Estimating: 45it [00:34,  1.46it/s]Extractor Estimating: 46it [00:35,  1.43it/s]Extractor Estimating: 47it [00:35,  1.48it/s]Extractor Estimating: 48it [00:36,  1.48it/s]Extractor Estimating: 49it [00:37,  1.49it/s]Extractor Estimating: 50it [00:37,  1.51it/s]Extractor Estimating: 51it [00:38,  1.55it/s]Extractor Estimating: 52it [00:39,  1.56it/s]Extractor Estimating: 53it [00:39,  1.56it/s]Extractor Estimating: 54it [00:40,  1.59it/s]Extractor Estimating: 55it [00:41,  1.53it/s]Extractor Estimating: 56it [00:41,  1.54it/s]Extractor Estimating: 57it [00:42,  1.55it/s]Extractor Estimating: 58it [00:43,  1.57it/s]Extractor Estimating: 59it [00:43,  1.53it/s]Extractor Estimating: 60it [00:44,  1.56it/s]Extractor Estimating: 61it [00:45,  1.46it/s]Extractor Estimating: 62it [00:45,  1.52it/s]Extractor Estimating: 63it [00:46,  1.51it/s]Extractor Estimating: 64it [00:47,  1.51it/s]Extractor Estimating: 65it [00:47,  1.54it/s]Extractor Estimating: 66it [00:48,  1.56it/s]Extractor Estimating: 67it [00:48,  1.54it/s]Extractor Estimating: 68it [00:49,  1.54it/s]Extractor Estimating: 69it [00:50,  1.51it/s]Extractor Estimating: 70it [00:50,  1.52it/s]Extractor Estimating: 71it [00:51,  1.54it/s]Extractor Estimating: 72it [00:52,  1.59it/s]Extractor Estimating: 73it [00:52,  1.62it/s]Extractor Estimating: 74it [00:53,  1.64it/s]Extractor Estimating: 75it [00:53,  1.64it/s]Extractor Estimating: 76it [00:54,  1.66it/s]Extractor Estimating: 77it [00:55,  1.64it/s]Extractor Estimating: 78it [00:55,  1.62it/s]Extractor Estimating: 79it [00:56,  1.60it/s]Extractor Estimating: 80it [00:57,  1.64it/s]Extractor Estimating: 81it [00:57,  1.68it/s]Extractor Estimating: 82it [00:58,  1.66it/s]Extractor Estimating: 83it [00:58,  1.62it/s]Extractor Estimating: 84it [00:59,  1.66it/s]Extractor Estimating: 85it [01:00,  1.67it/s]Extractor Estimating: 86it [01:00,  1.67it/s]Extractor Estimating: 87it [01:01,  1.62it/s]Extractor Estimating: 88it [01:03,  1.15s/it]Extractor Estimating: 89it [01:04,  1.01s/it]Extractor Estimating: 90it [01:04,  1.13it/s]Extractor Estimating: 91it [01:05,  1.23it/s]Extractor Estimating: 92it [01:06,  1.30it/s]Extractor Estimating: 93it [01:06,  1.37it/s]Extractor Estimating: 94it [01:07,  1.46it/s]Extractor Estimating: 95it [01:08,  1.51it/s]Extractor Estimating: 96it [01:08,  1.52it/s]Extractor Estimating: 97it [01:09,  1.55it/s]Extractor Estimating: 98it [01:09,  1.59it/s]Extractor Estimating: 99it [01:10,  1.61it/s]Extractor Estimating: 100it [01:11,  1.62it/s]Extractor Estimating: 101it [01:11,  1.62it/s]Extractor Estimating: 102it [01:12,  1.65it/s]Extractor Estimating: 103it [01:12,  1.70it/s]Extractor Estimating: 104it [01:13,  1.65it/s]Extractor Estimating: 105it [01:14,  1.69it/s]Extractor Estimating: 106it [01:14,  1.69it/s]Extractor Estimating: 107it [01:15,  1.61it/s]Extractor Estimating: 108it [01:16,  1.54it/s]Extractor Estimating: 109it [01:16,  1.61it/s]Extractor Estimating: 110it [01:17,  1.60it/s]Extractor Estimating: 111it [01:17,  1.60it/s]Extractor Estimating: 112it [01:18,  1.65it/s]Extractor Estimating: 113it [01:19,  1.63it/s]Extractor Estimating: 114it [01:19,  1.60it/s]Extractor Estimating: 115it [01:20,  1.59it/s]Extractor Estimating: 116it [01:20,  1.62it/s]Extractor Estimating: 117it [01:21,  1.60it/s]Extractor Estimating: 118it [01:22,  1.54it/s]Extractor Estimating: 119it [01:23,  1.52it/s]Extractor Estimating: 120it [01:23,  1.56it/s]Extractor Estimating: 121it [01:24,  1.62it/s]Extractor Estimating: 122it [01:24,  1.64it/s]Extractor Estimating: 123it [01:25,  1.71it/s]Extractor Estimating: 124it [01:25,  1.74it/s]Extractor Estimating: 125it [01:26,  1.68it/s]Extractor Estimating: 126it [01:27,  1.58it/s]Extractor Estimating: 127it [01:27,  1.55it/s]Extractor Estimating: 128it [01:28,  1.49it/s]Extractor Estimating: 129it [01:29,  1.45it/s]Extractor Estimating: 130it [01:30,  1.44it/s]Extractor Estimating: 131it [01:30,  1.42it/s]Extractor Estimating: 132it [01:31,  1.44it/s]Extractor Estimating: 133it [01:32,  1.39it/s]Extractor Estimating: 134it [01:32,  1.43it/s]Extractor Estimating: 135it [01:33,  1.44it/s]Extractor Estimating: 136it [01:34,  1.45it/s]Extractor Estimating: 137it [01:34,  1.48it/s]Extractor Estimating: 138it [01:35,  1.45it/s]Extractor Estimating: 139it [01:36,  1.44it/s]Extractor Estimating: 140it [01:37,  1.43it/s]Extractor Estimating: 141it [01:37,  1.41it/s]Extractor Estimating: 142it [01:38,  1.41it/s]Extractor Estimating: 143it [01:39,  1.41it/s]Extractor Estimating: 144it [01:39,  1.41it/s]Extractor Estimating: 145it [01:40,  1.27it/s]Extractor Estimating: 146it [01:41,  1.34it/s]Extractor Estimating: 147it [01:42,  1.36it/s]Extractor Estimating: 148it [01:42,  1.38it/s]Extractor Estimating: 149it [01:43,  1.44it/s]Extractor Estimating: 150it [01:44,  1.44it/s]Extractor Estimating: 151it [01:44,  1.52it/s]Extractor Estimating: 152it [01:45,  1.55it/s]Extractor Estimating: 153it [01:45,  1.63it/s]Extractor Estimating: 154it [01:46,  1.60it/s]Extractor Estimating: 155it [01:47,  1.62it/s]Extractor Estimating: 156it [01:47,  1.64it/s]Extractor Estimating: 157it [01:48,  1.63it/s]Extractor Estimating: 158it [01:49,  1.61it/s]Extractor Estimating: 159it [01:49,  1.67it/s]Extractor Estimating: 160it [01:50,  1.72it/s]Extractor Estimating: 161it [01:50,  1.75it/s]Extractor Estimating: 162it [01:51,  1.72it/s]Extractor Estimating: 163it [01:51,  1.73it/s]Extractor Estimating: 164it [01:52,  1.72it/s]Extractor Estimating: 165it [01:53,  1.69it/s]Extractor Estimating: 166it [01:53,  1.66it/s]Extractor Estimating: 167it [01:54,  1.66it/s]Extractor Estimating: 168it [01:54,  1.64it/s]Extractor Estimating: 169it [01:55,  1.62it/s]Extractor Estimating: 170it [01:56,  1.59it/s]Extractor Estimating: 171it [01:56,  1.62it/s]Extractor Estimating: 172it [01:57,  1.66it/s]Extractor Estimating: 173it [01:57,  1.68it/s]Extractor Estimating: 174it [01:58,  1.66it/s]Extractor Estimating: 175it [01:59,  1.63it/s]Extractor Estimating: 176it [01:59,  1.65it/s]Extractor Estimating: 177it [02:00,  1.70it/s]Extractor Estimating: 178it [02:00,  1.68it/s]Extractor Estimating: 179it [02:01,  1.68it/s]Extractor Estimating: 180it [02:02,  1.62it/s]Extractor Estimating: 181it [02:02,  1.68it/s]Extractor Estimating: 182it [02:03,  1.73it/s]Extractor Estimating: 183it [02:03,  1.69it/s]Extractor Estimating: 184it [02:04,  1.63it/s]Extractor Estimating: 185it [02:05,  1.60it/s]Extractor Estimating: 186it [02:05,  1.58it/s]Extractor Estimating: 187it [02:06,  1.60it/s]Extractor Estimating: 188it [02:07,  1.62it/s]Extractor Estimating: 189it [02:07,  1.61it/s]Extractor Estimating: 190it [02:08,  1.68it/s]Extractor Estimating: 191it [02:08,  1.66it/s]Extractor Estimating: 192it [02:09,  1.62it/s]Extractor Estimating: 193it [02:10,  1.68it/s]Extractor Estimating: 194it [02:10,  1.62it/s]Extractor Estimating: 195it [02:11,  1.67it/s]Extractor Estimating: 196it [02:11,  1.66it/s]Extractor Estimating: 197it [02:12,  1.68it/s]Extractor Estimating: 198it [02:13,  1.64it/s]Extractor Estimating: 199it [02:13,  1.76it/s]Extractor Estimating: 200it [02:14,  1.70it/s]Extractor Estimating: 201it [02:14,  1.68it/s]Extractor Estimating: 202it [02:15,  1.57it/s]Extractor Estimating: 203it [02:16,  1.56it/s]Extractor Estimating: 204it [02:16,  1.52it/s]Extractor Estimating: 205it [02:17,  1.51it/s]Extractor Estimating: 206it [02:18,  1.55it/s]Extractor Estimating: 207it [02:18,  1.52it/s]Extractor Estimating: 208it [02:19,  1.53it/s]Extractor Estimating: 209it [02:20,  1.55it/s]Extractor Estimating: 210it [02:20,  1.49it/s]Extractor Estimating: 211it [02:21,  1.45it/s]Extractor Estimating: 212it [02:22,  1.50it/s]Extractor Estimating: 213it [02:22,  1.47it/s]Extractor Estimating: 214it [02:23,  1.48it/s]Extractor Estimating: 215it [02:24,  1.50it/s]Extractor Estimating: 216it [02:24,  1.51it/s]Extractor Estimating: 217it [02:25,  1.58it/s]Extractor Estimating: 218it [02:26,  1.54it/s]Extractor Estimating: 219it [02:26,  1.47it/s]Extractor Estimating: 220it [02:27,  1.51it/s]Extractor Estimating: 221it [02:28,  1.51it/s]Extractor Estimating: 222it [02:29,  1.40it/s]Extractor Estimating: 223it [02:29,  1.43it/s]Extractor Estimating: 224it [02:30,  1.45it/s]Extractor Estimating: 225it [02:31,  1.48it/s]Extractor Estimating: 226it [02:31,  1.47it/s]Extractor Estimating: 227it [02:32,  1.50it/s]Extractor Estimating: 228it [02:32,  1.57it/s]Extractor Estimating: 229it [02:33,  1.50it/s]Extractor Estimating: 230it [02:34,  1.53it/s]Extractor Estimating: 231it [02:34,  1.55it/s]Extractor Estimating: 232it [02:35,  1.56it/s]Extractor Estimating: 233it [02:36,  1.57it/s]Extractor Estimating: 234it [02:36,  1.52it/s]Extractor Estimating: 235it [02:37,  1.47it/s]Extractor Estimating: 236it [02:38,  1.43it/s]Extractor Estimating: 237it [02:39,  1.43it/s]Extractor Estimating: 238it [02:39,  1.38it/s]Extractor Estimating: 239it [02:40,  1.44it/s]Extractor Estimating: 240it [02:41,  1.46it/s]Extractor Estimating: 241it [02:41,  1.50it/s]Extractor Estimating: 242it [02:42,  1.49it/s]Extractor Estimating: 243it [02:43,  1.51it/s]Extractor Estimating: 244it [02:43,  1.53it/s]Extractor Estimating: 245it [02:44,  1.55it/s]Extractor Estimating: 246it [02:44,  1.53it/s]Extractor Estimating: 247it [02:45,  1.47it/s]Extractor Estimating: 248it [02:46,  1.40it/s]Extractor Estimating: 249it [02:47,  1.49it/s]Extractor Estimating: 250it [02:47,  1.46it/s]Extractor Estimating: 251it [02:48,  1.47it/s]Extractor Estimating: 252it [02:49,  1.47it/s]Extractor Estimating: 253it [02:49,  1.43it/s]Extractor Estimating: 254it [02:50,  1.50it/s]Extractor Estimating: 255it [02:51,  1.53it/s]Extractor Estimating: 256it [02:51,  1.53it/s]Extractor Estimating: 257it [02:52,  1.51it/s]Extractor Estimating: 258it [02:53,  1.54it/s]Extractor Estimating: 259it [02:53,  1.52it/s]Extractor Estimating: 260it [02:54,  1.50it/s]Extractor Estimating: 261it [02:55,  1.51it/s]Extractor Estimating: 262it [02:55,  1.53it/s]Extractor Estimating: 263it [02:56,  1.55it/s]Extractor Estimating: 264it [02:56,  1.57it/s]Extractor Estimating: 265it [02:57,  1.51it/s]Extractor Estimating: 266it [02:58,  1.54it/s]Extractor Estimating: 267it [02:58,  1.56it/s]Extractor Estimating: 268it [02:59,  1.61it/s]Extractor Estimating: 269it [03:00,  1.55it/s]Extractor Estimating: 270it [03:00,  1.55it/s]Extractor Estimating: 271it [03:01,  1.57it/s]Extractor Estimating: 272it [03:02,  1.58it/s]Extractor Estimating: 273it [03:02,  1.56it/s]Extractor Estimating: 274it [03:03,  1.51it/s]Extractor Estimating: 275it [03:04,  1.52it/s]Extractor Estimating: 276it [03:04,  1.53it/s]Extractor Estimating: 277it [03:05,  1.49it/s]Extractor Estimating: 278it [03:06,  1.53it/s]Extractor Estimating: 279it [03:06,  1.52it/s]Extractor Estimating: 280it [03:07,  1.53it/s]Extractor Estimating: 281it [03:07,  1.55it/s]Extractor Estimating: 282it [03:08,  1.55it/s]Extractor Estimating: 283it [03:09,  1.55it/s]Extractor Estimating: 284it [03:09,  1.56it/s]Extractor Estimating: 285it [03:10,  1.52it/s]Extractor Estimating: 286it [03:11,  1.51it/s]Extractor Estimating: 287it [03:11,  1.54it/s]Extractor Estimating: 288it [03:12,  1.55it/s]Extractor Estimating: 289it [03:13,  1.57it/s]Extractor Estimating: 290it [03:13,  1.55it/s]Extractor Estimating: 291it [03:14,  1.56it/s]Extractor Estimating: 292it [03:15,  1.52it/s]Extractor Estimating: 293it [03:15,  1.50it/s]Extractor Estimating: 294it [03:16,  1.56it/s]Extractor Estimating: 295it [03:17,  1.51it/s]Extractor Estimating: 296it [03:17,  1.51it/s]Extractor Estimating: 297it [03:18,  1.41it/s]Extractor Estimating: 298it [03:19,  1.46it/s]Extractor Estimating: 299it [03:19,  1.44it/s]Extractor Estimating: 300it [03:20,  1.41it/s]Extractor Estimating: 301it [03:21,  1.53it/s]Extractor Estimating: 302it [03:21,  1.55it/s]Extractor Estimating: 303it [03:22,  1.58it/s]Extractor Estimating: 304it [03:23,  1.58it/s]Extractor Estimating: 305it [03:23,  1.50it/s]Extractor Estimating: 306it [03:24,  1.46it/s]Extractor Estimating: 307it [03:25,  1.51it/s]Extractor Estimating: 308it [03:25,  1.50it/s]Extractor Estimating: 309it [03:26,  1.53it/s]Extractor Estimating: 310it [03:27,  1.54it/s]Extractor Estimating: 311it [03:27,  1.54it/s]Extractor Estimating: 312it [03:28,  1.54it/s]Extractor Estimating: 313it [03:28,  1.63it/s]Extractor Estimating: 314it [03:29,  1.66it/s]Extractor Estimating: 315it [03:30,  1.61it/s]Extractor Estimating: 316it [03:30,  1.55it/s]Extractor Estimating: 317it [03:31,  1.59it/s]Extractor Estimating: 318it [03:32,  1.62it/s]Extractor Estimating: 319it [03:32,  1.64it/s]Extractor Estimating: 320it [03:33,  1.60it/s]Extractor Estimating: 321it [03:33,  1.62it/s]Extractor Estimating: 322it [03:34,  1.63it/s]Extractor Estimating: 323it [03:35,  1.57it/s]Extractor Estimating: 324it [03:35,  1.50it/s]Extractor Estimating: 325it [03:36,  1.58it/s]Extractor Estimating: 326it [03:37,  1.58it/s]Extractor Estimating: 327it [03:37,  1.58it/s]Extractor Estimating: 328it [03:38,  1.59it/s]Extractor Estimating: 329it [03:38,  1.62it/s]Extractor Estimating: 330it [03:39,  1.60it/s]Extractor Estimating: 331it [03:40,  1.61it/s]Extractor Estimating: 332it [03:40,  1.63it/s]Extractor Estimating: 333it [03:41,  1.60it/s]Extractor Estimating: 334it [03:42,  1.55it/s]Extractor Estimating: 335it [03:42,  1.57it/s]Extractor Estimating: 336it [03:43,  1.62it/s]Extractor Estimating: 337it [03:43,  1.63it/s]Extractor Estimating: 338it [03:44,  1.63it/s]Extractor Estimating: 339it [03:45,  1.61it/s]Extractor Estimating: 340it [03:45,  1.62it/s]Extractor Estimating: 341it [03:46,  1.64it/s]Extractor Estimating: 342it [03:47,  1.53it/s]Extractor Estimating: 343it [03:47,  1.57it/s]Extractor Estimating: 344it [03:48,  1.59it/s]Extractor Estimating: 345it [03:49,  1.57it/s]Extractor Estimating: 346it [03:49,  1.61it/s]Extractor Estimating: 347it [03:50,  1.60it/s]Extractor Estimating: 348it [03:50,  1.53it/s]Extractor Estimating: 349it [03:51,  1.48it/s]Extractor Estimating: 350it [03:52,  1.45it/s]Extractor Estimating: 351it [03:53,  1.49it/s]Extractor Estimating: 352it [03:53,  1.49it/s]Extractor Estimating: 353it [03:54,  1.51it/s]Extractor Estimating: 354it [03:55,  1.51it/s]Extractor Estimating: 355it [03:55,  1.46it/s]Extractor Estimating: 356it [03:56,  1.48it/s]Extractor Estimating: 357it [03:57,  1.47it/s]Extractor Estimating: 358it [03:57,  1.48it/s]Extractor Estimating: 359it [03:58,  1.45it/s]Extractor Estimating: 360it [03:59,  1.45it/s]Extractor Estimating: 361it [03:59,  1.48it/s]Extractor Estimating: 362it [04:00,  1.49it/s]Extractor Estimating: 363it [04:01,  1.47it/s]Extractor Estimating: 364it [04:01,  1.53it/s]Extractor Estimating: 365it [04:02,  1.55it/s]Extractor Estimating: 366it [04:03,  1.54it/s]Extractor Estimating: 367it [04:03,  1.49it/s]Extractor Estimating: 368it [04:04,  1.51it/s]Extractor Estimating: 369it [04:05,  1.53it/s]Extractor Estimating: 370it [04:05,  1.58it/s]Extractor Estimating: 371it [04:06,  1.53it/s]Extractor Estimating: 372it [04:07,  1.52it/s]Extractor Estimating: 373it [04:07,  1.49it/s]Extractor Estimating: 374it [04:08,  1.46it/s]Extractor Estimating: 375it [04:09,  1.45it/s]Extractor Estimating: 376it [04:09,  1.45it/s]Extractor Estimating: 377it [04:10,  1.55it/s]Extractor Estimating: 378it [04:11,  1.49it/s]Extractor Estimating: 379it [04:11,  1.49it/s]Extractor Estimating: 380it [04:12,  1.54it/s]Extractor Estimating: 381it [04:12,  1.59it/s]Extractor Estimating: 382it [04:13,  1.62it/s]Extractor Estimating: 383it [04:14,  1.61it/s]Extractor Estimating: 384it [04:14,  1.59it/s]Extractor Estimating: 385it [04:15,  1.61it/s]Extractor Estimating: 386it [04:16,  1.63it/s]Extractor Estimating: 387it [04:16,  1.58it/s]Extractor Estimating: 388it [04:17,  1.54it/s]Extractor Estimating: 389it [04:18,  1.52it/s]Extractor Estimating: 390it [04:18,  1.55it/s]Extractor Estimating: 391it [04:19,  1.57it/s]Extractor Estimating: 392it [04:19,  1.60it/s]Extractor Estimating: 393it [04:20,  1.60it/s]Extractor Estimating: 394it [04:21,  1.60it/s]Extractor Estimating: 395it [04:21,  1.58it/s]Extractor Estimating: 396it [04:22,  1.59it/s]Extractor Estimating: 397it [04:22,  1.63it/s]Extractor Estimating: 398it [04:23,  1.62it/s]Extractor Estimating: 399it [04:24,  1.63it/s]Extractor Estimating: 400it [04:24,  1.63it/s]Extractor Estimating: 401it [04:25,  1.59it/s]Extractor Estimating: 402it [04:26,  1.56it/s]Extractor Estimating: 403it [04:26,  1.54it/s]Extractor Estimating: 404it [04:27,  1.50it/s]Extractor Estimating: 405it [04:28,  1.57it/s]Extractor Estimating: 406it [04:28,  1.45it/s]Extractor Estimating: 407it [04:29,  1.47it/s]Extractor Estimating: 408it [04:30,  1.51it/s]Extractor Estimating: 409it [04:30,  1.56it/s]Extractor Estimating: 410it [04:31,  1.54it/s]Extractor Estimating: 411it [04:32,  1.58it/s]Extractor Estimating: 412it [04:32,  1.63it/s]Extractor Estimating: 413it [04:33,  1.63it/s]Extractor Estimating: 414it [04:33,  1.62it/s]Extractor Estimating: 415it [04:34,  1.54it/s]Extractor Estimating: 416it [04:35,  1.55it/s]Extractor Estimating: 417it [04:35,  1.56it/s]Extractor Estimating: 418it [04:36,  1.47it/s]Extractor Estimating: 419it [04:37,  1.48it/s]Extractor Estimating: 420it [04:37,  1.45it/s]Extractor Estimating: 421it [04:38,  1.46it/s]Extractor Estimating: 422it [04:39,  1.50it/s]Extractor Estimating: 423it [04:39,  1.52it/s]Extractor Estimating: 424it [04:40,  1.51it/s]Extractor Estimating: 425it [04:41,  1.50it/s]Extractor Estimating: 426it [04:41,  1.48it/s]Extractor Estimating: 427it [04:43,  1.14it/s]Extractor Estimating: 428it [04:44,  1.22it/s]Extractor Estimating: 429it [04:44,  1.28it/s]Extractor Estimating: 430it [04:45,  1.34it/s]Extractor Estimating: 431it [04:45,  1.42it/s]Extractor Estimating: 432it [04:46,  1.47it/s]Extractor Estimating: 433it [04:47,  1.48it/s]Extractor Estimating: 434it [04:47,  1.45it/s]Extractor Estimating: 435it [04:48,  1.50it/s]Extractor Estimating: 436it [04:49,  1.45it/s]Extractor Estimating: 437it [04:49,  1.52it/s]Extractor Estimating: 438it [04:50,  1.54it/s]Extractor Estimating: 439it [04:51,  1.51it/s]Extractor Estimating: 440it [04:51,  1.55it/s]Extractor Estimating: 441it [04:52,  1.49it/s]Extractor Estimating: 442it [04:53,  1.54it/s]Extractor Estimating: 443it [04:53,  1.51it/s]Extractor Estimating: 444it [04:54,  1.52it/s]Extractor Estimating: 445it [04:55,  1.54it/s]Extractor Estimating: 446it [04:55,  1.53it/s]Extractor Estimating: 447it [04:56,  1.53it/s]Extractor Estimating: 448it [04:57,  1.58it/s]Extractor Estimating: 449it [04:57,  1.56it/s]Extractor Estimating: 450it [04:58,  1.60it/s]Extractor Estimating: 451it [04:58,  1.60it/s]Extractor Estimating: 452it [04:59,  1.67it/s]Extractor Estimating: 453it [05:00,  1.68it/s]Extractor Estimating: 454it [05:00,  1.71it/s]Extractor Estimating: 455it [05:01,  1.67it/s]Extractor Estimating: 456it [05:01,  1.70it/s]Extractor Estimating: 457it [05:02,  1.74it/s]Extractor Estimating: 458it [05:02,  1.70it/s]Extractor Estimating: 459it [05:03,  1.72it/s]Extractor Estimating: 460it [05:04,  1.72it/s]Extractor Estimating: 461it [05:04,  1.75it/s]Extractor Estimating: 462it [05:05,  1.73it/s]Extractor Estimating: 463it [05:05,  1.73it/s]Extractor Estimating: 464it [05:06,  1.68it/s]Extractor Estimating: 465it [05:07,  1.68it/s]Extractor Estimating: 466it [05:07,  1.69it/s]Extractor Estimating: 467it [05:08,  1.71it/s]Extractor Estimating: 468it [05:08,  1.71it/s]Extractor Estimating: 469it [05:09,  1.62it/s]Extractor Estimating: 470it [05:10,  1.67it/s]Extractor Estimating: 471it [05:10,  1.70it/s]Extractor Estimating: 472it [05:11,  1.77it/s]Extractor Estimating: 473it [05:11,  1.78it/s]Extractor Estimating: 474it [05:12,  1.78it/s]Extractor Estimating: 475it [05:12,  1.72it/s]Extractor Estimating: 476it [05:13,  1.68it/s]Extractor Estimating: 477it [05:14,  1.68it/s]Extractor Estimating: 478it [05:14,  1.63it/s]Extractor Estimating: 479it [05:15,  1.63it/s]Extractor Estimating: 480it [05:16,  1.49it/s]Extractor Estimating: 481it [05:16,  1.53it/s]Extractor Estimating: 482it [05:17,  1.51it/s]Extractor Estimating: 483it [05:18,  1.55it/s]Extractor Estimating: 484it [05:18,  1.57it/s]Extractor Estimating: 485it [05:19,  1.43it/s]Extractor Estimating: 486it [05:20,  1.47it/s]Extractor Estimating: 487it [05:20,  1.48it/s]Extractor Estimating: 488it [05:21,  1.50it/s]Extractor Estimating: 489it [05:22,  1.50it/s]Extractor Estimating: 490it [05:22,  1.51it/s]Extractor Estimating: 491it [05:23,  1.56it/s]Extractor Estimating: 492it [05:24,  1.53it/s]Extractor Estimating: 493it [05:24,  1.55it/s]Extractor Estimating: 494it [05:25,  1.59it/s]Extractor Estimating: 495it [05:25,  1.55it/s]Extractor Estimating: 496it [05:26,  1.57it/s]Extractor Estimating: 497it [05:27,  1.55it/s]Extractor Estimating: 498it [05:27,  1.52it/s]Extractor Estimating: 499it [05:28,  1.49it/s]Extractor Estimating: 500it [05:29,  1.63it/s]Extractor Estimating: 500it [05:29,  1.52it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9998 mean pseudo reward: 0.9371911349397293
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 28375
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28475, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28475, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.358, loss:913.3475
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.087, loss:880.6785
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.088, loss:893.4691
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.149, loss:890.4017
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.082, loss:856.7772
>> valid entity prec:0.6042, rec:0.4717, f1:0.5298
>> valid relation prec:0.4823, rec:0.0900, f1:0.1518
>> valid relation with NER prec:0.4823, rec:0.0900, f1:0.1518
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.468, loss:854.6812
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.092, loss:896.5044
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.074, loss:850.7303
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.085, loss:833.6200
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.075, loss:863.1274
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5457, rec:0.6279, f1:0.5839
>> valid relation prec:0.3879, rec:0.1414, f1:0.2072
>> valid relation with NER prec:0.3879, rec:0.1414, f1:0.2072
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.488, loss:846.1712
g_step 1200, step 366, avg_time 1.099, loss:854.3282
g_step 1300, step 49, avg_time 1.066, loss:805.2173
g_step 1400, step 149, avg_time 1.098, loss:798.8002
g_step 1500, step 249, avg_time 1.087, loss:787.8259
>> valid entity prec:0.5235, rec:0.5951, f1:0.5570
>> valid relation prec:0.3998, rec:0.1207, f1:0.1855
>> valid relation with NER prec:0.3998, rec:0.1207, f1:0.1855
g_step 1600, step 349, avg_time 2.493, loss:823.4707
g_step 1700, step 32, avg_time 1.080, loss:764.5024
g_step 1800, step 132, avg_time 1.076, loss:752.4753
g_step 1900, step 232, avg_time 1.094, loss:761.8499
g_step 2000, step 332, avg_time 1.077, loss:767.5878
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5509, rec:0.6192, f1:0.5831
>> valid relation prec:0.4274, rec:0.1546, f1:0.2270
>> valid relation with NER prec:0.4274, rec:0.1546, f1:0.2270
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 15, avg_time 2.484, loss:755.2492
g_step 2200, step 115, avg_time 1.088, loss:706.5960
g_step 2300, step 215, avg_time 1.081, loss:705.6773
g_step 2400, step 315, avg_time 1.085, loss:744.3290
g_step 2500, step 415, avg_time 1.083, loss:736.9352
>> valid entity prec:0.5692, rec:0.4852, f1:0.5239
>> valid relation prec:0.3304, rec:0.0978, f1:0.1509
>> valid relation with NER prec:0.3304, rec:0.0978, f1:0.1509
g_step 2600, step 98, avg_time 2.472, loss:675.2684
g_step 2700, step 198, avg_time 1.082, loss:681.7464
g_step 2800, step 298, avg_time 1.092, loss:684.4025
g_step 2900, step 398, avg_time 1.082, loss:717.9550
g_step 3000, step 81, avg_time 1.067, loss:647.1499
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5996, rec:0.5199, f1:0.5570
>> valid relation prec:0.3759, rec:0.1207, f1:0.1828
>> valid relation with NER prec:0.3759, rec:0.1207, f1:0.1828
g_step 3100, step 181, avg_time 2.484, loss:645.8000
g_step 3200, step 281, avg_time 1.083, loss:666.8896
g_step 3300, step 381, avg_time 1.086, loss:680.3502
g_step 3400, step 64, avg_time 1.081, loss:613.3499
g_step 3500, step 164, avg_time 1.084, loss:647.8440
>> valid entity prec:0.5324, rec:0.5565, f1:0.5442
>> valid relation prec:0.3368, rec:0.1477, f1:0.2053
>> valid relation with NER prec:0.3368, rec:0.1477, f1:0.2053
g_step 3600, step 264, avg_time 2.484, loss:605.4025
g_step 3700, step 364, avg_time 1.090, loss:632.0925
g_step 3800, step 47, avg_time 1.073, loss:619.7361
g_step 3900, step 147, avg_time 1.095, loss:603.5738
g_step 4000, step 247, avg_time 1.086, loss:626.1879
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5096, rec:0.5796, f1:0.5423
>> valid relation prec:0.3415, rec:0.1316, f1:0.1900
>> valid relation with NER prec:0.3415, rec:0.1316, f1:0.1900
g_step 4100, step 347, avg_time 2.493, loss:628.3888
g_step 4200, step 30, avg_time 1.071, loss:609.1627
g_step 4300, step 130, avg_time 1.084, loss:576.8801
g_step 4400, step 230, avg_time 1.081, loss:584.8131
g_step 4500, step 330, avg_time 1.087, loss:590.4331
>> valid entity prec:0.5786, rec:0.5402, f1:0.5587
>> valid relation prec:0.3545, rec:0.1187, f1:0.1779
>> valid relation with NER prec:0.3545, rec:0.1187, f1:0.1779
g_step 4600, step 13, avg_time 2.487, loss:592.9312
g_step 4700, step 113, avg_time 1.099, loss:554.5595
g_step 4800, step 213, avg_time 1.089, loss:572.5812
g_step 4900, step 313, avg_time 1.084, loss:552.6186
g_step 5000, step 413, avg_time 1.085, loss:586.5700
learning rate was adjusted to 0.0008
>> valid entity prec:0.5685, rec:0.5198, f1:0.5431
>> valid relation prec:0.3451, rec:0.1070, f1:0.1633
>> valid relation with NER prec:0.3451, rec:0.1070, f1:0.1633
g_step 5100, step 96, avg_time 2.474, loss:527.5194
g_step 5200, step 196, avg_time 1.077, loss:539.1304
g_step 5300, step 296, avg_time 1.092, loss:561.6471
g_step 5400, step 396, avg_time 1.083, loss:527.9747
g_step 5500, step 79, avg_time 1.084, loss:507.2509
>> valid entity prec:0.5777, rec:0.5562, f1:0.5668
>> valid relation prec:0.3236, rec:0.1236, f1:0.1789
>> valid relation with NER prec:0.3236, rec:0.1236, f1:0.1789
g_step 5600, step 179, avg_time 2.494, loss:502.5091
g_step 5700, step 279, avg_time 1.091, loss:533.6370
g_step 5800, step 379, avg_time 1.088, loss:551.2409
g_step 5900, step 62, avg_time 1.093, loss:511.8592
g_step 6000, step 162, avg_time 1.072, loss:512.9438
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5249, rec:0.5734, f1:0.5481
>> valid relation prec:0.2744, rec:0.1193, f1:0.1663
>> valid relation with NER prec:0.2744, rec:0.1193, f1:0.1663
g_step 6100, step 262, avg_time 2.496, loss:488.5096
g_step 6200, step 362, avg_time 1.079, loss:504.5953
g_step 6300, step 45, avg_time 1.087, loss:500.5904
g_step 6400, step 145, avg_time 1.091, loss:478.2861
g_step 6500, step 245, avg_time 1.079, loss:487.0681
>> valid entity prec:0.5734, rec:0.5692, f1:0.5713
>> valid relation prec:0.3509, rec:0.1336, f1:0.1936
>> valid relation with NER prec:0.3509, rec:0.1336, f1:0.1936
g_step 6600, step 345, avg_time 2.487, loss:475.6577
g_step 6700, step 28, avg_time 1.082, loss:489.7296
g_step 6800, step 128, avg_time 1.087, loss:459.8798
g_step 6900, step 228, avg_time 1.088, loss:474.0695
g_step 7000, step 328, avg_time 1.076, loss:456.0911
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5701, rec:0.5436, f1:0.5565
>> valid relation prec:0.3499, rec:0.1356, f1:0.1955
>> valid relation with NER prec:0.3499, rec:0.1356, f1:0.1955
g_step 7100, step 11, avg_time 2.495, loss:484.7152
g_step 7200, step 111, avg_time 1.079, loss:422.6617
g_step 7300, step 211, avg_time 1.090, loss:462.6019
g_step 7400, step 311, avg_time 1.098, loss:453.2477
g_step 7500, step 411, avg_time 1.091, loss:474.9965
>> valid entity prec:0.5607, rec:0.5598, f1:0.5603
>> valid relation prec:0.3010, rec:0.1540, f1:0.2038
>> valid relation with NER prec:0.3010, rec:0.1540, f1:0.2038
g_step 7600, step 94, avg_time 2.482, loss:440.3055
g_step 7700, step 194, avg_time 1.091, loss:437.1094
g_step 7800, step 294, avg_time 1.091, loss:427.4033
g_step 7900, step 394, avg_time 1.090, loss:456.6798
g_step 8000, step 77, avg_time 1.089, loss:411.3273
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5067, rec:0.5961, f1:0.5478
>> valid relation prec:0.3175, rec:0.1302, f1:0.1847
>> valid relation with NER prec:0.3175, rec:0.1302, f1:0.1847
g_step 8100, step 177, avg_time 2.500, loss:419.4973
g_step 8200, step 277, avg_time 1.081, loss:417.9269
g_step 8300, step 377, avg_time 1.084, loss:443.1521
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:16:54 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:16:54 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-16-54_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:16:55 - WARNING - datasets.builder -   Using custom data configuration default-e3c04ad904937692
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e3c04ad904937692/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:16:56,094 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:16:56,095 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:16:56,095 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:16:56,096 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:16:56,107 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:56,110 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:56,110 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:56,111 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:56,111 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:56,111 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:56,111 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:16:56,268 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:16:59,385 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:16:59,388 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e3c04ad904937692/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 14:16:59 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14ff89da5200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  3.15ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.96ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.22ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.36ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.44ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.50ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.51ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.54ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.55ba/s] 91%|█████████ | 10/11 [00:02<00:00,  3.75ba/s]100%|██████████| 11/11 [00:02<00:00,  4.52ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.05ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.22ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.33ba/s]100%|██████████| 4/4 [00:00<00:00,  5.39ba/s]100%|██████████| 4/4 [00:00<00:00,  4.90ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  8.59ba/s] 27%|██▋       | 3/11 [00:00<00:00, 10.04ba/s] 45%|████▌     | 5/11 [00:00<00:00, 10.32ba/s] 64%|██████▎   | 7/11 [00:00<00:00, 10.32ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.39ba/s]100%|██████████| 11/11 [00:00<00:00, 11.31ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.67ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.89ba/s]100%|██████████| 4/4 [00:00<00:00, 11.21ba/s]
[INFO|trainer.py:414] 2023-08-28 14:17:04,344 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:17:04,349 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:17:04,349 >>   Num examples = 10026
[INFO|trainer.py:1149] 2023-08-28 14:17:04,349 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:17:04,349 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:17:04,349 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:17:04,349 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:17:04,349 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:51,  3.38it/s]  0%|          | 2/785 [00:00<03:47,  3.44it/s]  0%|          | 3/785 [00:00<03:46,  3.46it/s]  1%|          | 4/785 [00:01<03:45,  3.47it/s]  1%|          | 5/785 [00:01<03:44,  3.47it/s]  1%|          | 6/785 [00:01<03:44,  3.47it/s]  1%|          | 7/785 [00:02<03:43,  3.47it/s]  1%|          | 8/785 [00:02<03:43,  3.47it/s]  1%|          | 9/785 [00:02<03:43,  3.48it/s]  1%|▏         | 10/785 [00:02<03:42,  3.48it/s]  1%|▏         | 11/785 [00:03<03:42,  3.48it/s]  2%|▏         | 12/785 [00:03<03:42,  3.48it/s]  2%|▏         | 13/785 [00:03<03:41,  3.48it/s]  2%|▏         | 14/785 [00:04<03:41,  3.48it/s]  2%|▏         | 15/785 [00:04<03:41,  3.48it/s]  2%|▏         | 16/785 [00:04<03:40,  3.48it/s]  2%|▏         | 17/785 [00:04<03:40,  3.48it/s]  2%|▏         | 18/785 [00:05<03:40,  3.48it/s]  2%|▏         | 19/785 [00:05<03:40,  3.48it/s]  3%|▎         | 20/785 [00:05<03:39,  3.48it/s]  3%|▎         | 21/785 [00:06<03:39,  3.48it/s]  3%|▎         | 22/785 [00:06<03:39,  3.48it/s]  3%|▎         | 23/785 [00:06<03:39,  3.48it/s]  3%|▎         | 24/785 [00:06<03:38,  3.48it/s]  3%|▎         | 25/785 [00:07<03:38,  3.48it/s]  3%|▎         | 26/785 [00:07<03:38,  3.48it/s]  3%|▎         | 27/785 [00:07<03:38,  3.48it/s]  4%|▎         | 28/785 [00:08<03:37,  3.48it/s]  4%|▎         | 29/785 [00:08<03:37,  3.48it/s]  4%|▍         | 30/785 [00:08<03:37,  3.48it/s]  4%|▍         | 31/785 [00:08<03:36,  3.47it/s]  4%|▍         | 32/785 [00:09<03:36,  3.47it/s]  4%|▍         | 33/785 [00:09<03:36,  3.48it/s]  4%|▍         | 34/785 [00:09<03:36,  3.47it/s]  4%|▍         | 35/785 [00:10<03:35,  3.47it/s]  5%|▍         | 36/785 [00:10<03:35,  3.47it/s]  5%|▍         | 37/785 [00:10<03:35,  3.47it/s]  5%|▍         | 38/785 [00:10<03:35,  3.47it/s]  5%|▍         | 39/785 [00:11<03:34,  3.47it/s]  5%|▌         | 40/785 [00:11<03:34,  3.47it/s]  5%|▌         | 41/785 [00:11<03:34,  3.47it/s]  5%|▌         | 42/785 [00:12<03:33,  3.47it/s]  5%|▌         | 43/785 [00:12<03:33,  3.47it/s]  6%|▌         | 44/785 [00:12<03:33,  3.47it/s]  6%|▌         | 45/785 [00:12<03:33,  3.47it/s]  6%|▌         | 46/785 [00:13<03:32,  3.47it/s]  6%|▌         | 47/785 [00:13<03:36,  3.41it/s]  6%|▌         | 48/785 [00:13<03:35,  3.43it/s]  6%|▌         | 49/785 [00:14<03:33,  3.44it/s]  6%|▋         | 50/785 [00:14<03:33,  3.45it/s]  6%|▋         | 51/785 [00:14<03:32,  3.46it/s]  7%|▋         | 52/785 [00:14<03:31,  3.46it/s]  7%|▋         | 53/785 [00:15<03:31,  3.46it/s]  7%|▋         | 54/785 [00:15<03:30,  3.47it/s]  7%|▋         | 55/785 [00:15<03:30,  3.47it/s]  7%|▋         | 56/785 [00:16<03:30,  3.47it/s]  7%|▋         | 57/785 [00:16<03:29,  3.47it/s]  7%|▋         | 58/785 [00:16<03:29,  3.47it/s]  8%|▊         | 59/785 [00:17<03:29,  3.47it/s]  8%|▊         | 60/785 [00:17<03:29,  3.47it/s]  8%|▊         | 61/785 [00:17<03:28,  3.47it/s]  8%|▊         | 62/785 [00:17<03:28,  3.47it/s]  8%|▊         | 63/785 [00:18<03:28,  3.47it/s]  8%|▊         | 64/785 [00:18<03:27,  3.47it/s]  8%|▊         | 65/785 [00:18<03:27,  3.47it/s]  8%|▊         | 66/785 [00:19<03:27,  3.47it/s]  9%|▊         | 67/785 [00:19<03:26,  3.47it/s]  9%|▊         | 68/785 [00:19<03:26,  3.47it/s]  9%|▉         | 69/785 [00:19<03:26,  3.47it/s]  9%|▉         | 70/785 [00:20<03:26,  3.47it/s]  9%|▉         | 71/785 [00:20<03:25,  3.47it/s]  9%|▉         | 72/785 [00:20<03:25,  3.47it/s]  9%|▉         | 73/785 [00:21<03:25,  3.47it/s]  9%|▉         | 74/785 [00:21<03:24,  3.47it/s] 10%|▉         | 75/785 [00:21<03:29,  3.38it/s] 10%|▉         | 76/785 [00:21<03:28,  3.41it/s] 10%|▉         | 77/785 [00:22<03:26,  3.42it/s] 10%|▉         | 78/785 [00:22<03:25,  3.44it/s] 10%|█         | 79/785 [00:22<03:24,  3.45it/s] 10%|█         | 80/785 [00:23<03:24,  3.45it/s] 10%|█         | 81/785 [00:23<03:23,  3.46it/s] 10%|█         | 82/785 [00:23<03:23,  3.46it/s] 11%|█         | 83/785 [00:23<03:22,  3.46it/s] 11%|█         | 84/785 [00:24<03:22,  3.46it/s] 11%|█         | 85/785 [00:24<03:22,  3.46it/s] 11%|█         | 86/785 [00:24<03:21,  3.47it/s] 11%|█         | 87/785 [00:25<03:21,  3.47it/s] 11%|█         | 88/785 [00:25<03:21,  3.47it/s] 11%|█▏        | 89/785 [00:25<03:20,  3.47it/s] 11%|█▏        | 90/785 [00:25<03:20,  3.47it/s] 12%|█▏        | 91/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 92/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 93/785 [00:26<03:19,  3.46it/s] 12%|█▏        | 94/785 [00:27<03:19,  3.46it/s] 12%|█▏        | 95/785 [00:27<03:19,  3.46it/s] 12%|█▏        | 96/785 [00:27<03:18,  3.46it/s] 12%|█▏        | 97/785 [00:27<03:18,  3.47it/s] 12%|█▏        | 98/785 [00:28<03:18,  3.46it/s] 13%|█▎        | 99/785 [00:28<03:17,  3.47it/s] 13%|█▎        | 100/785 [00:28<03:17,  3.47it/s] 13%|█▎        | 101/785 [00:29<03:17,  3.47it/s] 13%|█▎        | 102/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 103/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 104/785 [00:30<03:16,  3.46it/s] 13%|█▎        | 105/785 [00:30<03:16,  3.46it/s] 14%|█▎        | 106/785 [00:30<03:16,  3.46it/s] 14%|█▎        | 107/785 [00:30<03:16,  3.46it/s] 14%|█▍        | 108/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 109/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 110/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 111/785 [00:32<03:14,  3.46it/s] 14%|█▍        | 112/785 [00:32<03:14,  3.46it/s] 14%|█▍        | 113/785 [00:32<03:14,  3.46it/s] 15%|█▍        | 114/785 [00:32<03:14,  3.46it/s] 15%|█▍        | 115/785 [00:33<03:13,  3.46it/s] 15%|█▍        | 116/785 [00:33<03:13,  3.45it/s] 15%|█▍        | 117/785 [00:33<03:13,  3.45it/s] 15%|█▌        | 118/785 [00:34<03:13,  3.45it/s] 15%|█▌        | 119/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 120/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 121/785 [00:34<03:12,  3.46it/s] 16%|█▌        | 122/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 123/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 124/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 125/785 [00:36<03:10,  3.46it/s] 16%|█▌        | 126/785 [00:36<03:10,  3.45it/s] 16%|█▌        | 127/785 [00:36<03:10,  3.45it/s] 16%|█▋        | 128/785 [00:36<03:10,  3.46it/s] 16%|█▋        | 129/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 130/785 [00:37<03:09,  3.45it/s] 17%|█▋        | 131/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 132/785 [00:38<03:09,  3.45it/s] 17%|█▋        | 133/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 134/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 135/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 136/785 [00:39<03:07,  3.46it/s] 17%|█▋        | 137/785 [00:39<03:07,  3.46it/s] 18%|█▊        | 138/785 [00:39<03:07,  3.46it/s] 18%|█▊        | 139/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 140/785 [00:40<03:06,  3.45it/s] 18%|█▊        | 141/785 [00:40<03:06,  3.45it/s] 18%|█▊        | 142/785 [00:41<03:06,  3.45it/s] 18%|█▊        | 143/785 [00:41<03:05,  3.45it/s] 18%|█▊        | 144/785 [00:41<03:05,  3.45it/s] 18%|█▊        | 145/785 [00:41<03:05,  3.45it/s] 19%|█▊        | 146/785 [00:42<03:05,  3.45it/s] 19%|█▊        | 147/785 [00:42<03:04,  3.45it/s] 19%|█▉        | 148/785 [00:42<03:04,  3.45it/s] 19%|█▉        | 149/785 [00:43<03:04,  3.45it/s] 19%|█▉        | 150/785 [00:43<03:03,  3.45it/s] 19%|█▉        | 151/785 [00:43<03:03,  3.46it/s] 19%|█▉        | 152/785 [00:43<03:03,  3.45it/s] 19%|█▉        | 153/785 [00:44<03:03,  3.45it/s] 20%|█▉        | 154/785 [00:44<03:02,  3.45it/s] 20%|█▉        | 155/785 [00:44<03:02,  3.45it/s] 20%|█▉        | 156/785 [00:45<03:02,  3.45it/s] 20%|██        | 157/785 [00:45<02:46,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 14:17:49,623 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:17:49,623 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:17:49,623 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.11it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.18it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.45it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.62it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.29it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.99it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.69it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.55it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.45it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.46it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.48it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.41it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.50it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.53it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.57it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.50it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.44it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.36it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.42it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.46it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.45it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.37it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.47it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.31it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.47it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.38it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.41it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.34it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.40it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.39it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.40it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.42it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.47it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.41it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.44it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.38it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.43it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.44it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.37it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.26it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.30it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.31it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.39it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.41it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.31it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.22it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.18it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.07it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.04it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.08it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.16it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.12it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.32it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.36it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.38it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.32it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.37it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 40.92it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 42.46it/s][A
 69%|██████▉   | 303/437 [00:06<00:03, 43.61it/s][A
 70%|███████   | 308/437 [00:06<00:02, 44.44it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 44.99it/s][A
 73%|███████▎  | 318/437 [00:06<00:03, 38.88it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 40.92it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 42.44it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 43.63it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 44.42it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 45.09it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 45.36it/s][A
 81%|████████  | 353/437 [00:07<00:01, 45.75it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 45.58it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 45.80it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 45.93it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.00it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.07it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.04it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.08it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.08it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.00it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.00it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 45.95it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 46.00it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.15it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.27it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.27it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.33it/s][A                                                 
                                                 [A 20%|██        | 157/785 [00:54<02:46,  3.77it/s]
100%|██████████| 437/437 [00:09<00:00, 46.33it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:17:59,483 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-28 14:17:59,809 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:18:04,882 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:18:04,913 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:18:04,920 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:07<1:10:06,  6.71s/it] 20%|██        | 159/785 [01:07<49:57,  4.79s/it]   20%|██        | 160/785 [01:07<35:48,  3.44s/it] 21%|██        | 161/785 [01:07<25:55,  2.49s/it] 21%|██        | 162/785 [01:08<19:01,  1.83s/it] 21%|██        | 163/785 [01:08<14:11,  1.37s/it] 21%|██        | 164/785 [01:08<10:48,  1.04s/it] 21%|██        | 165/785 [01:09<08:26,  1.22it/s] 21%|██        | 166/785 [01:09<06:47,  1.52it/s] 21%|██▏       | 167/785 [01:09<05:38,  1.82it/s] 21%|██▏       | 168/785 [01:09<04:50,  2.13it/s] 22%|██▏       | 169/785 [01:10<04:15,  2.41it/s] 22%|██▏       | 170/785 [01:10<03:52,  2.64it/s] 22%|██▏       | 171/785 [01:10<03:35,  2.85it/s] 22%|██▏       | 172/785 [01:11<03:23,  3.01it/s] 22%|██▏       | 173/785 [01:11<03:15,  3.13it/s] 22%|██▏       | 174/785 [01:11<03:09,  3.23it/s] 22%|██▏       | 175/785 [01:11<03:04,  3.30it/s] 22%|██▏       | 176/785 [01:12<03:01,  3.35it/s] 23%|██▎       | 177/785 [01:12<02:59,  3.38it/s] 23%|██▎       | 178/785 [01:12<02:58,  3.41it/s] 23%|██▎       | 179/785 [01:13<02:56,  3.43it/s] 23%|██▎       | 180/785 [01:13<02:55,  3.44it/s] 23%|██▎       | 181/785 [01:13<02:56,  3.43it/s] 23%|██▎       | 182/785 [01:13<02:55,  3.44it/s] 23%|██▎       | 183/785 [01:14<02:54,  3.45it/s] 23%|██▎       | 184/785 [01:14<02:53,  3.46it/s] 24%|██▎       | 185/785 [01:14<02:53,  3.46it/s] 24%|██▎       | 186/785 [01:15<02:52,  3.46it/s] 24%|██▍       | 187/785 [01:15<02:52,  3.46it/s] 24%|██▍       | 188/785 [01:15<02:52,  3.47it/s] 24%|██▍       | 189/785 [01:15<02:51,  3.47it/s] 24%|██▍       | 190/785 [01:16<02:51,  3.47it/s] 24%|██▍       | 191/785 [01:16<02:51,  3.47it/s] 24%|██▍       | 192/785 [01:16<02:51,  3.45it/s] 25%|██▍       | 193/785 [01:17<02:51,  3.45it/s] 25%|██▍       | 194/785 [01:17<02:51,  3.46it/s] 25%|██▍       | 195/785 [01:17<02:50,  3.46it/s] 25%|██▍       | 196/785 [01:18<02:50,  3.46it/s] 25%|██▌       | 197/785 [01:18<02:49,  3.47it/s] 25%|██▌       | 198/785 [01:18<02:49,  3.47it/s] 25%|██▌       | 199/785 [01:18<02:48,  3.47it/s] 25%|██▌       | 200/785 [01:19<02:48,  3.47it/s] 26%|██▌       | 201/785 [01:19<02:48,  3.47it/s] 26%|██▌       | 202/785 [01:19<02:47,  3.47it/s] 26%|██▌       | 203/785 [01:20<02:51,  3.40it/s] 26%|██▌       | 204/785 [01:20<02:49,  3.42it/s] 26%|██▌       | 205/785 [01:20<02:48,  3.43it/s] 26%|██▌       | 206/785 [01:20<02:48,  3.45it/s] 26%|██▋       | 207/785 [01:21<02:47,  3.45it/s] 26%|██▋       | 208/785 [01:21<02:47,  3.45it/s] 27%|██▋       | 209/785 [01:21<02:46,  3.46it/s] 27%|██▋       | 210/785 [01:22<02:46,  3.46it/s] 27%|██▋       | 211/785 [01:22<02:45,  3.46it/s] 27%|██▋       | 212/785 [01:22<02:45,  3.46it/s] 27%|██▋       | 213/785 [01:22<02:45,  3.46it/s] 27%|██▋       | 214/785 [01:23<02:44,  3.46it/s] 27%|██▋       | 215/785 [01:23<02:44,  3.46it/s] 28%|██▊       | 216/785 [01:23<02:44,  3.46it/s] 28%|██▊       | 217/785 [01:24<02:44,  3.46it/s] 28%|██▊       | 218/785 [01:24<02:43,  3.46it/s] 28%|██▊       | 219/785 [01:24<02:43,  3.46it/s] 28%|██▊       | 220/785 [01:24<02:43,  3.46it/s] 28%|██▊       | 221/785 [01:25<02:42,  3.46it/s] 28%|██▊       | 222/785 [01:25<02:42,  3.46it/s] 28%|██▊       | 223/785 [01:25<02:42,  3.46it/s] 29%|██▊       | 224/785 [01:26<02:43,  3.44it/s] 29%|██▊       | 225/785 [01:26<02:42,  3.45it/s] 29%|██▉       | 226/785 [01:26<02:42,  3.45it/s] 29%|██▉       | 227/785 [01:26<02:41,  3.45it/s] 29%|██▉       | 228/785 [01:27<02:41,  3.45it/s] 29%|██▉       | 229/785 [01:27<02:40,  3.45it/s] 29%|██▉       | 230/785 [01:27<02:40,  3.45it/s] 29%|██▉       | 231/785 [01:28<02:40,  3.46it/s] 30%|██▉       | 232/785 [01:28<02:39,  3.46it/s] 30%|██▉       | 233/785 [01:28<02:39,  3.46it/s] 30%|██▉       | 234/785 [01:28<02:39,  3.46it/s] 30%|██▉       | 235/785 [01:29<02:40,  3.44it/s] 30%|███       | 236/785 [01:29<02:39,  3.45it/s] 30%|███       | 237/785 [01:29<02:38,  3.45it/s] 30%|███       | 238/785 [01:30<02:38,  3.45it/s] 30%|███       | 239/785 [01:30<02:38,  3.45it/s] 31%|███       | 240/785 [01:30<02:37,  3.45it/s] 31%|███       | 241/785 [01:31<02:37,  3.46it/s] 31%|███       | 242/785 [01:31<02:37,  3.46it/s] 31%|███       | 243/785 [01:31<02:36,  3.45it/s] 31%|███       | 244/785 [01:31<02:36,  3.45it/s] 31%|███       | 245/785 [01:32<02:36,  3.45it/s] 31%|███▏      | 246/785 [01:32<02:36,  3.45it/s] 31%|███▏      | 247/785 [01:32<02:35,  3.45it/s] 32%|███▏      | 248/785 [01:33<02:35,  3.45it/s] 32%|███▏      | 249/785 [01:33<02:35,  3.45it/s] 32%|███▏      | 250/785 [01:33<02:34,  3.45it/s] 32%|███▏      | 251/785 [01:33<02:34,  3.45it/s] 32%|███▏      | 252/785 [01:34<02:34,  3.45it/s] 32%|███▏      | 253/785 [01:34<02:34,  3.45it/s] 32%|███▏      | 254/785 [01:34<02:33,  3.45it/s] 32%|███▏      | 255/785 [01:35<02:33,  3.45it/s] 33%|███▎      | 256/785 [01:35<02:33,  3.45it/s] 33%|███▎      | 257/785 [01:35<02:33,  3.44it/s] 33%|███▎      | 258/785 [01:35<02:32,  3.45it/s] 33%|███▎      | 259/785 [01:36<02:32,  3.45it/s] 33%|███▎      | 260/785 [01:36<02:32,  3.45it/s] 33%|███▎      | 261/785 [01:36<02:31,  3.45it/s] 33%|███▎      | 262/785 [01:37<02:31,  3.45it/s] 34%|███▎      | 263/785 [01:37<02:31,  3.45it/s] 34%|███▎      | 264/785 [01:37<02:30,  3.45it/s] 34%|███▍      | 265/785 [01:37<02:30,  3.46it/s] 34%|███▍      | 266/785 [01:38<02:30,  3.46it/s] 34%|███▍      | 267/785 [01:38<02:29,  3.46it/s] 34%|███▍      | 268/785 [01:38<02:29,  3.45it/s] 34%|███▍      | 269/785 [01:39<02:29,  3.45it/s] 34%|███▍      | 270/785 [01:39<02:29,  3.45it/s] 35%|███▍      | 271/785 [01:39<02:28,  3.45it/s] 35%|███▍      | 272/785 [01:40<02:28,  3.45it/s] 35%|███▍      | 273/785 [01:40<02:28,  3.46it/s] 35%|███▍      | 274/785 [01:40<02:27,  3.46it/s] 35%|███▌      | 275/785 [01:40<02:27,  3.46it/s] 35%|███▌      | 276/785 [01:41<02:27,  3.46it/s] 35%|███▌      | 277/785 [01:41<02:26,  3.46it/s] 35%|███▌      | 278/785 [01:41<02:26,  3.46it/s] 36%|███▌      | 279/785 [01:42<02:26,  3.45it/s] 36%|███▌      | 280/785 [01:42<02:26,  3.45it/s] 36%|███▌      | 281/785 [01:42<02:25,  3.45it/s] 36%|███▌      | 282/785 [01:42<02:25,  3.45it/s] 36%|███▌      | 283/785 [01:43<02:25,  3.45it/s] 36%|███▌      | 284/785 [01:43<02:24,  3.46it/s] 36%|███▋      | 285/785 [01:43<02:24,  3.45it/s] 36%|███▋      | 286/785 [01:44<02:24,  3.46it/s] 37%|███▋      | 287/785 [01:44<02:24,  3.45it/s] 37%|███▋      | 288/785 [01:44<02:23,  3.46it/s] 37%|███▋      | 289/785 [01:44<02:23,  3.45it/s] 37%|███▋      | 290/785 [01:45<02:23,  3.45it/s] 37%|███▋      | 291/785 [01:45<02:23,  3.45it/s] 37%|███▋      | 292/785 [01:45<02:22,  3.45it/s] 37%|███▋      | 293/785 [01:46<02:22,  3.45it/s] 37%|███▋      | 294/785 [01:46<02:22,  3.45it/s] 38%|███▊      | 295/785 [01:46<02:21,  3.45it/s] 38%|███▊      | 296/785 [01:46<02:21,  3.46it/s] 38%|███▊      | 297/785 [01:47<02:21,  3.46it/s] 38%|███▊      | 298/785 [01:47<02:20,  3.46it/s] 38%|███▊      | 299/785 [01:47<02:20,  3.46it/s] 38%|███▊      | 300/785 [01:48<02:20,  3.45it/s] 38%|███▊      | 301/785 [01:48<02:20,  3.44it/s] 38%|███▊      | 302/785 [01:48<02:20,  3.45it/s] 39%|███▊      | 303/785 [01:48<02:19,  3.45it/s] 39%|███▊      | 304/785 [01:49<02:19,  3.45it/s] 39%|███▉      | 305/785 [01:49<02:19,  3.45it/s] 39%|███▉      | 306/785 [01:49<02:18,  3.45it/s] 39%|███▉      | 307/785 [01:50<02:18,  3.45it/s] 39%|███▉      | 308/785 [01:50<02:18,  3.45it/s] 39%|███▉      | 309/785 [01:50<02:17,  3.45it/s] 39%|███▉      | 310/785 [01:51<02:17,  3.45it/s] 40%|███▉      | 311/785 [01:51<02:17,  3.45it/s] 40%|███▉      | 312/785 [01:51<02:17,  3.44it/s] 40%|███▉      | 313/785 [01:51<02:17,  3.44it/s] 40%|████      | 314/785 [01:52<02:05,  3.76it/s][INFO|trainer.py:2140] 2023-08-28 14:18:56,447 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:18:56,447 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:18:56,447 >>   Batch size = 8
{'eval_loss': 0.9183201789855957, 'eval_runtime': 9.5323, 'eval_samples_per_second': 366.019, 'eval_steps_per_second': 45.844, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.05it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.20it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.50it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.60it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.15it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.78it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.53it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.10it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.21it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.27it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.35it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.43it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.50it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.43it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.46it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.36it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.28it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.17it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.27it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.33it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.41it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.45it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.47it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.44it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.37it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.36it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.25it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.20it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.14it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.20it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.19it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.18it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.09it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.08it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.07it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.00it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.02it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.00it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.07it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.05it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.09it/s][A
 49%|████▊     | 213/437 [00:04<00:05, 42.63it/s][A
 50%|████▉     | 218/437 [00:04<00:05, 43.79it/s][A
 51%|█████     | 223/437 [00:04<00:04, 44.61it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 45.10it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 45.55it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 45.87it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.02it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.23it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.01it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 45.95it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.12it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.23it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.29it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.37it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.44it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.43it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.33it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.22it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.17it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.19it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.21it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.33it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.36it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.29it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.44it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.41it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.31it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.15it/s][A
 81%|████████  | 353/437 [00:07<00:02, 41.78it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 43.06it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 44.10it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 44.75it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 45.30it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 45.65it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 45.84it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.10it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 45.90it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 45.86it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.04it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.16it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.23it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.36it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.30it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.28it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.22it/s][A                                                 
                                                 [A 40%|████      | 314/785 [02:01<02:05,  3.76it/s]
100%|██████████| 437/437 [00:09<00:00, 46.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:19:05,964 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-28 14:19:05,985 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:19:10,145 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:19:10,346 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:19:10,452 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:12<48:20,  6.17s/it] 40%|████      | 316/785 [02:12<34:27,  4.41s/it] 40%|████      | 317/785 [02:12<24:44,  3.17s/it] 41%|████      | 318/785 [02:12<17:57,  2.31s/it] 41%|████      | 319/785 [02:13<13:12,  1.70s/it] 41%|████      | 320/785 [02:13<09:54,  1.28s/it] 41%|████      | 321/785 [02:13<07:35,  1.02it/s] 41%|████      | 322/785 [02:14<05:57,  1.29it/s] 41%|████      | 323/785 [02:14<04:50,  1.59it/s] 41%|████▏     | 324/785 [02:14<04:02,  1.90it/s] 41%|████▏     | 325/785 [02:14<03:29,  2.20it/s] 42%|████▏     | 326/785 [02:15<03:05,  2.47it/s] 42%|████▏     | 327/785 [02:15<02:49,  2.70it/s] 42%|████▏     | 328/785 [02:15<02:38,  2.89it/s] 42%|████▏     | 329/785 [02:16<02:29,  3.04it/s] 42%|████▏     | 330/785 [02:16<02:24,  3.16it/s] 42%|████▏     | 331/785 [02:16<02:19,  3.25it/s] 42%|████▏     | 332/785 [02:16<02:16,  3.31it/s] 42%|████▏     | 333/785 [02:17<02:14,  3.35it/s] 43%|████▎     | 334/785 [02:17<02:13,  3.39it/s] 43%|████▎     | 335/785 [02:17<02:11,  3.41it/s] 43%|████▎     | 336/785 [02:18<02:11,  3.43it/s] 43%|████▎     | 337/785 [02:18<02:10,  3.44it/s] 43%|████▎     | 338/785 [02:18<02:10,  3.44it/s] 43%|████▎     | 339/785 [02:18<02:09,  3.44it/s] 43%|████▎     | 340/785 [02:19<02:08,  3.45it/s] 43%|████▎     | 341/785 [02:19<02:08,  3.45it/s] 44%|████▎     | 342/785 [02:19<02:08,  3.46it/s] 44%|████▎     | 343/785 [02:20<02:07,  3.46it/s] 44%|████▍     | 344/785 [02:20<02:07,  3.46it/s] 44%|████▍     | 345/785 [02:20<02:07,  3.46it/s] 44%|████▍     | 346/785 [02:21<02:06,  3.46it/s] 44%|████▍     | 347/785 [02:21<02:06,  3.46it/s] 44%|████▍     | 348/785 [02:21<02:06,  3.46it/s] 44%|████▍     | 349/785 [02:21<02:06,  3.45it/s] 45%|████▍     | 350/785 [02:22<02:05,  3.45it/s] 45%|████▍     | 351/785 [02:22<02:05,  3.46it/s] 45%|████▍     | 352/785 [02:22<02:05,  3.46it/s] 45%|████▍     | 353/785 [02:23<02:04,  3.46it/s] 45%|████▌     | 354/785 [02:23<02:04,  3.45it/s] 45%|████▌     | 355/785 [02:23<02:04,  3.46it/s] 45%|████▌     | 356/785 [02:23<02:04,  3.46it/s] 45%|████▌     | 357/785 [02:24<02:03,  3.46it/s] 46%|████▌     | 358/785 [02:24<02:03,  3.46it/s] 46%|████▌     | 359/785 [02:24<02:03,  3.46it/s] 46%|████▌     | 360/785 [02:25<02:02,  3.46it/s] 46%|████▌     | 361/785 [02:25<02:02,  3.46it/s] 46%|████▌     | 362/785 [02:25<02:02,  3.46it/s] 46%|████▌     | 363/785 [02:25<02:02,  3.46it/s] 46%|████▋     | 364/785 [02:26<02:01,  3.45it/s] 46%|████▋     | 365/785 [02:26<02:01,  3.45it/s] 47%|████▋     | 366/785 [02:26<02:01,  3.45it/s] 47%|████▋     | 367/785 [02:27<02:01,  3.45it/s] 47%|████▋     | 368/785 [02:27<02:00,  3.46it/s] 47%|████▋     | 369/785 [02:27<02:00,  3.45it/s] 47%|████▋     | 370/785 [02:27<02:00,  3.45it/s] 47%|████▋     | 371/785 [02:28<01:59,  3.45it/s] 47%|████▋     | 372/785 [02:28<01:59,  3.45it/s] 48%|████▊     | 373/785 [02:28<01:59,  3.45it/s] 48%|████▊     | 374/785 [02:29<01:59,  3.45it/s] 48%|████▊     | 375/785 [02:29<01:58,  3.45it/s] 48%|████▊     | 376/785 [02:29<01:58,  3.45it/s] 48%|████▊     | 377/785 [02:29<01:58,  3.45it/s] 48%|████▊     | 378/785 [02:30<01:57,  3.45it/s] 48%|████▊     | 379/785 [02:30<01:57,  3.45it/s] 48%|████▊     | 380/785 [02:30<01:57,  3.43it/s] 49%|████▊     | 381/785 [02:31<01:57,  3.44it/s] 49%|████▊     | 382/785 [02:31<01:57,  3.44it/s] 49%|████▉     | 383/785 [02:31<01:56,  3.45it/s] 49%|████▉     | 384/785 [02:32<01:56,  3.45it/s] 49%|████▉     | 385/785 [02:32<01:55,  3.45it/s] 49%|████▉     | 386/785 [02:32<01:55,  3.45it/s] 49%|████▉     | 387/785 [02:32<01:55,  3.45it/s] 49%|████▉     | 388/785 [02:33<01:54,  3.45it/s] 50%|████▉     | 389/785 [02:33<01:54,  3.45it/s] 50%|████▉     | 390/785 [02:33<01:54,  3.45it/s] 50%|████▉     | 391/785 [02:34<01:54,  3.44it/s] 50%|████▉     | 392/785 [02:34<01:54,  3.44it/s] 50%|█████     | 393/785 [02:34<01:53,  3.45it/s] 50%|█████     | 394/785 [02:34<01:53,  3.45it/s] 50%|█████     | 395/785 [02:35<01:53,  3.45it/s] 50%|█████     | 396/785 [02:35<01:52,  3.45it/s] 51%|█████     | 397/785 [02:35<01:52,  3.45it/s] 51%|█████     | 398/785 [02:36<01:52,  3.45it/s] 51%|█████     | 399/785 [02:36<01:51,  3.45it/s] 51%|█████     | 400/785 [02:36<01:51,  3.45it/s] 51%|█████     | 401/785 [02:36<01:51,  3.45it/s] 51%|█████     | 402/785 [02:37<01:51,  3.44it/s] 51%|█████▏    | 403/785 [02:37<01:50,  3.45it/s] 51%|█████▏    | 404/785 [02:37<01:50,  3.45it/s] 52%|█████▏    | 405/785 [02:38<01:50,  3.45it/s] 52%|█████▏    | 406/785 [02:38<01:49,  3.45it/s] 52%|█████▏    | 407/785 [02:38<01:49,  3.45it/s] 52%|█████▏    | 408/785 [02:38<01:49,  3.45it/s] 52%|█████▏    | 409/785 [02:39<01:48,  3.45it/s] 52%|█████▏    | 410/785 [02:39<01:48,  3.45it/s] 52%|█████▏    | 411/785 [02:39<01:48,  3.45it/s] 52%|█████▏    | 412/785 [02:40<01:48,  3.45it/s] 53%|█████▎    | 413/785 [02:40<01:48,  3.43it/s] 53%|█████▎    | 414/785 [02:40<01:47,  3.44it/s] 53%|█████▎    | 415/785 [02:40<01:47,  3.44it/s] 53%|█████▎    | 416/785 [02:41<01:47,  3.45it/s] 53%|█████▎    | 417/785 [02:41<01:46,  3.45it/s] 53%|█████▎    | 418/785 [02:41<01:46,  3.45it/s] 53%|█████▎    | 419/785 [02:42<01:46,  3.45it/s] 54%|█████▎    | 420/785 [02:42<01:45,  3.45it/s] 54%|█████▎    | 421/785 [02:42<01:45,  3.45it/s] 54%|█████▍    | 422/785 [02:43<01:45,  3.45it/s] 54%|█████▍    | 423/785 [02:43<01:44,  3.45it/s] 54%|█████▍    | 424/785 [02:43<01:45,  3.42it/s] 54%|█████▍    | 425/785 [02:43<01:44,  3.43it/s] 54%|█████▍    | 426/785 [02:44<01:44,  3.44it/s] 54%|█████▍    | 427/785 [02:44<01:43,  3.44it/s] 55%|█████▍    | 428/785 [02:44<01:43,  3.45it/s] 55%|█████▍    | 429/785 [02:45<01:46,  3.35it/s] 55%|█████▍    | 430/785 [02:45<01:45,  3.37it/s] 55%|█████▍    | 431/785 [02:45<01:44,  3.39it/s] 55%|█████▌    | 432/785 [02:45<01:43,  3.41it/s] 55%|█████▌    | 433/785 [02:46<01:42,  3.43it/s] 55%|█████▌    | 434/785 [02:46<01:42,  3.43it/s] 55%|█████▌    | 435/785 [02:46<01:42,  3.43it/s] 56%|█████▌    | 436/785 [02:47<01:41,  3.43it/s] 56%|█████▌    | 437/785 [02:47<01:41,  3.44it/s] 56%|█████▌    | 438/785 [02:47<01:40,  3.44it/s] 56%|█████▌    | 439/785 [02:47<01:40,  3.44it/s] 56%|█████▌    | 440/785 [02:48<01:40,  3.45it/s] 56%|█████▌    | 441/785 [02:48<01:39,  3.45it/s] 56%|█████▋    | 442/785 [02:48<01:39,  3.45it/s] 56%|█████▋    | 443/785 [02:49<01:39,  3.45it/s] 57%|█████▋    | 444/785 [02:49<01:38,  3.45it/s] 57%|█████▋    | 445/785 [02:49<01:38,  3.45it/s] 57%|█████▋    | 446/785 [02:50<01:38,  3.44it/s] 57%|█████▋    | 447/785 [02:50<01:38,  3.44it/s] 57%|█████▋    | 448/785 [02:50<01:37,  3.45it/s] 57%|█████▋    | 449/785 [02:50<01:37,  3.45it/s] 57%|█████▋    | 450/785 [02:51<01:37,  3.45it/s] 57%|█████▋    | 451/785 [02:51<01:36,  3.45it/s] 58%|█████▊    | 452/785 [02:51<01:36,  3.45it/s] 58%|█████▊    | 453/785 [02:52<01:36,  3.45it/s] 58%|█████▊    | 454/785 [02:52<01:35,  3.45it/s] 58%|█████▊    | 455/785 [02:52<01:35,  3.45it/s] 58%|█████▊    | 456/785 [02:52<01:35,  3.45it/s] 58%|█████▊    | 457/785 [02:53<01:35,  3.44it/s] 58%|█████▊    | 458/785 [02:53<01:34,  3.44it/s] 58%|█████▊    | 459/785 [02:53<01:34,  3.45it/s] 59%|█████▊    | 460/785 [02:54<01:34,  3.45it/s] 59%|█████▊    | 461/785 [02:54<01:33,  3.45it/s] 59%|█████▉    | 462/785 [02:54<01:33,  3.45it/s] 59%|█████▉    | 463/785 [02:54<01:33,  3.45it/s] 59%|█████▉    | 464/785 [02:55<01:33,  3.45it/s] 59%|█████▉    | 465/785 [02:55<01:32,  3.45it/s] 59%|█████▉    | 466/785 [02:55<01:32,  3.45it/s] 59%|█████▉    | 467/785 [02:56<01:32,  3.45it/s] 60%|█████▉    | 468/785 [02:56<01:31,  3.45it/s] 60%|█████▉    | 469/785 [02:56<01:31,  3.45it/s] 60%|█████▉    | 470/785 [02:56<01:31,  3.45it/s] 60%|██████    | 471/785 [02:57<01:23,  3.76it/s][INFO|trainer.py:2140] 2023-08-28 14:20:01,544 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:20:01,544 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:20:01,544 >>   Batch size = 8
{'eval_loss': 0.9232273101806641, 'eval_runtime': 9.503, 'eval_samples_per_second': 367.145, 'eval_steps_per_second': 45.985, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.27it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.15it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.45it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.72it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.24it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.03it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.83it/s][A
 10%|▉         | 43/437 [00:00<00:09, 43.58it/s][A
 11%|█         | 48/437 [00:01<00:08, 44.34it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 44.95it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 45.42it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 45.81it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 46.03it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.12it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.21it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.09it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.02it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 46.05it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.06it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.04it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.11it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 46.13it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.04it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.01it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 45.87it/s][A
 30%|███       | 133/437 [00:02<00:06, 45.76it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 45.80it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 45.82it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 45.98it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.10it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.12it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.22it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.24it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.16it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.13it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 43.72it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 44.52it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 44.97it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 45.56it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 45.78it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 45.94it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.10it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.01it/s][A
 51%|█████     | 223/437 [00:04<00:04, 45.80it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 45.86it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 45.96it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.10it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.27it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.35it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.31it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.32it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.25it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.10it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.05it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.04it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.16it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.26it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.35it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.19it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.22it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.12it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 45.94it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 45.84it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 45.87it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.01it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 45.96it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.18it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.24it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.26it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.24it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.14it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.10it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.08it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.10it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.20it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.22it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.30it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.32it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.31it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.15it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.17it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.19it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.16it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.08it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.14it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.08it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [03:06<01:23,  3.76it/s]
100%|██████████| 437/437 [00:09<00:00, 46.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:20:11,066 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-28 14:20:11,090 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:20:15,014 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:20:15,196 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:20:15,294 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:17<32:28,  6.23s/it] 60%|██████    | 473/785 [03:17<23:07,  4.45s/it] 60%|██████    | 474/785 [03:17<16:34,  3.20s/it] 61%|██████    | 475/785 [03:18<12:00,  2.33s/it] 61%|██████    | 476/785 [03:18<08:49,  1.71s/it] 61%|██████    | 477/785 [03:18<06:36,  1.29s/it] 61%|██████    | 478/785 [03:19<05:03,  1.01it/s] 61%|██████    | 479/785 [03:19<03:57,  1.29it/s] 61%|██████    | 480/785 [03:19<03:12,  1.58it/s] 61%|██████▏   | 481/785 [03:19<02:40,  1.89it/s] 61%|██████▏   | 482/785 [03:20<02:18,  2.19it/s] 62%|██████▏   | 483/785 [03:20<02:02,  2.46it/s] 62%|██████▏   | 484/785 [03:20<01:51,  2.69it/s] 62%|██████▏   | 485/785 [03:21<01:43,  2.89it/s] 62%|██████▏   | 486/785 [03:21<01:38,  3.04it/s] 62%|██████▏   | 487/785 [03:21<01:34,  3.16it/s] 62%|██████▏   | 488/785 [03:21<01:31,  3.24it/s] 62%|██████▏   | 489/785 [03:22<01:29,  3.31it/s] 62%|██████▏   | 490/785 [03:22<01:27,  3.35it/s] 63%|██████▎   | 491/785 [03:22<01:26,  3.39it/s] 63%|██████▎   | 492/785 [03:23<01:25,  3.41it/s] 63%|██████▎   | 493/785 [03:23<01:25,  3.43it/s] 63%|██████▎   | 494/785 [03:23<01:24,  3.43it/s] 63%|██████▎   | 495/785 [03:23<01:24,  3.43it/s] 63%|██████▎   | 496/785 [03:24<01:23,  3.44it/s] 63%|██████▎   | 497/785 [03:24<01:23,  3.45it/s] 63%|██████▎   | 498/785 [03:24<01:23,  3.45it/s] 64%|██████▎   | 499/785 [03:25<01:22,  3.46it/s] 64%|██████▎   | 500/785 [03:25<01:22,  3.46it/s]                                                  64%|██████▎   | 500/785 [03:25<01:22,  3.46it/s] 64%|██████▍   | 501/785 [03:25<01:22,  3.46it/s] 64%|██████▍   | 502/785 [03:25<01:21,  3.46it/s] 64%|██████▍   | 503/785 [03:26<01:21,  3.46it/s] 64%|██████▍   | 504/785 [03:26<01:21,  3.46it/s] 64%|██████▍   | 505/785 [03:26<01:20,  3.46it/s] 64%|██████▍   | 506/785 [03:27<01:20,  3.46it/s] 65%|██████▍   | 507/785 [03:27<01:20,  3.46it/s] 65%|██████▍   | 508/785 [03:27<01:20,  3.46it/s] 65%|██████▍   | 509/785 [03:28<01:19,  3.46it/s] 65%|██████▍   | 510/785 [03:28<01:19,  3.46it/s] 65%|██████▌   | 511/785 [03:28<01:19,  3.46it/s] 65%|██████▌   | 512/785 [03:28<01:18,  3.46it/s] 65%|██████▌   | 513/785 [03:29<01:18,  3.45it/s] 65%|██████▌   | 514/785 [03:29<01:18,  3.45it/s] 66%|██████▌   | 515/785 [03:29<01:18,  3.46it/s] 66%|██████▌   | 516/785 [03:30<01:17,  3.46it/s] 66%|██████▌   | 517/785 [03:30<01:17,  3.46it/s] 66%|██████▌   | 518/785 [03:30<01:17,  3.46it/s] 66%|██████▌   | 519/785 [03:30<01:16,  3.46it/s] 66%|██████▌   | 520/785 [03:31<01:16,  3.46it/s] 66%|██████▋   | 521/785 [03:31<01:16,  3.46it/s] 66%|██████▋   | 522/785 [03:31<01:16,  3.45it/s] 67%|██████▋   | 523/785 [03:32<01:15,  3.46it/s] 67%|██████▋   | 524/785 [03:32<01:15,  3.45it/s] 67%|██████▋   | 525/785 [03:32<01:15,  3.45it/s] 67%|██████▋   | 526/785 [03:32<01:15,  3.45it/s] 67%|██████▋   | 527/785 [03:33<01:14,  3.45it/s] 67%|██████▋   | 528/785 [03:33<01:14,  3.45it/s] 67%|██████▋   | 529/785 [03:33<01:14,  3.45it/s] 68%|██████▊   | 530/785 [03:34<01:13,  3.45it/s] 68%|██████▊   | 531/785 [03:34<01:13,  3.45it/s] 68%|██████▊   | 532/785 [03:34<01:13,  3.45it/s] 68%|██████▊   | 533/785 [03:34<01:12,  3.46it/s] 68%|██████▊   | 534/785 [03:35<01:12,  3.46it/s] 68%|██████▊   | 535/785 [03:35<01:12,  3.44it/s] 68%|██████▊   | 536/785 [03:35<01:12,  3.45it/s] 68%|██████▊   | 537/785 [03:36<01:11,  3.45it/s] 69%|██████▊   | 538/785 [03:36<01:11,  3.45it/s] 69%|██████▊   | 539/785 [03:36<01:11,  3.45it/s] 69%|██████▉   | 540/785 [03:36<01:10,  3.45it/s] 69%|██████▉   | 541/785 [03:37<01:10,  3.46it/s] 69%|██████▉   | 542/785 [03:37<01:10,  3.46it/s] 69%|██████▉   | 543/785 [03:37<01:10,  3.45it/s] 69%|██████▉   | 544/785 [03:38<01:09,  3.45it/s] 69%|██████▉   | 545/785 [03:38<01:09,  3.46it/s] 70%|██████▉   | 546/785 [03:38<01:09,  3.45it/s] 70%|██████▉   | 547/785 [03:39<01:09,  3.45it/s] 70%|██████▉   | 548/785 [03:39<01:08,  3.45it/s] 70%|██████▉   | 549/785 [03:39<01:08,  3.45it/s] 70%|███████   | 550/785 [03:39<01:08,  3.45it/s] 70%|███████   | 551/785 [03:40<01:07,  3.45it/s] 70%|███████   | 552/785 [03:40<01:07,  3.45it/s] 70%|███████   | 553/785 [03:40<01:07,  3.45it/s] 71%|███████   | 554/785 [03:41<01:06,  3.45it/s] 71%|███████   | 555/785 [03:41<01:06,  3.45it/s] 71%|███████   | 556/785 [03:41<01:06,  3.45it/s] 71%|███████   | 557/785 [03:41<01:06,  3.44it/s] 71%|███████   | 558/785 [03:42<01:05,  3.45it/s] 71%|███████   | 559/785 [03:42<01:05,  3.45it/s] 71%|███████▏  | 560/785 [03:42<01:05,  3.45it/s] 71%|███████▏  | 561/785 [03:43<01:04,  3.45it/s] 72%|███████▏  | 562/785 [03:43<01:04,  3.45it/s] 72%|███████▏  | 563/785 [03:43<01:04,  3.45it/s] 72%|███████▏  | 564/785 [03:43<01:04,  3.45it/s] 72%|███████▏  | 565/785 [03:44<01:03,  3.45it/s] 72%|███████▏  | 566/785 [03:44<01:03,  3.45it/s] 72%|███████▏  | 567/785 [03:44<01:03,  3.45it/s] 72%|███████▏  | 568/785 [03:45<01:05,  3.32it/s] 72%|███████▏  | 569/785 [03:45<01:04,  3.35it/s] 73%|███████▎  | 570/785 [03:45<01:03,  3.38it/s] 73%|███████▎  | 571/785 [03:46<01:02,  3.40it/s] 73%|███████▎  | 572/785 [03:46<01:02,  3.42it/s] 73%|███████▎  | 573/785 [03:46<01:01,  3.43it/s] 73%|███████▎  | 574/785 [03:46<01:01,  3.44it/s] 73%|███████▎  | 575/785 [03:47<01:01,  3.44it/s] 73%|███████▎  | 576/785 [03:47<01:00,  3.44it/s] 74%|███████▎  | 577/785 [03:47<01:00,  3.45it/s] 74%|███████▎  | 578/785 [03:48<01:00,  3.45it/s] 74%|███████▍  | 579/785 [03:48<00:59,  3.43it/s] 74%|███████▍  | 580/785 [03:48<00:59,  3.44it/s] 74%|███████▍  | 581/785 [03:48<00:59,  3.44it/s] 74%|███████▍  | 582/785 [03:49<00:58,  3.45it/s] 74%|███████▍  | 583/785 [03:49<00:58,  3.45it/s] 74%|███████▍  | 584/785 [03:49<00:58,  3.45it/s] 75%|███████▍  | 585/785 [03:50<00:57,  3.45it/s] 75%|███████▍  | 586/785 [03:50<00:57,  3.45it/s] 75%|███████▍  | 587/785 [03:50<00:57,  3.45it/s] 75%|███████▍  | 588/785 [03:50<00:57,  3.45it/s] 75%|███████▌  | 589/785 [03:51<00:56,  3.45it/s] 75%|███████▌  | 590/785 [03:51<00:56,  3.43it/s] 75%|███████▌  | 591/785 [03:51<00:56,  3.44it/s] 75%|███████▌  | 592/785 [03:52<00:56,  3.44it/s] 76%|███████▌  | 593/785 [03:52<00:55,  3.45it/s] 76%|███████▌  | 594/785 [03:52<00:55,  3.45it/s] 76%|███████▌  | 595/785 [03:52<00:55,  3.45it/s] 76%|███████▌  | 596/785 [03:53<00:54,  3.45it/s] 76%|███████▌  | 597/785 [03:53<00:54,  3.45it/s] 76%|███████▌  | 598/785 [03:53<00:54,  3.45it/s] 76%|███████▋  | 599/785 [03:54<00:53,  3.45it/s] 76%|███████▋  | 600/785 [03:54<00:53,  3.45it/s] 77%|███████▋  | 601/785 [03:54<00:53,  3.44it/s] 77%|███████▋  | 602/785 [03:55<00:53,  3.45it/s] 77%|███████▋  | 603/785 [03:55<00:52,  3.45it/s] 77%|███████▋  | 604/785 [03:55<00:52,  3.45it/s] 77%|███████▋  | 605/785 [03:55<00:52,  3.45it/s] 77%|███████▋  | 606/785 [03:56<00:51,  3.45it/s] 77%|███████▋  | 607/785 [03:56<00:51,  3.45it/s] 77%|███████▋  | 608/785 [03:56<00:51,  3.45it/s] 78%|███████▊  | 609/785 [03:57<00:50,  3.45it/s] 78%|███████▊  | 610/785 [03:57<00:50,  3.45it/s] 78%|███████▊  | 611/785 [03:57<00:50,  3.45it/s] 78%|███████▊  | 612/785 [03:57<00:50,  3.45it/s] 78%|███████▊  | 613/785 [03:58<00:49,  3.45it/s] 78%|███████▊  | 614/785 [03:58<00:49,  3.45it/s] 78%|███████▊  | 615/785 [03:58<00:49,  3.45it/s] 78%|███████▊  | 616/785 [03:59<00:48,  3.45it/s] 79%|███████▊  | 617/785 [03:59<00:48,  3.45it/s] 79%|███████▊  | 618/785 [03:59<00:48,  3.45it/s] 79%|███████▉  | 619/785 [03:59<00:48,  3.44it/s] 79%|███████▉  | 620/785 [04:00<00:47,  3.44it/s] 79%|███████▉  | 621/785 [04:00<00:47,  3.45it/s] 79%|███████▉  | 622/785 [04:00<00:47,  3.45it/s] 79%|███████▉  | 623/785 [04:01<00:46,  3.45it/s] 79%|███████▉  | 624/785 [04:01<00:46,  3.45it/s] 80%|███████▉  | 625/785 [04:01<00:46,  3.45it/s] 80%|███████▉  | 626/785 [04:01<00:46,  3.45it/s] 80%|███████▉  | 627/785 [04:02<00:45,  3.45it/s] 80%|████████  | 628/785 [04:02<00:41,  3.76it/s][INFO|trainer.py:2140] 2023-08-28 14:21:06,814 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:21:06,814 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:21:06,814 >>   Batch size = 8
{'eval_loss': 0.9300096035003662, 'eval_runtime': 9.4982, 'eval_samples_per_second': 367.333, 'eval_steps_per_second': 46.009, 'epoch': 3.0}
{'loss': 0.7474, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.24it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.79it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.10it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.49it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.17it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.94it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.74it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.47it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.40it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.37it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.35it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.42it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.40it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.44it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.42it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.43it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.32it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.29it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.27it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.28it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.31it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.42it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.36it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.44it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.37it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.36it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.34it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.33it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.36it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.22it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 42.69it/s][A
 37%|███▋      | 163/437 [00:03<00:06, 43.77it/s][A
 38%|███▊      | 168/437 [00:03<00:06, 44.59it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 45.10it/s][A
 41%|████      | 178/437 [00:03<00:05, 45.49it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 45.70it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.02it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.19it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.00it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.10it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.13it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.23it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.30it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.29it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.30it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.38it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.38it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.29it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.24it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.26it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.29it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.34it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.42it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.35it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.36it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.33it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.26it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.27it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.24it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.27it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.26it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.22it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.23it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.16it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.14it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.11it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.14it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.21it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.21it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.17it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.22it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.31it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.39it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.28it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.27it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.27it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.34it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.34it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.31it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.25it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.27it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.40it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.46it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.30it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.22it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.15it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [04:11<00:41,  3.76it/s]
100%|██████████| 437/437 [00:09<00:00, 46.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:21:16,295 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 14:21:16,316 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:21:20,382 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:21:20,597 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:21:20,693 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:22<16:15,  6.26s/it] 80%|████████  | 630/785 [04:22<11:32,  4.47s/it] 80%|████████  | 631/785 [04:23<08:14,  3.21s/it] 81%|████████  | 632/785 [04:23<05:57,  2.34s/it] 81%|████████  | 633/785 [04:23<04:21,  1.72s/it] 81%|████████  | 634/785 [04:24<03:15,  1.29s/it] 81%|████████  | 635/785 [04:24<02:28,  1.01it/s] 81%|████████  | 636/785 [04:24<01:56,  1.28it/s] 81%|████████  | 637/785 [04:25<01:33,  1.58it/s] 81%|████████▏ | 638/785 [04:25<01:17,  1.89it/s] 81%|████████▏ | 639/785 [04:25<01:06,  2.19it/s] 82%|████████▏ | 640/785 [04:25<00:58,  2.46it/s] 82%|████████▏ | 641/785 [04:26<00:53,  2.68it/s] 82%|████████▏ | 642/785 [04:26<00:49,  2.88it/s] 82%|████████▏ | 643/785 [04:26<00:46,  3.03it/s] 82%|████████▏ | 644/785 [04:27<00:44,  3.15it/s] 82%|████████▏ | 645/785 [04:27<00:43,  3.24it/s] 82%|████████▏ | 646/785 [04:27<00:42,  3.31it/s] 82%|████████▏ | 647/785 [04:27<00:41,  3.35it/s] 83%|████████▎ | 648/785 [04:28<00:40,  3.39it/s] 83%|████████▎ | 649/785 [04:28<00:39,  3.41it/s] 83%|████████▎ | 650/785 [04:28<00:39,  3.43it/s] 83%|████████▎ | 651/785 [04:29<00:38,  3.44it/s] 83%|████████▎ | 652/785 [04:29<00:38,  3.45it/s] 83%|████████▎ | 653/785 [04:29<00:38,  3.45it/s] 83%|████████▎ | 654/785 [04:29<00:37,  3.45it/s] 83%|████████▎ | 655/785 [04:30<00:37,  3.46it/s] 84%|████████▎ | 656/785 [04:30<00:37,  3.45it/s] 84%|████████▎ | 657/785 [04:30<00:37,  3.45it/s] 84%|████████▍ | 658/785 [04:31<00:36,  3.45it/s] 84%|████████▍ | 659/785 [04:31<00:36,  3.46it/s] 84%|████████▍ | 660/785 [04:31<00:36,  3.46it/s] 84%|████████▍ | 661/785 [04:31<00:35,  3.46it/s] 84%|████████▍ | 662/785 [04:32<00:35,  3.46it/s] 84%|████████▍ | 663/785 [04:32<00:35,  3.46it/s] 85%|████████▍ | 664/785 [04:32<00:34,  3.46it/s] 85%|████████▍ | 665/785 [04:33<00:34,  3.46it/s] 85%|████████▍ | 666/785 [04:33<00:34,  3.46it/s] 85%|████████▍ | 667/785 [04:33<00:34,  3.44it/s] 85%|████████▌ | 668/785 [04:33<00:33,  3.44it/s] 85%|████████▌ | 669/785 [04:34<00:33,  3.45it/s] 85%|████████▌ | 670/785 [04:34<00:33,  3.45it/s] 85%|████████▌ | 671/785 [04:34<00:32,  3.46it/s] 86%|████████▌ | 672/785 [04:35<00:32,  3.46it/s] 86%|████████▌ | 673/785 [04:35<00:32,  3.46it/s] 86%|████████▌ | 674/785 [04:35<00:32,  3.46it/s] 86%|████████▌ | 675/785 [04:35<00:31,  3.46it/s] 86%|████████▌ | 676/785 [04:36<00:31,  3.46it/s] 86%|████████▌ | 677/785 [04:36<00:31,  3.46it/s] 86%|████████▋ | 678/785 [04:36<00:31,  3.45it/s] 86%|████████▋ | 679/785 [04:37<00:30,  3.45it/s] 87%|████████▋ | 680/785 [04:37<00:30,  3.45it/s] 87%|████████▋ | 681/785 [04:37<00:30,  3.45it/s] 87%|████████▋ | 682/785 [04:38<00:29,  3.45it/s] 87%|████████▋ | 683/785 [04:38<00:29,  3.46it/s] 87%|████████▋ | 684/785 [04:38<00:29,  3.46it/s] 87%|████████▋ | 685/785 [04:38<00:28,  3.46it/s] 87%|████████▋ | 686/785 [04:39<00:28,  3.46it/s] 88%|████████▊ | 687/785 [04:39<00:28,  3.46it/s] 88%|████████▊ | 688/785 [04:39<00:28,  3.46it/s] 88%|████████▊ | 689/785 [04:40<00:28,  3.42it/s] 88%|████████▊ | 690/785 [04:40<00:27,  3.43it/s] 88%|████████▊ | 691/785 [04:40<00:27,  3.44it/s] 88%|████████▊ | 692/785 [04:40<00:27,  3.44it/s] 88%|████████▊ | 693/785 [04:41<00:26,  3.45it/s] 88%|████████▊ | 694/785 [04:41<00:26,  3.45it/s] 89%|████████▊ | 695/785 [04:41<00:26,  3.45it/s] 89%|████████▊ | 696/785 [04:42<00:25,  3.45it/s] 89%|████████▉ | 697/785 [04:42<00:25,  3.45it/s] 89%|████████▉ | 698/785 [04:42<00:25,  3.45it/s] 89%|████████▉ | 699/785 [04:42<00:24,  3.45it/s] 89%|████████▉ | 700/785 [04:43<00:24,  3.43it/s] 89%|████████▉ | 701/785 [04:43<00:24,  3.44it/s] 89%|████████▉ | 702/785 [04:43<00:24,  3.44it/s] 90%|████████▉ | 703/785 [04:44<00:23,  3.45it/s] 90%|████████▉ | 704/785 [04:44<00:23,  3.45it/s] 90%|████████▉ | 705/785 [04:44<00:23,  3.45it/s] 90%|████████▉ | 706/785 [04:44<00:22,  3.45it/s] 90%|█████████ | 707/785 [04:45<00:22,  3.45it/s] 90%|█████████ | 708/785 [04:45<00:22,  3.45it/s] 90%|█████████ | 709/785 [04:45<00:22,  3.45it/s] 90%|█████████ | 710/785 [04:46<00:21,  3.45it/s] 91%|█████████ | 711/785 [04:46<00:21,  3.44it/s] 91%|█████████ | 712/785 [04:46<00:21,  3.44it/s] 91%|█████████ | 713/785 [04:47<00:20,  3.45it/s] 91%|█████████ | 714/785 [04:47<00:20,  3.45it/s] 91%|█████████ | 715/785 [04:47<00:20,  3.45it/s] 91%|█████████ | 716/785 [04:47<00:19,  3.45it/s] 91%|█████████▏| 717/785 [04:48<00:19,  3.45it/s] 91%|█████████▏| 718/785 [04:48<00:19,  3.45it/s] 92%|█████████▏| 719/785 [04:48<00:19,  3.45it/s] 92%|█████████▏| 720/785 [04:49<00:18,  3.45it/s] 92%|█████████▏| 721/785 [04:49<00:18,  3.45it/s] 92%|█████████▏| 722/785 [04:49<00:18,  3.44it/s] 92%|█████████▏| 723/785 [04:49<00:18,  3.44it/s] 92%|█████████▏| 724/785 [04:50<00:17,  3.45it/s] 92%|█████████▏| 725/785 [04:50<00:17,  3.45it/s] 92%|█████████▏| 726/785 [04:50<00:17,  3.45it/s] 93%|█████████▎| 727/785 [04:51<00:16,  3.45it/s] 93%|█████████▎| 728/785 [04:51<00:16,  3.45it/s] 93%|█████████▎| 729/785 [04:51<00:16,  3.45it/s] 93%|█████████▎| 730/785 [04:51<00:15,  3.45it/s] 93%|█████████▎| 731/785 [04:52<00:15,  3.45it/s] 93%|█████████▎| 732/785 [04:52<00:15,  3.45it/s] 93%|█████████▎| 733/785 [04:52<00:15,  3.44it/s] 94%|█████████▎| 734/785 [04:53<00:14,  3.44it/s] 94%|█████████▎| 735/785 [04:53<00:14,  3.44it/s] 94%|█████████▍| 736/785 [04:53<00:14,  3.45it/s] 94%|█████████▍| 737/785 [04:53<00:13,  3.44it/s] 94%|█████████▍| 738/785 [04:54<00:13,  3.45it/s] 94%|█████████▍| 739/785 [04:54<00:13,  3.45it/s] 94%|█████████▍| 740/785 [04:54<00:13,  3.45it/s] 94%|█████████▍| 741/785 [04:55<00:12,  3.45it/s] 95%|█████████▍| 742/785 [04:55<00:12,  3.45it/s] 95%|█████████▍| 743/785 [04:55<00:12,  3.45it/s] 95%|█████████▍| 744/785 [04:55<00:11,  3.45it/s] 95%|█████████▍| 745/785 [04:56<00:11,  3.45it/s] 95%|█████████▌| 746/785 [04:56<00:11,  3.45it/s] 95%|█████████▌| 747/785 [04:56<00:11,  3.45it/s] 95%|█████████▌| 748/785 [04:57<00:10,  3.45it/s] 95%|█████████▌| 749/785 [04:57<00:10,  3.45it/s] 96%|█████████▌| 750/785 [04:57<00:10,  3.45it/s] 96%|█████████▌| 751/785 [04:58<00:09,  3.45it/s] 96%|█████████▌| 752/785 [04:58<00:09,  3.45it/s] 96%|█████████▌| 753/785 [04:58<00:09,  3.45it/s] 96%|█████████▌| 754/785 [04:58<00:08,  3.45it/s] 96%|█████████▌| 755/785 [04:59<00:08,  3.45it/s] 96%|█████████▋| 756/785 [04:59<00:08,  3.45it/s] 96%|█████████▋| 757/785 [04:59<00:08,  3.45it/s] 97%|█████████▋| 758/785 [05:00<00:07,  3.45it/s] 97%|█████████▋| 759/785 [05:00<00:07,  3.45it/s] 97%|█████████▋| 760/785 [05:00<00:07,  3.45it/s] 97%|█████████▋| 761/785 [05:00<00:06,  3.45it/s] 97%|█████████▋| 762/785 [05:01<00:06,  3.42it/s] 97%|█████████▋| 763/785 [05:01<00:06,  3.43it/s] 97%|█████████▋| 764/785 [05:01<00:06,  3.44it/s] 97%|█████████▋| 765/785 [05:02<00:05,  3.44it/s] 98%|█████████▊| 766/785 [05:02<00:05,  3.44it/s] 98%|█████████▊| 767/785 [05:02<00:05,  3.45it/s] 98%|█████████▊| 768/785 [05:02<00:04,  3.45it/s] 98%|█████████▊| 769/785 [05:03<00:04,  3.45it/s] 98%|█████████▊| 770/785 [05:03<00:04,  3.45it/s] 98%|█████████▊| 771/785 [05:03<00:04,  3.45it/s] 98%|█████████▊| 772/785 [05:04<00:03,  3.45it/s] 98%|█████████▊| 773/785 [05:04<00:03,  3.44it/s] 99%|█████████▊| 774/785 [05:04<00:03,  3.44it/s] 99%|█████████▊| 775/785 [05:04<00:02,  3.44it/s] 99%|█████████▉| 776/785 [05:05<00:02,  3.44it/s] 99%|█████████▉| 777/785 [05:05<00:02,  3.44it/s] 99%|█████████▉| 778/785 [05:05<00:02,  3.44it/s] 99%|█████████▉| 779/785 [05:06<00:01,  3.45it/s] 99%|█████████▉| 780/785 [05:06<00:01,  3.45it/s] 99%|█████████▉| 781/785 [05:06<00:01,  3.45it/s]100%|█████████▉| 782/785 [05:07<00:00,  3.45it/s]100%|█████████▉| 783/785 [05:07<00:00,  3.45it/s]100%|█████████▉| 784/785 [05:07<00:00,  3.31it/s]100%|██████████| 785/785 [05:07<00:00,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 14:22:12,203 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:22:12,204 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:22:12,204 >>   Batch size = 8
{'eval_loss': 0.9391592144966125, 'eval_runtime': 9.4609, 'eval_samples_per_second': 368.78, 'eval_steps_per_second': 46.19, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.29it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.23it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.48it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.75it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.31it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.00it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.70it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.38it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.36it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.32it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.38it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.39it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.42it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.47it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.54it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.42it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.22it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.21it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.22it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.28it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.37it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.35it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.41it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.46it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.44it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.23it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.14it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.27it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.25it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.34it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.37it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.36it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.38it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.40it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.38it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.27it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.22it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.31it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.25it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.34it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.35it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.34it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.39it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.35it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.30it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.24it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.23it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.28it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.28it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.37it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.34it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.29it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.30it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.04it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.08it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.04it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 45.95it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 45.96it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.07it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.21it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.24it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.32it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.25it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.31it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.23it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.21it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.28it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.20it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.27it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.35it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.36it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.39it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.29it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.28it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.24it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.27it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.31it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.24it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.27it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.35it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.26it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.25it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.17it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.14it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.13it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.10it/s][A                                                 
                                                 [A100%|██████████| 785/785 [05:17<00:00,  3.65it/s]
100%|██████████| 437/437 [00:09<00:00, 46.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:22:21,657 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 14:22:21,675 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:22:26,133 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:22:26,367 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:22:26,465 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:22:32,648 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:22:32,650 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157 (score: 0.9183201789855957).
                                                 100%|██████████| 785/785 [05:33<00:00,  3.65it/s]100%|██████████| 785/785 [05:33<00:00,  2.35it/s]
[INFO|trainer.py:1894] 2023-08-28 14:22:37,974 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 14:22:37,994 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:22:40,567 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:22:40,581 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:22:40,591 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:22:40,803 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:40,803 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:40,803 >>   train_loss               =     0.7348
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:40,803 >>   train_runtime            = 0:05:33.61
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:40,803 >>   train_samples            =      10026
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:40,804 >>   train_samples_per_second =    150.262
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:40,804 >>   train_steps_per_second   =      2.353
{'eval_loss': 0.9387191534042358, 'eval_runtime': 9.4349, 'eval_samples_per_second': 369.798, 'eval_steps_per_second': 46.317, 'epoch': 5.0}
{'train_runtime': 333.6173, 'train_samples_per_second': 150.262, 'train_steps_per_second': 2.353, 'train_loss': 0.7348407137925458, 'epoch': 5.0}
08/28/2023 14:22:40 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:22:40,841 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:22:40,841 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:22:40,841 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.16it/s]  3%|▎         | 12/437 [00:00<00:08, 50.90it/s]  4%|▍         | 18/437 [00:00<00:08, 48.87it/s]  5%|▌         | 23/437 [00:00<00:08, 48.21it/s]  6%|▋         | 28/437 [00:00<00:08, 47.75it/s]  8%|▊         | 33/437 [00:00<00:08, 47.48it/s]  9%|▊         | 38/437 [00:00<00:08, 47.30it/s] 10%|▉         | 43/437 [00:00<00:08, 47.18it/s] 11%|█         | 48/437 [00:00<00:08, 47.01it/s] 12%|█▏        | 53/437 [00:01<00:08, 46.95it/s] 13%|█▎        | 58/437 [00:01<00:08, 46.94it/s] 14%|█▍        | 63/437 [00:01<00:07, 46.91it/s] 16%|█▌        | 68/437 [00:01<00:07, 46.88it/s] 17%|█▋        | 73/437 [00:01<00:07, 46.91it/s] 18%|█▊        | 78/437 [00:01<00:07, 46.86it/s] 19%|█▉        | 83/437 [00:01<00:07, 46.86it/s] 20%|██        | 88/437 [00:01<00:07, 46.85it/s] 21%|██▏       | 93/437 [00:01<00:07, 46.87it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.90it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.90it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.81it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.79it/s] 27%|██▋       | 118/437 [00:02<00:06, 46.76it/s] 28%|██▊       | 123/437 [00:02<00:06, 46.71it/s] 29%|██▉       | 128/437 [00:02<00:06, 46.79it/s] 30%|███       | 133/437 [00:02<00:06, 46.81it/s] 32%|███▏      | 138/437 [00:02<00:06, 46.82it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.85it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.86it/s] 35%|███▌      | 153/437 [00:03<00:06, 46.77it/s] 36%|███▌      | 158/437 [00:03<00:05, 46.70it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.71it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.73it/s] 40%|███▉      | 173/437 [00:03<00:05, 46.77it/s] 41%|████      | 178/437 [00:03<00:05, 46.82it/s] 42%|████▏     | 183/437 [00:03<00:05, 46.84it/s] 43%|████▎     | 188/437 [00:03<00:05, 46.85it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.75it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.81it/s] 46%|████▋     | 203/437 [00:04<00:04, 46.80it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.77it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.73it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.74it/s] 51%|█████     | 223/437 [00:04<00:04, 46.74it/s] 52%|█████▏    | 228/437 [00:04<00:04, 46.77it/s] 53%|█████▎    | 233/437 [00:04<00:04, 46.74it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.69it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.69it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.71it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.76it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.77it/s] 60%|██████    | 263/437 [00:05<00:03, 46.63it/s] 61%|██████▏   | 268/437 [00:05<00:03, 46.66it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.68it/s] 64%|██████▎   | 278/437 [00:05<00:03, 46.68it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.73it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.64it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.70it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.76it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.73it/s] 70%|███████   | 308/437 [00:06<00:02, 46.63it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.68it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.68it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.69it/s] 75%|███████▌  | 328/437 [00:06<00:02, 46.71it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.65it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.66it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.69it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.70it/s] 81%|████████  | 353/437 [00:07<00:01, 46.67it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.58it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.61it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.68it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.70it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.70it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.71it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.71it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.67it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.65it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.60it/s] 93%|█████████▎| 408/437 [00:08<00:00, 46.61it/s] 95%|█████████▍| 413/437 [00:08<00:00, 44.54it/s] 96%|█████████▌| 418/437 [00:08<00:00, 45.18it/s] 97%|█████████▋| 423/437 [00:09<00:00, 45.60it/s] 98%|█████████▊| 428/437 [00:09<00:00, 45.98it/s] 99%|█████████▉| 433/437 [00:09<00:00, 46.21it/s]100%|██████████| 437/437 [00:09<00:00, 46.79it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:22:50,203 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:50,203 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:50,203 >>   eval_loss               =     0.9183
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:50,203 >>   eval_runtime            = 0:00:09.36
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:50,203 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:50,203 >>   eval_samples_per_second =    372.671
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:50,203 >>   eval_steps_per_second   =     46.677
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:50,203 >>   perplexity              =     2.5051
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:56,764 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:56,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:56,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:56,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:56,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:22:57,374 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:22:57,375 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:22:57,943 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:22:58,975 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:22:58,976 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:01,865 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:01,873 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:01,873 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:01,873 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:01,873 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:23:02,502 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:23:02,503 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:23:03,063 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:23:03,217 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:23:03,217 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.45it/s]Extractor Predicting: 20it [00:13,  1.38it/s]Extractor Predicting: 21it [00:14,  1.41it/s]Extractor Predicting: 22it [00:15,  1.42it/s]Extractor Predicting: 23it [00:15,  1.44it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:17,  1.45it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:19,  1.46it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.50it/s]Extractor Predicting: 31it [00:21,  1.49it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:22,  1.53it/s]Extractor Predicting: 34it [00:23,  1.52it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:24,  1.47it/s]Extractor Predicting: 37it [00:25,  1.46it/s]Extractor Predicting: 38it [00:25,  1.45it/s]Extractor Predicting: 39it [00:26,  1.44it/s]Extractor Predicting: 40it [00:27,  1.45it/s]Extractor Predicting: 41it [00:27,  1.44it/s]Extractor Predicting: 42it [00:28,  1.45it/s]Extractor Predicting: 43it [00:29,  1.45it/s]Extractor Predicting: 44it [00:30,  1.43it/s]Extractor Predicting: 45it [00:30,  1.44it/s]Extractor Predicting: 46it [00:31,  1.46it/s]Extractor Predicting: 47it [00:32,  1.45it/s]Extractor Predicting: 48it [00:32,  1.44it/s]Extractor Predicting: 49it [00:33,  1.45it/s]Extractor Predicting: 50it [00:34,  1.43it/s]Extractor Predicting: 51it [00:34,  1.43it/s]Extractor Predicting: 52it [00:35,  1.46it/s]Extractor Predicting: 53it [00:36,  1.45it/s]Extractor Predicting: 54it [00:36,  1.44it/s]Extractor Predicting: 55it [00:37,  1.42it/s]Extractor Predicting: 56it [00:38,  1.43it/s]Extractor Predicting: 57it [00:39,  1.43it/s]Extractor Predicting: 58it [00:39,  1.43it/s]Extractor Predicting: 59it [00:40,  1.41it/s]Extractor Predicting: 60it [00:41,  1.41it/s]Extractor Predicting: 61it [00:41,  1.42it/s]Extractor Predicting: 62it [00:42,  1.46it/s]Extractor Predicting: 63it [00:43,  1.47it/s]Extractor Predicting: 64it [00:43,  1.46it/s]Extractor Predicting: 65it [00:44,  1.48it/s]Extractor Predicting: 66it [00:45,  1.47it/s]Extractor Predicting: 67it [00:45,  1.50it/s]Extractor Predicting: 68it [00:46,  1.50it/s]Extractor Predicting: 69it [00:47,  1.52it/s]Extractor Predicting: 70it [00:47,  1.54it/s]Extractor Predicting: 71it [00:48,  1.53it/s]Extractor Predicting: 72it [00:49,  1.52it/s]Extractor Predicting: 73it [00:49,  1.52it/s]Extractor Predicting: 74it [00:50,  1.53it/s]Extractor Predicting: 75it [00:51,  1.48it/s]Extractor Predicting: 76it [00:51,  1.47it/s]Extractor Predicting: 77it [00:52,  1.49it/s]Extractor Predicting: 78it [00:53,  1.52it/s]Extractor Predicting: 79it [00:53,  1.52it/s]Extractor Predicting: 80it [00:54,  1.52it/s]Extractor Predicting: 81it [00:55,  1.51it/s]Extractor Predicting: 82it [00:55,  1.52it/s]Extractor Predicting: 83it [00:56,  1.52it/s]Extractor Predicting: 84it [00:57,  1.49it/s]Extractor Predicting: 85it [00:57,  1.51it/s]Extractor Predicting: 86it [00:58,  1.53it/s]Extractor Predicting: 87it [00:59,  1.54it/s]Extractor Predicting: 88it [00:59,  1.52it/s]Extractor Predicting: 89it [01:00,  1.51it/s]Extractor Predicting: 90it [01:01,  1.51it/s]Extractor Predicting: 91it [01:01,  1.51it/s]Extractor Predicting: 92it [01:02,  1.46it/s]Extractor Predicting: 93it [01:03,  1.48it/s]Extractor Predicting: 94it [01:03,  1.49it/s]Extractor Predicting: 95it [01:04,  1.46it/s]Extractor Predicting: 96it [01:05,  1.46it/s]Extractor Predicting: 97it [01:05,  1.47it/s]Extractor Predicting: 98it [01:06,  1.50it/s]Extractor Predicting: 99it [01:07,  1.48it/s]Extractor Predicting: 100it [01:08,  1.34it/s]Extractor Predicting: 101it [01:08,  1.39it/s]Extractor Predicting: 102it [01:09,  1.40it/s]Extractor Predicting: 103it [01:10,  1.40it/s]Extractor Predicting: 104it [01:10,  1.43it/s]Extractor Predicting: 105it [01:11,  1.43it/s]Extractor Predicting: 106it [01:12,  1.44it/s]Extractor Predicting: 107it [01:12,  1.46it/s]Extractor Predicting: 108it [01:13,  1.47it/s]Extractor Predicting: 109it [01:14,  1.43it/s]Extractor Predicting: 110it [01:14,  1.43it/s]Extractor Predicting: 111it [01:15,  1.43it/s]Extractor Predicting: 112it [01:16,  1.44it/s]Extractor Predicting: 113it [01:17,  1.45it/s]Extractor Predicting: 114it [01:17,  1.46it/s]Extractor Predicting: 115it [01:18,  1.45it/s]Extractor Predicting: 116it [01:19,  1.47it/s]Extractor Predicting: 117it [01:19,  1.47it/s]Extractor Predicting: 118it [01:20,  1.48it/s]Extractor Predicting: 119it [01:21,  1.51it/s]Extractor Predicting: 120it [01:21,  1.54it/s]Extractor Predicting: 121it [01:22,  1.54it/s]Extractor Predicting: 122it [01:22,  1.54it/s]Extractor Predicting: 123it [01:23,  1.51it/s]Extractor Predicting: 124it [01:24,  1.51it/s]Extractor Predicting: 125it [01:24,  1.52it/s]Extractor Predicting: 126it [01:25,  1.51it/s]Extractor Predicting: 127it [01:26,  1.51it/s]Extractor Predicting: 128it [01:26,  1.48it/s]Extractor Predicting: 129it [01:27,  1.52it/s]Extractor Predicting: 130it [01:28,  1.48it/s]Extractor Predicting: 131it [01:29,  1.48it/s]Extractor Predicting: 132it [01:29,  1.50it/s]Extractor Predicting: 133it [01:30,  1.48it/s]Extractor Predicting: 134it [01:31,  1.46it/s]Extractor Predicting: 135it [01:31,  1.49it/s]Extractor Predicting: 136it [01:32,  1.50it/s]Extractor Predicting: 137it [01:33,  1.48it/s]Extractor Predicting: 138it [01:33,  1.47it/s]Extractor Predicting: 139it [01:34,  1.49it/s]Extractor Predicting: 140it [01:35,  1.49it/s]Extractor Predicting: 141it [01:35,  1.48it/s]Extractor Predicting: 142it [01:36,  1.48it/s]Extractor Predicting: 143it [01:37,  1.49it/s]Extractor Predicting: 144it [01:37,  1.49it/s]Extractor Predicting: 145it [01:37,  1.94it/s]Extractor Predicting: 145it [01:37,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:50,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:50,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:50,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:50,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:50,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:24:50,787 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:24:50,788 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:24:51,340 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:24:52,376 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:24:52,376 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:55,231 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:55,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:55,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:55,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:55,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:24:55,870 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:24:55,872 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:24:56,426 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:24:56,585 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:24:56,585 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5463414634146342,
  "recall": 0.16050444253367727,
  "score": 0.24811696942844483,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:20,  1.43it/s]Extractor Predicting: 31it [00:21,  1.43it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:22,  1.44it/s]Extractor Predicting: 34it [00:23,  1.44it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.48it/s]Extractor Predicting: 37it [00:25,  1.53it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:26,  1.52it/s]Extractor Predicting: 40it [00:27,  1.52it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:28,  1.51it/s]Extractor Predicting: 43it [00:29,  1.50it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:30,  1.48it/s]Extractor Predicting: 46it [00:31,  1.49it/s]Extractor Predicting: 47it [00:31,  1.49it/s]Extractor Predicting: 48it [00:32,  1.50it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:33,  1.47it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.50it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:37,  1.46it/s]Extractor Predicting: 56it [00:37,  1.46it/s]Extractor Predicting: 57it [00:38,  1.49it/s]Extractor Predicting: 58it [00:39,  1.52it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.59it/s]Extractor Predicting: 61it [00:40,  1.60it/s]Extractor Predicting: 62it [00:41,  1.62it/s]Extractor Predicting: 63it [00:42,  1.63it/s]Extractor Predicting: 64it [00:42,  1.65it/s]Extractor Predicting: 65it [00:43,  1.66it/s]Extractor Predicting: 66it [00:43,  1.65it/s]Extractor Predicting: 67it [00:44,  1.67it/s]Extractor Predicting: 68it [00:45,  1.70it/s]Extractor Predicting: 69it [00:45,  1.73it/s]Extractor Predicting: 70it [00:46,  1.71it/s]Extractor Predicting: 71it [00:46,  1.71it/s]Extractor Predicting: 72it [00:47,  1.71it/s]Extractor Predicting: 73it [00:47,  1.71it/s]Extractor Predicting: 74it [00:48,  1.68it/s]Extractor Predicting: 75it [00:49,  1.67it/s]Extractor Predicting: 76it [00:49,  1.70it/s]Extractor Predicting: 77it [00:50,  1.72it/s]Extractor Predicting: 78it [00:50,  1.71it/s]Extractor Predicting: 79it [00:51,  1.72it/s]Extractor Predicting: 80it [00:52,  1.73it/s]Extractor Predicting: 81it [00:52,  1.71it/s]Extractor Predicting: 82it [00:53,  1.69it/s]Extractor Predicting: 83it [00:53,  1.67it/s]Extractor Predicting: 84it [00:54,  1.66it/s]Extractor Predicting: 85it [00:55,  1.50it/s]Extractor Predicting: 86it [00:55,  1.51it/s]Extractor Predicting: 87it [00:56,  1.50it/s]Extractor Predicting: 88it [00:57,  1.49it/s]Extractor Predicting: 89it [00:58,  1.47it/s]Extractor Predicting: 90it [00:58,  1.43it/s]Extractor Predicting: 91it [00:59,  1.45it/s]Extractor Predicting: 92it [01:00,  1.43it/s]Extractor Predicting: 93it [01:00,  1.46it/s]Extractor Predicting: 94it [01:01,  1.39it/s]Extractor Predicting: 95it [01:02,  1.41it/s]Extractor Predicting: 96it [01:02,  1.42it/s]Extractor Predicting: 97it [01:03,  1.42it/s]Extractor Predicting: 98it [01:04,  1.43it/s]Extractor Predicting: 99it [01:05,  1.44it/s]Extractor Predicting: 100it [01:05,  1.44it/s]Extractor Predicting: 101it [01:06,  1.44it/s]Extractor Predicting: 102it [01:07,  1.43it/s]Extractor Predicting: 103it [01:07,  1.46it/s]Extractor Predicting: 104it [01:08,  1.43it/s]Extractor Predicting: 105it [01:09,  1.43it/s]Extractor Predicting: 106it [01:09,  1.41it/s]Extractor Predicting: 107it [01:10,  1.40it/s]Extractor Predicting: 108it [01:11,  1.40it/s]Extractor Predicting: 109it [01:12,  1.43it/s]Extractor Predicting: 110it [01:12,  1.43it/s]Extractor Predicting: 111it [01:13,  1.41it/s]Extractor Predicting: 112it [01:14,  1.38it/s]Extractor Predicting: 113it [01:14,  1.40it/s]Extractor Predicting: 114it [01:15,  1.43it/s]Extractor Predicting: 115it [01:16,  1.42it/s]Extractor Predicting: 116it [01:17,  1.44it/s]Extractor Predicting: 117it [01:17,  1.46it/s]Extractor Predicting: 118it [01:18,  1.42it/s]Extractor Predicting: 119it [01:19,  1.39it/s]Extractor Predicting: 120it [01:19,  1.40it/s]Extractor Predicting: 121it [01:20,  1.40it/s]Extractor Predicting: 122it [01:21,  1.42it/s]Extractor Predicting: 123it [01:21,  1.43it/s]Extractor Predicting: 124it [01:22,  1.42it/s]Extractor Predicting: 125it [01:23,  1.41it/s]Extractor Predicting: 126it [01:24,  1.38it/s]Extractor Predicting: 127it [01:24,  1.38it/s]Extractor Predicting: 128it [01:25,  1.36it/s]Extractor Predicting: 129it [01:26,  1.38it/s]Extractor Predicting: 130it [01:27,  1.37it/s]Extractor Predicting: 131it [01:27,  1.40it/s]Extractor Predicting: 132it [01:28,  1.41it/s]Extractor Predicting: 133it [01:29,  1.41it/s]Extractor Predicting: 134it [01:29,  1.43it/s]Extractor Predicting: 135it [01:30,  1.42it/s]Extractor Predicting: 136it [01:31,  1.38it/s]Extractor Predicting: 137it [01:32,  1.39it/s]Extractor Predicting: 138it [01:32,  1.38it/s]Extractor Predicting: 139it [01:33,  1.37it/s]Extractor Predicting: 140it [01:34,  1.39it/s]Extractor Predicting: 141it [01:34,  1.39it/s]Extractor Predicting: 142it [01:35,  1.40it/s]Extractor Predicting: 143it [01:36,  1.36it/s]Extractor Predicting: 144it [01:37,  1.36it/s]Extractor Predicting: 145it [01:37,  1.37it/s]Extractor Predicting: 146it [01:38,  1.39it/s]Extractor Predicting: 147it [01:39,  1.42it/s]Extractor Predicting: 148it [01:39,  1.44it/s]Extractor Predicting: 149it [01:40,  1.40it/s]Extractor Predicting: 150it [01:41,  1.41it/s]Extractor Predicting: 151it [01:42,  1.44it/s]Extractor Predicting: 152it [01:42,  1.46it/s]Extractor Predicting: 153it [01:43,  1.46it/s]Extractor Predicting: 154it [01:44,  1.49it/s]Extractor Predicting: 155it [01:44,  1.50it/s]Extractor Predicting: 156it [01:45,  1.51it/s]Extractor Predicting: 157it [01:46,  1.50it/s]Extractor Predicting: 158it [01:46,  1.52it/s]Extractor Predicting: 159it [01:47,  1.57it/s]Extractor Predicting: 160it [01:47,  1.52it/s]Extractor Predicting: 161it [01:48,  1.50it/s]Extractor Predicting: 162it [01:49,  1.47it/s]Extractor Predicting: 163it [01:50,  1.48it/s]Extractor Predicting: 164it [01:50,  1.46it/s]Extractor Predicting: 165it [01:51,  1.46it/s]Extractor Predicting: 166it [01:52,  1.46it/s]Extractor Predicting: 167it [01:52,  1.46it/s]Extractor Predicting: 168it [01:53,  1.45it/s]Extractor Predicting: 169it [01:54,  1.44it/s]Extractor Predicting: 170it [01:54,  1.47it/s]Extractor Predicting: 171it [01:55,  1.46it/s]Extractor Predicting: 172it [01:56,  1.44it/s]Extractor Predicting: 173it [01:56,  1.45it/s]Extractor Predicting: 174it [01:57,  1.46it/s]Extractor Predicting: 175it [01:58,  1.45it/s]Extractor Predicting: 176it [01:58,  1.45it/s]Extractor Predicting: 177it [01:59,  1.49it/s]Extractor Predicting: 178it [02:00,  1.48it/s]Extractor Predicting: 179it [02:00,  1.50it/s]Extractor Predicting: 180it [02:01,  1.52it/s]Extractor Predicting: 181it [02:02,  1.50it/s]Extractor Predicting: 182it [02:02,  1.54it/s]Extractor Predicting: 183it [02:03,  1.53it/s]Extractor Predicting: 184it [02:04,  1.52it/s]Extractor Predicting: 185it [02:04,  1.54it/s]Extractor Predicting: 186it [02:05,  1.54it/s]Extractor Predicting: 187it [02:06,  1.50it/s]Extractor Predicting: 188it [02:06,  1.54it/s]Extractor Predicting: 189it [02:07,  1.55it/s]Extractor Predicting: 190it [02:08,  1.59it/s]Extractor Predicting: 191it [02:08,  1.53it/s]Extractor Predicting: 192it [02:09,  1.55it/s]Extractor Predicting: 193it [02:09,  1.56it/s]Extractor Predicting: 194it [02:10,  1.40it/s]Extractor Predicting: 195it [02:11,  1.43it/s]Extractor Predicting: 196it [02:12,  1.46it/s]Extractor Predicting: 197it [02:12,  1.50it/s]Extractor Predicting: 198it [02:13,  1.50it/s]Extractor Predicting: 199it [02:14,  1.48it/s]Extractor Predicting: 200it [02:14,  1.50it/s]Extractor Predicting: 201it [02:15,  1.50it/s]Extractor Predicting: 202it [02:16,  1.53it/s]Extractor Predicting: 203it [02:16,  1.57it/s]Extractor Predicting: 204it [02:17,  1.58it/s]Extractor Predicting: 205it [02:17,  1.58it/s]Extractor Predicting: 206it [02:18,  1.64it/s]Extractor Predicting: 207it [02:19,  1.61it/s]Extractor Predicting: 208it [02:19,  1.65it/s]Extractor Predicting: 209it [02:20,  1.62it/s]Extractor Predicting: 210it [02:21,  1.62it/s]Extractor Predicting: 211it [02:21,  1.61it/s]Extractor Predicting: 212it [02:22,  1.64it/s]Extractor Predicting: 213it [02:22,  1.66it/s]Extractor Predicting: 214it [02:23,  1.67it/s]Extractor Predicting: 215it [02:23,  1.70it/s]Extractor Predicting: 216it [02:24,  1.69it/s]Extractor Predicting: 217it [02:25,  1.64it/s]Extractor Predicting: 218it [02:25,  1.62it/s]Extractor Predicting: 219it [02:26,  1.64it/s]Extractor Predicting: 220it [02:27,  1.64it/s]Extractor Predicting: 221it [02:27,  1.65it/s]Extractor Predicting: 222it [02:28,  1.70it/s]Extractor Predicting: 223it [02:28,  1.64it/s]Extractor Predicting: 224it [02:29,  1.67it/s]Extractor Predicting: 225it [02:30,  1.65it/s]Extractor Predicting: 226it [02:30,  1.66it/s]Extractor Predicting: 227it [02:31,  1.68it/s]Extractor Predicting: 228it [02:31,  1.67it/s]Extractor Predicting: 229it [02:32,  1.58it/s]Extractor Predicting: 230it [02:33,  1.53it/s]Extractor Predicting: 231it [02:33,  1.46it/s]Extractor Predicting: 232it [02:34,  1.43it/s]Extractor Predicting: 233it [02:35,  1.43it/s]Extractor Predicting: 234it [02:36,  1.42it/s]Extractor Predicting: 235it [02:36,  1.42it/s]Extractor Predicting: 236it [02:37,  1.42it/s]Extractor Predicting: 237it [02:38,  1.39it/s]Extractor Predicting: 238it [02:39,  1.37it/s]Extractor Predicting: 239it [02:39,  1.38it/s]Extractor Predicting: 240it [02:40,  1.38it/s]Extractor Predicting: 241it [02:41,  1.40it/s]Extractor Predicting: 242it [02:41,  1.42it/s]Extractor Predicting: 243it [02:42,  1.37it/s]Extractor Predicting: 244it [02:43,  1.40it/s]Extractor Predicting: 245it [02:44,  1.40it/s]Extractor Predicting: 246it [02:44,  1.41it/s]Extractor Predicting: 247it [02:45,  1.37it/s]Extractor Predicting: 248it [02:46,  1.37it/s]Extractor Predicting: 249it [02:47,  1.34it/s]Extractor Predicting: 250it [02:47,  1.35it/s]Extractor Predicting: 251it [02:48,  1.36it/s]Extractor Predicting: 252it [02:49,  1.38it/s]Extractor Predicting: 253it [02:49,  1.39it/s]Extractor Predicting: 254it [02:50,  1.38it/s]Extractor Predicting: 255it [02:51,  1.38it/s]Extractor Predicting: 256it [02:52,  1.40it/s]Extractor Predicting: 257it [02:52,  1.45it/s]Extractor Predicting: 258it [02:53,  1.46it/s]Extractor Predicting: 259it [02:54,  1.47it/s]Extractor Predicting: 260it [02:54,  1.49it/s]Extractor Predicting: 261it [02:55,  1.49it/s]Extractor Predicting: 262it [02:56,  1.49it/s]Extractor Predicting: 263it [02:56,  1.50it/s]Extractor Predicting: 264it [02:57,  1.50it/s]Extractor Predicting: 265it [02:58,  1.48it/s]Extractor Predicting: 266it [02:58,  1.50it/s]Extractor Predicting: 267it [02:59,  1.53it/s]Extractor Predicting: 268it [03:00,  1.50it/s]Extractor Predicting: 269it [03:00,  1.51it/s]Extractor Predicting: 270it [03:01,  1.49it/s]Extractor Predicting: 271it [03:02,  1.48it/s]Extractor Predicting: 272it [03:02,  1.47it/s]Extractor Predicting: 273it [03:03,  1.47it/s]Extractor Predicting: 274it [03:04,  1.46it/s]Extractor Predicting: 275it [03:04,  1.49it/s]Extractor Predicting: 276it [03:05,  1.49it/s]Extractor Predicting: 277it [03:06,  1.47it/s]Extractor Predicting: 278it [03:06,  1.44it/s]Extractor Predicting: 279it [03:07,  1.46it/s]Extractor Predicting: 280it [03:08,  1.45it/s]Extractor Predicting: 281it [03:08,  1.48it/s]Extractor Predicting: 282it [03:09,  1.47it/s]Extractor Predicting: 283it [03:10,  1.47it/s]Extractor Predicting: 284it [03:10,  1.48it/s]Extractor Predicting: 285it [03:11,  1.46it/s]Extractor Predicting: 286it [03:12,  1.45it/s]Extractor Predicting: 287it [03:12,  1.48it/s]Extractor Predicting: 288it [03:13,  1.47it/s]Extractor Predicting: 289it [03:14,  1.46it/s]Extractor Predicting: 290it [03:15,  1.31it/s]Extractor Predicting: 291it [03:15,  1.35it/s]Extractor Predicting: 292it [03:16,  1.38it/s]Extractor Predicting: 293it [03:17,  1.40it/s]Extractor Predicting: 294it [03:18,  1.42it/s]Extractor Predicting: 295it [03:18,  1.44it/s]Extractor Predicting: 296it [03:19,  1.44it/s]Extractor Predicting: 297it [03:20,  1.46it/s]Extractor Predicting: 298it [03:20,  1.44it/s]Extractor Predicting: 299it [03:21,  1.48it/s]Extractor Predicting: 300it [03:22,  1.47it/s]Extractor Predicting: 301it [03:22,  1.48it/s]Extractor Predicting: 302it [03:23,  1.47it/s]Extractor Predicting: 303it [03:24,  1.51it/s]Extractor Predicting: 304it [03:24,  1.50it/s]Extractor Predicting: 305it [03:25,  1.54it/s]Extractor Predicting: 306it [03:26,  1.52it/s]Extractor Predicting: 307it [03:26,  1.50it/s]Extractor Predicting: 308it [03:27,  1.49it/s]Extractor Predicting: 309it [03:28,  1.51it/s]Extractor Predicting: 310it [03:28,  1.46it/s]Extractor Predicting: 311it [03:29,  1.46it/s]Extractor Predicting: 312it [03:30,  1.46it/s]Extractor Predicting: 313it [03:30,  1.48it/s]Extractor Predicting: 314it [03:31,  1.48it/s]Extractor Predicting: 315it [03:32,  1.51it/s]Extractor Predicting: 316it [03:32,  1.49it/s]Extractor Predicting: 317it [03:33,  1.48it/s]Extractor Predicting: 318it [03:34,  1.53it/s]Extractor Predicting: 319it [03:34,  1.52it/s]Extractor Predicting: 320it [03:35,  1.55it/s]Extractor Predicting: 321it [03:36,  1.53it/s]Extractor Predicting: 322it [03:36,  1.51it/s]Extractor Predicting: 323it [03:37,  1.51it/s]Extractor Predicting: 324it [03:38,  1.49it/s]Extractor Predicting: 325it [03:38,  1.48it/s]Extractor Predicting: 326it [03:39,  1.48it/s]Extractor Predicting: 327it [03:40,  1.46it/s]Extractor Predicting: 328it [03:40,  1.46it/s]Extractor Predicting: 329it [03:41,  1.42it/s]Extractor Predicting: 330it [03:42,  1.43it/s]Extractor Predicting: 331it [03:42,  1.44it/s]Extractor Predicting: 332it [03:43,  1.48it/s]Extractor Predicting: 333it [03:44,  1.48it/s]Extractor Predicting: 334it [03:44,  1.48it/s]Extractor Predicting: 335it [03:45,  1.48it/s]Extractor Predicting: 336it [03:46,  1.47it/s]Extractor Predicting: 337it [03:46,  1.47it/s]Extractor Predicting: 338it [03:47,  1.47it/s]Extractor Predicting: 339it [03:48,  1.46it/s]Extractor Predicting: 340it [03:48,  1.50it/s]Extractor Predicting: 341it [03:49,  1.48it/s]Extractor Predicting: 342it [03:50,  1.51it/s]Extractor Predicting: 343it [03:51,  1.46it/s]Extractor Predicting: 344it [03:51,  1.47it/s]Extractor Predicting: 345it [03:52,  1.45it/s]Extractor Predicting: 346it [03:53,  1.47it/s]Extractor Predicting: 347it [03:53,  1.48it/s]Extractor Predicting: 348it [03:54,  1.50it/s]Extractor Predicting: 349it [03:55,  1.43it/s]Extractor Predicting: 350it [03:55,  1.42it/s]Extractor Predicting: 351it [03:56,  1.41it/s]Extractor Predicting: 352it [03:57,  1.43it/s]Extractor Predicting: 353it [03:57,  1.42it/s]Extractor Predicting: 354it [03:58,  1.44it/s]Extractor Predicting: 355it [03:59,  1.43it/s]Extractor Predicting: 356it [04:00,  1.46it/s]Extractor Predicting: 357it [04:00,  1.44it/s]Extractor Predicting: 358it [04:01,  1.43it/s]Extractor Predicting: 359it [04:02,  1.42it/s]Extractor Predicting: 360it [04:02,  1.40it/s]Extractor Predicting: 361it [04:03,  1.44it/s]Extractor Predicting: 362it [04:04,  1.46it/s]Extractor Predicting: 363it [04:04,  1.45it/s]Extractor Predicting: 364it [04:05,  1.47it/s]Extractor Predicting: 365it [04:06,  1.48it/s]Extractor Predicting: 366it [04:06,  1.45it/s]Extractor Predicting: 367it [04:07,  1.45it/s]Extractor Predicting: 368it [04:08,  1.44it/s]Extractor Predicting: 369it [04:09,  1.45it/s]Extractor Predicting: 370it [04:09,  1.44it/s]Extractor Predicting: 371it [04:10,  1.46it/s]Extractor Predicting: 372it [04:11,  1.47it/s]Extractor Predicting: 373it [04:11,  1.48it/s]Extractor Predicting: 374it [04:12,  1.48it/s]Extractor Predicting: 375it [04:13,  1.47it/s]Extractor Predicting: 376it [04:13,  1.48it/s]Extractor Predicting: 377it [04:14,  1.49it/s]Extractor Predicting: 378it [04:15,  1.50it/s]Extractor Predicting: 379it [04:15,  1.50it/s]Extractor Predicting: 380it [04:16,  1.50it/s]Extractor Predicting: 381it [04:17,  1.50it/s]Extractor Predicting: 382it [04:17,  1.47it/s]Extractor Predicting: 383it [04:18,  1.50it/s]Extractor Predicting: 384it [04:19,  1.51it/s]Extractor Predicting: 385it [04:19,  1.51it/s]Extractor Predicting: 386it [04:20,  1.53it/s]Extractor Predicting: 387it [04:20,  1.54it/s]Extractor Predicting: 388it [04:21,  1.51it/s]Extractor Predicting: 389it [04:22,  1.51it/s]Extractor Predicting: 390it [04:23,  1.50it/s]Extractor Predicting: 391it [04:23,  1.50it/s]Extractor Predicting: 392it [04:24,  1.51it/s]Extractor Predicting: 393it [04:25,  1.52it/s]Extractor Predicting: 394it [04:25,  1.53it/s]Extractor Predicting: 395it [04:26,  1.48it/s]Extractor Predicting: 396it [04:27,  1.50it/s]Extractor Predicting: 397it [04:27,  1.52it/s]Extractor Predicting: 398it [04:28,  1.49it/s]Extractor Predicting: 399it [04:29,  1.49it/s]Extractor Predicting: 400it [04:29,  1.49it/s]Extractor Predicting: 401it [04:30,  1.50it/s]Extractor Predicting: 402it [04:30,  1.54it/s]Extractor Predicting: 403it [04:31,  1.52it/s]Extractor Predicting: 404it [04:32,  1.53it/s]Extractor Predicting: 405it [04:32,  1.53it/s]Extractor Predicting: 406it [04:33,  1.53it/s]Extractor Predicting: 407it [04:34,  1.52it/s]Extractor Predicting: 408it [04:34,  1.55it/s]Extractor Predicting: 409it [04:35,  1.56it/s]Extractor Predicting: 410it [04:36,  1.55it/s]Extractor Predicting: 411it [04:36,  1.51it/s]Extractor Predicting: 412it [04:37,  1.53it/s]Extractor Predicting: 413it [04:38,  1.53it/s]Extractor Predicting: 414it [04:38,  1.54it/s]Extractor Predicting: 415it [04:39,  1.56it/s]Extractor Predicting: 416it [04:40,  1.56it/s]Extractor Predicting: 417it [04:40,  1.53it/s]Extractor Predicting: 418it [04:41,  1.55it/s]Extractor Predicting: 419it [04:42,  1.53it/s]Extractor Predicting: 420it [04:42,  1.53it/s]Extractor Predicting: 421it [04:43,  1.52it/s]Extractor Predicting: 422it [04:44,  1.51it/s]Extractor Predicting: 423it [04:44,  1.51it/s]Extractor Predicting: 424it [04:45,  1.52it/s]Extractor Predicting: 425it [04:45,  1.72it/s]Extractor Predicting: 425it [04:45,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:51,627 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:51,631 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:51,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:51,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:51,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:29:52,222 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:29:52,223 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:29:52,902 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:29:53,968 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:29:53,968 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:56,810 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:56,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:56,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:56,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:56,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:29:57,433 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:29:57,434 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:29:58,009 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:29:58,167 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:29:58,167 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.38625890238793464,
  "recall": 0.0905163950520322,
  "score": 0.1466634852461624,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 4it [00:02,  1.34it/s]Extractor Predicting: 5it [00:03,  1.34it/s]Extractor Predicting: 6it [00:04,  1.38it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 7it [00:04,  1.47it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:30:03,386 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:30:03,387 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:30:03,390 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:30:03,391 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:30:03,393 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:30:06,395 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:30:06,403 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:30:06,420 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:30:06,420 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:30:06,425 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:06,429 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:06,429 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:06,429 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:06,429 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:06,429 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:06,429 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7777777777777778,
  "recall": 0.022292993630573247,
  "score": 0.04334365325077399,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:30:06,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:07,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:08,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:09,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:09,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:10,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:11,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:12,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:12,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:13,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:14,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:15,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:16,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:16,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:17,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:18,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:19,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:19,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:20,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:21,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:22,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:23,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:17<05:35, 17.63s/it][WARNING|generation_utils.py:914] 2023-08-28 14:30:24,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:25,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:26,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:26,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:27,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:28,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:29,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:29,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:30,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:31,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:32,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:32,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:33,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:34,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:35,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:36,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:37,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:37,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:38,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:39,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:40,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:41,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:42,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:42,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:37<05:36, 18.72s/it][WARNING|generation_utils.py:914] 2023-08-28 14:30:43,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:44,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:45,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:46,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:46,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:47,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:48,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:49,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:49,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:50,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:51,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:52,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:53,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:53,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:54,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:55,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:56,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:56,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:57,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:58,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:58,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:59,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:00,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:54<05:07, 18.12s/it][WARNING|generation_utils.py:914] 2023-08-28 14:31:01,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:01,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:02,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:03,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:03,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:04,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:05,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:05,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:06,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:07,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:08,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:08,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:09,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:10,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:10,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:11,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:12,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:12,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:13,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:14,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:14,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:15,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:09<04:29, 16.87s/it][WARNING|generation_utils.py:914] 2023-08-28 14:31:16,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:16,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:17,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:18,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:19,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:19,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:20,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:21,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:21,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:22,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:23,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:24,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:24,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:25,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:26,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:27,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:28,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:29,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:29,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:30,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:31,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:32,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:32,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:33,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:27<04:19, 17.33s/it][WARNING|generation_utils.py:914] 2023-08-28 14:31:34,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:35,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:35,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:36,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:37,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:38,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:38,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:39,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:40,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:40,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:41,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:42,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:43,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:43,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:44,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:45,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:46,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:46,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:47,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:48,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:49,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:43<03:56, 16.87s/it][WARNING|generation_utils.py:914] 2023-08-28 14:31:50,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:50,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:51,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:52,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:53,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:54,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:54,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:55,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:56,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:57,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:57,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:58,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:59,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:00,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:00,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:01,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:02,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:03,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:04,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:04,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:05,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:06,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:06,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:07,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:01<03:44, 17.27s/it][WARNING|generation_utils.py:914] 2023-08-28 14:32:08,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:09,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:09,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:10,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:11,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:11,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:12,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:13,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:13,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:14,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:15,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:16,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:16,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:17,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:18,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:19,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:19,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:20,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:21,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:22,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:23,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:24,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:25,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:25,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:26,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:27,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:21<03:35, 17.93s/it][WARNING|generation_utils.py:914] 2023-08-28 14:32:27,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:28,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:29,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:30,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:30,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:31,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:32,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:33,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:34,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:35,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:35,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:36,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:37,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:37,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:38,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:39,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:40,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:41,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:41,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:42,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:43,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:44,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:45,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:46,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:40<03:23, 18.48s/it][WARNING|generation_utils.py:914] 2023-08-28 14:32:47,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:48,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:49,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:49,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:50,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:51,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:52,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:53,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:54,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:54,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:55,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:56,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:57,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:58,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:58,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:59,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:00,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:01,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:01,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:02,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:03,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:04,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:58<03:02, 18.29s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:05,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:06,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:06,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:07,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:08,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:09,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:09,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:10,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:11,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:11,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:12,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:13,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:14,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:15,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:15,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:16,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:17,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:18,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:18,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:19,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:20,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:21,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:15<02:40, 17.87s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:22,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:23,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:23,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:24,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:25,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:26,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:27,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:28,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:29,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:30,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:31,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:31,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:32,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:33,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:34,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:35,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:36,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:36,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:37,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:38,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:39,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:40,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:41,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:35<02:27, 18.42s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:41,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:42,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:43,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:43,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:44,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:45,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:45,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:46,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:47,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:47,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:48,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:49,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:50,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:51,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:51,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:52,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:53,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:53,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:54,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:55,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:55,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:50<02:01, 17.38s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:56,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:57,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:58,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:59,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:59,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:00,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:01,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:02,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:03,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:03,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:04,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:05,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:06,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:07,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:07,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:08,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:09,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:10,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:11,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:12,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:12,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:13,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:14,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:08<01:46, 17.71s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:15,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:16,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:16,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:17,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:18,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:19,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:19,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:20,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:21,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:22,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:23,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:23,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:24,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:25,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:26,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:27,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:27,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:28,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:29,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:30,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:31,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:31,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:32,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:26<01:29, 17.82s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:33,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:34,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:34,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:35,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:36,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:37,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:38,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:39,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:39,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:40,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:41,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:42,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:43,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:44,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:44,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:45,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:46,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:47,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:47,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:48,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:49,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:50,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:51,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:51,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:46<01:13, 18.26s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:52,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:53,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:54,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:55,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:55,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:56,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:57,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:57,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:58,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:59,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:59,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:00,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:01,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:02,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:03,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:03,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:04,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:05,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:05,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:06,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [05:00<00:51, 17.18s/it][WARNING|generation_utils.py:914] 2023-08-28 14:35:07,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:08,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:08,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:09,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:10,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:11,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:12,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:12,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:13,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:14,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:15,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:15,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:16,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:17,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:17,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:18,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:19,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:20,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:20,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:21,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:22,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:16<00:33, 16.72s/it][WARNING|generation_utils.py:914] 2023-08-28 14:35:22,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:23,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:24,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:25,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:25,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:26,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:27,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:27,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:28,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:29,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:29,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:30,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:31,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:31,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:32,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:33,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:34,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:34,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:35,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:36,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:36,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:37,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:31<00:16, 16.38s/it][WARNING|generation_utils.py:914] 2023-08-28 14:35:38,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:39,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:40,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:40,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:41,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:42,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:43,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:43,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:44,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:45,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:46,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:46,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:47,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:48,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:49,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:49,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:50,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:51,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:51,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:52,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:53,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:47<00:00, 16.18s/it]Generating: 100%|██████████| 20/20 [05:47<00:00, 17.38s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:01,263 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:01,268 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:01,268 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:01,268 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:01,268 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:36:01,885 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:36:01,886 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:36:02,453 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:36:03,526 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:36:03,526 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:06,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:06,816 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:06,816 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:06,816 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:06,816 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:36:07,450 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:36:07,452 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:36:08,007 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:36:08,164 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:36:08,164 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : has part .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : location .', 'success_rate': 0.796875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : Later in the year , the court appointed him as a judge at the trial of a prisoner in the First World War . Head Entity : first World War , Tail Entity : Chief Justice .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : position held .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8059895833333334, 'errors': {'', '(\'Stephanie Richey\', \'competition class\', \'\', \'She won two national championship with the " Miss USA " in 2003 , beating American gymnast Stephanie Richey , but lost to former runner - up Maria Sharapova in the women \\\'s 100 m butterfly .\')'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 297, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 372, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 418, 'raw': 576}
{'target': 600, 'success': 442, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 485, 'raw': 672}
{'target': 600, 'success': 509, 'raw': 704}
{'target': 600, 'success': 529, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 579, 'raw': 800}
{'target': 600, 'success': 604, 'raw': 832}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7259615384615384, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : father .', 'success_rate': 0.8125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : instrument . Context : Later in the year ( October 1887 ) , a young musician of French ancestry named Pierre de Vigny invited him to play with him in the chamber of the Marquise Bélanger at the Marquise de Vigny 's house in Montréal , Quebec . Head Entity : Pierre de Vigny , Tail Entity : soprano .\n"]
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8165760869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 576, 'raw': 736}
{'target': 600, 'success': 604, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7864583333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9484375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9315476190476191, 'errors': {''}}
['Relation : position played on team / speciality . Context : On 31 occasions with the club , he played for a total of eight teams during a career spanning four decades , including four Premier League matches in 1958 , 1960 , 1970 , 1971 , 1972 , 1974 , 1975 , 1976 and 1977 . Head Entity : 1973 , Tail Entity : Premier League .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8693181818181818, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 13092
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13192, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.21it/s]Extractor Estimating: 2it [00:01,  1.37it/s]Extractor Estimating: 3it [00:02,  1.46it/s]Extractor Estimating: 4it [00:02,  1.47it/s]Extractor Estimating: 5it [00:03,  1.46it/s]Extractor Estimating: 6it [00:04,  1.50it/s]Extractor Estimating: 7it [00:04,  1.44it/s]Extractor Estimating: 8it [00:05,  1.44it/s]Extractor Estimating: 9it [00:06,  1.44it/s]Extractor Estimating: 10it [00:06,  1.43it/s]Extractor Estimating: 11it [00:07,  1.45it/s]Extractor Estimating: 12it [00:08,  1.42it/s]Extractor Estimating: 13it [00:09,  1.38it/s]Extractor Estimating: 14it [00:09,  1.34it/s]Extractor Estimating: 15it [00:10,  1.35it/s]Extractor Estimating: 16it [00:11,  1.37it/s]Extractor Estimating: 17it [00:12,  1.39it/s]Extractor Estimating: 18it [00:12,  1.36it/s]Extractor Estimating: 19it [00:13,  1.43it/s]Extractor Estimating: 20it [00:14,  1.46it/s]Extractor Estimating: 21it [00:14,  1.48it/s]Extractor Estimating: 22it [00:15,  1.46it/s]Extractor Estimating: 23it [00:16,  1.39it/s]Extractor Estimating: 24it [00:16,  1.40it/s]Extractor Estimating: 25it [00:17,  1.37it/s]Extractor Estimating: 26it [00:18,  1.39it/s]Extractor Estimating: 27it [00:19,  1.44it/s]Extractor Estimating: 28it [00:19,  1.49it/s]Extractor Estimating: 29it [00:20,  1.47it/s]Extractor Estimating: 30it [00:21,  1.45it/s]Extractor Estimating: 31it [00:21,  1.48it/s]Extractor Estimating: 32it [00:22,  1.51it/s]Extractor Estimating: 33it [00:23,  1.47it/s]Extractor Estimating: 34it [00:23,  1.50it/s]Extractor Estimating: 35it [00:24,  1.49it/s]Extractor Estimating: 36it [00:25,  1.48it/s]Extractor Estimating: 37it [00:25,  1.52it/s]Extractor Estimating: 38it [00:26,  1.52it/s]Extractor Estimating: 39it [00:27,  1.48it/s]Extractor Estimating: 40it [00:27,  1.49it/s]Extractor Estimating: 41it [00:28,  1.44it/s]Extractor Estimating: 42it [00:29,  1.47it/s]Extractor Estimating: 43it [00:29,  1.49it/s]Extractor Estimating: 44it [00:30,  1.47it/s]Extractor Estimating: 45it [00:31,  1.43it/s]Extractor Estimating: 46it [00:31,  1.41it/s]Extractor Estimating: 47it [00:32,  1.45it/s]Extractor Estimating: 48it [00:33,  1.48it/s]Extractor Estimating: 49it [00:33,  1.49it/s]Extractor Estimating: 50it [00:34,  1.43it/s]Extractor Estimating: 51it [00:35,  1.49it/s]Extractor Estimating: 52it [00:35,  1.53it/s]Extractor Estimating: 53it [00:36,  1.52it/s]Extractor Estimating: 54it [00:37,  1.55it/s]Extractor Estimating: 55it [00:37,  1.55it/s]Extractor Estimating: 56it [00:38,  1.53it/s]Extractor Estimating: 57it [00:39,  1.52it/s]Extractor Estimating: 58it [00:39,  1.50it/s]Extractor Estimating: 59it [00:40,  1.51it/s]Extractor Estimating: 60it [00:41,  1.47it/s]Extractor Estimating: 61it [00:41,  1.52it/s]Extractor Estimating: 62it [00:42,  1.46it/s]Extractor Estimating: 63it [00:43,  1.46it/s]Extractor Estimating: 64it [00:44,  1.43it/s]Extractor Estimating: 65it [00:44,  1.46it/s]Extractor Estimating: 66it [00:45,  1.47it/s]Extractor Estimating: 67it [00:45,  1.47it/s]Extractor Estimating: 68it [00:46,  1.52it/s]Extractor Estimating: 69it [00:47,  1.53it/s]Extractor Estimating: 70it [00:47,  1.52it/s]Extractor Estimating: 71it [00:48,  1.58it/s]Extractor Estimating: 72it [00:49,  1.56it/s]Extractor Estimating: 73it [00:49,  1.55it/s]Extractor Estimating: 74it [00:50,  1.50it/s]Extractor Estimating: 75it [00:51,  1.55it/s]Extractor Estimating: 76it [00:51,  1.58it/s]Extractor Estimating: 77it [00:52,  1.62it/s]Extractor Estimating: 78it [00:52,  1.61it/s]Extractor Estimating: 79it [00:53,  1.63it/s]Extractor Estimating: 80it [00:54,  1.65it/s]Extractor Estimating: 81it [00:54,  1.62it/s]Extractor Estimating: 82it [00:55,  1.53it/s]Extractor Estimating: 83it [00:56,  1.58it/s]Extractor Estimating: 84it [00:56,  1.55it/s]Extractor Estimating: 85it [00:57,  1.40it/s]Extractor Estimating: 86it [00:58,  1.42it/s]Extractor Estimating: 87it [00:58,  1.45it/s]Extractor Estimating: 88it [00:59,  1.50it/s]Extractor Estimating: 89it [01:00,  1.50it/s]Extractor Estimating: 90it [01:00,  1.57it/s]Extractor Estimating: 91it [01:01,  1.60it/s]Extractor Estimating: 92it [01:02,  1.60it/s]Extractor Estimating: 93it [01:02,  1.63it/s]Extractor Estimating: 94it [01:03,  1.68it/s]Extractor Estimating: 95it [01:03,  1.64it/s]Extractor Estimating: 96it [01:04,  1.61it/s]Extractor Estimating: 97it [01:05,  1.57it/s]Extractor Estimating: 98it [01:05,  1.55it/s]Extractor Estimating: 99it [01:06,  1.56it/s]Extractor Estimating: 100it [01:06,  1.62it/s]Extractor Estimating: 101it [01:07,  1.61it/s]Extractor Estimating: 102it [01:08,  1.67it/s]Extractor Estimating: 103it [01:08,  1.69it/s]Extractor Estimating: 104it [01:09,  1.61it/s]Extractor Estimating: 105it [01:10,  1.62it/s]Extractor Estimating: 106it [01:10,  1.64it/s]Extractor Estimating: 107it [01:11,  1.64it/s]Extractor Estimating: 108it [01:11,  1.63it/s]Extractor Estimating: 109it [01:12,  1.68it/s]Extractor Estimating: 110it [01:13,  1.64it/s]Extractor Estimating: 111it [01:13,  1.66it/s]Extractor Estimating: 112it [01:14,  1.63it/s]Extractor Estimating: 113it [01:14,  1.65it/s]Extractor Estimating: 114it [01:15,  1.61it/s]Extractor Estimating: 115it [01:16,  1.59it/s]Extractor Estimating: 116it [01:16,  1.63it/s]Extractor Estimating: 117it [01:17,  1.63it/s]Extractor Estimating: 118it [01:17,  1.68it/s]Extractor Estimating: 119it [01:18,  1.60it/s]Extractor Estimating: 120it [01:19,  1.57it/s]Extractor Estimating: 121it [01:19,  1.56it/s]Extractor Estimating: 122it [01:20,  1.58it/s]Extractor Estimating: 123it [01:21,  1.58it/s]Extractor Estimating: 124it [01:21,  1.58it/s]Extractor Estimating: 125it [01:22,  1.56it/s]Extractor Estimating: 126it [01:23,  1.51it/s]Extractor Estimating: 127it [01:23,  1.48it/s]Extractor Estimating: 128it [01:24,  1.52it/s]Extractor Estimating: 129it [01:25,  1.49it/s]Extractor Estimating: 130it [01:25,  1.45it/s]Extractor Estimating: 131it [01:26,  1.47it/s]Extractor Estimating: 132it [01:27,  1.47it/s]Extractor Estimating: 133it [01:27,  1.49it/s]Extractor Estimating: 134it [01:28,  1.48it/s]Extractor Estimating: 135it [01:29,  1.46it/s]Extractor Estimating: 136it [01:30,  1.44it/s]Extractor Estimating: 137it [01:30,  1.48it/s]Extractor Estimating: 138it [01:31,  1.46it/s]Extractor Estimating: 139it [01:32,  1.44it/s]Extractor Estimating: 140it [01:32,  1.43it/s]Extractor Estimating: 141it [01:33,  1.47it/s]Extractor Estimating: 142it [01:34,  1.46it/s]Extractor Estimating: 143it [01:34,  1.45it/s]Extractor Estimating: 144it [01:35,  1.50it/s]Extractor Estimating: 145it [01:36,  1.50it/s]Extractor Estimating: 146it [01:36,  1.51it/s]Extractor Estimating: 147it [01:37,  1.47it/s]Extractor Estimating: 148it [01:38,  1.48it/s]Extractor Estimating: 149it [01:38,  1.44it/s]Extractor Estimating: 150it [01:39,  1.44it/s]Extractor Estimating: 151it [01:40,  1.51it/s]Extractor Estimating: 152it [01:40,  1.51it/s]Extractor Estimating: 153it [01:41,  1.55it/s]Extractor Estimating: 154it [01:42,  1.55it/s]Extractor Estimating: 155it [01:42,  1.56it/s]Extractor Estimating: 156it [01:43,  1.57it/s]Extractor Estimating: 157it [01:43,  1.64it/s]Extractor Estimating: 158it [01:44,  1.72it/s]Extractor Estimating: 159it [01:45,  1.63it/s]Extractor Estimating: 160it [01:45,  1.63it/s]Extractor Estimating: 161it [01:46,  1.67it/s]Extractor Estimating: 162it [01:46,  1.61it/s]Extractor Estimating: 163it [01:47,  1.60it/s]Extractor Estimating: 164it [01:48,  1.61it/s]Extractor Estimating: 165it [01:48,  1.62it/s]Extractor Estimating: 166it [01:49,  1.63it/s]Extractor Estimating: 167it [01:50,  1.59it/s]Extractor Estimating: 168it [01:50,  1.58it/s]Extractor Estimating: 169it [01:51,  1.44it/s]Extractor Estimating: 170it [01:52,  1.47it/s]Extractor Estimating: 171it [01:52,  1.57it/s]Extractor Estimating: 172it [01:53,  1.64it/s]Extractor Estimating: 173it [01:53,  1.66it/s]Extractor Estimating: 174it [01:54,  1.68it/s]Extractor Estimating: 175it [01:55,  1.63it/s]Extractor Estimating: 176it [01:55,  1.63it/s]Extractor Estimating: 177it [01:56,  1.63it/s]Extractor Estimating: 178it [01:56,  1.64it/s]Extractor Estimating: 179it [01:57,  1.66it/s]Extractor Estimating: 180it [01:58,  1.62it/s]Extractor Estimating: 181it [01:58,  1.63it/s]Extractor Estimating: 182it [01:59,  1.65it/s]Extractor Estimating: 183it [02:00,  1.62it/s]Extractor Estimating: 184it [02:00,  1.64it/s]Extractor Estimating: 185it [02:01,  1.63it/s]Extractor Estimating: 186it [02:01,  1.59it/s]Extractor Estimating: 187it [02:02,  1.51it/s]Extractor Estimating: 188it [02:03,  1.51it/s]Extractor Estimating: 189it [02:03,  1.61it/s]Extractor Estimating: 190it [02:04,  1.64it/s]Extractor Estimating: 191it [02:05,  1.65it/s]Extractor Estimating: 192it [02:05,  1.60it/s]Extractor Estimating: 193it [02:06,  1.57it/s]Extractor Estimating: 194it [02:06,  1.58it/s]Extractor Estimating: 195it [02:07,  1.54it/s]Extractor Estimating: 196it [02:08,  1.61it/s]Extractor Estimating: 197it [02:08,  1.63it/s]Extractor Estimating: 198it [02:09,  1.64it/s]Extractor Estimating: 199it [02:10,  1.62it/s]Extractor Estimating: 200it [02:10,  1.56it/s]Extractor Estimating: 201it [02:11,  1.62it/s]Extractor Estimating: 202it [02:11,  1.57it/s]Extractor Estimating: 203it [02:12,  1.57it/s]Extractor Estimating: 204it [02:13,  1.60it/s]Extractor Estimating: 205it [02:13,  1.60it/s]Extractor Estimating: 206it [02:14,  1.50it/s]Extractor Estimating: 207it [02:15,  1.53it/s]Extractor Estimating: 208it [02:15,  1.59it/s]Extractor Estimating: 209it [02:16,  1.54it/s]Extractor Estimating: 210it [02:17,  1.50it/s]Extractor Estimating: 211it [02:17,  1.52it/s]Extractor Estimating: 212it [02:18,  1.54it/s]Extractor Estimating: 213it [02:19,  1.51it/s]Extractor Estimating: 214it [02:19,  1.52it/s]Extractor Estimating: 215it [02:20,  1.52it/s]Extractor Estimating: 216it [02:21,  1.58it/s]Extractor Estimating: 217it [02:21,  1.57it/s]Extractor Estimating: 218it [02:22,  1.57it/s]Extractor Estimating: 219it [02:22,  1.56it/s]Extractor Estimating: 220it [02:23,  1.59it/s]Extractor Estimating: 221it [02:24,  1.60it/s]Extractor Estimating: 222it [02:24,  1.56it/s]Extractor Estimating: 223it [02:25,  1.49it/s]Extractor Estimating: 224it [02:26,  1.53it/s]Extractor Estimating: 225it [02:26,  1.56it/s]Extractor Estimating: 226it [02:27,  1.47it/s]Extractor Estimating: 227it [02:28,  1.48it/s]Extractor Estimating: 228it [02:29,  1.44it/s]Extractor Estimating: 229it [02:29,  1.43it/s]Extractor Estimating: 230it [02:30,  1.47it/s]Extractor Estimating: 231it [02:31,  1.38it/s]Extractor Estimating: 232it [02:31,  1.40it/s]Extractor Estimating: 233it [02:32,  1.37it/s]Extractor Estimating: 234it [02:33,  1.37it/s]Extractor Estimating: 235it [02:34,  1.43it/s]Extractor Estimating: 236it [02:34,  1.39it/s]Extractor Estimating: 237it [02:35,  1.40it/s]Extractor Estimating: 238it [02:36,  1.45it/s]Extractor Estimating: 239it [02:36,  1.43it/s]Extractor Estimating: 240it [02:37,  1.46it/s]Extractor Estimating: 241it [02:38,  1.44it/s]Extractor Estimating: 242it [02:38,  1.48it/s]Extractor Estimating: 243it [02:39,  1.47it/s]Extractor Estimating: 244it [02:40,  1.46it/s]Extractor Estimating: 245it [02:40,  1.48it/s]Extractor Estimating: 246it [02:41,  1.44it/s]Extractor Estimating: 247it [02:42,  1.48it/s]Extractor Estimating: 248it [02:42,  1.44it/s]Extractor Estimating: 249it [02:43,  1.43it/s]Extractor Estimating: 250it [02:44,  1.46it/s]Extractor Estimating: 251it [02:45,  1.43it/s]Extractor Estimating: 252it [02:45,  1.46it/s]Extractor Estimating: 253it [02:46,  1.38it/s]Extractor Estimating: 254it [02:47,  1.40it/s]Extractor Estimating: 255it [02:47,  1.48it/s]Extractor Estimating: 256it [02:48,  1.50it/s]Extractor Estimating: 257it [02:49,  1.50it/s]Extractor Estimating: 258it [02:49,  1.49it/s]Extractor Estimating: 259it [02:50,  1.56it/s]Extractor Estimating: 260it [02:51,  1.56it/s]Extractor Estimating: 261it [02:51,  1.53it/s]Extractor Estimating: 262it [02:52,  1.49it/s]Extractor Estimating: 263it [02:53,  1.47it/s]Extractor Estimating: 264it [02:53,  1.53it/s]Extractor Estimating: 265it [02:54,  1.55it/s]Extractor Estimating: 266it [02:54,  1.55it/s]Extractor Estimating: 267it [02:55,  1.53it/s]Extractor Estimating: 268it [02:56,  1.52it/s]Extractor Estimating: 269it [02:57,  1.50it/s]Extractor Estimating: 270it [02:57,  1.47it/s]Extractor Estimating: 271it [02:58,  1.48it/s]Extractor Estimating: 272it [02:58,  1.54it/s]Extractor Estimating: 273it [02:59,  1.54it/s]Extractor Estimating: 274it [03:00,  1.46it/s]Extractor Estimating: 275it [03:01,  1.49it/s]Extractor Estimating: 276it [03:01,  1.46it/s]Extractor Estimating: 277it [03:02,  1.46it/s]Extractor Estimating: 278it [03:03,  1.53it/s]Extractor Estimating: 279it [03:03,  1.49it/s]Extractor Estimating: 280it [03:04,  1.50it/s]Extractor Estimating: 281it [03:05,  1.48it/s]Extractor Estimating: 282it [03:05,  1.49it/s]Extractor Estimating: 283it [03:06,  1.44it/s]Extractor Estimating: 284it [03:07,  1.49it/s]Extractor Estimating: 285it [03:07,  1.50it/s]Extractor Estimating: 286it [03:08,  1.54it/s]Extractor Estimating: 287it [03:09,  1.50it/s]Extractor Estimating: 288it [03:09,  1.49it/s]Extractor Estimating: 289it [03:10,  1.49it/s]Extractor Estimating: 290it [03:11,  1.50it/s]Extractor Estimating: 291it [03:11,  1.48it/s]Extractor Estimating: 292it [03:12,  1.47it/s]Extractor Estimating: 293it [03:13,  1.48it/s]Extractor Estimating: 294it [03:13,  1.47it/s]Extractor Estimating: 295it [03:14,  1.46it/s]Extractor Estimating: 296it [03:15,  1.49it/s]Extractor Estimating: 297it [03:15,  1.46it/s]Extractor Estimating: 298it [03:16,  1.44it/s]Extractor Estimating: 299it [03:17,  1.48it/s]Extractor Estimating: 300it [03:17,  1.50it/s]Extractor Estimating: 301it [03:18,  1.54it/s]Extractor Estimating: 302it [03:19,  1.54it/s]Extractor Estimating: 303it [03:19,  1.57it/s]Extractor Estimating: 304it [03:20,  1.59it/s]Extractor Estimating: 305it [03:20,  1.60it/s]Extractor Estimating: 306it [03:21,  1.55it/s]Extractor Estimating: 307it [03:22,  1.61it/s]Extractor Estimating: 308it [03:22,  1.59it/s]Extractor Estimating: 309it [03:23,  1.57it/s]Extractor Estimating: 310it [03:24,  1.55it/s]Extractor Estimating: 311it [03:24,  1.55it/s]Extractor Estimating: 312it [03:25,  1.41it/s]Extractor Estimating: 313it [03:26,  1.37it/s]Extractor Estimating: 314it [03:27,  1.42it/s]Extractor Estimating: 315it [03:27,  1.47it/s]Extractor Estimating: 316it [03:28,  1.50it/s]Extractor Estimating: 317it [03:29,  1.52it/s]Extractor Estimating: 318it [03:29,  1.56it/s]Extractor Estimating: 319it [03:30,  1.55it/s]Extractor Estimating: 320it [03:30,  1.56it/s]Extractor Estimating: 321it [03:31,  1.53it/s]Extractor Estimating: 322it [03:32,  1.54it/s]Extractor Estimating: 323it [03:32,  1.57it/s]Extractor Estimating: 324it [03:33,  1.57it/s]Extractor Estimating: 325it [03:34,  1.54it/s]Extractor Estimating: 326it [03:34,  1.55it/s]Extractor Estimating: 327it [03:35,  1.52it/s]Extractor Estimating: 328it [03:36,  1.58it/s]Extractor Estimating: 329it [03:36,  1.61it/s]Extractor Estimating: 330it [03:37,  1.63it/s]Extractor Estimating: 331it [03:37,  1.64it/s]Extractor Estimating: 332it [03:38,  1.65it/s]Extractor Estimating: 333it [03:39,  1.61it/s]Extractor Estimating: 334it [03:39,  1.61it/s]Extractor Estimating: 335it [03:40,  1.58it/s]Extractor Estimating: 336it [03:40,  1.61it/s]Extractor Estimating: 337it [03:41,  1.57it/s]Extractor Estimating: 338it [03:42,  1.59it/s]Extractor Estimating: 339it [03:42,  1.61it/s]Extractor Estimating: 340it [03:43,  1.58it/s]Extractor Estimating: 341it [03:44,  1.56it/s]Extractor Estimating: 342it [03:44,  1.55it/s]Extractor Estimating: 343it [03:45,  1.37it/s]Extractor Estimating: 344it [03:46,  1.39it/s]Extractor Estimating: 345it [03:47,  1.42it/s]Extractor Estimating: 346it [03:47,  1.45it/s]Extractor Estimating: 347it [03:48,  1.46it/s]Extractor Estimating: 348it [03:49,  1.48it/s]Extractor Estimating: 349it [03:49,  1.51it/s]Extractor Estimating: 350it [03:50,  1.55it/s]Extractor Estimating: 351it [03:51,  1.52it/s]Extractor Estimating: 352it [03:51,  1.46it/s]Extractor Estimating: 353it [03:52,  1.48it/s]Extractor Estimating: 354it [03:53,  1.52it/s]Extractor Estimating: 355it [03:53,  1.46it/s]Extractor Estimating: 356it [03:54,  1.44it/s]Extractor Estimating: 357it [03:55,  1.44it/s]Extractor Estimating: 358it [03:55,  1.47it/s]Extractor Estimating: 359it [03:56,  1.42it/s]Extractor Estimating: 360it [03:57,  1.48it/s]Extractor Estimating: 361it [03:57,  1.49it/s]Extractor Estimating: 362it [03:58,  1.53it/s]Extractor Estimating: 363it [03:59,  1.51it/s]Extractor Estimating: 364it [03:59,  1.50it/s]Extractor Estimating: 365it [04:00,  1.48it/s]Extractor Estimating: 366it [04:01,  1.47it/s]Extractor Estimating: 367it [04:01,  1.45it/s]Extractor Estimating: 368it [04:02,  1.49it/s]Extractor Estimating: 369it [04:03,  1.52it/s]Extractor Estimating: 370it [04:03,  1.53it/s]Extractor Estimating: 371it [04:04,  1.46it/s]Extractor Estimating: 372it [04:05,  1.46it/s]Extractor Estimating: 373it [04:06,  1.44it/s]Extractor Estimating: 374it [04:06,  1.44it/s]Extractor Estimating: 375it [04:07,  1.49it/s]Extractor Estimating: 376it [04:08,  1.46it/s]Extractor Estimating: 377it [04:08,  1.44it/s]Extractor Estimating: 378it [04:09,  1.44it/s]Extractor Estimating: 379it [04:10,  1.43it/s]Extractor Estimating: 380it [04:10,  1.50it/s]Extractor Estimating: 381it [04:11,  1.37it/s]Extractor Estimating: 382it [04:12,  1.40it/s]Extractor Estimating: 383it [04:12,  1.43it/s]Extractor Estimating: 384it [04:13,  1.46it/s]Extractor Estimating: 385it [04:14,  1.53it/s]Extractor Estimating: 386it [04:14,  1.51it/s]Extractor Estimating: 387it [04:15,  1.48it/s]Extractor Estimating: 388it [04:16,  1.47it/s]Extractor Estimating: 389it [04:16,  1.53it/s]Extractor Estimating: 390it [04:17,  1.53it/s]Extractor Estimating: 391it [04:18,  1.49it/s]Extractor Estimating: 392it [04:18,  1.53it/s]Extractor Estimating: 393it [04:19,  1.55it/s]Extractor Estimating: 394it [04:20,  1.45it/s]Extractor Estimating: 395it [04:20,  1.45it/s]Extractor Estimating: 396it [04:21,  1.49it/s]Extractor Estimating: 397it [04:22,  1.52it/s]Extractor Estimating: 398it [04:22,  1.49it/s]Extractor Estimating: 399it [04:23,  1.48it/s]Extractor Estimating: 400it [04:24,  1.46it/s]Extractor Estimating: 401it [04:24,  1.49it/s]Extractor Estimating: 402it [04:25,  1.43it/s]Extractor Estimating: 403it [04:26,  1.46it/s]Extractor Estimating: 404it [04:26,  1.50it/s]Extractor Estimating: 405it [04:27,  1.52it/s]Extractor Estimating: 406it [04:28,  1.57it/s]Extractor Estimating: 407it [04:28,  1.59it/s]Extractor Estimating: 408it [04:29,  1.60it/s]Extractor Estimating: 409it [04:30,  1.61it/s]Extractor Estimating: 410it [04:30,  1.62it/s]Extractor Estimating: 411it [04:31,  1.65it/s]Extractor Estimating: 412it [04:31,  1.57it/s]Extractor Estimating: 413it [04:32,  1.60it/s]Extractor Estimating: 414it [04:33,  1.60it/s]Extractor Estimating: 415it [04:33,  1.54it/s]Extractor Estimating: 416it [04:34,  1.54it/s]Extractor Estimating: 417it [04:35,  1.52it/s]Extractor Estimating: 418it [04:35,  1.49it/s]Extractor Estimating: 419it [04:36,  1.52it/s]Extractor Estimating: 420it [04:37,  1.56it/s]Extractor Estimating: 421it [04:37,  1.57it/s]Extractor Estimating: 422it [04:38,  1.54it/s]Extractor Estimating: 423it [04:39,  1.58it/s]Extractor Estimating: 424it [04:39,  1.59it/s]Extractor Estimating: 425it [04:40,  1.57it/s]Extractor Estimating: 426it [04:40,  1.59it/s]Extractor Estimating: 427it [04:41,  1.50it/s]Extractor Estimating: 428it [04:42,  1.50it/s]Extractor Estimating: 429it [04:42,  1.53it/s]Extractor Estimating: 430it [04:43,  1.53it/s]Extractor Estimating: 431it [04:44,  1.53it/s]Extractor Estimating: 432it [04:45,  1.44it/s]Extractor Estimating: 433it [04:45,  1.47it/s]Extractor Estimating: 434it [04:46,  1.47it/s]Extractor Estimating: 435it [04:47,  1.47it/s]Extractor Estimating: 436it [04:47,  1.47it/s]Extractor Estimating: 437it [04:48,  1.45it/s]Extractor Estimating: 438it [04:49,  1.48it/s]Extractor Estimating: 439it [04:49,  1.35it/s]Extractor Estimating: 440it [04:50,  1.38it/s]Extractor Estimating: 441it [04:51,  1.43it/s]Extractor Estimating: 442it [04:51,  1.46it/s]Extractor Estimating: 443it [04:52,  1.49it/s]Extractor Estimating: 444it [04:53,  1.51it/s]Extractor Estimating: 445it [04:53,  1.56it/s]Extractor Estimating: 446it [04:54,  1.54it/s]Extractor Estimating: 447it [04:55,  1.50it/s]Extractor Estimating: 448it [04:55,  1.50it/s]Extractor Estimating: 449it [04:56,  1.52it/s]Extractor Estimating: 450it [04:57,  1.50it/s]Extractor Estimating: 451it [04:57,  1.55it/s]Extractor Estimating: 452it [04:58,  1.60it/s]Extractor Estimating: 453it [04:58,  1.63it/s]Extractor Estimating: 454it [04:59,  1.63it/s]Extractor Estimating: 455it [05:00,  1.66it/s]Extractor Estimating: 456it [05:00,  1.64it/s]Extractor Estimating: 457it [05:01,  1.68it/s]Extractor Estimating: 458it [05:01,  1.71it/s]Extractor Estimating: 459it [05:02,  1.73it/s]Extractor Estimating: 460it [05:03,  1.74it/s]Extractor Estimating: 461it [05:03,  1.74it/s]Extractor Estimating: 462it [05:04,  1.76it/s]Extractor Estimating: 463it [05:04,  1.75it/s]Extractor Estimating: 464it [05:05,  1.71it/s]Extractor Estimating: 465it [05:05,  1.69it/s]Extractor Estimating: 466it [05:06,  1.74it/s]Extractor Estimating: 467it [05:07,  1.74it/s]Extractor Estimating: 468it [05:07,  1.73it/s]Extractor Estimating: 469it [05:08,  1.70it/s]Extractor Estimating: 470it [05:08,  1.69it/s]Extractor Estimating: 471it [05:09,  1.72it/s]Extractor Estimating: 472it [05:09,  1.74it/s]Extractor Estimating: 473it [05:10,  1.74it/s]Extractor Estimating: 474it [05:11,  1.77it/s]Extractor Estimating: 475it [05:11,  1.63it/s]Extractor Estimating: 476it [05:12,  1.53it/s]Extractor Estimating: 477it [05:13,  1.53it/s]Extractor Estimating: 478it [05:13,  1.50it/s]Extractor Estimating: 479it [05:14,  1.54it/s]Extractor Estimating: 480it [05:15,  1.58it/s]Extractor Estimating: 481it [05:15,  1.57it/s]Extractor Estimating: 482it [05:16,  1.52it/s]Extractor Estimating: 483it [05:17,  1.53it/s]Extractor Estimating: 484it [05:17,  1.57it/s]Extractor Estimating: 485it [05:18,  1.58it/s]Extractor Estimating: 486it [05:19,  1.52it/s]Extractor Estimating: 487it [05:19,  1.58it/s]Extractor Estimating: 488it [05:20,  1.63it/s]Extractor Estimating: 489it [05:20,  1.57it/s]Extractor Estimating: 490it [05:21,  1.63it/s]Extractor Estimating: 491it [05:22,  1.61it/s]Extractor Estimating: 492it [05:22,  1.64it/s]Extractor Estimating: 493it [05:23,  1.60it/s]Extractor Estimating: 494it [05:24,  1.56it/s]Extractor Estimating: 495it [05:24,  1.61it/s]Extractor Estimating: 496it [05:25,  1.68it/s]Extractor Estimating: 497it [05:25,  1.65it/s]Extractor Estimating: 498it [05:26,  1.58it/s]Extractor Estimating: 499it [05:27,  1.61it/s]Extractor Estimating: 500it [05:27,  1.67it/s]Extractor Estimating: 500it [05:27,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:49,206 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:49,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:49,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:49,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:49,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:41:49,814 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:41:49,815 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:41:50,377 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:41:51,453 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:41:51,454 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:54,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:54,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:54,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:54,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:54,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:41:54,934 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:41:54,935 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:41:55,506 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:41:55,661 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:41:55,662 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:54:04,191 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:54:04,201 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9984 mean pseudo reward: 0.950730601743894
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 25239
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25339, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25339, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.101, loss:770.1175
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.124, loss:705.2305
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.106, loss:696.7833
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.149, loss:714.1775
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.113, loss:646.4342
>> valid entity prec:0.5621, rec:0.6012, f1:0.5810
>> valid relation prec:0.3980, rec:0.1354, f1:0.2020
>> valid relation with NER prec:0.3980, rec:0.1354, f1:0.2020
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.514, loss:691.3922
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.111, loss:688.1183
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.126, loss:707.0636
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.118, loss:679.6675
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.125, loss:667.6452
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5696, rec:0.5771, f1:0.5733
>> valid relation prec:0.3670, rec:0.1417, f1:0.2044
>> valid relation with NER prec:0.3670, rec:0.1417, f1:0.2044
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 268, avg_time 2.519, loss:703.2900
g_step 1200, step 368, avg_time 1.101, loss:738.9395
g_step 1300, step 52, avg_time 1.116, loss:692.0423
g_step 1400, step 152, avg_time 1.131, loss:642.5106
g_step 1500, step 252, avg_time 1.121, loss:668.9759
>> valid entity prec:0.5837, rec:0.5171, f1:0.5484
>> valid relation prec:0.4158, rec:0.1374, f1:0.2065
>> valid relation with NER prec:0.4158, rec:0.1374, f1:0.2065
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 352, avg_time 2.498, loss:661.0127
g_step 1700, step 36, avg_time 1.127, loss:681.9154
g_step 1800, step 136, avg_time 1.110, loss:627.3338
g_step 1900, step 236, avg_time 1.118, loss:663.3849
g_step 2000, step 336, avg_time 1.128, loss:632.2484
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5653, rec:0.5030, f1:0.5323
>> valid relation prec:0.4268, rec:0.1087, f1:0.1733
>> valid relation with NER prec:0.4268, rec:0.1087, f1:0.1733
g_step 2100, step 20, avg_time 2.501, loss:620.2038
g_step 2200, step 120, avg_time 1.110, loss:589.7875
g_step 2300, step 220, avg_time 1.117, loss:602.6510
g_step 2400, step 320, avg_time 1.107, loss:628.9271
g_step 2500, step 4, avg_time 1.122, loss:623.4644
>> valid entity prec:0.5372, rec:0.5940, f1:0.5641
>> valid relation prec:0.3797, rec:0.1520, f1:0.2171
>> valid relation with NER prec:0.3797, rec:0.1520, f1:0.2171
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 104, avg_time 2.519, loss:565.8723
g_step 2700, step 204, avg_time 1.116, loss:579.6493
g_step 2800, step 304, avg_time 1.120, loss:589.5918
g_step 2900, step 404, avg_time 1.120, loss:602.5465
g_step 3000, step 88, avg_time 1.130, loss:540.8818
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5318, rec:0.5823, f1:0.5559
>> valid relation prec:0.3808, rec:0.1448, f1:0.2098
>> valid relation with NER prec:0.3808, rec:0.1448, f1:0.2098
g_step 3100, step 188, avg_time 2.514, loss:555.6847
g_step 3200, step 288, avg_time 1.104, loss:569.3829
g_step 3300, step 388, avg_time 1.117, loss:575.9055
g_step 3400, step 72, avg_time 1.122, loss:537.5006
g_step 3500, step 172, avg_time 1.113, loss:533.3296
>> valid entity prec:0.5619, rec:0.5297, f1:0.5453
>> valid relation prec:0.3706, rec:0.1179, f1:0.1789
>> valid relation with NER prec:0.3706, rec:0.1179, f1:0.1789
g_step 3600, step 272, avg_time 2.512, loss:520.7438
g_step 3700, step 372, avg_time 1.119, loss:569.7528
g_step 3800, step 56, avg_time 1.103, loss:520.7627
g_step 3900, step 156, avg_time 1.113, loss:525.9244
g_step 4000, step 256, avg_time 1.131, loss:521.9593
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5874, rec:0.5178, f1:0.5504
>> valid relation prec:0.3406, rec:0.1480, f1:0.2063
>> valid relation with NER prec:0.3406, rec:0.1480, f1:0.2063
g_step 4100, step 356, avg_time 2.517, loss:513.8737
g_step 4200, step 40, avg_time 1.101, loss:516.4151
g_step 4300, step 140, avg_time 1.115, loss:487.4270
g_step 4400, step 240, avg_time 1.126, loss:486.4805
g_step 4500, step 340, avg_time 1.111, loss:510.5307
>> valid entity prec:0.5670, rec:0.5379, f1:0.5521
>> valid relation prec:0.3263, rec:0.1414, f1:0.1973
>> valid relation with NER prec:0.3263, rec:0.1414, f1:0.1973
g_step 4600, step 24, avg_time 2.519, loss:486.8448
g_step 4700, step 124, avg_time 1.107, loss:461.2379
g_step 4800, step 224, avg_time 1.106, loss:491.4747
g_step 4900, step 324, avg_time 1.121, loss:503.7197
g_step 5000, step 8, avg_time 1.132, loss:496.6409
learning rate was adjusted to 0.0008
>> valid entity prec:0.5842, rec:0.5132, f1:0.5464
>> valid relation prec:0.3011, rec:0.1018, f1:0.1522
>> valid relation with NER prec:0.3011, rec:0.1018, f1:0.1522
g_step 5100, step 108, avg_time 2.499, loss:442.3854
g_step 5200, step 208, avg_time 1.118, loss:483.6657
g_step 5300, step 308, avg_time 1.120, loss:465.7092
g_step 5400, step 408, avg_time 1.120, loss:481.8632
g_step 5500, step 92, avg_time 1.133, loss:423.7796
>> valid entity prec:0.5656, rec:0.5254, f1:0.5448
>> valid relation prec:0.3495, rec:0.1491, f1:0.2090
>> valid relation with NER prec:0.3495, rec:0.1491, f1:0.2090
g_step 5600, step 192, avg_time 2.509, loss:444.6964
g_step 5700, step 292, avg_time 1.119, loss:455.0156
g_step 5800, step 392, avg_time 1.100, loss:457.4420
g_step 5900, step 76, avg_time 1.133, loss:435.6625
g_step 6000, step 176, avg_time 1.104, loss:418.9953
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5075, rec:0.5238, f1:0.5156
>> valid relation prec:0.3011, rec:0.1213, f1:0.1729
>> valid relation with NER prec:0.3011, rec:0.1213, f1:0.1729
g_step 6100, step 276, avg_time 2.521, loss:437.1292
g_step 6200, step 376, avg_time 1.110, loss:445.7378
g_step 6300, step 60, avg_time 1.113, loss:425.8028
g_step 6400, step 160, avg_time 1.119, loss:420.0373
g_step 6500, step 260, avg_time 1.114, loss:418.4201
>> valid entity prec:0.5286, rec:0.5644, f1:0.5459
>> valid relation prec:0.3088, rec:0.1204, f1:0.1733
>> valid relation with NER prec:0.3088, rec:0.1204, f1:0.1733
g_step 6600, step 360, avg_time 2.519, loss:438.2710
g_step 6700, step 44, avg_time 1.117, loss:416.8183
g_step 6800, step 144, avg_time 1.102, loss:392.2736
g_step 6900, step 244, avg_time 1.107, loss:416.6408
g_step 7000, step 344, avg_time 1.131, loss:417.5037
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5658, rec:0.5608, f1:0.5633
>> valid relation prec:0.2899, rec:0.1348, f1:0.1840
>> valid relation with NER prec:0.2899, rec:0.1348, f1:0.1840
g_step 7100, step 28, avg_time 2.528, loss:389.6358
g_step 7200, step 128, avg_time 1.116, loss:368.6845
g_step 7300, step 228, avg_time 1.110, loss:389.7521
g_step 7400, step 328, avg_time 1.114, loss:396.9316
g_step 7500, step 12, avg_time 1.104, loss:413.5274
>> valid entity prec:0.5684, rec:0.5256, f1:0.5461
>> valid relation prec:0.3004, rec:0.1308, f1:0.1822
>> valid relation with NER prec:0.3004, rec:0.1308, f1:0.1822
g_step 7600, step 112, avg_time 2.509, loss:362.6772
g_step 7700, step 212, avg_time 1.113, loss:365.7266
g_step 7800, step 312, avg_time 1.110, loss:383.2795
g_step 7900, step 412, avg_time 1.134, loss:406.0425
g_step 8000, step 96, avg_time 1.122, loss:365.1518
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5347, rec:0.5407, f1:0.5377
>> valid relation prec:0.3262, rec:0.1362, f1:0.1922
>> valid relation with NER prec:0.3262, rec:0.1362, f1:0.1922
g_step 8100, step 196, avg_time 2.521, loss:349.6731
g_step 8200, step 296, avg_time 1.101, loss:367.9772
g_step 8300, step 396, avg_time 1.108, loss:370.6751
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:54:04 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:54:04 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-54-04_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:54:05 - WARNING - datasets.builder -   Using custom data configuration default-bd3a80489ab573f8
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-bd3a80489ab573f8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:54:05,643 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:54:05,644 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:54:05,644 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:54:05,645 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:54:05,663 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:05,666 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:05,666 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:05,666 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:05,666 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:05,666 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:05,666 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:54:05,803 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:54:08,938 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:54:08,990 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-bd3a80489ab573f8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.16ba/s] 20%|██        | 2/10 [00:00<00:02,  3.93ba/s] 30%|███       | 3/10 [00:00<00:01,  4.27ba/s] 40%|████      | 4/10 [00:00<00:01,  4.41ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.49ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.53ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.57ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.58ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.59ba/s]100%|██████████| 10/10 [00:02<00:00,  4.60ba/s]100%|██████████| 10/10 [00:02<00:00,  4.44ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.69ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.03ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.49ba/s]100%|██████████| 4/4 [00:00<00:00,  4.60ba/s]100%|██████████| 4/4 [00:00<00:00,  4.05ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  7.01ba/s] 30%|███       | 3/10 [00:00<00:00,  9.14ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.82ba/s] 70%|███████   | 7/10 [00:00<00:00, 10.09ba/s] 90%|█████████ | 9/10 [00:00<00:00, 10.22ba/s]100%|██████████| 10/10 [00:01<00:00,  9.94ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.76ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  6.06ba/s]100%|██████████| 4/4 [00:00<00:00,  6.66ba/s]
[INFO|trainer.py:414] 2023-08-28 17:54:14,933 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:54:14,983 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:54:14,983 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-28 17:54:14,983 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:54:14,983 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:54:14,983 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:54:14,983 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:54:14,983 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:58,  3.27it/s]  0%|          | 2/780 [00:00<04:01,  3.22it/s]  0%|          | 3/780 [00:00<03:53,  3.33it/s]  1%|          | 4/780 [00:01<03:49,  3.38it/s]  1%|          | 5/780 [00:01<03:46,  3.42it/s]  1%|          | 6/780 [00:01<03:45,  3.43it/s]  1%|          | 7/780 [00:02<03:44,  3.45it/s]  1%|          | 8/780 [00:02<03:43,  3.46it/s]  1%|          | 9/780 [00:02<03:42,  3.46it/s]  1%|▏         | 10/780 [00:02<03:42,  3.47it/s]  1%|▏         | 11/780 [00:03<03:41,  3.47it/s]  2%|▏         | 12/780 [00:03<03:41,  3.47it/s]  2%|▏         | 13/780 [00:03<03:53,  3.29it/s]  2%|▏         | 14/780 [00:04<03:49,  3.34it/s]  2%|▏         | 15/780 [00:04<03:46,  3.38it/s]  2%|▏         | 16/780 [00:04<03:44,  3.41it/s]  2%|▏         | 17/780 [00:04<03:42,  3.43it/s]  2%|▏         | 18/780 [00:05<03:41,  3.44it/s]  2%|▏         | 19/780 [00:05<03:40,  3.45it/s]  3%|▎         | 20/780 [00:05<03:39,  3.46it/s]  3%|▎         | 21/780 [00:06<03:39,  3.46it/s]  3%|▎         | 22/780 [00:06<03:38,  3.46it/s]  3%|▎         | 23/780 [00:06<03:38,  3.47it/s]  3%|▎         | 24/780 [00:07<03:53,  3.24it/s]  3%|▎         | 25/780 [00:07<03:48,  3.31it/s]  3%|▎         | 26/780 [00:07<03:44,  3.36it/s]  3%|▎         | 27/780 [00:07<03:42,  3.39it/s]  4%|▎         | 28/780 [00:08<03:40,  3.41it/s]  4%|▎         | 29/780 [00:08<03:39,  3.42it/s]  4%|▍         | 30/780 [00:08<03:37,  3.44it/s]  4%|▍         | 31/780 [00:09<03:37,  3.45it/s]  4%|▍         | 32/780 [00:09<03:36,  3.46it/s]  4%|▍         | 33/780 [00:09<03:35,  3.46it/s]  4%|▍         | 34/780 [00:09<03:35,  3.46it/s]  4%|▍         | 35/780 [00:10<05:16,  2.36it/s]  5%|▍         | 36/780 [00:10<04:45,  2.61it/s]  5%|▍         | 37/780 [00:11<04:23,  2.82it/s]  5%|▍         | 38/780 [00:11<04:08,  2.99it/s]  5%|▌         | 39/780 [00:11<03:57,  3.12it/s]  5%|▌         | 40/780 [00:12<03:50,  3.22it/s]  5%|▌         | 41/780 [00:12<03:44,  3.29it/s]  5%|▌         | 42/780 [00:12<03:41,  3.34it/s]  6%|▌         | 43/780 [00:13<03:38,  3.38it/s]  6%|▌         | 44/780 [00:13<03:48,  3.22it/s]  6%|▌         | 45/780 [00:13<03:43,  3.29it/s]  6%|▌         | 46/780 [00:13<03:39,  3.34it/s]  6%|▌         | 47/780 [00:14<03:36,  3.38it/s]  6%|▌         | 48/780 [00:14<03:34,  3.41it/s]  6%|▋         | 49/780 [00:14<03:33,  3.42it/s]  6%|▋         | 50/780 [00:15<03:32,  3.44it/s]  7%|▋         | 51/780 [00:15<03:31,  3.44it/s]  7%|▋         | 52/780 [00:15<03:31,  3.45it/s]  7%|▋         | 53/780 [00:15<03:30,  3.45it/s]  7%|▋         | 54/780 [00:16<03:30,  3.45it/s]  7%|▋         | 55/780 [00:16<03:33,  3.40it/s]  7%|▋         | 56/780 [00:16<03:31,  3.42it/s]  7%|▋         | 57/780 [00:17<03:30,  3.43it/s]  7%|▋         | 58/780 [00:17<03:29,  3.44it/s]  8%|▊         | 59/780 [00:17<03:29,  3.45it/s]  8%|▊         | 60/780 [00:17<03:28,  3.45it/s]  8%|▊         | 61/780 [00:18<03:28,  3.46it/s]  8%|▊         | 62/780 [00:18<03:27,  3.46it/s]  8%|▊         | 63/780 [00:18<03:27,  3.45it/s]  8%|▊         | 64/780 [00:19<03:27,  3.46it/s]  8%|▊         | 65/780 [00:19<03:26,  3.46it/s]  8%|▊         | 66/780 [00:20<04:47,  2.48it/s]  9%|▊         | 67/780 [00:20<04:22,  2.71it/s]  9%|▊         | 68/780 [00:20<04:05,  2.90it/s]  9%|▉         | 69/780 [00:20<03:52,  3.05it/s]  9%|▉         | 70/780 [00:21<03:44,  3.16it/s]  9%|▉         | 71/780 [00:21<03:38,  3.25it/s]  9%|▉         | 72/780 [00:21<03:33,  3.31it/s]  9%|▉         | 73/780 [00:22<03:30,  3.36it/s]  9%|▉         | 74/780 [00:22<03:28,  3.39it/s] 10%|▉         | 75/780 [00:22<03:26,  3.41it/s] 10%|▉         | 76/780 [00:23<03:53,  3.01it/s] 10%|▉         | 77/780 [00:23<03:44,  3.14it/s] 10%|█         | 78/780 [00:23<03:51,  3.03it/s] 10%|█         | 79/780 [00:24<03:42,  3.15it/s] 10%|█         | 80/780 [00:24<03:36,  3.24it/s] 10%|█         | 81/780 [00:24<03:31,  3.31it/s] 11%|█         | 82/780 [00:24<03:28,  3.35it/s] 11%|█         | 83/780 [00:25<03:25,  3.39it/s] 11%|█         | 84/780 [00:25<03:24,  3.41it/s] 11%|█         | 85/780 [00:25<03:22,  3.43it/s] 11%|█         | 86/780 [00:26<03:21,  3.44it/s] 11%|█         | 87/780 [00:26<03:21,  3.45it/s] 11%|█▏        | 88/780 [00:26<03:20,  3.45it/s] 11%|█▏        | 89/780 [00:26<03:20,  3.44it/s] 12%|█▏        | 90/780 [00:27<03:20,  3.45it/s] 12%|█▏        | 91/780 [00:27<03:19,  3.45it/s] 12%|█▏        | 92/780 [00:27<03:19,  3.46it/s] 12%|█▏        | 93/780 [00:28<03:18,  3.46it/s] 12%|█▏        | 94/780 [00:28<03:18,  3.46it/s] 12%|█▏        | 95/780 [00:28<03:17,  3.46it/s] 12%|█▏        | 96/780 [00:28<03:17,  3.46it/s] 12%|█▏        | 97/780 [00:29<03:17,  3.46it/s] 13%|█▎        | 98/780 [00:29<03:16,  3.47it/s] 13%|█▎        | 99/780 [00:29<03:16,  3.47it/s] 13%|█▎        | 100/780 [00:30<03:17,  3.44it/s] 13%|█▎        | 101/780 [00:30<03:16,  3.45it/s] 13%|█▎        | 102/780 [00:30<03:16,  3.45it/s] 13%|█▎        | 103/780 [00:30<03:15,  3.45it/s] 13%|█▎        | 104/780 [00:31<03:15,  3.46it/s] 13%|█▎        | 105/780 [00:31<03:15,  3.46it/s] 14%|█▎        | 106/780 [00:31<03:14,  3.46it/s] 14%|█▎        | 107/780 [00:32<03:14,  3.46it/s] 14%|█▍        | 108/780 [00:32<03:14,  3.46it/s] 14%|█▍        | 109/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 110/780 [00:33<03:13,  3.46it/s] 14%|█▍        | 111/780 [00:33<03:22,  3.31it/s] 14%|█▍        | 112/780 [00:33<03:19,  3.35it/s] 14%|█▍        | 113/780 [00:33<03:17,  3.38it/s] 15%|█▍        | 114/780 [00:34<03:15,  3.40it/s] 15%|█▍        | 115/780 [00:34<03:14,  3.42it/s] 15%|█▍        | 116/780 [00:34<03:13,  3.43it/s] 15%|█▌        | 117/780 [00:35<03:12,  3.44it/s] 15%|█▌        | 118/780 [00:35<03:11,  3.45it/s] 15%|█▌        | 119/780 [00:35<03:11,  3.45it/s] 15%|█▌        | 120/780 [00:35<03:10,  3.46it/s] 16%|█▌        | 121/780 [00:36<03:10,  3.46it/s] 16%|█▌        | 122/780 [00:36<03:13,  3.40it/s] 16%|█▌        | 123/780 [00:36<03:12,  3.42it/s] 16%|█▌        | 124/780 [00:37<03:11,  3.43it/s] 16%|█▌        | 125/780 [00:37<03:10,  3.44it/s] 16%|█▌        | 126/780 [00:37<03:09,  3.45it/s] 16%|█▋        | 127/780 [00:37<03:09,  3.45it/s] 16%|█▋        | 128/780 [00:38<03:08,  3.45it/s] 17%|█▋        | 129/780 [00:38<03:08,  3.46it/s] 17%|█▋        | 130/780 [00:38<03:08,  3.46it/s] 17%|█▋        | 131/780 [00:39<03:07,  3.46it/s] 17%|█▋        | 132/780 [00:39<03:07,  3.46it/s] 17%|█▋        | 133/780 [00:39<03:07,  3.44it/s] 17%|█▋        | 134/780 [00:40<03:07,  3.45it/s] 17%|█▋        | 135/780 [00:40<03:06,  3.45it/s] 17%|█▋        | 136/780 [00:40<03:33,  3.02it/s] 18%|█▊        | 137/780 [00:41<03:25,  3.13it/s] 18%|█▊        | 138/780 [00:41<03:24,  3.14it/s] 18%|█▊        | 139/780 [00:41<03:18,  3.22it/s] 18%|█▊        | 140/780 [00:41<03:14,  3.29it/s] 18%|█▊        | 141/780 [00:42<03:11,  3.34it/s] 18%|█▊        | 142/780 [00:42<03:09,  3.37it/s] 18%|█▊        | 143/780 [00:42<03:08,  3.38it/s] 18%|█▊        | 144/780 [00:43<03:06,  3.41it/s] 19%|█▊        | 145/780 [00:43<03:05,  3.42it/s] 19%|█▊        | 146/780 [00:43<03:04,  3.43it/s] 19%|█▉        | 147/780 [00:43<03:04,  3.44it/s] 19%|█▉        | 148/780 [00:44<03:03,  3.44it/s] 19%|█▉        | 149/780 [00:44<03:03,  3.45it/s] 19%|█▉        | 150/780 [00:44<03:02,  3.45it/s] 19%|█▉        | 151/780 [00:45<03:02,  3.45it/s] 19%|█▉        | 152/780 [00:45<03:01,  3.45it/s] 20%|█▉        | 153/780 [00:45<03:01,  3.46it/s] 20%|█▉        | 154/780 [00:45<03:02,  3.44it/s] 20%|█▉        | 155/780 [00:46<03:01,  3.44it/s] 20%|██        | 156/780 [00:46<03:01,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 17:55:01,585 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:55:01,586 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:55:01,586 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.66it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.94it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.15it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.46it/s][A
  6%|▋         | 28/437 [00:00<00:08, 46.96it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.93it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.61it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.62it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.50it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.54it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.56it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.64it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.51it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.59it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.65it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.69it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.70it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.56it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.56it/s][A
 24%|██▎       | 103/437 [00:02<00:08, 41.07it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 42.58it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 43.72it/s][A
 27%|██▋       | 118/437 [00:02<00:07, 44.58it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 45.20it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 45.52it/s][A
 30%|███       | 133/437 [00:02<00:06, 45.88it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.11it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.16it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.29it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.40it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.42it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.48it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.52it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.65it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.67it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.52it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.63it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.65it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.56it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.65it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.65it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.59it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.60it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.60it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.45it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.46it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.54it/s][A
 56%|█████▌    | 243/437 [00:05<00:05, 33.15it/s][A
 57%|█████▋    | 248/437 [00:05<00:05, 36.28it/s][A
 58%|█████▊    | 253/437 [00:05<00:04, 38.79it/s][A
 59%|█████▉    | 258/437 [00:05<00:04, 40.89it/s][A
 60%|██████    | 263/437 [00:05<00:04, 42.47it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 43.65it/s][A
 62%|██████▏   | 273/437 [00:06<00:03, 44.48it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 45.11it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 45.42it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 45.76it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.04it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.18it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.31it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.41it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.43it/s][A
 73%|███████▎  | 318/437 [00:07<00:02, 46.54it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 46.58it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.55it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.47it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.55it/s][A
 78%|███████▊  | 343/437 [00:08<00:02, 46.58it/s][A
 80%|███████▉  | 348/437 [00:08<00:07, 11.39it/s][A
 81%|████████  | 353/437 [00:08<00:05, 14.73it/s][A
 82%|████████▏ | 358/437 [00:08<00:04, 18.54it/s][A
 83%|████████▎ | 363/437 [00:09<00:03, 22.63it/s][A
 84%|████████▍ | 368/437 [00:09<00:02, 26.76it/s][A
 85%|████████▌ | 373/437 [00:09<00:02, 30.69it/s][A
 86%|████████▋ | 378/437 [00:09<00:01, 34.18it/s][A
 88%|████████▊ | 383/437 [00:09<00:01, 37.17it/s][A
 89%|████████▉ | 388/437 [00:09<00:01, 39.44it/s][A
 90%|████████▉ | 393/437 [00:09<00:01, 41.41it/s][A
 91%|█████████ | 398/437 [00:09<00:00, 42.89it/s][A
 92%|█████████▏| 403/437 [00:09<00:00, 43.92it/s][A
 93%|█████████▎| 408/437 [00:10<00:00, 44.62it/s][A
 95%|█████████▍| 413/437 [00:10<00:00, 45.15it/s][A
 96%|█████████▌| 418/437 [00:10<00:00, 45.56it/s][A
 97%|█████████▋| 423/437 [00:10<00:00, 45.84it/s][A
 98%|█████████▊| 428/437 [00:10<00:00, 45.98it/s][A
 99%|█████████▉| 433/437 [00:10<00:00, 45.94it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 45.94it/s][A 20%|██        | 156/780 [00:57<03:01,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:55:12,618 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 17:55:12,667 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:55:19,244 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:55:19,613 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:55:19,667 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:16<1:35:25,  9.19s/it] 20%|██        | 158/780 [01:16<1:07:45,  6.54s/it] 20%|██        | 159/780 [01:17<48:15,  4.66s/it]   21%|██        | 160/780 [01:17<34:36,  3.35s/it] 21%|██        | 161/780 [01:17<25:05,  2.43s/it] 21%|██        | 162/780 [01:18<18:25,  1.79s/it] 21%|██        | 163/780 [01:18<13:45,  1.34s/it] 21%|██        | 164/780 [01:18<10:30,  1.02s/it] 21%|██        | 165/780 [01:18<08:13,  1.25it/s] 21%|██▏       | 166/780 [01:19<06:38,  1.54it/s] 21%|██▏       | 167/780 [01:19<05:31,  1.85it/s] 22%|██▏       | 168/780 [01:19<04:44,  2.15it/s] 22%|██▏       | 169/780 [01:20<04:21,  2.34it/s] 22%|██▏       | 170/780 [01:20<03:55,  2.59it/s] 22%|██▏       | 171/780 [01:20<03:37,  2.80it/s] 22%|██▏       | 172/780 [01:20<03:24,  2.97it/s] 22%|██▏       | 173/780 [01:21<03:15,  3.11it/s] 22%|██▏       | 174/780 [01:21<03:08,  3.21it/s] 22%|██▏       | 175/780 [01:21<03:04,  3.28it/s] 23%|██▎       | 176/780 [01:22<03:01,  3.33it/s] 23%|██▎       | 177/780 [01:22<02:58,  3.37it/s] 23%|██▎       | 178/780 [01:22<02:57,  3.40it/s] 23%|██▎       | 179/780 [01:22<02:56,  3.41it/s] 23%|██▎       | 180/780 [01:23<03:04,  3.25it/s] 23%|██▎       | 181/780 [01:23<03:01,  3.31it/s] 23%|██▎       | 182/780 [01:23<02:58,  3.35it/s] 23%|██▎       | 183/780 [01:24<02:56,  3.39it/s] 24%|██▎       | 184/780 [01:24<02:54,  3.41it/s] 24%|██▎       | 185/780 [01:24<02:53,  3.43it/s] 24%|██▍       | 186/780 [01:25<02:52,  3.44it/s] 24%|██▍       | 187/780 [01:25<02:52,  3.45it/s] 24%|██▍       | 188/780 [01:25<02:51,  3.45it/s] 24%|██▍       | 189/780 [01:25<02:50,  3.46it/s] 24%|██▍       | 190/780 [01:26<02:50,  3.46it/s] 24%|██▍       | 191/780 [01:26<03:10,  3.10it/s] 25%|██▍       | 192/780 [01:26<03:03,  3.20it/s] 25%|██▍       | 193/780 [01:27<02:59,  3.27it/s] 25%|██▍       | 194/780 [01:27<02:56,  3.32it/s] 25%|██▌       | 195/780 [01:27<02:53,  3.36it/s] 25%|██▌       | 196/780 [01:28<02:52,  3.39it/s] 25%|██▌       | 197/780 [01:28<02:50,  3.41it/s] 25%|██▌       | 198/780 [01:28<02:49,  3.43it/s] 26%|██▌       | 199/780 [01:28<02:49,  3.44it/s] 26%|██▌       | 200/780 [01:29<02:48,  3.45it/s] 26%|██▌       | 201/780 [01:29<02:54,  3.32it/s] 26%|██▌       | 202/780 [01:29<02:51,  3.36it/s] 26%|██▌       | 203/780 [01:30<02:49,  3.40it/s] 26%|██▌       | 204/780 [01:30<02:48,  3.41it/s] 26%|██▋       | 205/780 [01:30<02:47,  3.43it/s] 26%|██▋       | 206/780 [01:30<02:46,  3.44it/s] 27%|██▋       | 207/780 [01:31<02:46,  3.45it/s] 27%|██▋       | 208/780 [01:31<02:45,  3.45it/s] 27%|██▋       | 209/780 [01:31<02:45,  3.45it/s] 27%|██▋       | 210/780 [01:32<02:44,  3.46it/s] 27%|██▋       | 211/780 [01:32<02:44,  3.46it/s] 27%|██▋       | 212/780 [01:32<02:43,  3.46it/s] 27%|██▋       | 213/780 [01:32<02:43,  3.46it/s] 27%|██▋       | 214/780 [01:33<03:01,  3.12it/s] 28%|██▊       | 215/780 [01:33<02:55,  3.21it/s] 28%|██▊       | 216/780 [01:33<02:51,  3.29it/s] 28%|██▊       | 217/780 [01:34<02:48,  3.33it/s] 28%|██▊       | 218/780 [01:34<02:46,  3.37it/s] 28%|██▊       | 219/780 [01:34<02:45,  3.40it/s] 28%|██▊       | 220/780 [01:35<02:43,  3.42it/s] 28%|██▊       | 221/780 [01:35<02:42,  3.43it/s] 28%|██▊       | 222/780 [01:35<02:42,  3.44it/s] 29%|██▊       | 223/780 [01:35<02:41,  3.45it/s] 29%|██▊       | 224/780 [01:36<02:41,  3.45it/s] 29%|██▉       | 225/780 [01:36<02:42,  3.42it/s] 29%|██▉       | 226/780 [01:36<02:41,  3.44it/s] 29%|██▉       | 227/780 [01:37<02:40,  3.44it/s] 29%|██▉       | 228/780 [01:37<02:40,  3.45it/s] 29%|██▉       | 229/780 [01:37<02:39,  3.45it/s] 29%|██▉       | 230/780 [01:38<02:39,  3.45it/s] 30%|██▉       | 231/780 [01:38<02:38,  3.45it/s] 30%|██▉       | 232/780 [01:38<02:38,  3.46it/s] 30%|██▉       | 233/780 [01:38<02:38,  3.46it/s] 30%|███       | 234/780 [01:39<02:37,  3.46it/s] 30%|███       | 235/780 [01:39<02:37,  3.46it/s] 30%|███       | 236/780 [01:39<02:39,  3.42it/s] 30%|███       | 237/780 [01:40<02:38,  3.43it/s] 31%|███       | 238/780 [01:40<02:37,  3.44it/s] 31%|███       | 239/780 [01:41<04:54,  1.84it/s] 31%|███       | 240/780 [01:41<04:13,  2.13it/s] 31%|███       | 241/780 [01:42<03:43,  2.41it/s] 31%|███       | 242/780 [01:42<03:22,  2.65it/s] 31%|███       | 243/780 [01:42<03:08,  2.86it/s] 31%|███▏      | 244/780 [01:42<02:58,  3.00it/s] 31%|███▏      | 245/780 [01:43<02:51,  3.13it/s] 32%|███▏      | 246/780 [01:43<02:45,  3.22it/s] 32%|███▏      | 247/780 [01:43<02:42,  3.29it/s] 32%|███▏      | 248/780 [01:44<02:39,  3.34it/s] 32%|███▏      | 249/780 [01:44<02:37,  3.38it/s] 32%|███▏      | 250/780 [01:44<02:35,  3.40it/s] 32%|███▏      | 251/780 [01:44<02:34,  3.42it/s] 32%|███▏      | 252/780 [01:45<02:33,  3.43it/s] 32%|███▏      | 253/780 [01:45<02:33,  3.44it/s] 33%|███▎      | 254/780 [01:45<02:32,  3.45it/s] 33%|███▎      | 255/780 [01:46<02:33,  3.42it/s] 33%|███▎      | 256/780 [01:46<02:32,  3.44it/s] 33%|███▎      | 257/780 [01:46<02:31,  3.44it/s] 33%|███▎      | 258/780 [01:46<02:31,  3.45it/s] 33%|███▎      | 259/780 [01:47<02:30,  3.45it/s] 33%|███▎      | 260/780 [01:47<02:30,  3.46it/s] 33%|███▎      | 261/780 [01:47<02:30,  3.46it/s] 34%|███▎      | 262/780 [01:48<02:29,  3.45it/s] 34%|███▎      | 263/780 [01:48<02:29,  3.46it/s] 34%|███▍      | 264/780 [01:48<02:29,  3.46it/s] 34%|███▍      | 265/780 [01:48<02:28,  3.46it/s] 34%|███▍      | 266/780 [01:49<02:33,  3.34it/s] 34%|███▍      | 267/780 [01:49<02:31,  3.38it/s] 34%|███▍      | 268/780 [01:49<02:30,  3.40it/s] 34%|███▍      | 269/780 [01:50<02:29,  3.42it/s] 35%|███▍      | 270/780 [01:50<02:28,  3.43it/s] 35%|███▍      | 271/780 [01:50<02:27,  3.44it/s] 35%|███▍      | 272/780 [01:51<02:27,  3.45it/s] 35%|███▌      | 273/780 [01:51<02:26,  3.45it/s] 35%|███▌      | 274/780 [01:51<02:26,  3.45it/s] 35%|███▌      | 275/780 [01:51<02:26,  3.45it/s] 35%|███▌      | 276/780 [01:52<02:25,  3.46it/s] 36%|███▌      | 277/780 [01:52<02:28,  3.40it/s] 36%|███▌      | 278/780 [01:52<02:26,  3.42it/s] 36%|███▌      | 279/780 [01:53<02:26,  3.43it/s] 36%|███▌      | 280/780 [01:53<02:25,  3.44it/s] 36%|███▌      | 281/780 [01:53<02:24,  3.45it/s] 36%|███▌      | 282/780 [01:53<02:24,  3.45it/s] 36%|███▋      | 283/780 [01:54<02:23,  3.45it/s] 36%|███▋      | 284/780 [01:54<02:23,  3.46it/s] 37%|███▋      | 285/780 [01:54<02:23,  3.46it/s] 37%|███▋      | 286/780 [01:55<02:22,  3.46it/s] 37%|███▋      | 287/780 [01:55<02:22,  3.46it/s] 37%|███▋      | 288/780 [01:55<02:22,  3.44it/s] 37%|███▋      | 289/780 [01:55<02:22,  3.45it/s] 37%|███▋      | 290/780 [01:56<02:21,  3.45it/s] 37%|███▋      | 291/780 [01:56<02:21,  3.46it/s] 37%|███▋      | 292/780 [01:56<02:21,  3.46it/s] 38%|███▊      | 293/780 [01:57<02:20,  3.46it/s] 38%|███▊      | 294/780 [01:57<02:20,  3.46it/s] 38%|███▊      | 295/780 [01:57<02:20,  3.46it/s] 38%|███▊      | 296/780 [01:58<02:19,  3.46it/s] 38%|███▊      | 297/780 [01:58<02:19,  3.46it/s] 38%|███▊      | 298/780 [01:58<02:19,  3.46it/s] 38%|███▊      | 299/780 [01:58<02:20,  3.43it/s] 38%|███▊      | 300/780 [01:59<02:19,  3.44it/s] 39%|███▊      | 301/780 [01:59<02:19,  3.44it/s] 39%|███▊      | 302/780 [01:59<02:18,  3.45it/s] 39%|███▉      | 303/780 [02:00<02:18,  3.45it/s] 39%|███▉      | 304/780 [02:00<02:17,  3.46it/s] 39%|███▉      | 305/780 [02:00<02:17,  3.46it/s] 39%|███▉      | 306/780 [02:00<02:17,  3.46it/s] 39%|███▉      | 307/780 [02:01<02:16,  3.46it/s] 39%|███▉      | 308/780 [02:01<02:16,  3.46it/s] 40%|███▉      | 309/780 [02:01<02:16,  3.46it/s] 40%|███▉      | 310/780 [02:02<02:15,  3.46it/s] 40%|███▉      | 311/780 [02:02<02:15,  3.46it/s] 40%|████      | 312/780 [02:02<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 17:56:17,676 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:56:17,676 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:56:17,676 >>   Batch size = 8
{'eval_loss': 0.9312083721160889, 'eval_runtime': 10.7455, 'eval_samples_per_second': 324.694, 'eval_steps_per_second': 40.668, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.86it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.50it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.66it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.96it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.37it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.09it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.94it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.55it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.54it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 45.52it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 45.83it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 45.99it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 46.10it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.10it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.10it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.08it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.02it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.14it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.22it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.32it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.34it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.49it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.50it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.53it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.41it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.41it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.34it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.35it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.43it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.48it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.51it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.53it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.49it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.47it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.40it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.36it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.30it/s][A
 44%|████▍     | 193/437 [00:04<00:07, 34.07it/s][A
 45%|████▌     | 198/437 [00:04<00:06, 37.01it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 39.40it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 41.34it/s][A
 49%|████▊     | 213/437 [00:04<00:05, 42.78it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 43.92it/s][A
 51%|█████     | 223/437 [00:04<00:04, 44.73it/s][A
 52%|█████▏    | 228/437 [00:05<00:04, 45.24it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 45.31it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 45.64it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 45.98it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.20it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.24it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.40it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.41it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.43it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.55it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.49it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.48it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.59it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.46it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.51it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.56it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.47it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.50it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.52it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 46.49it/s][A
 75%|███████▌  | 328/437 [00:07<00:04, 23.02it/s][A
 76%|███████▌  | 333/437 [00:07<00:03, 27.14it/s][A
 77%|███████▋  | 338/437 [00:07<00:03, 31.02it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 34.47it/s][A
 80%|███████▉  | 348/437 [00:07<00:02, 37.13it/s][A
 81%|████████  | 353/437 [00:08<00:02, 39.65it/s][A
 82%|████████▏ | 358/437 [00:08<00:01, 41.54it/s][A
 83%|████████▎ | 363/437 [00:08<00:01, 42.94it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 43.74it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 44.56it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 45.12it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 45.61it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 45.96it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.10it/s][A
 91%|█████████ | 398/437 [00:09<00:00, 46.17it/s][A
 92%|█████████▏| 403/437 [00:09<00:00, 46.17it/s][A
 93%|█████████▎| 408/437 [00:09<00:00, 46.22it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 46.10it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.21it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.27it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.39it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.44it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.44it/s][A 40%|████      | 312/780 [02:12<02:15,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:56:27,717 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 17:56:27,969 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:56:31,509 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:56:31,857 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:56:31,913 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:24<52:06,  6.70s/it] 40%|████      | 314/780 [02:24<37:04,  4.77s/it] 40%|████      | 315/780 [02:24<26:34,  3.43s/it] 41%|████      | 316/780 [02:25<19:13,  2.49s/it] 41%|████      | 317/780 [02:25<14:05,  1.83s/it] 41%|████      | 318/780 [02:25<10:30,  1.37s/it] 41%|████      | 319/780 [02:26<08:00,  1.04s/it] 41%|████      | 320/780 [02:26<06:15,  1.23it/s] 41%|████      | 321/780 [02:26<05:01,  1.52it/s] 41%|████▏     | 322/780 [02:26<04:10,  1.83it/s] 41%|████▏     | 323/780 [02:27<03:34,  2.13it/s] 42%|████▏     | 324/780 [02:27<03:09,  2.41it/s] 42%|████▏     | 325/780 [02:27<02:51,  2.65it/s] 42%|████▏     | 326/780 [02:28<02:39,  2.85it/s] 42%|████▏     | 327/780 [02:28<02:30,  3.01it/s] 42%|████▏     | 328/780 [02:28<02:24,  3.14it/s] 42%|████▏     | 329/780 [02:28<02:19,  3.23it/s] 42%|████▏     | 330/780 [02:29<02:16,  3.30it/s] 42%|████▏     | 331/780 [02:29<02:14,  3.35it/s] 43%|████▎     | 332/780 [02:29<02:12,  3.38it/s] 43%|████▎     | 333/780 [02:30<02:11,  3.40it/s] 43%|████▎     | 334/780 [02:30<02:10,  3.42it/s] 43%|████▎     | 335/780 [02:30<02:09,  3.44it/s] 43%|████▎     | 336/780 [02:30<02:09,  3.43it/s] 43%|████▎     | 337/780 [02:31<02:08,  3.44it/s] 43%|████▎     | 338/780 [02:31<02:08,  3.45it/s] 43%|████▎     | 339/780 [02:31<02:07,  3.45it/s] 44%|████▎     | 340/780 [02:32<02:07,  3.46it/s] 44%|████▎     | 341/780 [02:32<02:06,  3.46it/s] 44%|████▍     | 342/780 [02:32<02:06,  3.46it/s] 44%|████▍     | 343/780 [02:32<02:06,  3.46it/s] 44%|████▍     | 344/780 [02:33<02:05,  3.47it/s] 44%|████▍     | 345/780 [02:33<02:05,  3.47it/s] 44%|████▍     | 346/780 [02:33<02:05,  3.47it/s] 44%|████▍     | 347/780 [02:34<02:04,  3.47it/s] 45%|████▍     | 348/780 [02:34<02:04,  3.47it/s] 45%|████▍     | 349/780 [02:34<02:04,  3.47it/s] 45%|████▍     | 350/780 [02:35<02:08,  3.35it/s] 45%|████▌     | 351/780 [02:35<02:06,  3.38it/s] 45%|████▌     | 352/780 [02:35<02:05,  3.41it/s] 45%|████▌     | 353/780 [02:35<02:04,  3.42it/s] 45%|████▌     | 354/780 [02:36<02:04,  3.43it/s] 46%|████▌     | 355/780 [02:36<02:03,  3.44it/s] 46%|████▌     | 356/780 [02:36<02:03,  3.44it/s] 46%|████▌     | 357/780 [02:37<02:02,  3.44it/s] 46%|████▌     | 358/780 [02:37<02:02,  3.45it/s] 46%|████▌     | 359/780 [02:37<02:02,  3.45it/s] 46%|████▌     | 360/780 [02:37<02:01,  3.45it/s] 46%|████▋     | 361/780 [02:38<02:10,  3.21it/s] 46%|████▋     | 362/780 [02:38<02:16,  3.07it/s] 47%|████▋     | 363/780 [02:38<02:11,  3.17it/s] 47%|████▋     | 364/780 [02:39<02:07,  3.25it/s] 47%|████▋     | 365/780 [02:39<02:05,  3.31it/s] 47%|████▋     | 366/780 [02:39<02:03,  3.36it/s] 47%|████▋     | 367/780 [02:40<02:01,  3.39it/s] 47%|████▋     | 368/780 [02:40<02:00,  3.41it/s] 47%|████▋     | 369/780 [02:41<04:34,  1.50it/s] 47%|████▋     | 370/780 [02:42<03:53,  1.76it/s] 48%|████▊     | 371/780 [02:42<03:18,  2.06it/s] 48%|████▊     | 372/780 [02:42<02:53,  2.35it/s] 48%|████▊     | 373/780 [02:43<02:36,  2.60it/s] 48%|████▊     | 374/780 [02:43<02:24,  2.81it/s] 48%|████▊     | 375/780 [02:43<02:16,  2.98it/s] 48%|████▊     | 376/780 [02:43<02:10,  3.11it/s] 48%|████▊     | 377/780 [02:44<02:05,  3.21it/s] 48%|████▊     | 378/780 [02:44<02:02,  3.28it/s] 49%|████▊     | 379/780 [02:44<02:00,  3.33it/s] 49%|████▊     | 380/780 [02:45<01:58,  3.37it/s] 49%|████▉     | 381/780 [02:45<01:57,  3.39it/s] 49%|████▉     | 382/780 [02:45<01:56,  3.41it/s] 49%|████▉     | 383/780 [02:45<01:55,  3.42it/s] 49%|████▉     | 384/780 [02:46<01:55,  3.43it/s] 49%|████▉     | 385/780 [02:46<01:54,  3.44it/s] 49%|████▉     | 386/780 [02:46<01:54,  3.45it/s] 50%|████▉     | 387/780 [02:47<01:53,  3.45it/s] 50%|████▉     | 388/780 [02:47<01:53,  3.45it/s] 50%|████▉     | 389/780 [02:47<01:53,  3.45it/s] 50%|█████     | 390/780 [02:48<01:52,  3.46it/s] 50%|█████     | 391/780 [02:48<01:52,  3.46it/s] 50%|█████     | 392/780 [02:48<01:53,  3.42it/s] 50%|█████     | 393/780 [02:48<01:52,  3.43it/s] 51%|█████     | 394/780 [02:49<01:52,  3.43it/s] 51%|█████     | 395/780 [02:49<01:51,  3.44it/s] 51%|█████     | 396/780 [02:49<01:51,  3.44it/s] 51%|█████     | 397/780 [02:50<01:50,  3.45it/s] 51%|█████     | 398/780 [02:50<01:50,  3.45it/s] 51%|█████     | 399/780 [02:50<01:50,  3.46it/s] 51%|█████▏    | 400/780 [02:50<01:49,  3.46it/s] 51%|█████▏    | 401/780 [02:51<01:49,  3.46it/s] 52%|█████▏    | 402/780 [02:51<01:49,  3.46it/s] 52%|█████▏    | 403/780 [02:51<01:58,  3.17it/s] 52%|█████▏    | 404/780 [02:52<01:55,  3.25it/s] 52%|█████▏    | 405/780 [02:52<01:53,  3.31it/s] 52%|█████▏    | 406/780 [02:52<01:51,  3.35it/s] 52%|█████▏    | 407/780 [02:53<01:50,  3.39it/s] 52%|█████▏    | 408/780 [02:53<01:49,  3.41it/s] 52%|█████▏    | 409/780 [02:53<01:48,  3.42it/s] 53%|█████▎    | 410/780 [02:53<01:47,  3.43it/s] 53%|█████▎    | 411/780 [02:54<01:47,  3.44it/s] 53%|█████▎    | 412/780 [02:54<01:46,  3.45it/s] 53%|█████▎    | 413/780 [02:54<01:46,  3.45it/s] 53%|█████▎    | 414/780 [02:55<01:47,  3.42it/s] 53%|█████▎    | 415/780 [02:55<01:46,  3.43it/s] 53%|█████▎    | 416/780 [02:55<01:45,  3.44it/s] 53%|█████▎    | 417/780 [02:55<01:45,  3.44it/s] 54%|█████▎    | 418/780 [02:56<01:44,  3.45it/s] 54%|█████▎    | 419/780 [02:56<01:44,  3.45it/s] 54%|█████▍    | 420/780 [02:56<01:44,  3.45it/s] 54%|█████▍    | 421/780 [02:57<01:43,  3.45it/s] 54%|█████▍    | 422/780 [02:57<01:43,  3.46it/s] 54%|█████▍    | 423/780 [02:57<01:43,  3.46it/s] 54%|█████▍    | 424/780 [02:57<01:42,  3.46it/s] 54%|█████▍    | 425/780 [02:58<01:44,  3.38it/s] 55%|█████▍    | 426/780 [02:58<01:43,  3.41it/s] 55%|█████▍    | 427/780 [02:58<01:43,  3.42it/s] 55%|█████▍    | 428/780 [02:59<01:42,  3.43it/s] 55%|█████▌    | 429/780 [02:59<01:41,  3.44it/s] 55%|█████▌    | 430/780 [02:59<01:41,  3.45it/s] 55%|█████▌    | 431/780 [02:59<01:41,  3.45it/s] 55%|█████▌    | 432/780 [03:00<01:40,  3.45it/s] 56%|█████▌    | 433/780 [03:00<01:40,  3.45it/s] 56%|█████▌    | 434/780 [03:00<01:40,  3.46it/s] 56%|█████▌    | 435/780 [03:01<01:39,  3.46it/s] 56%|█████▌    | 436/780 [03:01<01:39,  3.45it/s] 56%|█████▌    | 437/780 [03:01<01:39,  3.45it/s] 56%|█████▌    | 438/780 [03:02<01:39,  3.45it/s] 56%|█████▋    | 439/780 [03:02<01:38,  3.46it/s] 56%|█████▋    | 440/780 [03:02<01:38,  3.46it/s] 57%|█████▋    | 441/780 [03:02<01:38,  3.45it/s] 57%|█████▋    | 442/780 [03:03<01:37,  3.45it/s] 57%|█████▋    | 443/780 [03:03<01:37,  3.46it/s] 57%|█████▋    | 444/780 [03:03<01:37,  3.45it/s] 57%|█████▋    | 445/780 [03:04<01:36,  3.45it/s] 57%|█████▋    | 446/780 [03:04<01:36,  3.45it/s] 57%|█████▋    | 447/780 [03:04<01:36,  3.45it/s] 57%|█████▋    | 448/780 [03:04<01:36,  3.45it/s] 58%|█████▊    | 449/780 [03:05<01:35,  3.45it/s] 58%|█████▊    | 450/780 [03:05<01:42,  3.23it/s] 58%|█████▊    | 451/780 [03:05<01:39,  3.29it/s] 58%|█████▊    | 452/780 [03:06<01:38,  3.34it/s] 58%|█████▊    | 453/780 [03:06<01:36,  3.38it/s] 58%|█████▊    | 454/780 [03:06<01:35,  3.40it/s] 58%|█████▊    | 455/780 [03:07<01:35,  3.42it/s] 58%|█████▊    | 456/780 [03:07<01:34,  3.43it/s] 59%|█████▊    | 457/780 [03:07<01:33,  3.44it/s] 59%|█████▊    | 458/780 [03:07<01:33,  3.44it/s] 59%|█████▉    | 459/780 [03:08<01:33,  3.45it/s] 59%|█████▉    | 460/780 [03:08<01:32,  3.45it/s] 59%|█████▉    | 461/780 [03:08<01:34,  3.36it/s] 59%|█████▉    | 462/780 [03:09<01:33,  3.39it/s] 59%|█████▉    | 463/780 [03:09<01:32,  3.41it/s] 59%|█████▉    | 464/780 [03:09<01:32,  3.42it/s] 60%|█████▉    | 465/780 [03:09<01:31,  3.43it/s] 60%|█████▉    | 466/780 [03:10<01:31,  3.43it/s] 60%|█████▉    | 467/780 [03:10<01:31,  3.44it/s] 60%|██████    | 468/780 [03:10<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 17:57:25,832 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:57:25,832 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:57:25,832 >>   Batch size = 8
{'eval_loss': 0.9389128088951111, 'eval_runtime': 9.9287, 'eval_samples_per_second': 351.404, 'eval_steps_per_second': 44.014, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.67it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.16it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.22it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.62it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.26it/s][A
  8%|▊         | 33/437 [00:00<00:14, 28.57it/s][A
  9%|▊         | 38/437 [00:01<00:12, 32.49it/s][A
 10%|▉         | 43/437 [00:01<00:10, 35.84it/s][A
 11%|█         | 48/437 [00:01<00:10, 38.58it/s][A
 12%|█▏        | 53/437 [00:01<00:09, 40.68it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 42.22it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 43.36it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 44.25it/s][A
 17%|█▋        | 73/437 [00:01<00:08, 44.67it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 45.07it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 45.50it/s][A
 20%|██        | 88/437 [00:02<00:07, 45.78it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 45.98it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.02it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.20it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.21it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.32it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.36it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.36it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.32it/s][A
 30%|███       | 133/437 [00:03<00:06, 46.39it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 46.28it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.39it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.30it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.18it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.18it/s][A
 37%|███▋      | 163/437 [00:03<00:06, 43.53it/s][A
 38%|███▊      | 168/437 [00:03<00:06, 44.28it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 44.75it/s][A
 41%|████      | 178/437 [00:04<00:05, 45.14it/s][A
 42%|████▏     | 183/437 [00:04<00:05, 45.38it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 45.57it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 45.82it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.00it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.10it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.15it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.21it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.29it/s][A
 51%|█████     | 223/437 [00:05<00:04, 46.37it/s][A
 52%|█████▏    | 228/437 [00:05<00:04, 46.34it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.27it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.36it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.30it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.19it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.27it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.31it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.42it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.45it/s][A
 62%|██████▏   | 273/437 [00:06<00:03, 46.43it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.32it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.39it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.35it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.30it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.27it/s][A
 69%|██████▉   | 303/437 [00:06<00:03, 42.90it/s][A
 70%|███████   | 308/437 [00:06<00:02, 43.90it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 44.66it/s][A
 73%|███████▎  | 318/437 [00:07<00:02, 45.12it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 45.60it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 45.90it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.08it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.20it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 45.83it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 45.97it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.11it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.22it/s][A
 83%|████████▎ | 363/437 [00:08<00:01, 46.31it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 46.32it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.39it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.42it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.33it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.28it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.14it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.14it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.21it/s][A
 93%|█████████▎| 408/437 [00:09<00:00, 46.35it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 46.39it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.43it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.36it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.46it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.37it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.37it/s][A 60%|██████    | 468/780 [03:20<01:30,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:57:36,121 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 17:57:36,974 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:57:42,548 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:57:42,753 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:57:42,797 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:38<43:35,  8.41s/it] 60%|██████    | 470/780 [03:38<30:51,  5.97s/it] 60%|██████    | 471/780 [03:38<22:10,  4.31s/it] 61%|██████    | 472/780 [03:39<15:55,  3.10s/it] 61%|██████    | 473/780 [03:39<11:33,  2.26s/it] 61%|██████    | 474/780 [03:39<08:30,  1.67s/it] 61%|██████    | 475/780 [03:40<06:22,  1.25s/it] 61%|██████    | 476/780 [03:40<04:53,  1.04it/s] 61%|██████    | 477/780 [03:40<03:50,  1.31it/s] 61%|██████▏   | 478/780 [03:40<03:14,  1.56it/s] 61%|██████▏   | 479/780 [03:41<02:41,  1.86it/s] 62%|██████▏   | 480/780 [03:43<05:12,  1.04s/it] 62%|██████▏   | 481/780 [03:43<04:05,  1.22it/s] 62%|██████▏   | 482/780 [03:44<03:16,  1.51it/s] 62%|██████▏   | 483/780 [03:44<02:43,  1.82it/s] 62%|██████▏   | 484/780 [03:44<02:20,  2.10it/s] 62%|██████▏   | 485/780 [03:44<02:03,  2.39it/s] 62%|██████▏   | 486/780 [03:45<01:51,  2.63it/s] 62%|██████▏   | 487/780 [03:45<01:43,  2.84it/s] 63%|██████▎   | 488/780 [03:45<01:37,  3.00it/s] 63%|██████▎   | 489/780 [03:46<01:32,  3.13it/s] 63%|██████▎   | 490/780 [03:46<01:29,  3.23it/s] 63%|██████▎   | 491/780 [03:46<01:27,  3.30it/s] 63%|██████▎   | 492/780 [03:46<01:26,  3.35it/s] 63%|██████▎   | 493/780 [03:47<01:24,  3.38it/s] 63%|██████▎   | 494/780 [03:47<01:23,  3.41it/s] 63%|██████▎   | 495/780 [03:47<01:23,  3.39it/s] 64%|██████▎   | 496/780 [03:48<01:23,  3.41it/s] 64%|██████▎   | 497/780 [03:48<01:22,  3.43it/s] 64%|██████▍   | 498/780 [03:48<01:21,  3.44it/s] 64%|██████▍   | 499/780 [03:48<01:21,  3.45it/s] 64%|██████▍   | 500/780 [03:49<01:21,  3.46it/s]                                                  64%|██████▍   | 500/780 [03:49<01:21,  3.46it/s] 64%|██████▍   | 501/780 [03:49<01:20,  3.46it/s] 64%|██████▍   | 502/780 [03:49<01:20,  3.46it/s] 64%|██████▍   | 503/780 [03:50<01:20,  3.46it/s] 65%|██████▍   | 504/780 [03:50<01:19,  3.46it/s] 65%|██████▍   | 505/780 [03:50<01:19,  3.46it/s] 65%|██████▍   | 506/780 [03:51<01:30,  3.03it/s] 65%|██████▌   | 507/780 [03:51<01:26,  3.15it/s] 65%|██████▌   | 508/780 [03:51<01:24,  3.24it/s] 65%|██████▌   | 509/780 [03:52<01:22,  3.30it/s] 65%|██████▌   | 510/780 [03:52<01:20,  3.35it/s] 66%|██████▌   | 511/780 [03:52<01:19,  3.38it/s] 66%|██████▌   | 512/780 [03:52<01:18,  3.41it/s] 66%|██████▌   | 513/780 [03:53<01:18,  3.42it/s] 66%|██████▌   | 514/780 [03:53<01:17,  3.43it/s] 66%|██████▌   | 515/780 [03:53<01:17,  3.44it/s] 66%|██████▌   | 516/780 [03:54<01:21,  3.26it/s] 66%|██████▋   | 517/780 [03:54<01:19,  3.32it/s] 66%|██████▋   | 518/780 [03:54<01:18,  3.35it/s] 67%|██████▋   | 519/780 [03:54<01:17,  3.38it/s] 67%|██████▋   | 520/780 [03:55<01:16,  3.41it/s] 67%|██████▋   | 521/780 [03:55<01:15,  3.42it/s] 67%|██████▋   | 522/780 [03:55<01:15,  3.43it/s] 67%|██████▋   | 523/780 [03:56<01:14,  3.44it/s] 67%|██████▋   | 524/780 [03:56<01:14,  3.45it/s] 67%|██████▋   | 525/780 [03:56<01:13,  3.45it/s] 67%|██████▋   | 526/780 [03:56<01:13,  3.45it/s] 68%|██████▊   | 527/780 [03:57<01:14,  3.39it/s] 68%|██████▊   | 528/780 [03:57<01:13,  3.41it/s] 68%|██████▊   | 529/780 [03:57<01:13,  3.43it/s] 68%|██████▊   | 530/780 [03:58<01:12,  3.44it/s] 68%|██████▊   | 531/780 [03:58<01:12,  3.44it/s] 68%|██████▊   | 532/780 [03:58<01:11,  3.45it/s] 68%|██████▊   | 533/780 [03:59<01:11,  3.45it/s] 68%|██████▊   | 534/780 [03:59<01:11,  3.45it/s] 69%|██████▊   | 535/780 [03:59<01:10,  3.45it/s] 69%|██████▊   | 536/780 [03:59<01:10,  3.46it/s] 69%|██████▉   | 537/780 [04:00<01:10,  3.46it/s] 69%|██████▉   | 538/780 [04:00<01:11,  3.39it/s] 69%|██████▉   | 539/780 [04:00<01:10,  3.41it/s] 69%|██████▉   | 540/780 [04:01<01:10,  3.43it/s] 69%|██████▉   | 541/780 [04:01<01:09,  3.44it/s] 69%|██████▉   | 542/780 [04:01<01:09,  3.44it/s] 70%|██████▉   | 543/780 [04:01<01:08,  3.44it/s] 70%|██████▉   | 544/780 [04:02<01:08,  3.44it/s] 70%|██████▉   | 545/780 [04:02<01:08,  3.45it/s] 70%|███████   | 546/780 [04:02<01:07,  3.45it/s] 70%|███████   | 547/780 [04:03<01:07,  3.45it/s] 70%|███████   | 548/780 [04:03<01:07,  3.45it/s] 70%|███████   | 549/780 [04:03<01:10,  3.29it/s] 71%|███████   | 550/780 [04:04<01:08,  3.34it/s] 71%|███████   | 551/780 [04:04<01:07,  3.38it/s] 71%|███████   | 552/780 [04:04<01:07,  3.40it/s] 71%|███████   | 553/780 [04:04<01:06,  3.42it/s] 71%|███████   | 554/780 [04:05<01:05,  3.43it/s] 71%|███████   | 555/780 [04:05<01:05,  3.44it/s] 71%|███████▏  | 556/780 [04:05<01:05,  3.44it/s] 71%|███████▏  | 557/780 [04:06<01:04,  3.45it/s] 72%|███████▏  | 558/780 [04:06<01:04,  3.45it/s] 72%|███████▏  | 559/780 [04:06<01:04,  3.45it/s] 72%|███████▏  | 560/780 [04:06<01:09,  3.18it/s] 72%|███████▏  | 561/780 [04:07<01:07,  3.26it/s] 72%|███████▏  | 562/780 [04:07<01:05,  3.32it/s] 72%|███████▏  | 563/780 [04:07<01:04,  3.36it/s] 72%|███████▏  | 564/780 [04:08<01:03,  3.38it/s] 72%|███████▏  | 565/780 [04:08<01:03,  3.40it/s] 73%|███████▎  | 566/780 [04:08<01:02,  3.42it/s] 73%|███████▎  | 567/780 [04:09<01:02,  3.43it/s] 73%|███████▎  | 568/780 [04:09<01:01,  3.43it/s] 73%|███████▎  | 569/780 [04:09<01:01,  3.44it/s] 73%|███████▎  | 570/780 [04:09<01:01,  3.44it/s] 73%|███████▎  | 571/780 [04:10<01:01,  3.39it/s] 73%|███████▎  | 572/780 [04:10<01:01,  3.41it/s] 73%|███████▎  | 573/780 [04:10<01:00,  3.42it/s] 74%|███████▎  | 574/780 [04:11<01:00,  3.43it/s] 74%|███████▎  | 575/780 [04:11<00:59,  3.44it/s] 74%|███████▍  | 576/780 [04:11<00:59,  3.44it/s] 74%|███████▍  | 577/780 [04:11<00:58,  3.45it/s] 74%|███████▍  | 578/780 [04:12<00:58,  3.45it/s] 74%|███████▍  | 579/780 [04:12<00:58,  3.45it/s] 74%|███████▍  | 580/780 [04:12<00:57,  3.45it/s] 74%|███████▍  | 581/780 [04:13<00:57,  3.45it/s] 75%|███████▍  | 582/780 [04:13<00:59,  3.33it/s] 75%|███████▍  | 583/780 [04:13<00:58,  3.37it/s] 75%|███████▍  | 584/780 [04:13<00:57,  3.40it/s] 75%|███████▌  | 585/780 [04:14<00:57,  3.41it/s] 75%|███████▌  | 586/780 [04:14<00:56,  3.43it/s] 75%|███████▌  | 587/780 [04:14<00:56,  3.43it/s] 75%|███████▌  | 588/780 [04:15<00:55,  3.44it/s] 76%|███████▌  | 589/780 [04:15<00:55,  3.44it/s] 76%|███████▌  | 590/780 [04:15<00:55,  3.45it/s] 76%|███████▌  | 591/780 [04:16<00:54,  3.45it/s] 76%|███████▌  | 592/780 [04:16<00:54,  3.45it/s] 76%|███████▌  | 593/780 [04:16<00:54,  3.43it/s] 76%|███████▌  | 594/780 [04:16<00:54,  3.44it/s] 76%|███████▋  | 595/780 [04:17<00:53,  3.44it/s] 76%|███████▋  | 596/780 [04:17<00:53,  3.45it/s] 77%|███████▋  | 597/780 [04:17<00:53,  3.45it/s] 77%|███████▋  | 598/780 [04:18<00:52,  3.45it/s] 77%|███████▋  | 599/780 [04:18<00:52,  3.45it/s] 77%|███████▋  | 600/780 [04:18<00:52,  3.45it/s] 77%|███████▋  | 601/780 [04:18<00:51,  3.45it/s] 77%|███████▋  | 602/780 [04:19<00:51,  3.45it/s] 77%|███████▋  | 603/780 [04:19<00:51,  3.45it/s] 77%|███████▋  | 604/780 [04:19<00:51,  3.41it/s] 78%|███████▊  | 605/780 [04:20<00:51,  3.42it/s] 78%|███████▊  | 606/780 [04:20<00:50,  3.43it/s] 78%|███████▊  | 607/780 [04:20<00:50,  3.44it/s] 78%|███████▊  | 608/780 [04:20<00:49,  3.44it/s] 78%|███████▊  | 609/780 [04:21<00:49,  3.44it/s] 78%|███████▊  | 610/780 [04:21<00:49,  3.44it/s] 78%|███████▊  | 611/780 [04:21<00:49,  3.45it/s] 78%|███████▊  | 612/780 [04:22<00:48,  3.45it/s] 79%|███████▊  | 613/780 [04:22<00:48,  3.45it/s] 79%|███████▊  | 614/780 [04:22<00:48,  3.45it/s] 79%|███████▉  | 615/780 [04:22<00:47,  3.44it/s] 79%|███████▉  | 616/780 [04:23<00:47,  3.44it/s] 79%|███████▉  | 617/780 [04:23<00:47,  3.45it/s] 79%|███████▉  | 618/780 [04:23<00:46,  3.45it/s] 79%|███████▉  | 619/780 [04:24<00:46,  3.45it/s] 79%|███████▉  | 620/780 [04:24<00:46,  3.45it/s] 80%|███████▉  | 621/780 [04:24<00:46,  3.45it/s] 80%|███████▉  | 622/780 [04:25<00:45,  3.45it/s] 80%|███████▉  | 623/780 [04:25<00:45,  3.45it/s] 80%|████████  | 624/780 [04:25<00:45,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 17:58:40,624 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:58:40,624 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:58:40,624 >>   Batch size = 8
{'eval_loss': 0.9456563591957092, 'eval_runtime': 9.7168, 'eval_samples_per_second': 359.068, 'eval_steps_per_second': 44.974, 'epoch': 3.0}
{'loss': 0.6567, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.04it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.22it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.45it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.74it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.38it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.20it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.86it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.44it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.48it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.37it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.39it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.45it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.53it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.57it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.62it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.35it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.26it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.28it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.32it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.32it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.40it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.49it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.46it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.51it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.48it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.50it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.47it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.43it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.32it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 44.91it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 45.40it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 45.85it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.06it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.21it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.39it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.37it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.31it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.22it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.20it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.29it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.36it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.43it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.42it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.56it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.58it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.55it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.38it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.31it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.36it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.39it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.41it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.43it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.60it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.56it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.48it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.36it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.30it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 45.66it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.01it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.25it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.20it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.30it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.35it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.39it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.37it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.31it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.20it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.31it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.30it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.46it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.59it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.52it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.56it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.47it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.39it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.29it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.26it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.27it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.39it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.40it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 44.61it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 45.14it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 45.55it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 45.95it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.08it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.10it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.10it/s][A 80%|████████  | 624/780 [04:35<00:45,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:58:50,288 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 17:58:50,360 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:58:55,405 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:58:55,430 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:58:55,440 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:49<18:47,  7.27s/it] 80%|████████  | 626/780 [04:49<13:17,  5.18s/it] 80%|████████  | 627/780 [04:49<09:27,  3.71s/it] 81%|████████  | 628/780 [04:50<06:48,  2.68s/it] 81%|████████  | 629/780 [04:50<04:56,  1.97s/it] 81%|████████  | 630/780 [04:50<03:39,  1.46s/it] 81%|████████  | 631/780 [04:50<02:45,  1.11s/it] 81%|████████  | 632/780 [04:51<02:07,  1.16it/s] 81%|████████  | 633/780 [04:51<01:41,  1.45it/s] 81%|████████▏ | 634/780 [04:51<01:23,  1.75it/s] 81%|████████▏ | 635/780 [04:52<01:10,  2.06it/s] 82%|████████▏ | 636/780 [04:52<01:01,  2.34it/s] 82%|████████▏ | 637/780 [04:52<00:56,  2.52it/s] 82%|████████▏ | 638/780 [04:52<00:51,  2.74it/s] 82%|████████▏ | 639/780 [04:53<00:48,  2.93it/s] 82%|████████▏ | 640/780 [04:53<00:45,  3.07it/s] 82%|████████▏ | 641/780 [04:53<00:43,  3.18it/s] 82%|████████▏ | 642/780 [04:54<00:42,  3.26it/s] 82%|████████▏ | 643/780 [04:54<00:41,  3.32it/s] 83%|████████▎ | 644/780 [04:54<00:40,  3.36it/s] 83%|████████▎ | 645/780 [04:54<00:39,  3.39it/s] 83%|████████▎ | 646/780 [04:55<00:39,  3.41it/s] 83%|████████▎ | 647/780 [04:55<00:38,  3.43it/s] 83%|████████▎ | 648/780 [04:55<00:43,  3.02it/s] 83%|████████▎ | 649/780 [04:56<00:41,  3.14it/s] 83%|████████▎ | 650/780 [04:56<00:40,  3.23it/s] 83%|████████▎ | 651/780 [04:56<00:39,  3.30it/s] 84%|████████▎ | 652/780 [04:57<00:38,  3.35it/s] 84%|████████▎ | 653/780 [04:57<00:37,  3.38it/s] 84%|████████▍ | 654/780 [04:57<00:36,  3.41it/s] 84%|████████▍ | 655/780 [04:57<00:36,  3.42it/s] 84%|████████▍ | 656/780 [04:58<00:36,  3.44it/s] 84%|████████▍ | 657/780 [04:58<00:35,  3.45it/s] 84%|████████▍ | 658/780 [04:58<00:37,  3.28it/s] 84%|████████▍ | 659/780 [04:59<00:36,  3.34it/s] 85%|████████▍ | 660/780 [04:59<00:35,  3.38it/s] 85%|████████▍ | 661/780 [04:59<00:34,  3.40it/s] 85%|████████▍ | 662/780 [05:00<00:34,  3.42it/s] 85%|████████▌ | 663/780 [05:00<00:34,  3.44it/s] 85%|████████▌ | 664/780 [05:00<00:33,  3.44it/s] 85%|████████▌ | 665/780 [05:00<00:33,  3.45it/s] 85%|████████▌ | 666/780 [05:01<00:33,  3.45it/s] 86%|████████▌ | 667/780 [05:01<00:32,  3.46it/s] 86%|████████▌ | 668/780 [05:01<00:32,  3.46it/s] 86%|████████▌ | 669/780 [05:02<00:37,  3.00it/s] 86%|████████▌ | 670/780 [05:02<00:35,  3.12it/s] 86%|████████▌ | 671/780 [05:02<00:33,  3.22it/s] 86%|████████▌ | 672/780 [05:03<00:32,  3.29it/s] 86%|████████▋ | 673/780 [05:03<00:32,  3.34it/s] 86%|████████▋ | 674/780 [05:03<00:31,  3.37it/s] 87%|████████▋ | 675/780 [05:03<00:30,  3.40it/s] 87%|████████▋ | 676/780 [05:04<00:30,  3.42it/s] 87%|████████▋ | 677/780 [05:04<00:30,  3.43it/s] 87%|████████▋ | 678/780 [05:04<00:29,  3.44it/s] 87%|████████▋ | 679/780 [05:05<00:31,  3.22it/s] 87%|████████▋ | 680/780 [05:05<00:30,  3.28it/s] 87%|████████▋ | 681/780 [05:05<00:29,  3.33it/s] 87%|████████▋ | 682/780 [05:06<00:29,  3.37it/s] 88%|████████▊ | 683/780 [05:06<00:28,  3.40it/s] 88%|████████▊ | 684/780 [05:06<00:28,  3.42it/s] 88%|████████▊ | 685/780 [05:06<00:27,  3.43it/s] 88%|████████▊ | 686/780 [05:07<00:27,  3.44it/s] 88%|████████▊ | 687/780 [05:07<00:26,  3.44it/s] 88%|████████▊ | 688/780 [05:07<00:26,  3.45it/s] 88%|████████▊ | 689/780 [05:08<00:26,  3.45it/s] 88%|████████▊ | 690/780 [05:08<00:26,  3.45it/s] 89%|████████▊ | 691/780 [05:08<00:25,  3.45it/s] 89%|████████▊ | 692/780 [05:08<00:25,  3.45it/s] 89%|████████▉ | 693/780 [05:09<00:25,  3.46it/s] 89%|████████▉ | 694/780 [05:09<00:24,  3.46it/s] 89%|████████▉ | 695/780 [05:09<00:24,  3.46it/s] 89%|████████▉ | 696/780 [05:10<00:24,  3.46it/s] 89%|████████▉ | 697/780 [05:10<00:23,  3.46it/s] 89%|████████▉ | 698/780 [05:10<00:23,  3.46it/s] 90%|████████▉ | 699/780 [05:10<00:23,  3.46it/s] 90%|████████▉ | 700/780 [05:11<00:23,  3.46it/s] 90%|████████▉ | 701/780 [05:11<00:25,  3.12it/s] 90%|█████████ | 702/780 [05:11<00:24,  3.22it/s] 90%|█████████ | 703/780 [05:12<00:23,  3.29it/s] 90%|█████████ | 704/780 [05:12<00:22,  3.34it/s] 90%|█████████ | 705/780 [05:12<00:22,  3.38it/s] 91%|█████████ | 706/780 [05:13<00:21,  3.40it/s] 91%|█████████ | 707/780 [05:13<00:21,  3.42it/s] 91%|█████████ | 708/780 [05:13<00:20,  3.43it/s] 91%|█████████ | 709/780 [05:13<00:20,  3.44it/s] 91%|█████████ | 710/780 [05:14<00:20,  3.45it/s] 91%|█████████ | 711/780 [05:14<00:20,  3.45it/s] 91%|█████████▏| 712/780 [05:14<00:21,  3.18it/s] 91%|█████████▏| 713/780 [05:15<00:20,  3.26it/s] 92%|█████████▏| 714/780 [05:15<00:19,  3.32it/s] 92%|█████████▏| 715/780 [05:15<00:19,  3.36it/s] 92%|█████████▏| 716/780 [05:16<00:18,  3.39it/s] 92%|█████████▏| 717/780 [05:16<00:18,  3.41it/s] 92%|█████████▏| 718/780 [05:16<00:18,  3.42it/s] 92%|█████████▏| 719/780 [05:16<00:17,  3.44it/s] 92%|█████████▏| 720/780 [05:17<00:17,  3.44it/s] 92%|█████████▏| 721/780 [05:17<00:17,  3.45it/s] 93%|█████████▎| 722/780 [05:17<00:16,  3.45it/s] 93%|█████████▎| 723/780 [05:18<00:18,  3.02it/s] 93%|█████████▎| 724/780 [05:18<00:17,  3.14it/s] 93%|█████████▎| 725/780 [05:18<00:17,  3.23it/s] 93%|█████████▎| 726/780 [05:19<00:16,  3.30it/s] 93%|█████████▎| 727/780 [05:19<00:15,  3.34it/s] 93%|█████████▎| 728/780 [05:19<00:15,  3.38it/s] 93%|█████████▎| 729/780 [05:19<00:14,  3.41it/s] 94%|█████████▎| 730/780 [05:20<00:14,  3.42it/s] 94%|█████████▎| 731/780 [05:20<00:14,  3.44it/s] 94%|█████████▍| 732/780 [05:20<00:13,  3.44it/s] 94%|█████████▍| 733/780 [05:21<00:14,  3.15it/s] 94%|█████████▍| 734/780 [05:21<00:14,  3.23it/s] 94%|█████████▍| 735/780 [05:21<00:13,  3.30it/s] 94%|█████████▍| 736/780 [05:22<00:13,  3.34it/s] 94%|█████████▍| 737/780 [05:22<00:12,  3.38it/s] 95%|█████████▍| 738/780 [05:22<00:12,  3.40it/s] 95%|█████████▍| 739/780 [05:22<00:11,  3.42it/s] 95%|█████████▍| 740/780 [05:23<00:11,  3.43it/s] 95%|█████████▌| 741/780 [05:23<00:11,  3.44it/s] 95%|█████████▌| 742/780 [05:23<00:11,  3.45it/s] 95%|█████████▌| 743/780 [05:24<00:10,  3.45it/s] 95%|█████████▌| 744/780 [05:24<00:10,  3.42it/s] 96%|█████████▌| 745/780 [05:24<00:10,  3.43it/s] 96%|█████████▌| 746/780 [05:24<00:09,  3.44it/s] 96%|█████████▌| 747/780 [05:25<00:09,  3.45it/s] 96%|█████████▌| 748/780 [05:25<00:09,  3.45it/s] 96%|█████████▌| 749/780 [05:25<00:08,  3.45it/s] 96%|█████████▌| 750/780 [05:26<00:08,  3.45it/s] 96%|█████████▋| 751/780 [05:26<00:08,  3.45it/s] 96%|█████████▋| 752/780 [05:26<00:08,  3.45it/s] 97%|█████████▋| 753/780 [05:27<00:07,  3.46it/s] 97%|█████████▋| 754/780 [05:27<00:07,  3.46it/s] 97%|█████████▋| 755/780 [05:27<00:08,  2.84it/s] 97%|█████████▋| 756/780 [05:28<00:07,  3.00it/s] 97%|█████████▋| 757/780 [05:28<00:07,  3.13it/s] 97%|█████████▋| 758/780 [05:28<00:06,  3.22it/s] 97%|█████████▋| 759/780 [05:28<00:06,  3.29it/s] 97%|█████████▋| 760/780 [05:29<00:05,  3.34it/s] 98%|█████████▊| 761/780 [05:29<00:05,  3.37it/s] 98%|█████████▊| 762/780 [05:29<00:05,  3.39it/s] 98%|█████████▊| 763/780 [05:30<00:04,  3.41it/s] 98%|█████████▊| 764/780 [05:30<00:04,  3.42it/s] 98%|█████████▊| 765/780 [05:30<00:05,  2.94it/s] 98%|█████████▊| 766/780 [05:31<00:04,  3.08it/s] 98%|█████████▊| 767/780 [05:31<00:04,  3.18it/s] 98%|█████████▊| 768/780 [05:31<00:03,  3.26it/s] 99%|█████████▊| 769/780 [05:32<00:03,  3.32it/s] 99%|█████████▊| 770/780 [05:32<00:02,  3.36it/s] 99%|█████████▉| 771/780 [05:32<00:02,  3.39it/s] 99%|█████████▉| 772/780 [05:32<00:02,  3.41it/s] 99%|█████████▉| 773/780 [05:33<00:02,  3.43it/s] 99%|█████████▉| 774/780 [05:33<00:01,  3.44it/s] 99%|█████████▉| 775/780 [05:33<00:01,  3.13it/s] 99%|█████████▉| 776/780 [05:34<00:01,  3.22it/s]100%|█████████▉| 777/780 [05:34<00:00,  3.29it/s]100%|█████████▉| 778/780 [05:34<00:00,  3.34it/s]100%|█████████▉| 779/780 [05:34<00:00,  3.37it/s]100%|██████████| 780/780 [05:35<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 17:59:50,273 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:59:50,273 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:59:50,273 >>   Batch size = 8
{'eval_loss': 0.9507215023040771, 'eval_runtime': 9.4617, 'eval_samples_per_second': 368.749, 'eval_steps_per_second': 46.186, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.77it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.55it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.67it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.84it/s][A
  6%|▋         | 28/437 [00:00<00:10, 39.47it/s][A
  8%|▊         | 33/437 [00:00<00:09, 41.56it/s][A
  9%|▊         | 38/437 [00:00<00:09, 43.06it/s][A
 10%|▉         | 43/437 [00:00<00:08, 44.16it/s][A
 11%|█         | 48/437 [00:01<00:08, 44.79it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 45.36it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 45.78it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.08it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 45.88it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 45.99it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.19it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.33it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.45it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 46.45it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.50it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.58it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.47it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.42it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.39it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.27it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.38it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.50it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 46.49it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.52it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.60it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.54it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.48it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.35it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.35it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.34it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.40it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.46it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.56it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.60it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.60it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.50it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.55it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.32it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.39it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.44it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.42it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.52it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.63it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.56it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.54it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.46it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.33it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.32it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.47it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.51it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.53it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.49it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.49it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.47it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.49it/s][A
 69%|██████▉   | 303/437 [00:06<00:03, 42.83it/s][A
 70%|███████   | 308/437 [00:06<00:02, 43.85it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 44.62it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 45.19it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 45.65it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 45.96it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.21it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.31it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.05it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.04it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.22it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.34it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.39it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.38it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.54it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.37it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.58it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.40it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.30it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.35it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.29it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.46it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.49it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.48it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.55it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.50it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.41it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.41it/s][A100%|██████████| 780/780 [05:44<00:00,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:59:59,790 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 17:59:59,812 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:00:05,656 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:00:05,674 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:00:05,739 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:00:16,731 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:00:16,740 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156 (score: 0.9312083721160889).
                                                 100%|██████████| 780/780 [06:06<00:00,  3.40it/s]100%|██████████| 780/780 [06:06<00:00,  2.13it/s]
[INFO|trainer.py:1894] 2023-08-28 18:00:21,704 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 18:00:21,877 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:00:26,950 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:00:26,990 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:00:27,009 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:00:27,364 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:27,365 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:27,365 >>   train_loss               =     0.6449
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:27,365 >>   train_runtime            = 0:06:06.65
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:27,365 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:27,365 >>   train_samples_per_second =     136.37
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:27,365 >>   train_steps_per_second   =      2.127
{'eval_loss': 0.9526326060295105, 'eval_runtime': 9.4876, 'eval_samples_per_second': 367.742, 'eval_steps_per_second': 46.06, 'epoch': 5.0}
{'train_runtime': 366.6506, 'train_samples_per_second': 136.37, 'train_steps_per_second': 2.127, 'train_loss': 0.6449320279634916, 'epoch': 5.0}
08/28/2023 18:00:27 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:00:27,403 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:00:27,403 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 18:00:27,403 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.61it/s]  3%|▎         | 12/437 [00:00<00:08, 50.97it/s]  4%|▍         | 18/437 [00:00<00:08, 49.01it/s]  5%|▌         | 23/437 [00:00<00:08, 48.27it/s]  6%|▋         | 28/437 [00:00<00:08, 47.93it/s]  8%|▊         | 33/437 [00:00<00:08, 47.68it/s]  9%|▊         | 38/437 [00:00<00:08, 47.47it/s] 10%|▉         | 43/437 [00:00<00:08, 47.22it/s] 11%|█         | 48/437 [00:00<00:08, 46.99it/s] 12%|█▏        | 53/437 [00:01<00:08, 46.96it/s] 13%|█▎        | 58/437 [00:01<00:08, 47.00it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.02it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.01it/s] 17%|█▋        | 73/437 [00:01<00:07, 46.90it/s] 18%|█▊        | 78/437 [00:01<00:07, 46.86it/s] 19%|█▉        | 83/437 [00:01<00:07, 46.94it/s] 20%|██        | 88/437 [00:01<00:07, 46.92it/s] 21%|██▏       | 93/437 [00:01<00:07, 46.85it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.84it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.89it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.75it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.84it/s] 27%|██▋       | 118/437 [00:02<00:06, 46.93it/s] 28%|██▊       | 123/437 [00:02<00:06, 46.99it/s] 29%|██▉       | 128/437 [00:02<00:07, 41.37it/s] 30%|███       | 133/437 [00:02<00:07, 42.87it/s] 32%|███▏      | 138/437 [00:02<00:06, 43.97it/s] 33%|███▎      | 143/437 [00:03<00:06, 44.86it/s] 34%|███▍      | 148/437 [00:03<00:06, 45.48it/s] 35%|███▌      | 153/437 [00:03<00:06, 45.92it/s] 36%|███▌      | 158/437 [00:03<00:06, 46.14it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.32it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.34it/s] 40%|███▉      | 173/437 [00:03<00:05, 46.42it/s] 41%|████      | 178/437 [00:03<00:05, 46.56it/s] 42%|████▏     | 183/437 [00:03<00:05, 46.64it/s] 43%|████▎     | 188/437 [00:04<00:05, 46.69it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.70it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.65it/s] 46%|████▋     | 203/437 [00:04<00:05, 46.70it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.80it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.79it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.71it/s] 51%|█████     | 223/437 [00:04<00:04, 46.81it/s] 52%|█████▏    | 228/437 [00:04<00:04, 46.72it/s] 53%|█████▎    | 233/437 [00:04<00:04, 46.80it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.80it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.86it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.70it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.73it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.73it/s] 60%|██████    | 263/437 [00:05<00:03, 46.77it/s] 61%|██████▏   | 268/437 [00:05<00:03, 45.70it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.02it/s] 64%|██████▎   | 278/437 [00:05<00:03, 46.20it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.40it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.59it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.60it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.71it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.69it/s] 70%|███████   | 308/437 [00:06<00:02, 46.68it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.70it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.78it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.72it/s] 75%|███████▌  | 328/437 [00:07<00:02, 46.69it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.76it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.67it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.61it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.69it/s] 81%|████████  | 353/437 [00:07<00:01, 46.72it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.70it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.81it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.64it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.67it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.73it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.73it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.82it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.88it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.67it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.66it/s] 93%|█████████▎| 408/437 [00:08<00:00, 38.81it/s] 95%|█████████▍| 413/437 [00:08<00:00, 40.87it/s] 96%|█████████▌| 418/437 [00:09<00:00, 42.50it/s] 97%|█████████▋| 423/437 [00:09<00:00, 43.67it/s] 98%|█████████▊| 428/437 [00:09<00:00, 44.51it/s] 99%|█████████▉| 433/437 [00:09<00:00, 45.17it/s]100%|██████████| 437/437 [00:09<00:00, 46.28it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:00:36,867 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:36,867 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:36,867 >>   eval_loss               =     0.9312
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:36,867 >>   eval_runtime            = 0:00:09.46
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:36,867 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:36,867 >>   eval_samples_per_second =     368.68
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:36,867 >>   eval_steps_per_second   =     46.177
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:36,867 >>   perplexity              =     2.5376
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:43,099 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:43,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:43,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:43,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:43,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:00:43,425 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:00:43,426 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:00:43,699 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:00:44,756 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:00:44,756 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:46,106 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:46,111 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:46,111 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:46,111 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:46,111 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:00:46,928 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:00:46,929 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:00:47,671 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:00:47,826 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:00:47,826 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:08,  1.51it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.46it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:16,  1.46it/s]Extractor Predicting: 25it [00:16,  1.45it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.48it/s]Extractor Predicting: 31it [00:20,  1.48it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:22,  1.53it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:24,  1.47it/s]Extractor Predicting: 37it [00:25,  1.46it/s]Extractor Predicting: 38it [00:25,  1.45it/s]Extractor Predicting: 39it [00:26,  1.44it/s]Extractor Predicting: 40it [00:27,  1.45it/s]Extractor Predicting: 41it [00:27,  1.44it/s]Extractor Predicting: 42it [00:28,  1.45it/s]Extractor Predicting: 43it [00:29,  1.45it/s]Extractor Predicting: 44it [00:29,  1.44it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:31,  1.45it/s]Extractor Predicting: 47it [00:32,  1.34it/s]Extractor Predicting: 48it [00:32,  1.35it/s]Extractor Predicting: 49it [00:33,  1.38it/s]Extractor Predicting: 50it [00:34,  1.39it/s]Extractor Predicting: 51it [00:34,  1.40it/s]Extractor Predicting: 52it [00:35,  1.43it/s]Extractor Predicting: 53it [00:36,  1.42it/s]Extractor Predicting: 54it [00:37,  1.42it/s]Extractor Predicting: 55it [00:37,  1.40it/s]Extractor Predicting: 56it [00:38,  1.42it/s]Extractor Predicting: 57it [00:39,  1.42it/s]Extractor Predicting: 58it [00:39,  1.43it/s]Extractor Predicting: 59it [00:40,  1.40it/s]Extractor Predicting: 60it [00:41,  1.41it/s]Extractor Predicting: 61it [00:42,  1.41it/s]Extractor Predicting: 62it [00:42,  1.45it/s]Extractor Predicting: 63it [00:43,  1.45it/s]Extractor Predicting: 64it [00:44,  1.45it/s]Extractor Predicting: 65it [00:44,  1.47it/s]Extractor Predicting: 66it [00:45,  1.47it/s]Extractor Predicting: 67it [00:46,  1.48it/s]Extractor Predicting: 68it [00:46,  1.49it/s]Extractor Predicting: 69it [00:47,  1.50it/s]Extractor Predicting: 70it [00:47,  1.52it/s]Extractor Predicting: 71it [00:48,  1.52it/s]Extractor Predicting: 72it [00:49,  1.51it/s]Extractor Predicting: 73it [00:49,  1.51it/s]Extractor Predicting: 74it [00:50,  1.50it/s]Extractor Predicting: 75it [00:51,  1.46it/s]Extractor Predicting: 76it [00:52,  1.45it/s]Extractor Predicting: 77it [00:52,  1.48it/s]Extractor Predicting: 78it [00:53,  1.51it/s]Extractor Predicting: 79it [00:54,  1.50it/s]Extractor Predicting: 80it [00:54,  1.52it/s]Extractor Predicting: 81it [00:55,  1.52it/s]Extractor Predicting: 82it [00:55,  1.53it/s]Extractor Predicting: 83it [00:56,  1.52it/s]Extractor Predicting: 84it [00:57,  1.49it/s]Extractor Predicting: 85it [00:57,  1.52it/s]Extractor Predicting: 86it [00:58,  1.53it/s]Extractor Predicting: 87it [00:59,  1.53it/s]Extractor Predicting: 88it [00:59,  1.53it/s]Extractor Predicting: 89it [01:00,  1.50it/s]Extractor Predicting: 90it [01:01,  1.51it/s]Extractor Predicting: 91it [01:01,  1.51it/s]Extractor Predicting: 92it [01:02,  1.46it/s]Extractor Predicting: 93it [01:03,  1.48it/s]Extractor Predicting: 94it [01:04,  1.48it/s]Extractor Predicting: 95it [01:04,  1.46it/s]Extractor Predicting: 96it [01:05,  1.46it/s]Extractor Predicting: 97it [01:06,  1.47it/s]Extractor Predicting: 98it [01:06,  1.50it/s]Extractor Predicting: 99it [01:07,  1.48it/s]Extractor Predicting: 100it [01:08,  1.45it/s]Extractor Predicting: 101it [01:08,  1.48it/s]Extractor Predicting: 102it [01:09,  1.46it/s]Extractor Predicting: 103it [01:10,  1.44it/s]Extractor Predicting: 104it [01:10,  1.45it/s]Extractor Predicting: 105it [01:11,  1.45it/s]Extractor Predicting: 106it [01:12,  1.45it/s]Extractor Predicting: 107it [01:12,  1.47it/s]Extractor Predicting: 108it [01:13,  1.48it/s]Extractor Predicting: 109it [01:14,  1.44it/s]Extractor Predicting: 110it [01:15,  1.43it/s]Extractor Predicting: 111it [01:15,  1.43it/s]Extractor Predicting: 112it [01:16,  1.44it/s]Extractor Predicting: 113it [01:17,  1.45it/s]Extractor Predicting: 114it [01:17,  1.46it/s]Extractor Predicting: 115it [01:18,  1.44it/s]Extractor Predicting: 116it [01:19,  1.47it/s]Extractor Predicting: 117it [01:19,  1.47it/s]Extractor Predicting: 118it [01:20,  1.48it/s]Extractor Predicting: 119it [01:21,  1.51it/s]Extractor Predicting: 120it [01:21,  1.48it/s]Extractor Predicting: 121it [01:22,  1.50it/s]Extractor Predicting: 122it [01:23,  1.51it/s]Extractor Predicting: 123it [01:23,  1.49it/s]Extractor Predicting: 124it [01:24,  1.50it/s]Extractor Predicting: 125it [01:25,  1.50it/s]Extractor Predicting: 126it [01:25,  1.50it/s]Extractor Predicting: 127it [01:26,  1.50it/s]Extractor Predicting: 128it [01:27,  1.47it/s]Extractor Predicting: 129it [01:27,  1.51it/s]Extractor Predicting: 130it [01:28,  1.46it/s]Extractor Predicting: 131it [01:29,  1.47it/s]Extractor Predicting: 132it [01:29,  1.48it/s]Extractor Predicting: 133it [01:30,  1.47it/s]Extractor Predicting: 134it [01:31,  1.46it/s]Extractor Predicting: 135it [01:31,  1.48it/s]Extractor Predicting: 136it [01:32,  1.50it/s]Extractor Predicting: 137it [01:33,  1.35it/s]Extractor Predicting: 138it [01:34,  1.37it/s]Extractor Predicting: 139it [01:34,  1.42it/s]Extractor Predicting: 140it [01:35,  1.44it/s]Extractor Predicting: 141it [01:36,  1.44it/s]Extractor Predicting: 142it [01:36,  1.45it/s]Extractor Predicting: 143it [01:37,  1.47it/s]Extractor Predicting: 144it [01:38,  1.48it/s]Extractor Predicting: 145it [01:38,  1.79it/s]Extractor Predicting: 145it [01:38,  1.47it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:35,994 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:36,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:36,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:36,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:36,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:02:36,366 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:02:36,367 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:02:36,636 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:02:37,924 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:02:37,924 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:39,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:39,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:39,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:39,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:39,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:02:40,170 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:02:40,171 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:02:40,523 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:02:40,693 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:02:40,693 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5644444444444444,
  "recall": 0.14560045858412152,
  "score": 0.23148781043517885,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.46it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.45it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.40it/s]Extractor Predicting: 24it [00:16,  1.40it/s]Extractor Predicting: 25it [00:17,  1.41it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:18,  1.45it/s]Extractor Predicting: 28it [00:19,  1.43it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.42it/s]Extractor Predicting: 31it [00:21,  1.43it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:22,  1.41it/s]Extractor Predicting: 34it [00:23,  1.42it/s]Extractor Predicting: 35it [00:24,  1.44it/s]Extractor Predicting: 36it [00:24,  1.47it/s]Extractor Predicting: 37it [00:25,  1.51it/s]Extractor Predicting: 38it [00:26,  1.43it/s]Extractor Predicting: 39it [00:26,  1.45it/s]Extractor Predicting: 40it [00:27,  1.47it/s]Extractor Predicting: 41it [00:28,  1.48it/s]Extractor Predicting: 42it [00:28,  1.49it/s]Extractor Predicting: 43it [00:29,  1.48it/s]Extractor Predicting: 44it [00:30,  1.50it/s]Extractor Predicting: 45it [00:30,  1.48it/s]Extractor Predicting: 46it [00:31,  1.49it/s]Extractor Predicting: 47it [00:32,  1.49it/s]Extractor Predicting: 48it [00:32,  1.49it/s]Extractor Predicting: 49it [00:33,  1.49it/s]Extractor Predicting: 50it [00:34,  1.47it/s]Extractor Predicting: 51it [00:34,  1.48it/s]Extractor Predicting: 52it [00:35,  1.48it/s]Extractor Predicting: 53it [00:36,  1.48it/s]Extractor Predicting: 54it [00:36,  1.49it/s]Extractor Predicting: 55it [00:37,  1.45it/s]Extractor Predicting: 56it [00:38,  1.46it/s]Extractor Predicting: 57it [00:38,  1.49it/s]Extractor Predicting: 58it [00:39,  1.52it/s]Extractor Predicting: 59it [00:40,  1.51it/s]Extractor Predicting: 60it [00:40,  1.58it/s]Extractor Predicting: 61it [00:41,  1.60it/s]Extractor Predicting: 62it [00:41,  1.62it/s]Extractor Predicting: 63it [00:42,  1.62it/s]Extractor Predicting: 64it [00:43,  1.65it/s]Extractor Predicting: 65it [00:43,  1.66it/s]Extractor Predicting: 66it [00:44,  1.65it/s]Extractor Predicting: 67it [00:44,  1.67it/s]Extractor Predicting: 68it [00:45,  1.70it/s]Extractor Predicting: 69it [00:46,  1.71it/s]Extractor Predicting: 70it [00:46,  1.70it/s]Extractor Predicting: 71it [00:47,  1.70it/s]Extractor Predicting: 72it [00:47,  1.70it/s]Extractor Predicting: 73it [00:48,  1.70it/s]Extractor Predicting: 74it [00:49,  1.69it/s]Extractor Predicting: 75it [00:49,  1.67it/s]Extractor Predicting: 76it [00:50,  1.70it/s]Extractor Predicting: 77it [00:50,  1.72it/s]Extractor Predicting: 78it [00:51,  1.71it/s]Extractor Predicting: 79it [00:51,  1.71it/s]Extractor Predicting: 80it [00:52,  1.72it/s]Extractor Predicting: 81it [00:53,  1.67it/s]Extractor Predicting: 82it [00:53,  1.66it/s]Extractor Predicting: 83it [00:54,  1.65it/s]Extractor Predicting: 84it [00:54,  1.65it/s]Extractor Predicting: 85it [00:55,  1.66it/s]Extractor Predicting: 86it [00:56,  1.58it/s]Extractor Predicting: 87it [00:56,  1.55it/s]Extractor Predicting: 88it [00:57,  1.53it/s]Extractor Predicting: 89it [00:58,  1.49it/s]Extractor Predicting: 90it [00:59,  1.46it/s]Extractor Predicting: 91it [00:59,  1.46it/s]Extractor Predicting: 92it [01:00,  1.44it/s]Extractor Predicting: 93it [01:01,  1.47it/s]Extractor Predicting: 94it [01:01,  1.46it/s]Extractor Predicting: 95it [01:02,  1.47it/s]Extractor Predicting: 96it [01:03,  1.45it/s]Extractor Predicting: 97it [01:03,  1.45it/s]Extractor Predicting: 98it [01:04,  1.45it/s]Extractor Predicting: 99it [01:05,  1.47it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:06,  1.46it/s]Extractor Predicting: 102it [01:07,  1.45it/s]Extractor Predicting: 103it [01:07,  1.47it/s]Extractor Predicting: 104it [01:08,  1.44it/s]Extractor Predicting: 105it [01:09,  1.44it/s]Extractor Predicting: 106it [01:10,  1.42it/s]Extractor Predicting: 107it [01:10,  1.41it/s]Extractor Predicting: 108it [01:11,  1.41it/s]Extractor Predicting: 109it [01:12,  1.44it/s]Extractor Predicting: 110it [01:12,  1.43it/s]Extractor Predicting: 111it [01:13,  1.42it/s]Extractor Predicting: 112it [01:14,  1.39it/s]Extractor Predicting: 113it [01:15,  1.40it/s]Extractor Predicting: 114it [01:15,  1.45it/s]Extractor Predicting: 115it [01:16,  1.43it/s]Extractor Predicting: 116it [01:17,  1.45it/s]Extractor Predicting: 117it [01:17,  1.47it/s]Extractor Predicting: 118it [01:18,  1.43it/s]Extractor Predicting: 119it [01:19,  1.40it/s]Extractor Predicting: 120it [01:19,  1.41it/s]Extractor Predicting: 121it [01:20,  1.39it/s]Extractor Predicting: 122it [01:21,  1.42it/s]Extractor Predicting: 123it [01:22,  1.43it/s]Extractor Predicting: 124it [01:22,  1.42it/s]Extractor Predicting: 125it [01:23,  1.41it/s]Extractor Predicting: 126it [01:24,  1.39it/s]Extractor Predicting: 127it [01:24,  1.39it/s]Extractor Predicting: 128it [01:25,  1.37it/s]Extractor Predicting: 129it [01:26,  1.38it/s]Extractor Predicting: 130it [01:27,  1.38it/s]Extractor Predicting: 131it [01:27,  1.40it/s]Extractor Predicting: 132it [01:28,  1.41it/s]Extractor Predicting: 133it [01:29,  1.27it/s]Extractor Predicting: 134it [01:30,  1.32it/s]Extractor Predicting: 135it [01:30,  1.30it/s]Extractor Predicting: 136it [01:31,  1.30it/s]Extractor Predicting: 137it [01:32,  1.33it/s]Extractor Predicting: 138it [01:33,  1.34it/s]Extractor Predicting: 139it [01:33,  1.33it/s]Extractor Predicting: 140it [01:34,  1.36it/s]Extractor Predicting: 141it [01:35,  1.37it/s]Extractor Predicting: 142it [01:36,  1.39it/s]Extractor Predicting: 143it [01:36,  1.36it/s]Extractor Predicting: 144it [01:37,  1.31it/s]Extractor Predicting: 145it [01:38,  1.34it/s]Extractor Predicting: 146it [01:39,  1.36it/s]Extractor Predicting: 147it [01:39,  1.40it/s]Extractor Predicting: 148it [01:40,  1.43it/s]Extractor Predicting: 149it [01:41,  1.37it/s]Extractor Predicting: 150it [01:41,  1.39it/s]Extractor Predicting: 151it [01:42,  1.42it/s]Extractor Predicting: 152it [01:43,  1.44it/s]Extractor Predicting: 153it [01:43,  1.46it/s]Extractor Predicting: 154it [01:44,  1.48it/s]Extractor Predicting: 155it [01:45,  1.50it/s]Extractor Predicting: 156it [01:45,  1.51it/s]Extractor Predicting: 157it [01:46,  1.50it/s]Extractor Predicting: 158it [01:47,  1.51it/s]Extractor Predicting: 159it [01:47,  1.56it/s]Extractor Predicting: 160it [01:48,  1.52it/s]Extractor Predicting: 161it [01:49,  1.50it/s]Extractor Predicting: 162it [01:49,  1.47it/s]Extractor Predicting: 163it [01:50,  1.48it/s]Extractor Predicting: 164it [01:51,  1.47it/s]Extractor Predicting: 165it [01:51,  1.46it/s]Extractor Predicting: 166it [01:52,  1.46it/s]Extractor Predicting: 167it [01:53,  1.46it/s]Extractor Predicting: 168it [01:53,  1.45it/s]Extractor Predicting: 169it [01:54,  1.44it/s]Extractor Predicting: 170it [01:55,  1.47it/s]Extractor Predicting: 171it [01:56,  1.46it/s]Extractor Predicting: 172it [01:56,  1.44it/s]Extractor Predicting: 173it [01:57,  1.45it/s]Extractor Predicting: 174it [01:58,  1.45it/s]Extractor Predicting: 175it [01:58,  1.45it/s]Extractor Predicting: 176it [01:59,  1.45it/s]Extractor Predicting: 177it [02:00,  1.49it/s]Extractor Predicting: 178it [02:00,  1.47it/s]Extractor Predicting: 179it [02:01,  1.49it/s]Extractor Predicting: 180it [02:02,  1.51it/s]Extractor Predicting: 181it [02:02,  1.49it/s]Extractor Predicting: 182it [02:03,  1.54it/s]Extractor Predicting: 183it [02:04,  1.53it/s]Extractor Predicting: 184it [02:04,  1.51it/s]Extractor Predicting: 185it [02:05,  1.54it/s]Extractor Predicting: 186it [02:06,  1.54it/s]Extractor Predicting: 187it [02:06,  1.50it/s]Extractor Predicting: 188it [02:07,  1.53it/s]Extractor Predicting: 189it [02:07,  1.54it/s]Extractor Predicting: 190it [02:08,  1.58it/s]Extractor Predicting: 191it [02:09,  1.51it/s]Extractor Predicting: 192it [02:09,  1.54it/s]Extractor Predicting: 193it [02:10,  1.55it/s]Extractor Predicting: 194it [02:11,  1.58it/s]Extractor Predicting: 195it [02:11,  1.55it/s]Extractor Predicting: 196it [02:12,  1.54it/s]Extractor Predicting: 197it [02:13,  1.57it/s]Extractor Predicting: 198it [02:13,  1.53it/s]Extractor Predicting: 199it [02:14,  1.50it/s]Extractor Predicting: 200it [02:15,  1.52it/s]Extractor Predicting: 201it [02:15,  1.52it/s]Extractor Predicting: 202it [02:16,  1.55it/s]Extractor Predicting: 203it [02:17,  1.58it/s]Extractor Predicting: 204it [02:17,  1.59it/s]Extractor Predicting: 205it [02:18,  1.58it/s]Extractor Predicting: 206it [02:18,  1.63it/s]Extractor Predicting: 207it [02:19,  1.61it/s]Extractor Predicting: 208it [02:20,  1.65it/s]Extractor Predicting: 209it [02:20,  1.62it/s]Extractor Predicting: 210it [02:21,  1.58it/s]Extractor Predicting: 211it [02:22,  1.58it/s]Extractor Predicting: 212it [02:22,  1.63it/s]Extractor Predicting: 213it [02:23,  1.65it/s]Extractor Predicting: 214it [02:23,  1.65it/s]Extractor Predicting: 215it [02:24,  1.53it/s]Extractor Predicting: 216it [02:25,  1.57it/s]Extractor Predicting: 217it [02:25,  1.57it/s]Extractor Predicting: 218it [02:26,  1.56it/s]Extractor Predicting: 219it [02:27,  1.60it/s]Extractor Predicting: 220it [02:27,  1.61it/s]Extractor Predicting: 221it [02:28,  1.63it/s]Extractor Predicting: 222it [02:28,  1.68it/s]Extractor Predicting: 223it [02:29,  1.64it/s]Extractor Predicting: 224it [02:30,  1.67it/s]Extractor Predicting: 225it [02:30,  1.66it/s]Extractor Predicting: 226it [02:31,  1.65it/s]Extractor Predicting: 227it [02:31,  1.69it/s]Extractor Predicting: 228it [02:32,  1.68it/s]Extractor Predicting: 229it [02:33,  1.59it/s]Extractor Predicting: 230it [02:33,  1.55it/s]Extractor Predicting: 231it [02:34,  1.44it/s]Extractor Predicting: 232it [02:35,  1.42it/s]Extractor Predicting: 233it [02:36,  1.43it/s]Extractor Predicting: 234it [02:36,  1.41it/s]Extractor Predicting: 235it [02:37,  1.42it/s]Extractor Predicting: 236it [02:38,  1.40it/s]Extractor Predicting: 237it [02:38,  1.38it/s]Extractor Predicting: 238it [02:39,  1.37it/s]Extractor Predicting: 239it [02:40,  1.38it/s]Extractor Predicting: 240it [02:41,  1.38it/s]Extractor Predicting: 241it [02:41,  1.40it/s]Extractor Predicting: 242it [02:42,  1.42it/s]Extractor Predicting: 243it [02:43,  1.38it/s]Extractor Predicting: 244it [02:43,  1.40it/s]Extractor Predicting: 245it [02:44,  1.41it/s]Extractor Predicting: 246it [02:45,  1.42it/s]Extractor Predicting: 247it [02:46,  1.39it/s]Extractor Predicting: 248it [02:46,  1.38it/s]Extractor Predicting: 249it [02:47,  1.35it/s]Extractor Predicting: 250it [02:48,  1.36it/s]Extractor Predicting: 251it [02:49,  1.35it/s]Extractor Predicting: 252it [02:49,  1.38it/s]Extractor Predicting: 253it [02:50,  1.25it/s]Extractor Predicting: 254it [02:51,  1.28it/s]Extractor Predicting: 255it [02:52,  1.31it/s]Extractor Predicting: 256it [02:52,  1.34it/s]Extractor Predicting: 257it [02:53,  1.41it/s]Extractor Predicting: 258it [02:54,  1.43it/s]Extractor Predicting: 259it [02:54,  1.45it/s]Extractor Predicting: 260it [02:55,  1.48it/s]Extractor Predicting: 261it [02:56,  1.48it/s]Extractor Predicting: 262it [02:56,  1.49it/s]Extractor Predicting: 263it [02:57,  1.50it/s]Extractor Predicting: 264it [02:58,  1.51it/s]Extractor Predicting: 265it [02:58,  1.47it/s]Extractor Predicting: 266it [02:59,  1.50it/s]Extractor Predicting: 267it [03:00,  1.53it/s]Extractor Predicting: 268it [03:00,  1.50it/s]Extractor Predicting: 269it [03:01,  1.51it/s]Extractor Predicting: 270it [03:02,  1.48it/s]Extractor Predicting: 271it [03:02,  1.47it/s]Extractor Predicting: 272it [03:03,  1.47it/s]Extractor Predicting: 273it [03:04,  1.47it/s]Extractor Predicting: 274it [03:04,  1.47it/s]Extractor Predicting: 275it [03:05,  1.49it/s]Extractor Predicting: 276it [03:06,  1.49it/s]Extractor Predicting: 277it [03:06,  1.48it/s]Extractor Predicting: 278it [03:07,  1.44it/s]Extractor Predicting: 279it [03:08,  1.47it/s]Extractor Predicting: 280it [03:09,  1.45it/s]Extractor Predicting: 281it [03:09,  1.48it/s]Extractor Predicting: 282it [03:10,  1.47it/s]Extractor Predicting: 283it [03:11,  1.48it/s]Extractor Predicting: 284it [03:11,  1.49it/s]Extractor Predicting: 285it [03:12,  1.45it/s]Extractor Predicting: 286it [03:13,  1.46it/s]Extractor Predicting: 287it [03:13,  1.48it/s]Extractor Predicting: 288it [03:14,  1.48it/s]Extractor Predicting: 289it [03:15,  1.47it/s]Extractor Predicting: 290it [03:15,  1.47it/s]Extractor Predicting: 291it [03:16,  1.47it/s]Extractor Predicting: 292it [03:17,  1.48it/s]Extractor Predicting: 293it [03:17,  1.47it/s]Extractor Predicting: 294it [03:18,  1.47it/s]Extractor Predicting: 295it [03:19,  1.48it/s]Extractor Predicting: 296it [03:19,  1.47it/s]Extractor Predicting: 297it [03:20,  1.49it/s]Extractor Predicting: 298it [03:21,  1.46it/s]Extractor Predicting: 299it [03:21,  1.50it/s]Extractor Predicting: 300it [03:22,  1.49it/s]Extractor Predicting: 301it [03:23,  1.49it/s]Extractor Predicting: 302it [03:23,  1.48it/s]Extractor Predicting: 303it [03:24,  1.52it/s]Extractor Predicting: 304it [03:25,  1.51it/s]Extractor Predicting: 305it [03:25,  1.55it/s]Extractor Predicting: 306it [03:26,  1.51it/s]Extractor Predicting: 307it [03:27,  1.50it/s]Extractor Predicting: 308it [03:27,  1.49it/s]Extractor Predicting: 309it [03:28,  1.51it/s]Extractor Predicting: 310it [03:29,  1.47it/s]Extractor Predicting: 311it [03:29,  1.46it/s]Extractor Predicting: 312it [03:30,  1.47it/s]Extractor Predicting: 313it [03:31,  1.49it/s]Extractor Predicting: 314it [03:31,  1.49it/s]Extractor Predicting: 315it [03:32,  1.52it/s]Extractor Predicting: 316it [03:33,  1.50it/s]Extractor Predicting: 317it [03:33,  1.49it/s]Extractor Predicting: 318it [03:34,  1.54it/s]Extractor Predicting: 319it [03:35,  1.53it/s]Extractor Predicting: 320it [03:35,  1.56it/s]Extractor Predicting: 321it [03:36,  1.53it/s]Extractor Predicting: 322it [03:37,  1.52it/s]Extractor Predicting: 323it [03:37,  1.51it/s]Extractor Predicting: 324it [03:38,  1.50it/s]Extractor Predicting: 325it [03:39,  1.48it/s]Extractor Predicting: 326it [03:39,  1.49it/s]Extractor Predicting: 327it [03:40,  1.47it/s]Extractor Predicting: 328it [03:41,  1.46it/s]Extractor Predicting: 329it [03:41,  1.43it/s]Extractor Predicting: 330it [03:42,  1.45it/s]Extractor Predicting: 331it [03:43,  1.45it/s]Extractor Predicting: 332it [03:43,  1.49it/s]Extractor Predicting: 333it [03:44,  1.49it/s]Extractor Predicting: 334it [03:45,  1.49it/s]Extractor Predicting: 335it [03:45,  1.48it/s]Extractor Predicting: 336it [03:46,  1.47it/s]Extractor Predicting: 337it [03:47,  1.46it/s]Extractor Predicting: 338it [03:48,  1.47it/s]Extractor Predicting: 339it [03:48,  1.46it/s]Extractor Predicting: 340it [03:49,  1.51it/s]Extractor Predicting: 341it [03:50,  1.48it/s]Extractor Predicting: 342it [03:50,  1.51it/s]Extractor Predicting: 343it [03:51,  1.46it/s]Extractor Predicting: 344it [03:52,  1.48it/s]Extractor Predicting: 345it [03:52,  1.45it/s]Extractor Predicting: 346it [03:53,  1.47it/s]Extractor Predicting: 347it [03:54,  1.48it/s]Extractor Predicting: 348it [03:54,  1.50it/s]Extractor Predicting: 349it [03:55,  1.44it/s]Extractor Predicting: 350it [03:56,  1.43it/s]Extractor Predicting: 351it [03:56,  1.40it/s]Extractor Predicting: 352it [03:57,  1.42it/s]Extractor Predicting: 353it [03:58,  1.42it/s]Extractor Predicting: 354it [03:59,  1.44it/s]Extractor Predicting: 355it [03:59,  1.42it/s]Extractor Predicting: 356it [04:00,  1.45it/s]Extractor Predicting: 357it [04:01,  1.43it/s]Extractor Predicting: 358it [04:01,  1.43it/s]Extractor Predicting: 359it [04:02,  1.42it/s]Extractor Predicting: 360it [04:03,  1.40it/s]Extractor Predicting: 361it [04:04,  1.28it/s]Extractor Predicting: 362it [04:04,  1.34it/s]Extractor Predicting: 363it [04:05,  1.37it/s]Extractor Predicting: 364it [04:06,  1.41it/s]Extractor Predicting: 365it [04:06,  1.44it/s]Extractor Predicting: 366it [04:07,  1.42it/s]Extractor Predicting: 367it [04:08,  1.43it/s]Extractor Predicting: 368it [04:09,  1.42it/s]Extractor Predicting: 369it [04:09,  1.44it/s]Extractor Predicting: 370it [04:10,  1.43it/s]Extractor Predicting: 371it [04:11,  1.46it/s]Extractor Predicting: 372it [04:11,  1.47it/s]Extractor Predicting: 373it [04:12,  1.49it/s]Extractor Predicting: 374it [04:13,  1.50it/s]Extractor Predicting: 375it [04:13,  1.48it/s]Extractor Predicting: 376it [04:14,  1.49it/s]Extractor Predicting: 377it [04:15,  1.49it/s]Extractor Predicting: 378it [04:15,  1.51it/s]Extractor Predicting: 379it [04:16,  1.52it/s]Extractor Predicting: 380it [04:17,  1.51it/s]Extractor Predicting: 381it [04:18,  1.30it/s]Extractor Predicting: 382it [04:18,  1.34it/s]Extractor Predicting: 383it [04:19,  1.40it/s]Extractor Predicting: 384it [04:20,  1.44it/s]Extractor Predicting: 385it [04:20,  1.47it/s]Extractor Predicting: 386it [04:21,  1.49it/s]Extractor Predicting: 387it [04:21,  1.52it/s]Extractor Predicting: 388it [04:22,  1.49it/s]Extractor Predicting: 389it [04:23,  1.50it/s]Extractor Predicting: 390it [04:24,  1.50it/s]Extractor Predicting: 391it [04:24,  1.32it/s]Extractor Predicting: 392it [04:25,  1.38it/s]Extractor Predicting: 393it [04:26,  1.42it/s]Extractor Predicting: 394it [04:26,  1.47it/s]Extractor Predicting: 395it [04:27,  1.44it/s]Extractor Predicting: 396it [04:28,  1.44it/s]Extractor Predicting: 397it [04:28,  1.48it/s]Extractor Predicting: 398it [04:29,  1.46it/s]Extractor Predicting: 399it [04:30,  1.48it/s]Extractor Predicting: 400it [04:30,  1.48it/s]Extractor Predicting: 401it [04:31,  1.50it/s]Extractor Predicting: 402it [04:32,  1.54it/s]Extractor Predicting: 403it [04:32,  1.52it/s]Extractor Predicting: 404it [04:33,  1.54it/s]Extractor Predicting: 405it [04:34,  1.53it/s]Extractor Predicting: 406it [04:34,  1.53it/s]Extractor Predicting: 407it [04:35,  1.51it/s]Extractor Predicting: 408it [04:36,  1.55it/s]Extractor Predicting: 409it [04:36,  1.56it/s]Extractor Predicting: 410it [04:37,  1.56it/s]Extractor Predicting: 411it [04:38,  1.52it/s]Extractor Predicting: 412it [04:38,  1.54it/s]Extractor Predicting: 413it [04:39,  1.54it/s]Extractor Predicting: 414it [04:40,  1.54it/s]Extractor Predicting: 415it [04:40,  1.57it/s]Extractor Predicting: 416it [04:41,  1.55it/s]Extractor Predicting: 417it [04:42,  1.53it/s]Extractor Predicting: 418it [04:42,  1.55it/s]Extractor Predicting: 419it [04:43,  1.53it/s]Extractor Predicting: 420it [04:43,  1.53it/s]Extractor Predicting: 421it [04:44,  1.52it/s]Extractor Predicting: 422it [04:45,  1.51it/s]Extractor Predicting: 423it [04:45,  1.51it/s]Extractor Predicting: 424it [04:46,  1.52it/s]Extractor Predicting: 425it [04:47,  1.71it/s]Extractor Predicting: 425it [04:47,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:36,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:36,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:36,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:36,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:36,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:07:37,536 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:07:37,537 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:07:38,164 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:07:39,233 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:07:39,234 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:42,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:42,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:42,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:42,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:42,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:07:43,269 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:07:43,270 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:07:43,839 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:07:44,029 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:07:44,029 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35452404978136565,
  "recall": 0.10347535833496957,
  "score": 0.16019454365833272,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.36it/s]Extractor Predicting: 4it [00:02,  1.36it/s]Extractor Predicting: 5it [00:03,  1.35it/s]Extractor Predicting: 6it [00:04,  1.39it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 7it [00:04,  1.47it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:07:49,806 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:07:49,807 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:07:49,830 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:07:49,831 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:07:49,839 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:07:54,925 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:07:54,928 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:07:54,939 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:07:54,940 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:07:54,947 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:07:54,952 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:07:54,952 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:07:54,952 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:07:54,952 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:07:54,952 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:07:54,952 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5882352941176471,
  "recall": 0.03184713375796178,
  "score": 0.060422960725075525,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:07:55,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:55,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:56,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:57,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:58,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:58,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:59,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:00,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:01,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:02,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:02,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:03,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:04,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:05,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:05,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:06,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:07,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:08,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:08,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:09,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:50, 15.30s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:10,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:11,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:12,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:12,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:13,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:14,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:15,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:15,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:16,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:17,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:18,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:19,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:19,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:20,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:21,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:22,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:22,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:23,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:24,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:25,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:25,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:26,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:32<04:51, 16.20s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:27,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:27,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:28,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:29,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:30,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:30,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:31,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:32,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:33,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:33,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:34,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:35,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:35,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:36,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:37,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:38,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:38,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:39,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:40,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:40,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:41,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:42,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:47<04:31, 15.99s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:43,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:43,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:44,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:45,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:45,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:46,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:47,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:47,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:48,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:48,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:49,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:50,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:50,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:51,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:52,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:52,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:53,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:54,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:54,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:55,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:56,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:01<04:01, 15.11s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:56,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:57,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:58,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:58,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:59,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:00,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:00,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:01,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:02,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:03,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:03,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:04,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:05,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:05,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:06,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:07,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:08,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:08,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:09,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:10,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:10,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:11,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:17<03:48, 15.26s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:12,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:13,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:14,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:14,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:15,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:16,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:17,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:17,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:18,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:19,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:20,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:20,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:21,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:22,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:23,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:23,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:24,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:25,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:26,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:27,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:32<03:35, 15.37s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:27,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:28,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:29,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:30,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:31,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:31,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:32,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:33,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:33,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:34,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:35,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:35,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:36,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:37,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:38,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:38,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:39,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:40,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:40,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:41,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:42,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:43,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:44,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:50<03:28, 16.01s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:45,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:46,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:46,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:47,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:48,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:49,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:49,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:50,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:51,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:51,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:52,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:53,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:54,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:54,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:55,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:56,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:56,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:57,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:58,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:59,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:59,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:00,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:01,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:06<03:13, 16.15s/it][WARNING|generation_utils.py:914] 2023-08-28 18:10:01,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:02,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:03,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:04,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:04,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:05,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:06,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:07,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:08,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:08,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:09,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:10,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:11,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:12,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:13,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:14,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:14,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:15,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:16,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:17,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:17,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:18,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:24<03:02, 16.61s/it][WARNING|generation_utils.py:914] 2023-08-28 18:10:19,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:20,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:20,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:21,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:22,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:23,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:24,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:24,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:25,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:26,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:27,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:27,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:28,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:29,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:30,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:31,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:32,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:32,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:33,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:34,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:34,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:35,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:41<02:47, 16.77s/it][WARNING|generation_utils.py:914] 2023-08-28 18:10:36,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:37,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:38,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:38,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:39,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:40,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:41,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:41,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:42,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:43,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:43,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:44,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:45,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:46,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:46,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:47,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:48,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:48,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:49,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:50,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:51,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:56<02:26, 16.31s/it][WARNING|generation_utils.py:914] 2023-08-28 18:10:51,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:52,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:53,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:53,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:54,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:55,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:56,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:56,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:57,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:58,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:59,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:59,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:00,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:01,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:03,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:03,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:04,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:05,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:06,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:07,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:08,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:09,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:14<02:14, 16.85s/it][WARNING|generation_utils.py:914] 2023-08-28 18:11:09,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:10,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:11,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:11,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:12,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:13,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:14,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:14,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:15,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:16,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:16,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:17,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:17,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:18,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:19,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:20,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:20,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:21,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:22,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:22,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:28<01:51, 15.89s/it][WARNING|generation_utils.py:914] 2023-08-28 18:11:23,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:24,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:25,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:25,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:26,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:27,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:28,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:29,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:30,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:30,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:31,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:32,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:33,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:33,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:34,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:35,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:36,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:37,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:37,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:38,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:39,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:45<01:36, 16.16s/it][WARNING|generation_utils.py:914] 2023-08-28 18:11:40,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:40,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:41,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:42,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:43,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:43,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:44,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:45,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:46,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:47,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:47,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:48,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:49,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:49,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:50,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:51,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:52,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:52,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:53,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:54,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:55,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:55,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:01<01:21, 16.29s/it][WARNING|generation_utils.py:914] 2023-08-28 18:11:56,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:57,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:58,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:59,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:00,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:00,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:01,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:02,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:03,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:03,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:04,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:05,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:06,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:07,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:08,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:08,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:09,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:10,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:11,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:11,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:12,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:13,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:18<01:06, 16.56s/it][WARNING|generation_utils.py:914] 2023-08-28 18:12:14,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:14,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:15,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:16,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:16,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:17,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:18,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:19,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:19,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:20,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:21,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:21,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:22,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:23,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:23,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:24,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:25,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:26,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:26,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:27,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:33<00:47, 15.88s/it][WARNING|generation_utils.py:914] 2023-08-28 18:12:28,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:29,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:29,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:30,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:31,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:32,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:32,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:33,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:34,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:34,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:35,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:36,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:36,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:37,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:38,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:39,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:39,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:40,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:41,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:42,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:47<00:31, 15.50s/it][WARNING|generation_utils.py:914] 2023-08-28 18:12:42,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:43,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:44,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:44,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:45,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:46,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:47,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:47,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:48,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:48,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:49,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:50,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:51,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:51,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:52,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:53,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:54,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:54,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:55,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:56,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:56,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:02<00:15, 15.26s/it][WARNING|generation_utils.py:914] 2023-08-28 18:12:57,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:58,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:59,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:59,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:00,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:01,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:02,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:02,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:03,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:04,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:04,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:05,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:06,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:06,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:07,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:08,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:08,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:09,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:10,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:10,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:16<00:00, 14.92s/it]Generating: 100%|██████████| 20/20 [05:16<00:00, 15.83s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:18,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:18,431 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:18,431 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:18,431 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:18,431 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:13:18,831 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:13:18,832 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:13:19,122 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:13:20,264 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:13:20,264 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:21,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:21,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:21,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:21,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:21,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:13:22,031 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:13:22,032 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:13:22,328 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:13:22,530 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:13:22,530 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : has part .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : location .', 'success_rate': 0.8565340909090909, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : position held .', 'success_rate': 0.8650568181818182, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8315217391304348, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : father .', 'success_rate': 0.8778409090909091, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : instrument .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.953125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8536931818181818, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.96875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 10709
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10809, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.31it/s]Extractor Estimating: 2it [00:01,  1.33it/s]Extractor Estimating: 3it [00:02,  1.42it/s]Extractor Estimating: 4it [00:02,  1.36it/s]Extractor Estimating: 5it [00:03,  1.40it/s]Extractor Estimating: 6it [00:04,  1.44it/s]Extractor Estimating: 7it [00:04,  1.45it/s]Extractor Estimating: 8it [00:05,  1.49it/s]Extractor Estimating: 9it [00:06,  1.49it/s]Extractor Estimating: 10it [00:06,  1.52it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:08,  1.50it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:09,  1.55it/s]Extractor Estimating: 15it [00:10,  1.54it/s]Extractor Estimating: 16it [00:10,  1.52it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:12,  1.57it/s]Extractor Estimating: 19it [00:12,  1.60it/s]Extractor Estimating: 20it [00:13,  1.62it/s]Extractor Estimating: 21it [00:13,  1.59it/s]Extractor Estimating: 22it [00:14,  1.55it/s]Extractor Estimating: 23it [00:15,  1.54it/s]Extractor Estimating: 24it [00:15,  1.52it/s]Extractor Estimating: 25it [00:16,  1.48it/s]Extractor Estimating: 26it [00:17,  1.51it/s]Extractor Estimating: 27it [00:17,  1.54it/s]Extractor Estimating: 28it [00:18,  1.60it/s]Extractor Estimating: 29it [00:19,  1.59it/s]Extractor Estimating: 30it [00:19,  1.64it/s]Extractor Estimating: 31it [00:20,  1.62it/s]Extractor Estimating: 32it [00:20,  1.64it/s]Extractor Estimating: 33it [00:21,  1.61it/s]Extractor Estimating: 34it [00:22,  1.63it/s]Extractor Estimating: 35it [00:22,  1.59it/s]Extractor Estimating: 36it [00:23,  1.61it/s]Extractor Estimating: 37it [00:24,  1.62it/s]Extractor Estimating: 38it [00:24,  1.59it/s]Extractor Estimating: 39it [00:25,  1.55it/s]Extractor Estimating: 40it [00:25,  1.55it/s]Extractor Estimating: 41it [00:26,  1.52it/s]Extractor Estimating: 42it [00:27,  1.58it/s]Extractor Estimating: 43it [00:27,  1.59it/s]Extractor Estimating: 44it [00:28,  1.60it/s]Extractor Estimating: 45it [00:29,  1.56it/s]Extractor Estimating: 46it [00:29,  1.48it/s]Extractor Estimating: 47it [00:30,  1.53it/s]Extractor Estimating: 48it [00:31,  1.55it/s]Extractor Estimating: 49it [00:31,  1.51it/s]Extractor Estimating: 50it [00:32,  1.51it/s]Extractor Estimating: 51it [00:33,  1.55it/s]Extractor Estimating: 52it [00:33,  1.56it/s]Extractor Estimating: 53it [00:34,  1.59it/s]Extractor Estimating: 54it [00:34,  1.60it/s]Extractor Estimating: 55it [00:35,  1.61it/s]Extractor Estimating: 56it [00:36,  1.60it/s]Extractor Estimating: 57it [00:36,  1.59it/s]Extractor Estimating: 58it [00:37,  1.58it/s]Extractor Estimating: 59it [00:38,  1.57it/s]Extractor Estimating: 60it [00:38,  1.55it/s]Extractor Estimating: 61it [00:39,  1.56it/s]Extractor Estimating: 62it [00:40,  1.56it/s]Extractor Estimating: 63it [00:40,  1.58it/s]Extractor Estimating: 64it [00:41,  1.57it/s]Extractor Estimating: 65it [00:42,  1.54it/s]Extractor Estimating: 66it [00:42,  1.48it/s]Extractor Estimating: 67it [00:43,  1.53it/s]Extractor Estimating: 68it [00:43,  1.56it/s]Extractor Estimating: 69it [00:44,  1.60it/s]Extractor Estimating: 70it [00:45,  1.59it/s]Extractor Estimating: 71it [00:45,  1.48it/s]Extractor Estimating: 72it [00:46,  1.34it/s]Extractor Estimating: 73it [00:47,  1.39it/s]Extractor Estimating: 74it [00:48,  1.46it/s]Extractor Estimating: 75it [00:48,  1.51it/s]Extractor Estimating: 76it [00:49,  1.53it/s]Extractor Estimating: 77it [00:49,  1.60it/s]Extractor Estimating: 78it [00:50,  1.59it/s]Extractor Estimating: 79it [00:51,  1.62it/s]Extractor Estimating: 80it [00:51,  1.66it/s]Extractor Estimating: 81it [00:52,  1.64it/s]Extractor Estimating: 82it [00:52,  1.63it/s]Extractor Estimating: 83it [00:53,  1.67it/s]Extractor Estimating: 84it [00:54,  1.64it/s]Extractor Estimating: 85it [00:54,  1.62it/s]Extractor Estimating: 86it [00:55,  1.62it/s]Extractor Estimating: 87it [00:56,  1.64it/s]Extractor Estimating: 88it [00:56,  1.54it/s]Extractor Estimating: 89it [00:57,  1.57it/s]Extractor Estimating: 90it [00:57,  1.64it/s]Extractor Estimating: 91it [00:58,  1.66it/s]Extractor Estimating: 92it [00:59,  1.66it/s]Extractor Estimating: 93it [00:59,  1.65it/s]Extractor Estimating: 94it [01:00,  1.68it/s]Extractor Estimating: 95it [01:00,  1.72it/s]Extractor Estimating: 96it [01:01,  1.66it/s]Extractor Estimating: 97it [01:02,  1.66it/s]Extractor Estimating: 98it [01:02,  1.65it/s]Extractor Estimating: 99it [01:03,  1.61it/s]Extractor Estimating: 100it [01:04,  1.60it/s]Extractor Estimating: 101it [01:04,  1.63it/s]Extractor Estimating: 102it [01:05,  1.64it/s]Extractor Estimating: 103it [01:05,  1.66it/s]Extractor Estimating: 104it [01:06,  1.61it/s]Extractor Estimating: 105it [01:06,  1.69it/s]Extractor Estimating: 106it [01:07,  1.71it/s]Extractor Estimating: 107it [01:08,  1.74it/s]Extractor Estimating: 108it [01:08,  1.74it/s]Extractor Estimating: 109it [01:09,  1.70it/s]Extractor Estimating: 110it [01:09,  1.71it/s]Extractor Estimating: 111it [01:10,  1.70it/s]Extractor Estimating: 112it [01:10,  1.77it/s]Extractor Estimating: 113it [01:11,  1.78it/s]Extractor Estimating: 114it [01:12,  1.75it/s]Extractor Estimating: 115it [01:12,  1.72it/s]Extractor Estimating: 116it [01:13,  1.70it/s]Extractor Estimating: 117it [01:13,  1.68it/s]Extractor Estimating: 118it [01:14,  1.69it/s]Extractor Estimating: 119it [01:15,  1.65it/s]Extractor Estimating: 120it [01:15,  1.68it/s]Extractor Estimating: 121it [01:16,  1.61it/s]Extractor Estimating: 122it [01:17,  1.62it/s]Extractor Estimating: 123it [01:17,  1.63it/s]Extractor Estimating: 124it [01:18,  1.67it/s]Extractor Estimating: 125it [01:18,  1.65it/s]Extractor Estimating: 126it [01:19,  1.51it/s]Extractor Estimating: 127it [01:20,  1.52it/s]Extractor Estimating: 128it [01:20,  1.49it/s]Extractor Estimating: 129it [01:21,  1.47it/s]Extractor Estimating: 130it [01:22,  1.45it/s]Extractor Estimating: 131it [01:23,  1.47it/s]Extractor Estimating: 132it [01:23,  1.49it/s]Extractor Estimating: 133it [01:24,  1.47it/s]Extractor Estimating: 134it [01:25,  1.45it/s]Extractor Estimating: 135it [01:25,  1.46it/s]Extractor Estimating: 136it [01:26,  1.50it/s]Extractor Estimating: 137it [01:27,  1.53it/s]Extractor Estimating: 138it [01:27,  1.54it/s]Extractor Estimating: 139it [01:28,  1.50it/s]Extractor Estimating: 140it [01:29,  1.50it/s]Extractor Estimating: 141it [01:29,  1.52it/s]Extractor Estimating: 142it [01:30,  1.53it/s]Extractor Estimating: 143it [01:31,  1.48it/s]Extractor Estimating: 144it [01:31,  1.46it/s]Extractor Estimating: 145it [01:32,  1.50it/s]Extractor Estimating: 146it [01:33,  1.51it/s]Extractor Estimating: 147it [01:33,  1.54it/s]Extractor Estimating: 148it [01:34,  1.56it/s]Extractor Estimating: 149it [01:34,  1.52it/s]Extractor Estimating: 150it [01:35,  1.50it/s]Extractor Estimating: 151it [01:36,  1.56it/s]Extractor Estimating: 152it [01:36,  1.59it/s]Extractor Estimating: 153it [01:37,  1.58it/s]Extractor Estimating: 154it [01:38,  1.58it/s]Extractor Estimating: 155it [01:38,  1.61it/s]Extractor Estimating: 156it [01:39,  1.64it/s]Extractor Estimating: 157it [01:39,  1.66it/s]Extractor Estimating: 158it [01:40,  1.73it/s]Extractor Estimating: 159it [01:40,  1.76it/s]Extractor Estimating: 160it [01:41,  1.75it/s]Extractor Estimating: 161it [01:42,  1.54it/s]Extractor Estimating: 162it [01:42,  1.61it/s]Extractor Estimating: 163it [01:43,  1.68it/s]Extractor Estimating: 164it [01:44,  1.69it/s]Extractor Estimating: 165it [01:44,  1.68it/s]Extractor Estimating: 166it [01:45,  1.69it/s]Extractor Estimating: 167it [01:45,  1.70it/s]Extractor Estimating: 168it [01:46,  1.72it/s]Extractor Estimating: 169it [01:46,  1.74it/s]Extractor Estimating: 170it [01:47,  1.78it/s]Extractor Estimating: 171it [01:48,  1.71it/s]Extractor Estimating: 172it [01:48,  1.72it/s]Extractor Estimating: 173it [01:49,  1.63it/s]Extractor Estimating: 174it [01:49,  1.68it/s]Extractor Estimating: 175it [01:50,  1.68it/s]Extractor Estimating: 176it [01:51,  1.66it/s]Extractor Estimating: 177it [01:51,  1.65it/s]Extractor Estimating: 178it [01:52,  1.54it/s]Extractor Estimating: 179it [01:53,  1.57it/s]Extractor Estimating: 180it [01:53,  1.60it/s]Extractor Estimating: 181it [01:54,  1.63it/s]Extractor Estimating: 182it [01:54,  1.67it/s]Extractor Estimating: 183it [01:55,  1.69it/s]Extractor Estimating: 184it [01:55,  1.71it/s]Extractor Estimating: 185it [01:56,  1.71it/s]Extractor Estimating: 186it [01:57,  1.71it/s]Extractor Estimating: 187it [01:57,  1.75it/s]Extractor Estimating: 188it [01:58,  1.76it/s]Extractor Estimating: 189it [01:58,  1.74it/s]Extractor Estimating: 190it [01:59,  1.66it/s]Extractor Estimating: 191it [02:00,  1.70it/s]Extractor Estimating: 192it [02:00,  1.72it/s]Extractor Estimating: 193it [02:01,  1.70it/s]Extractor Estimating: 194it [02:01,  1.71it/s]Extractor Estimating: 195it [02:02,  1.71it/s]Extractor Estimating: 196it [02:02,  1.71it/s]Extractor Estimating: 197it [02:03,  1.70it/s]Extractor Estimating: 198it [02:04,  1.73it/s]Extractor Estimating: 199it [02:04,  1.71it/s]Extractor Estimating: 200it [02:05,  1.70it/s]Extractor Estimating: 201it [02:05,  1.65it/s]Extractor Estimating: 202it [02:06,  1.65it/s]Extractor Estimating: 203it [02:07,  1.61it/s]Extractor Estimating: 204it [02:07,  1.63it/s]Extractor Estimating: 205it [02:08,  1.60it/s]Extractor Estimating: 206it [02:09,  1.62it/s]Extractor Estimating: 207it [02:09,  1.64it/s]Extractor Estimating: 208it [02:10,  1.65it/s]Extractor Estimating: 209it [02:11,  1.49it/s]Extractor Estimating: 210it [02:11,  1.47it/s]Extractor Estimating: 211it [02:12,  1.46it/s]Extractor Estimating: 212it [02:13,  1.49it/s]Extractor Estimating: 213it [02:13,  1.47it/s]Extractor Estimating: 214it [02:14,  1.51it/s]Extractor Estimating: 215it [02:15,  1.53it/s]Extractor Estimating: 216it [02:15,  1.49it/s]Extractor Estimating: 217it [02:16,  1.55it/s]Extractor Estimating: 218it [02:16,  1.57it/s]Extractor Estimating: 219it [02:17,  1.55it/s]Extractor Estimating: 220it [02:18,  1.60it/s]Extractor Estimating: 221it [02:18,  1.56it/s]Extractor Estimating: 222it [02:19,  1.58it/s]Extractor Estimating: 223it [02:20,  1.66it/s]Extractor Estimating: 224it [02:20,  1.66it/s]Extractor Estimating: 225it [02:21,  1.68it/s]Extractor Estimating: 226it [02:21,  1.69it/s]Extractor Estimating: 227it [02:22,  1.64it/s]Extractor Estimating: 228it [02:23,  1.62it/s]Extractor Estimating: 229it [02:23,  1.54it/s]Extractor Estimating: 230it [02:24,  1.59it/s]Extractor Estimating: 231it [02:25,  1.58it/s]Extractor Estimating: 232it [02:25,  1.55it/s]Extractor Estimating: 233it [02:26,  1.49it/s]Extractor Estimating: 234it [02:27,  1.48it/s]Extractor Estimating: 235it [02:27,  1.49it/s]Extractor Estimating: 236it [02:28,  1.56it/s]Extractor Estimating: 237it [02:28,  1.59it/s]Extractor Estimating: 238it [02:29,  1.57it/s]Extractor Estimating: 239it [02:30,  1.55it/s]Extractor Estimating: 240it [02:30,  1.54it/s]Extractor Estimating: 241it [02:31,  1.56it/s]Extractor Estimating: 242it [02:32,  1.59it/s]Extractor Estimating: 243it [02:32,  1.57it/s]Extractor Estimating: 244it [02:33,  1.49it/s]Extractor Estimating: 245it [02:34,  1.52it/s]Extractor Estimating: 246it [02:34,  1.56it/s]Extractor Estimating: 247it [02:35,  1.58it/s]Extractor Estimating: 248it [02:36,  1.62it/s]Extractor Estimating: 249it [02:36,  1.62it/s]Extractor Estimating: 250it [02:37,  1.54it/s]Extractor Estimating: 251it [02:37,  1.64it/s]Extractor Estimating: 252it [02:38,  1.54it/s]Extractor Estimating: 253it [02:39,  1.60it/s]Extractor Estimating: 254it [02:39,  1.64it/s]Extractor Estimating: 255it [02:40,  1.59it/s]Extractor Estimating: 256it [02:41,  1.63it/s]Extractor Estimating: 257it [02:41,  1.62it/s]Extractor Estimating: 258it [02:42,  1.63it/s]Extractor Estimating: 259it [02:43,  1.49it/s]Extractor Estimating: 260it [02:43,  1.54it/s]Extractor Estimating: 261it [02:44,  1.58it/s]Extractor Estimating: 262it [02:44,  1.59it/s]Extractor Estimating: 263it [02:45,  1.60it/s]Extractor Estimating: 264it [02:46,  1.59it/s]Extractor Estimating: 265it [02:46,  1.56it/s]Extractor Estimating: 266it [02:47,  1.62it/s]Extractor Estimating: 267it [02:47,  1.65it/s]Extractor Estimating: 268it [02:48,  1.63it/s]Extractor Estimating: 269it [02:49,  1.65it/s]Extractor Estimating: 270it [02:49,  1.71it/s]Extractor Estimating: 271it [02:50,  1.65it/s]Extractor Estimating: 272it [02:50,  1.66it/s]Extractor Estimating: 273it [02:51,  1.69it/s]Extractor Estimating: 274it [02:52,  1.69it/s]Extractor Estimating: 275it [02:52,  1.69it/s]Extractor Estimating: 276it [02:53,  1.62it/s]Extractor Estimating: 277it [02:53,  1.61it/s]Extractor Estimating: 278it [02:54,  1.59it/s]Extractor Estimating: 279it [02:55,  1.59it/s]Extractor Estimating: 280it [02:55,  1.62it/s]Extractor Estimating: 281it [02:56,  1.65it/s]Extractor Estimating: 282it [02:57,  1.63it/s]Extractor Estimating: 283it [02:57,  1.58it/s]Extractor Estimating: 284it [02:58,  1.60it/s]Extractor Estimating: 285it [02:58,  1.59it/s]Extractor Estimating: 286it [02:59,  1.61it/s]Extractor Estimating: 287it [03:00,  1.53it/s]Extractor Estimating: 288it [03:00,  1.57it/s]Extractor Estimating: 289it [03:01,  1.51it/s]Extractor Estimating: 290it [03:02,  1.52it/s]Extractor Estimating: 291it [03:02,  1.59it/s]Extractor Estimating: 292it [03:03,  1.50it/s]Extractor Estimating: 293it [03:04,  1.48it/s]Extractor Estimating: 294it [03:04,  1.55it/s]Extractor Estimating: 295it [03:05,  1.41it/s]Extractor Estimating: 296it [03:06,  1.40it/s]Extractor Estimating: 297it [03:07,  1.45it/s]Extractor Estimating: 298it [03:07,  1.46it/s]Extractor Estimating: 299it [03:08,  1.47it/s]Extractor Estimating: 300it [03:09,  1.50it/s]Extractor Estimating: 301it [03:09,  1.53it/s]Extractor Estimating: 302it [03:10,  1.52it/s]Extractor Estimating: 303it [03:11,  1.50it/s]Extractor Estimating: 304it [03:11,  1.59it/s]Extractor Estimating: 305it [03:12,  1.65it/s]Extractor Estimating: 306it [03:12,  1.58it/s]Extractor Estimating: 307it [03:13,  1.55it/s]Extractor Estimating: 308it [03:14,  1.58it/s]Extractor Estimating: 309it [03:14,  1.61it/s]Extractor Estimating: 310it [03:15,  1.66it/s]Extractor Estimating: 311it [03:15,  1.69it/s]Extractor Estimating: 312it [03:16,  1.71it/s]Extractor Estimating: 313it [03:16,  1.69it/s]Extractor Estimating: 314it [03:17,  1.69it/s]Extractor Estimating: 315it [03:18,  1.67it/s]Extractor Estimating: 316it [03:18,  1.62it/s]Extractor Estimating: 317it [03:19,  1.56it/s]Extractor Estimating: 318it [03:20,  1.58it/s]Extractor Estimating: 319it [03:20,  1.66it/s]Extractor Estimating: 320it [03:21,  1.63it/s]Extractor Estimating: 321it [03:21,  1.64it/s]Extractor Estimating: 322it [03:22,  1.59it/s]Extractor Estimating: 323it [03:23,  1.60it/s]Extractor Estimating: 324it [03:23,  1.59it/s]Extractor Estimating: 325it [03:24,  1.63it/s]Extractor Estimating: 326it [03:24,  1.68it/s]Extractor Estimating: 327it [03:25,  1.65it/s]Extractor Estimating: 328it [03:26,  1.66it/s]Extractor Estimating: 329it [03:26,  1.69it/s]Extractor Estimating: 330it [03:27,  1.66it/s]Extractor Estimating: 331it [03:27,  1.72it/s]Extractor Estimating: 332it [03:28,  1.73it/s]Extractor Estimating: 333it [03:29,  1.73it/s]Extractor Estimating: 334it [03:29,  1.73it/s]Extractor Estimating: 335it [03:30,  1.65it/s]Extractor Estimating: 336it [03:30,  1.68it/s]Extractor Estimating: 337it [03:31,  1.71it/s]Extractor Estimating: 338it [03:32,  1.64it/s]Extractor Estimating: 339it [03:32,  1.71it/s]Extractor Estimating: 340it [03:33,  1.69it/s]Extractor Estimating: 341it [03:33,  1.69it/s]Extractor Estimating: 342it [03:34,  1.65it/s]Extractor Estimating: 343it [03:35,  1.65it/s]Extractor Estimating: 344it [03:35,  1.62it/s]Extractor Estimating: 345it [03:36,  1.57it/s]Extractor Estimating: 346it [03:37,  1.62it/s]Extractor Estimating: 347it [03:37,  1.63it/s]Extractor Estimating: 348it [03:38,  1.63it/s]Extractor Estimating: 349it [03:38,  1.66it/s]Extractor Estimating: 350it [03:39,  1.69it/s]Extractor Estimating: 351it [03:39,  1.67it/s]Extractor Estimating: 352it [03:40,  1.66it/s]Extractor Estimating: 353it [03:41,  1.66it/s]Extractor Estimating: 354it [03:41,  1.67it/s]Extractor Estimating: 355it [03:42,  1.66it/s]Extractor Estimating: 356it [03:43,  1.62it/s]Extractor Estimating: 357it [03:43,  1.63it/s]Extractor Estimating: 358it [03:44,  1.57it/s]Extractor Estimating: 359it [03:44,  1.57it/s]Extractor Estimating: 360it [03:45,  1.53it/s]Extractor Estimating: 361it [03:46,  1.51it/s]Extractor Estimating: 362it [03:47,  1.45it/s]Extractor Estimating: 363it [03:47,  1.51it/s]Extractor Estimating: 364it [03:48,  1.55it/s]Extractor Estimating: 365it [03:48,  1.57it/s]Extractor Estimating: 366it [03:49,  1.61it/s]Extractor Estimating: 367it [03:50,  1.68it/s]Extractor Estimating: 368it [03:50,  1.64it/s]Extractor Estimating: 369it [03:51,  1.64it/s]Extractor Estimating: 370it [03:51,  1.64it/s]Extractor Estimating: 371it [03:52,  1.68it/s]Extractor Estimating: 372it [03:53,  1.65it/s]Extractor Estimating: 373it [03:53,  1.66it/s]Extractor Estimating: 374it [03:54,  1.65it/s]Extractor Estimating: 375it [03:54,  1.65it/s]Extractor Estimating: 376it [03:55,  1.66it/s]Extractor Estimating: 377it [03:56,  1.64it/s]Extractor Estimating: 378it [03:56,  1.60it/s]Extractor Estimating: 379it [03:57,  1.61it/s]Extractor Estimating: 380it [03:58,  1.63it/s]Extractor Estimating: 381it [03:58,  1.61it/s]Extractor Estimating: 382it [03:59,  1.62it/s]Extractor Estimating: 383it [03:59,  1.58it/s]Extractor Estimating: 384it [04:00,  1.62it/s]Extractor Estimating: 385it [04:01,  1.62it/s]Extractor Estimating: 386it [04:01,  1.67it/s]Extractor Estimating: 387it [04:02,  1.64it/s]Extractor Estimating: 388it [04:02,  1.68it/s]Extractor Estimating: 389it [04:03,  1.70it/s]Extractor Estimating: 390it [04:04,  1.61it/s]Extractor Estimating: 391it [04:04,  1.60it/s]Extractor Estimating: 392it [04:05,  1.62it/s]Extractor Estimating: 393it [04:06,  1.57it/s]Extractor Estimating: 394it [04:06,  1.63it/s]Extractor Estimating: 395it [04:07,  1.64it/s]Extractor Estimating: 396it [04:07,  1.64it/s]Extractor Estimating: 397it [04:08,  1.61it/s]Extractor Estimating: 398it [04:09,  1.60it/s]Extractor Estimating: 399it [04:09,  1.62it/s]Extractor Estimating: 400it [04:10,  1.63it/s]Extractor Estimating: 401it [04:10,  1.64it/s]Extractor Estimating: 402it [04:11,  1.57it/s]Extractor Estimating: 403it [04:12,  1.47it/s]Extractor Estimating: 404it [04:13,  1.48it/s]Extractor Estimating: 405it [04:13,  1.50it/s]Extractor Estimating: 406it [04:14,  1.55it/s]Extractor Estimating: 407it [04:14,  1.58it/s]Extractor Estimating: 408it [04:15,  1.58it/s]Extractor Estimating: 409it [04:16,  1.61it/s]Extractor Estimating: 410it [04:16,  1.63it/s]Extractor Estimating: 411it [04:17,  1.67it/s]Extractor Estimating: 412it [04:17,  1.68it/s]Extractor Estimating: 413it [04:18,  1.71it/s]Extractor Estimating: 414it [04:19,  1.71it/s]Extractor Estimating: 415it [04:19,  1.72it/s]Extractor Estimating: 416it [04:20,  1.66it/s]Extractor Estimating: 417it [04:20,  1.67it/s]Extractor Estimating: 418it [04:21,  1.64it/s]Extractor Estimating: 419it [04:22,  1.64it/s]Extractor Estimating: 420it [04:22,  1.56it/s]Extractor Estimating: 421it [04:23,  1.62it/s]Extractor Estimating: 422it [04:24,  1.59it/s]Extractor Estimating: 423it [04:24,  1.59it/s]Extractor Estimating: 424it [04:25,  1.68it/s]Extractor Estimating: 425it [04:25,  1.61it/s]Extractor Estimating: 426it [04:26,  1.52it/s]Extractor Estimating: 427it [04:27,  1.49it/s]Extractor Estimating: 428it [04:27,  1.52it/s]Extractor Estimating: 429it [04:28,  1.48it/s]Extractor Estimating: 430it [04:29,  1.49it/s]Extractor Estimating: 431it [04:29,  1.54it/s]Extractor Estimating: 432it [04:30,  1.57it/s]Extractor Estimating: 433it [04:31,  1.54it/s]Extractor Estimating: 434it [04:31,  1.55it/s]Extractor Estimating: 435it [04:32,  1.56it/s]Extractor Estimating: 436it [04:33,  1.59it/s]Extractor Estimating: 437it [04:33,  1.67it/s]Extractor Estimating: 438it [04:34,  1.64it/s]Extractor Estimating: 439it [04:34,  1.67it/s]Extractor Estimating: 440it [04:35,  1.63it/s]Extractor Estimating: 441it [04:36,  1.59it/s]Extractor Estimating: 442it [04:36,  1.56it/s]Extractor Estimating: 443it [04:37,  1.55it/s]Extractor Estimating: 444it [04:38,  1.56it/s]Extractor Estimating: 445it [04:38,  1.57it/s]Extractor Estimating: 446it [04:39,  1.53it/s]Extractor Estimating: 447it [04:40,  1.54it/s]Extractor Estimating: 448it [04:40,  1.54it/s]Extractor Estimating: 449it [04:41,  1.48it/s]Extractor Estimating: 450it [04:42,  1.49it/s]Extractor Estimating: 451it [04:42,  1.57it/s]Extractor Estimating: 452it [04:43,  1.60it/s]Extractor Estimating: 453it [04:43,  1.64it/s]Extractor Estimating: 454it [04:44,  1.71it/s]Extractor Estimating: 455it [04:44,  1.71it/s]Extractor Estimating: 456it [04:45,  1.75it/s]Extractor Estimating: 457it [04:45,  1.78it/s]Extractor Estimating: 458it [04:46,  1.85it/s]Extractor Estimating: 459it [04:47,  1.83it/s]Extractor Estimating: 460it [04:47,  1.80it/s]Extractor Estimating: 461it [04:48,  1.75it/s]Extractor Estimating: 462it [04:48,  1.81it/s]Extractor Estimating: 463it [04:49,  1.62it/s]Extractor Estimating: 464it [04:50,  1.63it/s]Extractor Estimating: 465it [04:50,  1.64it/s]Extractor Estimating: 466it [04:51,  1.66it/s]Extractor Estimating: 467it [04:51,  1.70it/s]Extractor Estimating: 468it [04:52,  1.73it/s]Extractor Estimating: 469it [04:52,  1.74it/s]Extractor Estimating: 470it [04:53,  1.78it/s]Extractor Estimating: 471it [04:54,  1.77it/s]Extractor Estimating: 472it [04:54,  1.74it/s]Extractor Estimating: 473it [04:55,  1.71it/s]Extractor Estimating: 474it [04:55,  1.71it/s]Extractor Estimating: 475it [04:56,  1.69it/s]Extractor Estimating: 476it [04:57,  1.66it/s]Extractor Estimating: 477it [04:57,  1.68it/s]Extractor Estimating: 478it [04:58,  1.64it/s]Extractor Estimating: 479it [04:59,  1.54it/s]Extractor Estimating: 480it [04:59,  1.62it/s]Extractor Estimating: 481it [05:00,  1.67it/s]Extractor Estimating: 482it [05:00,  1.66it/s]Extractor Estimating: 483it [05:01,  1.62it/s]Extractor Estimating: 484it [05:01,  1.66it/s]Extractor Estimating: 485it [05:02,  1.69it/s]Extractor Estimating: 486it [05:03,  1.66it/s]Extractor Estimating: 487it [05:03,  1.69it/s]Extractor Estimating: 488it [05:04,  1.71it/s]Extractor Estimating: 489it [05:05,  1.60it/s]Extractor Estimating: 490it [05:05,  1.65it/s]Extractor Estimating: 491it [05:06,  1.60it/s]Extractor Estimating: 492it [05:06,  1.58it/s]Extractor Estimating: 493it [05:07,  1.60it/s]Extractor Estimating: 494it [05:08,  1.65it/s]Extractor Estimating: 495it [05:08,  1.67it/s]Extractor Estimating: 496it [05:09,  1.65it/s]Extractor Estimating: 497it [05:09,  1.65it/s]Extractor Estimating: 498it [05:10,  1.69it/s]Extractor Estimating: 499it [05:11,  1.62it/s]Extractor Estimating: 500it [05:11,  1.63it/s]Extractor Estimating: 500it [05:11,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:48,175 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:48,260 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:48,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:48,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:48,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:18:48,672 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:18:48,673 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:18:49,358 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:18:50,481 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:18:50,481 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:52,060 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:52,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:52,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:52,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:52,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:18:52,415 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:18:52,417 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:18:52,677 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:18:52,851 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:18:52,851 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:27:49,718 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:27:49,720 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9996 mean pseudo reward: 0.9472449174884955
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 21284
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21384, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21384, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.091, loss:678.5658
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.082, loss:651.8032
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.090, loss:660.8112
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.093, loss:653.8474
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.102, loss:612.3607
>> valid entity prec:0.5727, rec:0.5505, f1:0.5614
>> valid relation prec:0.4125, rec:0.1440, f1:0.2134
>> valid relation with NER prec:0.4125, rec:0.1440, f1:0.2134
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.494, loss:658.9002
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.080, loss:658.9115
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.097, loss:659.8151
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.081, loss:613.3792
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.084, loss:637.8949
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6123, rec:0.4991, f1:0.5499
>> valid relation prec:0.4461, rec:0.1411, f1:0.2144
>> valid relation with NER prec:0.4461, rec:0.1411, f1:0.2144
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.499, loss:645.5919
g_step 1200, step 366, avg_time 1.092, loss:638.5757
g_step 1300, step 49, avg_time 1.080, loss:649.0527
g_step 1400, step 149, avg_time 1.077, loss:590.1321
g_step 1500, step 249, avg_time 1.108, loss:614.2706
>> valid entity prec:0.5710, rec:0.5207, f1:0.5447
>> valid relation prec:0.3395, rec:0.1219, f1:0.1794
>> valid relation with NER prec:0.3395, rec:0.1219, f1:0.1794
g_step 1600, step 349, avg_time 2.497, loss:628.4323
g_step 1700, step 32, avg_time 1.079, loss:582.6140
g_step 1800, step 132, avg_time 1.089, loss:572.9588
g_step 1900, step 232, avg_time 1.084, loss:580.9046
g_step 2000, step 332, avg_time 1.094, loss:577.6921
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5490, rec:0.5546, f1:0.5518
>> valid relation prec:0.4012, rec:0.1379, f1:0.2053
>> valid relation with NER prec:0.4012, rec:0.1379, f1:0.2053
g_step 2100, step 15, avg_time 2.484, loss:591.1224
g_step 2200, step 115, avg_time 1.098, loss:529.8086
g_step 2300, step 215, avg_time 1.082, loss:549.3064
g_step 2400, step 315, avg_time 1.074, loss:573.2780
g_step 2500, step 415, avg_time 1.095, loss:543.8121
>> valid entity prec:0.5047, rec:0.6461, f1:0.5667
>> valid relation prec:0.3212, rec:0.1336, f1:0.1887
>> valid relation with NER prec:0.3212, rec:0.1336, f1:0.1887
new max entity f1 on valid!
g_step 2600, step 98, avg_time 2.505, loss:510.8040
g_step 2700, step 198, avg_time 1.077, loss:506.6849
g_step 2800, step 298, avg_time 1.083, loss:538.3587
g_step 2900, step 398, avg_time 1.111, loss:558.0782
g_step 3000, step 81, avg_time 1.081, loss:492.6760
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5204, rec:0.5639, f1:0.5413
>> valid relation prec:0.3396, rec:0.1299, f1:0.1879
>> valid relation with NER prec:0.3396, rec:0.1299, f1:0.1879
g_step 3100, step 181, avg_time 2.483, loss:488.1878
g_step 3200, step 281, avg_time 1.095, loss:507.4746
g_step 3300, step 381, avg_time 1.090, loss:536.2830
g_step 3400, step 64, avg_time 1.080, loss:483.1096
g_step 3500, step 164, avg_time 1.106, loss:494.5036
>> valid entity prec:0.5671, rec:0.5569, f1:0.5620
>> valid relation prec:0.3596, rec:0.1348, f1:0.1961
>> valid relation with NER prec:0.3596, rec:0.1348, f1:0.1961
g_step 3600, step 264, avg_time 2.489, loss:478.8732
g_step 3700, step 364, avg_time 1.096, loss:501.6995
g_step 3800, step 47, avg_time 1.094, loss:456.5478
g_step 3900, step 147, avg_time 1.096, loss:455.3919
g_step 4000, step 247, avg_time 1.084, loss:480.3029
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5506, rec:0.5346, f1:0.5425
>> valid relation prec:0.3023, rec:0.1184, f1:0.1702
>> valid relation with NER prec:0.3023, rec:0.1184, f1:0.1702
g_step 4100, step 347, avg_time 2.497, loss:481.6011
g_step 4200, step 30, avg_time 1.079, loss:465.9631
g_step 4300, step 130, avg_time 1.105, loss:433.5146
g_step 4400, step 230, avg_time 1.093, loss:443.6087
g_step 4500, step 330, avg_time 1.082, loss:459.5491
>> valid entity prec:0.5216, rec:0.5682, f1:0.5439
>> valid relation prec:0.3184, rec:0.1247, f1:0.1793
>> valid relation with NER prec:0.3184, rec:0.1247, f1:0.1793
g_step 4600, step 13, avg_time 2.499, loss:458.7236
g_step 4700, step 113, avg_time 1.091, loss:425.0042
g_step 4800, step 213, avg_time 1.092, loss:436.1172
g_step 4900, step 313, avg_time 1.093, loss:426.3717
g_step 5000, step 413, avg_time 1.083, loss:455.4559
learning rate was adjusted to 0.0008
>> valid entity prec:0.5645, rec:0.5408, f1:0.5524
>> valid relation prec:0.3416, rec:0.1385, f1:0.1971
>> valid relation with NER prec:0.3416, rec:0.1385, f1:0.1971
g_step 5100, step 96, avg_time 2.481, loss:411.8286
g_step 5200, step 196, avg_time 1.093, loss:403.1380
g_step 5300, step 296, avg_time 1.093, loss:423.7731
g_step 5400, step 396, avg_time 1.094, loss:450.5551
g_step 5500, step 79, avg_time 1.089, loss:377.2873
>> valid entity prec:0.5543, rec:0.5459, f1:0.5501
>> valid relation prec:0.3140, rec:0.1239, f1:0.1777
>> valid relation with NER prec:0.3140, rec:0.1239, f1:0.1777
g_step 5600, step 179, avg_time 2.503, loss:395.3220
g_step 5700, step 279, avg_time 1.077, loss:434.7005
g_step 5800, step 379, avg_time 1.092, loss:412.3063
g_step 5900, step 62, avg_time 1.093, loss:392.1736
g_step 6000, step 162, avg_time 1.087, loss:382.4722
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5659, rec:0.5381, f1:0.5516
>> valid relation prec:0.3153, rec:0.1268, f1:0.1808
>> valid relation with NER prec:0.3153, rec:0.1268, f1:0.1808
g_step 6100, step 262, avg_time 2.484, loss:408.7943
g_step 6200, step 362, avg_time 1.085, loss:404.4426
g_step 6300, step 45, avg_time 1.096, loss:380.3584
g_step 6400, step 145, avg_time 1.111, loss:361.3864
g_step 6500, step 245, avg_time 1.074, loss:390.8662
>> valid entity prec:0.5515, rec:0.5342, f1:0.5427
>> valid relation prec:0.2932, rec:0.1144, f1:0.1646
>> valid relation with NER prec:0.2932, rec:0.1144, f1:0.1646
g_step 6600, step 345, avg_time 2.496, loss:394.0407
g_step 6700, step 28, avg_time 1.086, loss:380.4536
g_step 6800, step 128, avg_time 1.092, loss:385.1564
g_step 6900, step 228, avg_time 1.087, loss:364.2932
g_step 7000, step 328, avg_time 1.097, loss:380.0769
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5658, rec:0.5290, f1:0.5468
>> valid relation prec:0.3278, rec:0.1362, f1:0.1925
>> valid relation with NER prec:0.3278, rec:0.1362, f1:0.1925
g_step 7100, step 11, avg_time 2.487, loss:374.1323
g_step 7200, step 111, avg_time 1.082, loss:343.5074
g_step 7300, step 211, avg_time 1.098, loss:344.2067
g_step 7400, step 311, avg_time 1.084, loss:377.0677
g_step 7500, step 411, avg_time 1.096, loss:362.9843
>> valid entity prec:0.5612, rec:0.5099, f1:0.5343
>> valid relation prec:0.3079, rec:0.1136, f1:0.1659
>> valid relation with NER prec:0.3079, rec:0.1136, f1:0.1659
g_step 7600, step 94, avg_time 2.496, loss:327.8604
g_step 7700, step 194, avg_time 1.084, loss:333.9766
g_step 7800, step 294, avg_time 1.087, loss:355.0008
g_step 7900, step 394, avg_time 1.075, loss:371.1999
g_step 8000, step 77, avg_time 1.085, loss:312.2249
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5428, rec:0.5109, f1:0.5264
>> valid relation prec:0.3112, rec:0.1236, f1:0.1769
>> valid relation with NER prec:0.3112, rec:0.1236, f1:0.1769
g_step 8100, step 177, avg_time 2.484, loss:339.6060
g_step 8200, step 277, avg_time 1.098, loss:329.5632
g_step 8300, step 377, avg_time 1.080, loss:352.8590
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:27:49 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:27:49 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-27-49_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:27:50 - WARNING - datasets.builder -   Using custom data configuration default-c20756b005259eee
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c20756b005259eee/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:27:51,371 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:27:51,372 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:27:51,373 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:27:51,374 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:27:51,384 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:51,390 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:51,390 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:51,390 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:51,390 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:51,390 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:51,390 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:27:52,792 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:27:56,134 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:27:56,175 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c20756b005259eee/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:03,  2.93ba/s] 20%|██        | 2/10 [00:00<00:02,  3.81ba/s] 30%|███       | 3/10 [00:00<00:01,  4.25ba/s] 40%|████      | 4/10 [00:00<00:01,  4.52ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.63ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.69ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.73ba/s] 80%|████████  | 8/10 [00:01<00:00,  3.91ba/s] 90%|█████████ | 9/10 [00:02<00:00,  3.93ba/s]100%|██████████| 10/10 [00:02<00:00,  4.19ba/s]100%|██████████| 10/10 [00:02<00:00,  4.20ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.47ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.95ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.15ba/s]100%|██████████| 4/4 [00:00<00:00,  5.27ba/s]100%|██████████| 4/4 [00:00<00:00,  4.68ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  8.61ba/s] 30%|███       | 3/10 [00:00<00:00,  9.84ba/s] 50%|█████     | 5/10 [00:00<00:00, 10.15ba/s] 70%|███████   | 7/10 [00:00<00:00, 10.26ba/s] 90%|█████████ | 9/10 [00:00<00:00, 10.35ba/s]100%|██████████| 10/10 [00:00<00:00, 10.22ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.16ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.05ba/s]100%|██████████| 4/4 [00:00<00:00, 11.46ba/s]
[INFO|trainer.py:414] 2023-08-28 21:28:03,391 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:28:03,438 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:28:03,439 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-28 21:28:03,439 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:28:03,439 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:28:03,439 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:28:03,439 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:28:03,439 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:58,  3.27it/s]  0%|          | 2/780 [00:00<03:49,  3.39it/s]  0%|          | 3/780 [00:00<03:46,  3.43it/s]  1%|          | 4/780 [00:01<03:44,  3.46it/s]  1%|          | 5/780 [00:01<03:44,  3.46it/s]  1%|          | 6/780 [00:01<03:43,  3.46it/s]  1%|          | 7/780 [00:02<03:42,  3.47it/s]  1%|          | 8/780 [00:02<03:49,  3.36it/s]  1%|          | 9/780 [00:02<03:46,  3.40it/s]  1%|▏         | 10/780 [00:02<03:44,  3.42it/s]  1%|▏         | 11/780 [00:03<03:43,  3.44it/s]  2%|▏         | 12/780 [00:03<03:42,  3.45it/s]  2%|▏         | 13/780 [00:03<03:41,  3.46it/s]  2%|▏         | 14/780 [00:04<03:40,  3.47it/s]  2%|▏         | 15/780 [00:04<03:40,  3.47it/s]  2%|▏         | 16/780 [00:04<03:39,  3.48it/s]  2%|▏         | 17/780 [00:04<03:39,  3.48it/s]  2%|▏         | 18/780 [00:05<03:39,  3.48it/s]  2%|▏         | 19/780 [00:05<03:39,  3.47it/s]  3%|▎         | 20/780 [00:05<03:38,  3.47it/s]  3%|▎         | 21/780 [00:06<03:38,  3.47it/s]  3%|▎         | 22/780 [00:06<03:38,  3.47it/s]  3%|▎         | 23/780 [00:06<03:38,  3.47it/s]  3%|▎         | 24/780 [00:06<03:37,  3.48it/s]  3%|▎         | 25/780 [00:07<03:37,  3.48it/s]  3%|▎         | 26/780 [00:07<03:36,  3.48it/s]  3%|▎         | 27/780 [00:07<03:36,  3.48it/s]  4%|▎         | 28/780 [00:08<03:36,  3.48it/s]  4%|▎         | 29/780 [00:08<03:35,  3.48it/s]  4%|▍         | 30/780 [00:08<04:17,  2.91it/s]  4%|▍         | 31/780 [00:09<04:04,  3.06it/s]  4%|▍         | 32/780 [00:09<03:55,  3.18it/s]  4%|▍         | 33/780 [00:09<03:49,  3.26it/s]  4%|▍         | 34/780 [00:10<03:44,  3.32it/s]  4%|▍         | 35/780 [00:10<03:41,  3.37it/s]  5%|▍         | 36/780 [00:10<03:38,  3.40it/s]  5%|▍         | 37/780 [00:10<03:37,  3.42it/s]  5%|▍         | 38/780 [00:11<03:35,  3.44it/s]  5%|▌         | 39/780 [00:11<03:34,  3.45it/s]  5%|▌         | 40/780 [00:11<03:36,  3.42it/s]  5%|▌         | 41/780 [00:12<03:34,  3.44it/s]  5%|▌         | 42/780 [00:12<03:33,  3.45it/s]  6%|▌         | 43/780 [00:12<03:32,  3.46it/s]  6%|▌         | 44/780 [00:12<03:32,  3.47it/s]  6%|▌         | 45/780 [00:13<03:31,  3.47it/s]  6%|▌         | 46/780 [00:13<03:31,  3.47it/s]  6%|▌         | 47/780 [00:13<03:31,  3.47it/s]  6%|▌         | 48/780 [00:14<03:30,  3.48it/s]  6%|▋         | 49/780 [00:14<03:30,  3.48it/s]  6%|▋         | 50/780 [00:14<03:29,  3.48it/s]  7%|▋         | 51/780 [00:14<03:37,  3.36it/s]  7%|▋         | 52/780 [00:15<03:34,  3.40it/s]  7%|▋         | 53/780 [00:15<03:32,  3.42it/s]  7%|▋         | 54/780 [00:15<03:31,  3.44it/s]  7%|▋         | 55/780 [00:16<03:30,  3.45it/s]  7%|▋         | 56/780 [00:16<03:29,  3.46it/s]  7%|▋         | 57/780 [00:16<03:28,  3.47it/s]  7%|▋         | 58/780 [00:16<03:28,  3.47it/s]  8%|▊         | 59/780 [00:17<03:27,  3.47it/s]  8%|▊         | 60/780 [00:17<03:27,  3.47it/s]  8%|▊         | 61/780 [00:17<03:27,  3.47it/s]  8%|▊         | 62/780 [00:18<03:26,  3.47it/s]  8%|▊         | 63/780 [00:18<03:26,  3.47it/s]  8%|▊         | 64/780 [00:18<03:26,  3.47it/s]  8%|▊         | 65/780 [00:18<03:25,  3.47it/s]  8%|▊         | 66/780 [00:19<03:25,  3.47it/s]  9%|▊         | 67/780 [00:19<03:25,  3.48it/s]  9%|▊         | 68/780 [00:19<03:24,  3.48it/s]  9%|▉         | 69/780 [00:20<03:28,  3.40it/s]  9%|▉         | 70/780 [00:20<03:27,  3.43it/s]  9%|▉         | 71/780 [00:20<03:26,  3.44it/s]  9%|▉         | 72/780 [00:20<03:25,  3.45it/s]  9%|▉         | 73/780 [00:21<03:24,  3.46it/s]  9%|▉         | 74/780 [00:21<03:23,  3.47it/s] 10%|▉         | 75/780 [00:21<03:23,  3.47it/s] 10%|▉         | 76/780 [00:22<03:22,  3.47it/s] 10%|▉         | 77/780 [00:22<03:22,  3.47it/s] 10%|█         | 78/780 [00:22<03:22,  3.47it/s] 10%|█         | 79/780 [00:23<03:21,  3.47it/s] 10%|█         | 80/780 [00:23<03:25,  3.40it/s] 10%|█         | 81/780 [00:23<03:24,  3.42it/s] 11%|█         | 82/780 [00:23<03:23,  3.44it/s] 11%|█         | 83/780 [00:24<03:22,  3.45it/s] 11%|█         | 84/780 [00:24<03:21,  3.46it/s] 11%|█         | 85/780 [00:24<03:20,  3.46it/s] 11%|█         | 86/780 [00:25<03:35,  3.23it/s] 11%|█         | 87/780 [00:25<03:30,  3.28it/s] 11%|█▏        | 88/780 [00:25<03:28,  3.33it/s] 11%|█▏        | 89/780 [00:25<03:25,  3.37it/s] 12%|█▏        | 90/780 [00:26<03:25,  3.36it/s] 12%|█▏        | 91/780 [00:26<03:24,  3.37it/s] 12%|█▏        | 92/780 [00:26<03:22,  3.40it/s] 12%|█▏        | 93/780 [00:27<03:21,  3.41it/s] 12%|█▏        | 94/780 [00:27<03:20,  3.42it/s] 12%|█▏        | 95/780 [00:27<03:19,  3.43it/s] 12%|█▏        | 96/780 [00:28<03:18,  3.44it/s] 12%|█▏        | 97/780 [00:28<03:18,  3.45it/s] 13%|█▎        | 98/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 99/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 100/780 [00:29<03:16,  3.46it/s] 13%|█▎        | 101/780 [00:29<03:16,  3.46it/s] 13%|█▎        | 102/780 [00:29<03:21,  3.37it/s] 13%|█▎        | 103/780 [00:30<03:19,  3.40it/s] 13%|█▎        | 104/780 [00:30<03:17,  3.42it/s] 13%|█▎        | 105/780 [00:30<03:16,  3.43it/s] 14%|█▎        | 106/780 [00:30<03:16,  3.44it/s] 14%|█▎        | 107/780 [00:31<03:15,  3.44it/s] 14%|█▍        | 108/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 109/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 110/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 111/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 112/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 113/780 [00:32<03:15,  3.40it/s] 15%|█▍        | 114/780 [00:33<03:14,  3.42it/s] 15%|█▍        | 115/780 [00:33<03:13,  3.43it/s] 15%|█▍        | 116/780 [00:33<03:12,  3.44it/s] 15%|█▌        | 117/780 [00:34<03:12,  3.45it/s] 15%|█▌        | 118/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 119/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 120/780 [00:35<03:11,  3.46it/s] 16%|█▌        | 121/780 [00:35<03:19,  3.30it/s] 16%|█▌        | 122/780 [00:35<03:17,  3.34it/s] 16%|█▌        | 123/780 [00:35<03:14,  3.37it/s] 16%|█▌        | 124/780 [00:36<03:15,  3.36it/s] 16%|█▌        | 125/780 [00:36<03:13,  3.39it/s] 16%|█▌        | 126/780 [00:36<03:11,  3.41it/s] 16%|█▋        | 127/780 [00:37<03:10,  3.43it/s] 16%|█▋        | 128/780 [00:37<03:09,  3.44it/s] 17%|█▋        | 129/780 [00:37<03:09,  3.44it/s] 17%|█▋        | 130/780 [00:37<03:08,  3.44it/s] 17%|█▋        | 131/780 [00:38<03:08,  3.44it/s] 17%|█▋        | 132/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 133/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 134/780 [00:39<03:07,  3.45it/s] 17%|█▋        | 135/780 [00:39<03:17,  3.26it/s] 17%|█▋        | 136/780 [00:39<03:14,  3.32it/s] 18%|█▊        | 137/780 [00:40<03:11,  3.36it/s] 18%|█▊        | 138/780 [00:40<03:09,  3.39it/s] 18%|█▊        | 139/780 [00:40<03:07,  3.41it/s] 18%|█▊        | 140/780 [00:40<03:06,  3.42it/s] 18%|█▊        | 141/780 [00:41<03:06,  3.43it/s] 18%|█▊        | 142/780 [00:41<03:05,  3.44it/s] 18%|█▊        | 143/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 144/780 [00:42<03:19,  3.19it/s] 19%|█▊        | 145/780 [00:42<03:20,  3.16it/s] 19%|█▊        | 146/780 [00:42<03:15,  3.24it/s] 19%|█▉        | 147/780 [00:43<03:11,  3.30it/s] 19%|█▉        | 148/780 [00:43<03:09,  3.34it/s] 19%|█▉        | 149/780 [00:43<03:07,  3.37it/s] 19%|█▉        | 150/780 [00:43<03:05,  3.39it/s] 19%|█▉        | 151/780 [00:44<03:04,  3.41it/s] 19%|█▉        | 152/780 [00:44<03:03,  3.43it/s] 20%|█▉        | 153/780 [00:44<03:02,  3.44it/s] 20%|█▉        | 154/780 [00:45<03:01,  3.44it/s] 20%|█▉        | 155/780 [00:45<03:01,  3.45it/s] 20%|██        | 156/780 [00:45<03:36,  2.88it/s][INFO|trainer.py:2140] 2023-08-28 21:28:49,335 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:28:49,335 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:28:49,335 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.92it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.23it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.60it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.88it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.51it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.25it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.98it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.73it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.59it/s][A
 12%|█▏        | 53/437 [00:01<00:11, 33.18it/s][A
 13%|█▎        | 58/437 [00:01<00:10, 36.22it/s][A
 14%|█▍        | 63/437 [00:01<00:09, 38.87it/s][A
 16%|█▌        | 68/437 [00:01<00:09, 40.96it/s][A
 17%|█▋        | 73/437 [00:01<00:10, 35.99it/s][A
 18%|█▊        | 78/437 [00:01<00:09, 38.63it/s][A
 19%|█▉        | 83/437 [00:01<00:08, 40.76it/s][A
 20%|██        | 88/437 [00:02<00:08, 42.33it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 43.57it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 44.47it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 45.01it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 45.52it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 45.61it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 45.85it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.11it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.28it/s][A
 30%|███       | 133/437 [00:03<00:06, 46.34it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 46.54it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.56it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.52it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.60it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.45it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.48it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.55it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.44it/s][A
 41%|████      | 178/437 [00:04<00:06, 37.63it/s][A
 42%|████▏     | 183/437 [00:04<00:06, 39.89it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 41.77it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 43.22it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 44.15it/s][A
 46%|████▋     | 203/437 [00:04<00:06, 36.19it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 38.79it/s][A
 49%|████▊     | 213/437 [00:04<00:05, 40.88it/s][A
 50%|████▉     | 218/437 [00:05<00:05, 42.43it/s][A
 51%|█████     | 223/437 [00:05<00:04, 43.57it/s][A
 52%|█████▏    | 228/437 [00:05<00:04, 44.43it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 45.08it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 45.57it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 45.66it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 45.92it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.08it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.19it/s][A
 60%|██████    | 263/437 [00:06<00:03, 46.42it/s][A
 61%|██████▏   | 268/437 [00:06<00:03, 46.48it/s][A
 62%|██████▏   | 273/437 [00:06<00:03, 46.52it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.60it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.46it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.45it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.34it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.34it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.40it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.46it/s][A
 72%|███████▏  | 313/437 [00:07<00:02, 46.38it/s][A
 73%|███████▎  | 318/437 [00:07<00:02, 46.44it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 46.35it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.37it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.25it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.27it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 38.63it/s][A
 80%|███████▉  | 348/437 [00:07<00:02, 40.73it/s][A
 81%|████████  | 353/437 [00:08<00:01, 42.35it/s][A
 82%|████████▏ | 358/437 [00:08<00:01, 43.54it/s][A
 83%|████████▎ | 363/437 [00:08<00:01, 44.45it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 45.02it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 45.57it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 45.85it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 45.84it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 45.98it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.11it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.30it/s][A
 92%|█████████▏| 403/437 [00:09<00:00, 46.42it/s][A
 93%|█████████▎| 408/437 [00:09<00:00, 46.43it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 46.54it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.55it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.54it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.49it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.37it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.37it/s][A 20%|██        | 156/780 [00:55<03:36,  2.88it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:29:00,461 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 21:29:00,954 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:29:09,317 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:29:09,632 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:29:10,079 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:23<2:00:49, 11.64s/it] 20%|██        | 158/780 [01:24<1:25:20,  8.23s/it] 20%|██        | 159/780 [01:24<1:00:32,  5.85s/it] 21%|██        | 160/780 [01:24<43:12,  4.18s/it]   21%|██        | 161/780 [01:24<31:04,  3.01s/it] 21%|██        | 162/780 [01:25<22:36,  2.19s/it] 21%|██        | 163/780 [01:25<16:41,  1.62s/it] 21%|██        | 164/780 [01:25<12:36,  1.23s/it] 21%|██        | 165/780 [01:26<09:41,  1.06it/s] 21%|██▏       | 166/780 [01:26<07:48,  1.31it/s] 21%|██▏       | 167/780 [01:26<06:20,  1.61it/s] 22%|██▏       | 168/780 [01:27<05:18,  1.92it/s] 22%|██▏       | 169/780 [01:27<04:42,  2.16it/s] 22%|██▏       | 170/780 [01:27<04:10,  2.44it/s] 22%|██▏       | 171/780 [01:27<03:47,  2.68it/s] 22%|██▏       | 172/780 [01:28<03:31,  2.87it/s] 22%|██▏       | 173/780 [01:28<03:20,  3.03it/s] 22%|██▏       | 174/780 [01:28<03:12,  3.15it/s] 22%|██▏       | 175/780 [01:29<03:06,  3.24it/s] 23%|██▎       | 176/780 [01:29<03:02,  3.31it/s] 23%|██▎       | 177/780 [01:29<02:59,  3.36it/s] 23%|██▎       | 178/780 [01:29<02:57,  3.40it/s] 23%|██▎       | 179/780 [01:30<02:55,  3.42it/s] 23%|██▎       | 180/780 [01:30<03:24,  2.94it/s] 23%|██▎       | 181/780 [01:31<03:14,  3.08it/s] 23%|██▎       | 182/780 [01:31<03:07,  3.19it/s] 23%|██▎       | 183/780 [01:31<03:02,  3.27it/s] 24%|██▎       | 184/780 [01:31<02:59,  3.33it/s] 24%|██▎       | 185/780 [01:32<02:56,  3.37it/s] 24%|██▍       | 186/780 [01:32<02:54,  3.40it/s] 24%|██▍       | 187/780 [01:32<02:53,  3.42it/s] 24%|██▍       | 188/780 [01:33<02:52,  3.44it/s] 24%|██▍       | 189/780 [01:33<02:51,  3.45it/s] 24%|██▍       | 190/780 [01:33<02:52,  3.42it/s] 24%|██▍       | 191/780 [01:33<02:51,  3.44it/s] 25%|██▍       | 192/780 [01:34<02:50,  3.45it/s] 25%|██▍       | 193/780 [01:34<02:49,  3.46it/s] 25%|██▍       | 194/780 [01:34<02:49,  3.46it/s] 25%|██▌       | 195/780 [01:35<02:48,  3.47it/s] 25%|██▌       | 196/780 [01:35<02:48,  3.47it/s] 25%|██▌       | 197/780 [01:35<02:48,  3.47it/s] 25%|██▌       | 198/780 [01:35<02:47,  3.47it/s] 26%|██▌       | 199/780 [01:36<02:47,  3.47it/s] 26%|██▌       | 200/780 [01:36<02:47,  3.47it/s] 26%|██▌       | 201/780 [01:36<02:49,  3.42it/s] 26%|██▌       | 202/780 [01:37<02:48,  3.44it/s] 26%|██▌       | 203/780 [01:37<02:47,  3.45it/s] 26%|██▌       | 204/780 [01:37<02:46,  3.45it/s] 26%|██▋       | 205/780 [01:37<02:46,  3.46it/s] 26%|██▋       | 206/780 [01:38<02:45,  3.46it/s] 27%|██▋       | 207/780 [01:38<02:45,  3.47it/s] 27%|██▋       | 208/780 [01:38<02:44,  3.47it/s] 27%|██▋       | 209/780 [01:39<02:44,  3.46it/s] 27%|██▋       | 210/780 [01:39<02:44,  3.46it/s] 27%|██▋       | 211/780 [01:39<02:44,  3.46it/s] 27%|██▋       | 212/780 [01:39<02:44,  3.46it/s] 27%|██▋       | 213/780 [01:40<02:43,  3.46it/s] 27%|██▋       | 214/780 [01:40<02:43,  3.46it/s] 28%|██▊       | 215/780 [01:40<02:42,  3.47it/s] 28%|██▊       | 216/780 [01:41<02:42,  3.47it/s] 28%|██▊       | 217/780 [01:41<02:42,  3.47it/s] 28%|██▊       | 218/780 [01:41<02:42,  3.47it/s] 28%|██▊       | 219/780 [01:41<02:41,  3.47it/s] 28%|██▊       | 220/780 [01:42<02:41,  3.47it/s] 28%|██▊       | 221/780 [01:42<02:41,  3.47it/s] 28%|██▊       | 222/780 [01:42<02:40,  3.47it/s] 29%|██▊       | 223/780 [01:43<02:42,  3.43it/s] 29%|██▊       | 224/780 [01:43<02:41,  3.44it/s] 29%|██▉       | 225/780 [01:43<02:40,  3.45it/s] 29%|██▉       | 226/780 [01:44<02:40,  3.45it/s] 29%|██▉       | 227/780 [01:44<02:39,  3.46it/s] 29%|██▉       | 228/780 [01:44<02:39,  3.46it/s] 29%|██▉       | 229/780 [01:44<02:39,  3.46it/s] 29%|██▉       | 230/780 [01:45<02:38,  3.46it/s] 30%|██▉       | 231/780 [01:45<02:38,  3.46it/s] 30%|██▉       | 232/780 [01:45<02:38,  3.47it/s] 30%|██▉       | 233/780 [01:46<02:37,  3.46it/s] 30%|███       | 234/780 [01:46<02:39,  3.42it/s] 30%|███       | 235/780 [01:46<02:38,  3.44it/s] 30%|███       | 236/780 [01:46<02:37,  3.45it/s] 30%|███       | 237/780 [01:47<02:37,  3.45it/s] 31%|███       | 238/780 [01:47<02:36,  3.46it/s] 31%|███       | 239/780 [01:47<02:36,  3.46it/s] 31%|███       | 240/780 [01:48<02:35,  3.47it/s] 31%|███       | 241/780 [01:48<02:35,  3.47it/s] 31%|███       | 242/780 [01:48<02:35,  3.47it/s] 31%|███       | 243/780 [01:48<02:34,  3.47it/s] 31%|███▏      | 244/780 [01:49<02:34,  3.47it/s] 31%|███▏      | 245/780 [01:49<02:34,  3.47it/s] 32%|███▏      | 246/780 [01:49<02:33,  3.47it/s] 32%|███▏      | 247/780 [01:50<02:33,  3.47it/s] 32%|███▏      | 248/780 [01:50<02:33,  3.47it/s] 32%|███▏      | 249/780 [01:50<02:33,  3.47it/s] 32%|███▏      | 250/780 [01:50<02:32,  3.47it/s] 32%|███▏      | 251/780 [01:51<02:32,  3.47it/s] 32%|███▏      | 252/780 [01:51<02:32,  3.47it/s] 32%|███▏      | 253/780 [01:51<02:31,  3.47it/s] 33%|███▎      | 254/780 [01:52<02:31,  3.47it/s] 33%|███▎      | 255/780 [01:52<02:46,  3.16it/s] 33%|███▎      | 256/780 [01:52<02:41,  3.25it/s] 33%|███▎      | 257/780 [01:53<02:37,  3.32it/s] 33%|███▎      | 258/780 [01:53<02:35,  3.36it/s] 33%|███▎      | 259/780 [01:53<02:33,  3.40it/s] 33%|███▎      | 260/780 [01:53<02:31,  3.42it/s] 33%|███▎      | 261/780 [01:54<02:30,  3.44it/s] 34%|███▎      | 262/780 [01:54<02:30,  3.44it/s] 34%|███▎      | 263/780 [01:54<02:29,  3.45it/s] 34%|███▍      | 264/780 [01:55<02:29,  3.46it/s] 34%|███▍      | 265/780 [01:55<02:28,  3.46it/s] 34%|███▍      | 266/780 [01:56<03:25,  2.50it/s] 34%|███▍      | 267/780 [01:56<03:08,  2.73it/s] 34%|███▍      | 268/780 [01:56<02:55,  2.91it/s] 34%|███▍      | 269/780 [01:56<02:46,  3.06it/s] 35%|███▍      | 270/780 [01:57<02:41,  3.17it/s] 35%|███▍      | 271/780 [01:57<02:37,  3.23it/s] 35%|███▍      | 272/780 [01:57<02:34,  3.29it/s] 35%|███▌      | 273/780 [01:58<02:31,  3.34it/s] 35%|███▌      | 274/780 [01:58<02:30,  3.37it/s] 35%|███▌      | 275/780 [01:58<02:28,  3.40it/s] 35%|███▌      | 276/780 [01:58<02:35,  3.25it/s] 36%|███▌      | 277/780 [01:59<02:32,  3.31it/s] 36%|███▌      | 278/780 [01:59<02:29,  3.36it/s] 36%|███▌      | 279/780 [01:59<02:27,  3.39it/s] 36%|███▌      | 280/780 [02:00<02:26,  3.41it/s] 36%|███▌      | 281/780 [02:00<02:25,  3.43it/s] 36%|███▌      | 282/780 [02:00<02:24,  3.44it/s] 36%|███▋      | 283/780 [02:00<02:24,  3.45it/s] 36%|███▋      | 284/780 [02:01<02:23,  3.46it/s] 37%|███▋      | 285/780 [02:01<02:23,  3.46it/s] 37%|███▋      | 286/780 [02:01<02:22,  3.46it/s] 37%|███▋      | 287/780 [02:02<02:59,  2.75it/s] 37%|███▋      | 288/780 [02:02<02:48,  2.93it/s] 37%|███▋      | 289/780 [02:02<02:40,  3.07it/s] 37%|███▋      | 290/780 [02:03<02:34,  3.18it/s] 37%|███▋      | 291/780 [02:03<02:30,  3.26it/s] 37%|███▋      | 292/780 [02:03<02:27,  3.32it/s] 38%|███▊      | 293/780 [02:04<02:24,  3.36it/s] 38%|███▊      | 294/780 [02:04<02:23,  3.39it/s] 38%|███▊      | 295/780 [02:04<02:22,  3.41it/s] 38%|███▊      | 296/780 [02:04<02:21,  3.43it/s] 38%|███▊      | 297/780 [02:05<02:35,  3.11it/s] 38%|███▊      | 298/780 [02:05<02:30,  3.21it/s] 38%|███▊      | 299/780 [02:05<02:26,  3.28it/s] 38%|███▊      | 300/780 [02:06<02:23,  3.34it/s] 39%|███▊      | 301/780 [02:06<02:21,  3.38it/s] 39%|███▊      | 302/780 [02:06<02:20,  3.40it/s] 39%|███▉      | 303/780 [02:07<02:19,  3.42it/s] 39%|███▉      | 304/780 [02:07<02:18,  3.43it/s] 39%|███▉      | 305/780 [02:07<02:18,  3.44it/s] 39%|███▉      | 306/780 [02:07<02:17,  3.44it/s] 39%|███▉      | 307/780 [02:08<02:17,  3.45it/s] 39%|███▉      | 308/780 [02:08<02:23,  3.28it/s] 40%|███▉      | 309/780 [02:08<02:21,  3.33it/s] 40%|███▉      | 310/780 [02:09<02:19,  3.36it/s] 40%|███▉      | 311/780 [02:09<02:18,  3.39it/s] 40%|████      | 312/780 [02:09<02:17,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 21:30:13,239 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:30:13,239 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:30:13,239 >>   Batch size = 8
{'eval_loss': 0.9459603428840637, 'eval_runtime': 9.8642, 'eval_samples_per_second': 353.704, 'eval_steps_per_second': 44.302, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.08it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.37it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.62it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.97it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.40it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.04it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.92it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.60it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.62it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.52it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.61it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.63it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.62it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.69it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.57it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.57it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.46it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.45it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.52it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.50it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.56it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.54it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.50it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.53it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.66it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.62it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.54it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.49it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.46it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.51it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.44it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.59it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.58it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.50it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.64it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.56it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.54it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.51it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.55it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.45it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.54it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.48it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.55it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.55it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.59it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.56it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.55it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.47it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.44it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.45it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.55it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.42it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.52it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.57it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.51it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.54it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.52it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.41it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.53it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.53it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.49it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.49it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.49it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.58it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.51it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.49it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.46it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.44it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.48it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.51it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.43it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.44it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.49it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.44it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.58it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.49it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.52it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.41it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.45it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.49it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.37it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.48it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.54it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.44it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.55it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.56it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.56it/s][A 40%|████      | 312/780 [02:19<02:17,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:30:22,676 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 21:30:22,695 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:30:28,725 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:30:28,753 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:30:28,778 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:37<1:05:20,  8.40s/it] 40%|████      | 314/780 [02:37<46:23,  5.97s/it]   40%|████      | 315/780 [02:37<33:04,  4.27s/it] 41%|████      | 316/780 [02:37<23:46,  3.07s/it] 41%|████      | 317/780 [02:38<17:16,  2.24s/it] 41%|████      | 318/780 [02:38<12:43,  1.65s/it] 41%|████      | 319/780 [02:38<09:33,  1.24s/it] 41%|████      | 320/780 [02:39<07:20,  1.04it/s] 41%|████      | 321/780 [02:39<05:47,  1.32it/s] 41%|████▏     | 322/780 [02:39<04:42,  1.62it/s] 41%|████▏     | 323/780 [02:39<03:56,  1.93it/s] 42%|████▏     | 324/780 [02:40<03:24,  2.23it/s] 42%|████▏     | 325/780 [02:40<03:05,  2.45it/s] 42%|████▏     | 326/780 [02:40<02:48,  2.69it/s] 42%|████▏     | 327/780 [02:41<02:37,  2.88it/s] 42%|████▏     | 328/780 [02:41<02:28,  3.04it/s] 42%|████▏     | 329/780 [02:41<02:22,  3.15it/s] 42%|████▏     | 330/780 [02:42<02:18,  3.24it/s] 42%|████▏     | 331/780 [02:42<02:15,  3.31it/s] 43%|████▎     | 332/780 [02:42<02:13,  3.35it/s] 43%|████▎     | 333/780 [02:42<02:11,  3.39it/s] 43%|████▎     | 334/780 [02:43<02:10,  3.41it/s] 43%|████▎     | 335/780 [02:43<02:09,  3.43it/s] 43%|████▎     | 336/780 [02:43<02:11,  3.37it/s] 43%|████▎     | 337/780 [02:44<02:10,  3.40it/s] 43%|████▎     | 338/780 [02:44<02:09,  3.42it/s] 43%|████▎     | 339/780 [02:44<02:08,  3.43it/s] 44%|████▎     | 340/780 [02:44<02:07,  3.44it/s] 44%|████▎     | 341/780 [02:45<02:07,  3.45it/s] 44%|████▍     | 342/780 [02:45<02:06,  3.46it/s] 44%|████▍     | 343/780 [02:45<02:06,  3.46it/s] 44%|████▍     | 344/780 [02:46<02:05,  3.46it/s] 44%|████▍     | 345/780 [02:46<02:05,  3.47it/s] 44%|████▍     | 346/780 [02:46<02:05,  3.47it/s] 44%|████▍     | 347/780 [02:46<02:06,  3.43it/s] 45%|████▍     | 348/780 [02:47<02:05,  3.44it/s] 45%|████▍     | 349/780 [02:47<02:04,  3.45it/s] 45%|████▍     | 350/780 [02:47<02:04,  3.45it/s] 45%|████▌     | 351/780 [02:48<02:04,  3.46it/s] 45%|████▌     | 352/780 [02:48<02:03,  3.46it/s] 45%|████▌     | 353/780 [02:48<02:03,  3.46it/s] 45%|████▌     | 354/780 [02:48<02:02,  3.47it/s] 46%|████▌     | 355/780 [02:49<02:02,  3.46it/s] 46%|████▌     | 356/780 [02:49<02:02,  3.47it/s] 46%|████▌     | 357/780 [02:49<02:02,  3.47it/s] 46%|████▌     | 358/780 [02:50<02:02,  3.46it/s] 46%|████▌     | 359/780 [02:50<02:01,  3.46it/s] 46%|████▌     | 360/780 [02:50<02:01,  3.46it/s] 46%|████▋     | 361/780 [02:50<02:00,  3.46it/s] 46%|████▋     | 362/780 [02:51<02:00,  3.46it/s] 47%|████▋     | 363/780 [02:51<02:00,  3.47it/s] 47%|████▋     | 364/780 [02:51<01:59,  3.47it/s] 47%|████▋     | 365/780 [02:52<01:59,  3.47it/s] 47%|████▋     | 366/780 [02:52<01:59,  3.46it/s] 47%|████▋     | 367/780 [02:52<01:59,  3.46it/s] 47%|████▋     | 368/780 [02:53<01:58,  3.46it/s] 47%|████▋     | 369/780 [02:53<01:58,  3.46it/s] 47%|████▋     | 370/780 [02:53<01:58,  3.46it/s] 48%|████▊     | 371/780 [02:53<02:00,  3.40it/s] 48%|████▊     | 372/780 [02:54<01:59,  3.42it/s] 48%|████▊     | 373/780 [02:54<01:58,  3.43it/s] 48%|████▊     | 374/780 [02:54<01:57,  3.44it/s] 48%|████▊     | 375/780 [02:55<01:57,  3.45it/s] 48%|████▊     | 376/780 [02:55<01:56,  3.45it/s] 48%|████▊     | 377/780 [02:55<01:56,  3.46it/s] 48%|████▊     | 378/780 [02:55<01:56,  3.46it/s] 49%|████▊     | 379/780 [02:56<01:55,  3.46it/s] 49%|████▊     | 380/780 [02:56<01:55,  3.46it/s] 49%|████▉     | 381/780 [02:56<01:55,  3.46it/s] 49%|████▉     | 382/780 [02:57<02:01,  3.27it/s] 49%|████▉     | 383/780 [02:57<01:59,  3.32it/s] 49%|████▉     | 384/780 [02:57<01:57,  3.36it/s] 49%|████▉     | 385/780 [02:57<01:56,  3.39it/s] 49%|████▉     | 386/780 [02:58<01:55,  3.41it/s] 50%|████▉     | 387/780 [02:58<01:54,  3.42it/s] 50%|████▉     | 388/780 [02:58<01:54,  3.44it/s] 50%|████▉     | 389/780 [02:59<01:53,  3.45it/s] 50%|█████     | 390/780 [02:59<01:52,  3.45it/s] 50%|█████     | 391/780 [02:59<01:52,  3.46it/s] 50%|█████     | 392/780 [03:00<01:52,  3.46it/s] 50%|█████     | 393/780 [03:00<01:53,  3.42it/s] 51%|█████     | 394/780 [03:00<01:52,  3.43it/s] 51%|█████     | 395/780 [03:00<01:51,  3.44it/s] 51%|█████     | 396/780 [03:01<01:51,  3.45it/s] 51%|█████     | 397/780 [03:01<01:51,  3.45it/s] 51%|█████     | 398/780 [03:01<01:50,  3.45it/s] 51%|█████     | 399/780 [03:02<01:50,  3.45it/s] 51%|█████▏    | 400/780 [03:02<01:49,  3.46it/s] 51%|█████▏    | 401/780 [03:02<01:49,  3.46it/s] 52%|█████▏    | 402/780 [03:02<01:49,  3.46it/s] 52%|█████▏    | 403/780 [03:03<01:49,  3.46it/s] 52%|█████▏    | 404/780 [03:03<01:51,  3.37it/s] 52%|█████▏    | 405/780 [03:03<01:50,  3.40it/s] 52%|█████▏    | 406/780 [03:04<01:49,  3.42it/s] 52%|█████▏    | 407/780 [03:04<01:48,  3.43it/s] 52%|█████▏    | 408/780 [03:04<01:48,  3.44it/s] 52%|█████▏    | 409/780 [03:04<01:47,  3.44it/s] 53%|█████▎    | 410/780 [03:05<01:47,  3.44it/s] 53%|█████▎    | 411/780 [03:05<01:47,  3.44it/s] 53%|█████▎    | 412/780 [03:05<01:46,  3.45it/s] 53%|█████▎    | 413/780 [03:06<01:46,  3.45it/s] 53%|█████▎    | 414/780 [03:06<01:46,  3.45it/s] 53%|█████▎    | 415/780 [03:06<02:06,  2.88it/s] 53%|█████▎    | 416/780 [03:07<01:59,  3.03it/s] 53%|█████▎    | 417/780 [03:07<01:55,  3.15it/s] 54%|█████▎    | 418/780 [03:07<01:51,  3.24it/s] 54%|█████▎    | 419/780 [03:08<01:49,  3.30it/s] 54%|█████▍    | 420/780 [03:08<01:47,  3.35it/s] 54%|█████▍    | 421/780 [03:08<01:46,  3.38it/s] 54%|█████▍    | 422/780 [03:08<01:45,  3.40it/s] 54%|█████▍    | 423/780 [03:09<01:44,  3.42it/s] 54%|█████▍    | 424/780 [03:09<01:43,  3.43it/s] 54%|█████▍    | 425/780 [03:09<01:50,  3.21it/s] 55%|█████▍    | 426/780 [03:10<01:48,  3.28it/s] 55%|█████▍    | 427/780 [03:10<01:46,  3.33it/s] 55%|█████▍    | 428/780 [03:10<01:44,  3.36it/s] 55%|█████▌    | 429/780 [03:11<01:43,  3.39it/s] 55%|█████▌    | 430/780 [03:11<01:42,  3.41it/s] 55%|█████▌    | 431/780 [03:11<01:41,  3.43it/s] 55%|█████▌    | 432/780 [03:11<01:41,  3.44it/s] 56%|█████▌    | 433/780 [03:12<01:40,  3.44it/s] 56%|█████▌    | 434/780 [03:12<01:40,  3.45it/s] 56%|█████▌    | 435/780 [03:12<01:39,  3.45it/s] 56%|█████▌    | 436/780 [03:13<01:44,  3.29it/s] 56%|█████▌    | 437/780 [03:13<01:42,  3.34it/s] 56%|█████▌    | 438/780 [03:13<01:41,  3.37it/s] 56%|█████▋    | 439/780 [03:13<01:40,  3.40it/s] 56%|█████▋    | 440/780 [03:14<01:39,  3.41it/s] 57%|█████▋    | 441/780 [03:14<01:38,  3.43it/s] 57%|█████▋    | 442/780 [03:14<01:38,  3.44it/s] 57%|█████▋    | 443/780 [03:15<01:37,  3.44it/s] 57%|█████▋    | 444/780 [03:15<01:37,  3.45it/s] 57%|█████▋    | 445/780 [03:15<01:37,  3.45it/s] 57%|█████▋    | 446/780 [03:15<01:36,  3.45it/s] 57%|█████▋    | 447/780 [03:16<02:38,  2.11it/s] 57%|█████▋    | 448/780 [03:17<02:19,  2.39it/s] 58%|█████▊    | 449/780 [03:17<02:05,  2.63it/s] 58%|█████▊    | 450/780 [03:17<01:56,  2.84it/s] 58%|█████▊    | 451/780 [03:18<01:49,  3.00it/s] 58%|█████▊    | 452/780 [03:18<01:45,  3.12it/s] 58%|█████▊    | 453/780 [03:18<01:41,  3.21it/s] 58%|█████▊    | 454/780 [03:18<01:39,  3.28it/s] 58%|█████▊    | 455/780 [03:19<01:37,  3.33it/s] 58%|█████▊    | 456/780 [03:19<01:37,  3.33it/s] 59%|█████▊    | 457/780 [03:19<01:36,  3.36it/s] 59%|█████▊    | 458/780 [03:20<01:35,  3.39it/s] 59%|█████▉    | 459/780 [03:20<01:34,  3.41it/s] 59%|█████▉    | 460/780 [03:20<01:33,  3.42it/s] 59%|█████▉    | 461/780 [03:20<01:32,  3.43it/s] 59%|█████▉    | 462/780 [03:21<01:32,  3.44it/s] 59%|█████▉    | 463/780 [03:21<01:32,  3.44it/s] 59%|█████▉    | 464/780 [03:21<01:31,  3.44it/s] 60%|█████▉    | 465/780 [03:22<01:31,  3.44it/s] 60%|█████▉    | 466/780 [03:22<01:31,  3.44it/s] 60%|█████▉    | 467/780 [03:22<01:30,  3.44it/s] 60%|██████    | 468/780 [03:22<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 21:31:26,461 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:31:26,462 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:31:26,462 >>   Batch size = 8
{'eval_loss': 0.959118664264679, 'eval_runtime': 9.4086, 'eval_samples_per_second': 370.83, 'eval_steps_per_second': 46.447, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.91it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.41it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.71it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.98it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.45it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.97it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.69it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.47it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.38it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 44.93it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 45.44it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 45.70it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 46.02it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.20it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.34it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.19it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.20it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.09it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.27it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.31it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 45.31it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 45.70it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 45.95it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.07it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.26it/s][A
 30%|███       | 133/437 [00:02<00:07, 40.03it/s][A
 32%|███▏      | 138/437 [00:03<00:07, 42.08it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 43.07it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 44.17it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 44.77it/s][A
 36%|███▌      | 158/437 [00:03<00:10, 26.05it/s][A
 37%|███▋      | 163/437 [00:03<00:09, 30.08it/s][A
 38%|███▊      | 168/437 [00:03<00:07, 33.64it/s][A
 40%|███▉      | 173/437 [00:04<00:07, 36.73it/s][A
 41%|████      | 178/437 [00:04<00:08, 29.17it/s][A
 42%|████▏     | 183/437 [00:04<00:07, 32.81it/s][A
 43%|████▎     | 188/437 [00:04<00:06, 35.96it/s][A
 44%|████▍     | 193/437 [00:04<00:06, 38.64it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 40.71it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 42.38it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 43.48it/s][A
 49%|████▊     | 213/437 [00:05<00:05, 44.35it/s][A
 50%|████▉     | 218/437 [00:05<00:04, 44.88it/s][A
 51%|█████     | 223/437 [00:05<00:04, 45.21it/s][A
 52%|█████▏    | 228/437 [00:05<00:04, 45.60it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 45.92it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.05it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.28it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.30it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.43it/s][A
 59%|█████▉    | 258/437 [00:06<00:03, 46.46it/s][A
 60%|██████    | 263/437 [00:06<00:03, 46.46it/s][A
 61%|██████▏   | 268/437 [00:06<00:03, 46.43it/s][A
 62%|██████▏   | 273/437 [00:06<00:03, 46.27it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.35it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.49it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.41it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.46it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.49it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.47it/s][A
 70%|███████   | 308/437 [00:07<00:02, 46.47it/s][A
 72%|███████▏  | 313/437 [00:07<00:05, 22.31it/s][A
 73%|███████▎  | 318/437 [00:07<00:04, 26.41it/s][A
 74%|███████▍  | 323/437 [00:07<00:03, 30.34it/s][A
 75%|███████▌  | 328/437 [00:07<00:03, 33.90it/s][A
 76%|███████▌  | 333/437 [00:08<00:02, 36.92it/s][A
 77%|███████▋  | 338/437 [00:08<00:02, 39.30it/s][A
 78%|███████▊  | 343/437 [00:08<00:02, 41.13it/s][A
 80%|███████▉  | 348/437 [00:08<00:02, 42.76it/s][A
 81%|████████  | 353/437 [00:08<00:01, 43.63it/s][A
 82%|████████▏ | 358/437 [00:08<00:01, 44.49it/s][A
 83%|████████▎ | 363/437 [00:08<00:01, 45.07it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 45.48it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 45.75it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.05it/s][A
 88%|████████▊ | 383/437 [00:09<00:01, 46.14it/s][A
 89%|████████▉ | 388/437 [00:09<00:01, 46.30it/s][A
 90%|████████▉ | 393/437 [00:09<00:00, 46.39it/s][A
 91%|█████████ | 398/437 [00:09<00:00, 46.31it/s][A
 92%|█████████▏| 403/437 [00:09<00:00, 46.30it/s][A
 93%|█████████▎| 408/437 [00:09<00:00, 46.47it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 46.37it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.54it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.60it/s][A
 98%|█████████▊| 428/437 [00:10<00:00, 46.48it/s][A
 99%|█████████▉| 433/437 [00:10<00:00, 46.51it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 46.51it/s][A 60%|██████    | 468/780 [03:33<01:30,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:31:37,267 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:31:37,393 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:31:44,263 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:31:44,680 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:31:44,700 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:53<48:25,  9.34s/it] 60%|██████    | 470/780 [03:53<34:14,  6.63s/it] 60%|██████    | 471/780 [03:54<24:20,  4.73s/it] 61%|██████    | 472/780 [03:54<17:25,  3.39s/it] 61%|██████    | 473/780 [03:54<12:35,  2.46s/it] 61%|██████    | 474/780 [03:54<09:13,  1.81s/it] 61%|██████    | 475/780 [03:55<06:53,  1.36s/it] 61%|██████    | 476/780 [03:55<05:14,  1.04s/it] 61%|██████    | 477/780 [03:55<04:05,  1.23it/s] 61%|██████▏   | 478/780 [03:56<03:17,  1.53it/s] 61%|██████▏   | 479/780 [03:56<02:43,  1.84it/s] 62%|██████▏   | 480/780 [03:56<02:20,  2.14it/s] 62%|██████▏   | 481/780 [03:56<02:03,  2.42it/s] 62%|██████▏   | 482/780 [03:57<01:52,  2.66it/s] 62%|██████▏   | 483/780 [03:57<01:49,  2.71it/s] 62%|██████▏   | 484/780 [03:57<01:42,  2.89it/s] 62%|██████▏   | 485/780 [03:58<01:36,  3.04it/s] 62%|██████▏   | 486/780 [03:58<01:33,  3.15it/s] 62%|██████▏   | 487/780 [03:58<01:30,  3.24it/s] 63%|██████▎   | 488/780 [03:58<01:28,  3.31it/s] 63%|██████▎   | 489/780 [03:59<01:26,  3.36it/s] 63%|██████▎   | 490/780 [03:59<01:25,  3.39it/s] 63%|██████▎   | 491/780 [03:59<01:24,  3.41it/s] 63%|██████▎   | 492/780 [04:00<01:23,  3.43it/s] 63%|██████▎   | 493/780 [04:00<01:23,  3.44it/s] 63%|██████▎   | 494/780 [04:00<01:22,  3.45it/s] 63%|██████▎   | 495/780 [04:01<01:22,  3.46it/s] 64%|██████▎   | 496/780 [04:01<01:22,  3.46it/s] 64%|██████▎   | 497/780 [04:01<01:22,  3.45it/s] 64%|██████▍   | 498/780 [04:01<01:21,  3.46it/s] 64%|██████▍   | 499/780 [04:02<01:21,  3.46it/s] 64%|██████▍   | 500/780 [04:02<01:20,  3.46it/s]                                                  64%|██████▍   | 500/780 [04:02<01:20,  3.46it/s] 64%|██████▍   | 501/780 [04:02<01:20,  3.46it/s] 64%|██████▍   | 502/780 [04:03<01:20,  3.47it/s] 64%|██████▍   | 503/780 [04:03<01:19,  3.47it/s] 65%|██████▍   | 504/780 [04:03<01:19,  3.47it/s] 65%|██████▍   | 505/780 [04:03<01:19,  3.47it/s] 65%|██████▍   | 506/780 [04:04<01:19,  3.47it/s] 65%|██████▌   | 507/780 [04:04<01:18,  3.47it/s] 65%|██████▌   | 508/780 [04:04<01:20,  3.37it/s] 65%|██████▌   | 509/780 [04:05<01:19,  3.40it/s] 65%|██████▌   | 510/780 [04:05<01:19,  3.42it/s] 66%|██████▌   | 511/780 [04:05<01:18,  3.43it/s] 66%|██████▌   | 512/780 [04:05<01:17,  3.44it/s] 66%|██████▌   | 513/780 [04:06<01:17,  3.45it/s] 66%|██████▌   | 514/780 [04:06<01:16,  3.46it/s] 66%|██████▌   | 515/780 [04:06<01:16,  3.46it/s] 66%|██████▌   | 516/780 [04:07<01:16,  3.46it/s] 66%|██████▋   | 517/780 [04:07<01:16,  3.46it/s] 66%|██████▋   | 518/780 [04:07<01:15,  3.46it/s] 67%|██████▋   | 519/780 [04:07<01:16,  3.42it/s] 67%|██████▋   | 520/780 [04:08<01:15,  3.44it/s] 67%|██████▋   | 521/780 [04:08<01:15,  3.45it/s] 67%|██████▋   | 522/780 [04:08<01:14,  3.45it/s] 67%|██████▋   | 523/780 [04:09<01:14,  3.46it/s] 67%|██████▋   | 524/780 [04:09<01:14,  3.46it/s] 67%|██████▋   | 525/780 [04:09<01:13,  3.46it/s] 67%|██████▋   | 526/780 [04:09<01:13,  3.46it/s] 68%|██████▊   | 527/780 [04:10<01:13,  3.46it/s] 68%|██████▊   | 528/780 [04:10<01:12,  3.46it/s] 68%|██████▊   | 529/780 [04:10<01:12,  3.47it/s] 68%|██████▊   | 530/780 [04:11<01:14,  3.37it/s] 68%|██████▊   | 531/780 [04:11<01:13,  3.40it/s] 68%|██████▊   | 532/780 [04:11<01:12,  3.42it/s] 68%|██████▊   | 533/780 [04:12<01:12,  3.43it/s] 68%|██████▊   | 534/780 [04:12<01:11,  3.44it/s] 69%|██████▊   | 535/780 [04:12<01:11,  3.45it/s] 69%|██████▊   | 536/780 [04:12<01:10,  3.45it/s] 69%|██████▉   | 537/780 [04:13<01:10,  3.46it/s] 69%|██████▉   | 538/780 [04:13<01:09,  3.46it/s] 69%|██████▉   | 539/780 [04:13<01:09,  3.46it/s] 69%|██████▉   | 540/780 [04:14<01:09,  3.46it/s] 69%|██████▉   | 541/780 [04:14<01:10,  3.40it/s] 69%|██████▉   | 542/780 [04:14<01:09,  3.42it/s] 70%|██████▉   | 543/780 [04:14<01:09,  3.43it/s] 70%|██████▉   | 544/780 [04:15<01:08,  3.44it/s] 70%|██████▉   | 545/780 [04:15<01:08,  3.45it/s] 70%|███████   | 546/780 [04:15<01:07,  3.45it/s] 70%|███████   | 547/780 [04:16<01:07,  3.45it/s] 70%|███████   | 548/780 [04:16<01:07,  3.45it/s] 70%|███████   | 549/780 [04:16<01:06,  3.46it/s] 71%|███████   | 550/780 [04:16<01:06,  3.46it/s] 71%|███████   | 551/780 [04:17<01:06,  3.46it/s] 71%|███████   | 552/780 [04:17<01:06,  3.41it/s] 71%|███████   | 553/780 [04:17<01:06,  3.42it/s] 71%|███████   | 554/780 [04:18<01:05,  3.44it/s] 71%|███████   | 555/780 [04:18<01:05,  3.44it/s] 71%|███████▏  | 556/780 [04:18<01:04,  3.45it/s] 71%|███████▏  | 557/780 [04:19<01:04,  3.45it/s] 72%|███████▏  | 558/780 [04:19<01:04,  3.46it/s] 72%|███████▏  | 559/780 [04:19<01:03,  3.46it/s] 72%|███████▏  | 560/780 [04:19<01:03,  3.46it/s] 72%|███████▏  | 561/780 [04:20<01:03,  3.46it/s] 72%|███████▏  | 562/780 [04:20<01:03,  3.46it/s] 72%|███████▏  | 563/780 [04:20<01:03,  3.42it/s] 72%|███████▏  | 564/780 [04:21<01:02,  3.43it/s] 72%|███████▏  | 565/780 [04:21<01:02,  3.44it/s] 73%|███████▎  | 566/780 [04:21<01:02,  3.45it/s] 73%|███████▎  | 567/780 [04:21<01:01,  3.45it/s] 73%|███████▎  | 568/780 [04:22<01:01,  3.45it/s] 73%|███████▎  | 569/780 [04:22<01:01,  3.45it/s] 73%|███████▎  | 570/780 [04:22<01:00,  3.45it/s] 73%|███████▎  | 571/780 [04:23<01:00,  3.45it/s] 73%|███████▎  | 572/780 [04:23<01:00,  3.45it/s] 73%|███████▎  | 573/780 [04:23<01:00,  3.45it/s] 74%|███████▎  | 574/780 [04:23<00:59,  3.45it/s] 74%|███████▎  | 575/780 [04:24<00:59,  3.45it/s] 74%|███████▍  | 576/780 [04:24<00:59,  3.45it/s] 74%|███████▍  | 577/780 [04:24<00:58,  3.45it/s] 74%|███████▍  | 578/780 [04:25<00:58,  3.45it/s] 74%|███████▍  | 579/780 [04:25<00:58,  3.41it/s] 74%|███████▍  | 580/780 [04:25<00:58,  3.42it/s] 74%|███████▍  | 581/780 [04:26<01:05,  3.06it/s] 75%|███████▍  | 582/780 [04:26<01:02,  3.14it/s] 75%|███████▍  | 583/780 [04:26<01:02,  3.16it/s] 75%|███████▍  | 584/780 [04:26<01:00,  3.23it/s] 75%|███████▌  | 585/780 [04:27<00:59,  3.30it/s] 75%|███████▌  | 586/780 [04:27<00:58,  3.34it/s] 75%|███████▌  | 587/780 [04:27<00:57,  3.37it/s] 75%|███████▌  | 588/780 [04:28<00:56,  3.39it/s] 76%|███████▌  | 589/780 [04:28<00:55,  3.41it/s] 76%|███████▌  | 590/780 [04:28<00:55,  3.42it/s] 76%|███████▌  | 591/780 [04:29<00:55,  3.42it/s] 76%|███████▌  | 592/780 [04:29<00:54,  3.44it/s] 76%|███████▌  | 593/780 [04:29<00:54,  3.44it/s] 76%|███████▌  | 594/780 [04:29<00:54,  3.44it/s] 76%|███████▋  | 595/780 [04:30<00:53,  3.45it/s] 76%|███████▋  | 596/780 [04:30<00:53,  3.45it/s] 77%|███████▋  | 597/780 [04:30<00:53,  3.45it/s] 77%|███████▋  | 598/780 [04:31<00:52,  3.45it/s] 77%|███████▋  | 599/780 [04:31<00:52,  3.45it/s] 77%|███████▋  | 600/780 [04:31<00:52,  3.45it/s] 77%|███████▋  | 601/780 [04:31<00:51,  3.45it/s] 77%|███████▋  | 602/780 [04:32<00:52,  3.40it/s] 77%|███████▋  | 603/780 [04:32<00:51,  3.42it/s] 77%|███████▋  | 604/780 [04:32<00:51,  3.43it/s] 78%|███████▊  | 605/780 [04:33<00:50,  3.44it/s] 78%|███████▊  | 606/780 [04:33<00:50,  3.44it/s] 78%|███████▊  | 607/780 [04:33<00:50,  3.45it/s] 78%|███████▊  | 608/780 [04:33<00:49,  3.45it/s] 78%|███████▊  | 609/780 [04:34<00:49,  3.45it/s] 78%|███████▊  | 610/780 [04:34<00:49,  3.45it/s] 78%|███████▊  | 611/780 [04:34<00:48,  3.45it/s] 78%|███████▊  | 612/780 [04:35<00:48,  3.45it/s] 79%|███████▊  | 613/780 [04:35<00:48,  3.41it/s] 79%|███████▊  | 614/780 [04:35<00:48,  3.41it/s] 79%|███████▉  | 615/780 [04:36<00:48,  3.42it/s] 79%|███████▉  | 616/780 [04:36<00:47,  3.43it/s] 79%|███████▉  | 617/780 [04:36<00:47,  3.44it/s] 79%|███████▉  | 618/780 [04:36<00:47,  3.44it/s] 79%|███████▉  | 619/780 [04:37<00:46,  3.44it/s] 79%|███████▉  | 620/780 [04:37<00:46,  3.45it/s] 80%|███████▉  | 621/780 [04:37<00:46,  3.45it/s] 80%|███████▉  | 622/780 [04:38<00:45,  3.45it/s] 80%|███████▉  | 623/780 [04:38<00:45,  3.45it/s] 80%|████████  | 624/780 [04:38<00:46,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 21:32:42,140 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:32:42,141 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:32:42,141 >>   Batch size = 8
{'eval_loss': 0.969862163066864, 'eval_runtime': 10.3716, 'eval_samples_per_second': 336.4, 'eval_steps_per_second': 42.134, 'epoch': 3.0}
{'loss': 0.5278, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.22it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.61it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.67it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.83it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.33it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.97it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.78it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.40it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.45it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.45it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.45it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.48it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.50it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.41it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.45it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.31it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.04it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.15it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.33it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.48it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.22it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.44it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.33it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.47it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.53it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.55it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.29it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.39it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.29it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.47it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.49it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 45.82it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.68it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.58it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.35it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.43it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.36it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.46it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.50it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.41it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.42it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.33it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.44it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.48it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.45it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.36it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.42it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.30it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.43it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.45it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.40it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.45it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 44.52it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 45.07it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 45.33it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 45.75it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.00it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.16it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.27it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.29it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.15it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.21it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.16it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.38it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.22it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.43it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.44it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.51it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.51it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.39it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.24it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.42it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.44it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.45it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.40it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.50it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.39it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.46it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.42it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.39it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 42.47it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 43.57it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 44.40it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 45.00it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 45.48it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 45.75it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 45.75it/s][A 80%|████████  | 624/780 [04:48<00:46,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:32:51,642 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 21:32:51,659 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:32:57,019 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:32:57,033 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:32:57,040 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [05:04<20:27,  7.92s/it] 80%|████████  | 626/780 [05:04<14:27,  5.63s/it] 80%|████████  | 627/780 [05:04<10:16,  4.03s/it] 81%|████████  | 628/780 [05:05<07:21,  2.91s/it] 81%|████████  | 629/780 [05:05<05:20,  2.12s/it] 81%|████████  | 630/780 [05:05<03:55,  1.57s/it] 81%|████████  | 631/780 [05:06<02:56,  1.19s/it] 81%|████████  | 632/780 [05:06<02:15,  1.09it/s] 81%|████████  | 633/780 [05:06<01:47,  1.37it/s] 81%|████████▏ | 634/780 [05:06<01:27,  1.68it/s] 81%|████████▏ | 635/780 [05:07<01:13,  1.98it/s] 82%|████████▏ | 636/780 [05:07<01:03,  2.28it/s] 82%|████████▏ | 637/780 [05:07<00:56,  2.51it/s] 82%|████████▏ | 638/780 [05:08<00:51,  2.74it/s] 82%|████████▏ | 639/780 [05:08<00:48,  2.92it/s] 82%|████████▏ | 640/780 [05:08<00:45,  3.07it/s] 82%|████████▏ | 641/780 [05:08<00:43,  3.18it/s] 82%|████████▏ | 642/780 [05:09<00:42,  3.26it/s] 82%|████████▏ | 643/780 [05:09<00:41,  3.32it/s] 83%|████████▎ | 644/780 [05:09<00:40,  3.36it/s] 83%|████████▎ | 645/780 [05:10<00:39,  3.39it/s] 83%|████████▎ | 646/780 [05:10<00:39,  3.42it/s] 83%|████████▎ | 647/780 [05:10<00:38,  3.43it/s] 83%|████████▎ | 648/780 [05:11<00:39,  3.38it/s] 83%|████████▎ | 649/780 [05:11<00:38,  3.41it/s] 83%|████████▎ | 650/780 [05:11<00:37,  3.43it/s] 83%|████████▎ | 651/780 [05:11<00:37,  3.44it/s] 84%|████████▎ | 652/780 [05:12<00:37,  3.45it/s] 84%|████████▎ | 653/780 [05:12<00:36,  3.45it/s] 84%|████████▍ | 654/780 [05:12<00:36,  3.46it/s] 84%|████████▍ | 655/780 [05:13<00:36,  3.46it/s] 84%|████████▍ | 656/780 [05:13<00:35,  3.47it/s] 84%|████████▍ | 657/780 [05:13<00:35,  3.47it/s] 84%|████████▍ | 658/780 [05:13<00:35,  3.47it/s] 84%|████████▍ | 659/780 [05:14<00:35,  3.43it/s] 85%|████████▍ | 660/780 [05:14<00:34,  3.45it/s] 85%|████████▍ | 661/780 [05:14<00:34,  3.45it/s] 85%|████████▍ | 662/780 [05:15<00:34,  3.46it/s] 85%|████████▌ | 663/780 [05:15<00:33,  3.46it/s] 85%|████████▌ | 664/780 [05:15<00:33,  3.46it/s] 85%|████████▌ | 665/780 [05:15<00:33,  3.46it/s] 85%|████████▌ | 666/780 [05:16<00:32,  3.47it/s] 86%|████████▌ | 667/780 [05:16<00:32,  3.47it/s] 86%|████████▌ | 668/780 [05:16<00:32,  3.47it/s] 86%|████████▌ | 669/780 [05:17<00:32,  3.47it/s] 86%|████████▌ | 670/780 [05:17<00:31,  3.46it/s] 86%|████████▌ | 671/780 [05:17<00:31,  3.46it/s] 86%|████████▌ | 672/780 [05:17<00:31,  3.46it/s] 86%|████████▋ | 673/780 [05:18<00:30,  3.46it/s] 86%|████████▋ | 674/780 [05:18<00:30,  3.46it/s] 87%|████████▋ | 675/780 [05:18<00:30,  3.47it/s] 87%|████████▋ | 676/780 [05:19<00:29,  3.47it/s] 87%|████████▋ | 677/780 [05:19<00:29,  3.46it/s] 87%|████████▋ | 678/780 [05:19<00:29,  3.47it/s] 87%|████████▋ | 679/780 [05:19<00:29,  3.47it/s] 87%|████████▋ | 680/780 [05:20<00:28,  3.47it/s] 87%|████████▋ | 681/780 [05:20<00:29,  3.39it/s] 87%|████████▋ | 682/780 [05:20<00:28,  3.41it/s] 88%|████████▊ | 683/780 [05:21<00:28,  3.43it/s] 88%|████████▊ | 684/780 [05:21<00:27,  3.44it/s] 88%|████████▊ | 685/780 [05:21<00:27,  3.45it/s] 88%|████████▊ | 686/780 [05:22<00:27,  3.45it/s] 88%|████████▊ | 687/780 [05:22<00:26,  3.46it/s] 88%|████████▊ | 688/780 [05:22<00:26,  3.46it/s] 88%|████████▊ | 689/780 [05:22<00:26,  3.46it/s] 88%|████████▊ | 690/780 [05:23<00:26,  3.46it/s] 89%|████████▊ | 691/780 [05:23<00:25,  3.46it/s] 89%|████████▊ | 692/780 [05:23<00:27,  3.23it/s] 89%|████████▉ | 693/780 [05:24<00:26,  3.30it/s] 89%|████████▉ | 694/780 [05:24<00:25,  3.35it/s] 89%|████████▉ | 695/780 [05:24<00:25,  3.38it/s] 89%|████████▉ | 696/780 [05:24<00:24,  3.41it/s] 89%|████████▉ | 697/780 [05:25<00:24,  3.42it/s] 89%|████████▉ | 698/780 [05:25<00:24,  3.38it/s] 90%|████████▉ | 699/780 [05:25<00:23,  3.39it/s] 90%|████████▉ | 700/780 [05:26<00:24,  3.33it/s] 90%|████████▉ | 701/780 [05:26<00:23,  3.36it/s] 90%|█████████ | 702/780 [05:26<00:23,  3.37it/s] 90%|█████████ | 703/780 [05:27<00:22,  3.37it/s] 90%|█████████ | 704/780 [05:27<00:22,  3.35it/s] 90%|█████████ | 705/780 [05:27<00:22,  3.38it/s] 91%|█████████ | 706/780 [05:27<00:21,  3.41it/s] 91%|█████████ | 707/780 [05:28<00:21,  3.42it/s] 91%|█████████ | 708/780 [05:28<00:20,  3.43it/s] 91%|█████████ | 709/780 [05:28<00:20,  3.44it/s] 91%|█████████ | 710/780 [05:29<00:20,  3.44it/s] 91%|█████████ | 711/780 [05:29<00:20,  3.45it/s] 91%|█████████▏| 712/780 [05:29<00:19,  3.45it/s] 91%|█████████▏| 713/780 [05:29<00:19,  3.45it/s] 92%|█████████▏| 714/780 [05:30<00:19,  3.45it/s] 92%|█████████▏| 715/780 [05:30<00:20,  3.22it/s] 92%|█████████▏| 716/780 [05:30<00:19,  3.29it/s] 92%|█████████▏| 717/780 [05:31<00:18,  3.34it/s] 92%|█████████▏| 718/780 [05:31<00:18,  3.37it/s] 92%|█████████▏| 719/780 [05:31<00:17,  3.39it/s] 92%|█████████▏| 720/780 [05:32<00:17,  3.41it/s] 92%|█████████▏| 721/780 [05:32<00:17,  3.42it/s] 93%|█████████▎| 722/780 [05:32<00:16,  3.43it/s] 93%|█████████▎| 723/780 [05:32<00:16,  3.44it/s] 93%|█████████▎| 724/780 [05:33<00:16,  3.44it/s] 93%|█████████▎| 725/780 [05:33<00:15,  3.44it/s] 93%|█████████▎| 726/780 [05:33<00:17,  3.07it/s] 93%|█████████▎| 727/780 [05:34<00:16,  3.17it/s] 93%|█████████▎| 728/780 [05:34<00:15,  3.25it/s] 93%|█████████▎| 729/780 [05:34<00:15,  3.31it/s] 94%|█████████▎| 730/780 [05:35<00:14,  3.35it/s] 94%|█████████▎| 731/780 [05:35<00:14,  3.38it/s] 94%|█████████▍| 732/780 [05:35<00:14,  3.38it/s] 94%|█████████▍| 733/780 [05:35<00:14,  3.32it/s] 94%|█████████▍| 734/780 [05:36<00:13,  3.36it/s] 94%|█████████▍| 735/780 [05:36<00:13,  3.39it/s] 94%|█████████▍| 736/780 [05:36<00:13,  3.22it/s] 94%|█████████▍| 737/780 [05:37<00:13,  3.29it/s] 95%|█████████▍| 738/780 [05:37<00:12,  3.34it/s] 95%|█████████▍| 739/780 [05:37<00:12,  3.37it/s] 95%|█████████▍| 740/780 [05:38<00:11,  3.40it/s] 95%|█████████▌| 741/780 [05:38<00:11,  3.41it/s] 95%|█████████▌| 742/780 [05:38<00:11,  3.43it/s] 95%|█████████▌| 743/780 [05:38<00:10,  3.43it/s] 95%|█████████▌| 744/780 [05:39<00:10,  3.44it/s] 96%|█████████▌| 745/780 [05:39<00:10,  3.44it/s] 96%|█████████▌| 746/780 [05:39<00:09,  3.45it/s] 96%|█████████▌| 747/780 [05:40<00:09,  3.43it/s] 96%|█████████▌| 748/780 [05:40<00:09,  3.44it/s] 96%|█████████▌| 749/780 [05:40<00:09,  3.44it/s] 96%|█████████▌| 750/780 [05:40<00:08,  3.44it/s] 96%|█████████▋| 751/780 [05:41<00:08,  3.45it/s] 96%|█████████▋| 752/780 [05:41<00:08,  3.45it/s] 97%|█████████▋| 753/780 [05:41<00:07,  3.45it/s] 97%|█████████▋| 754/780 [05:42<00:07,  3.45it/s] 97%|█████████▋| 755/780 [05:42<00:07,  3.37it/s] 97%|█████████▋| 756/780 [05:42<00:07,  3.39it/s] 97%|█████████▋| 757/780 [05:43<00:06,  3.41it/s] 97%|█████████▋| 758/780 [05:43<00:06,  3.34it/s] 97%|█████████▋| 759/780 [05:43<00:06,  3.37it/s] 97%|█████████▋| 760/780 [05:43<00:05,  3.39it/s] 98%|█████████▊| 761/780 [05:44<00:05,  3.41it/s] 98%|█████████▊| 762/780 [05:44<00:05,  3.43it/s] 98%|█████████▊| 763/780 [05:44<00:04,  3.43it/s] 98%|█████████▊| 764/780 [05:45<00:04,  3.44it/s] 98%|█████████▊| 765/780 [05:45<00:04,  3.44it/s] 98%|█████████▊| 766/780 [05:45<00:04,  3.45it/s] 98%|█████████▊| 767/780 [05:45<00:03,  3.45it/s] 98%|█████████▊| 768/780 [05:46<00:03,  3.45it/s] 99%|█████████▊| 769/780 [05:46<00:03,  3.40it/s] 99%|█████████▊| 770/780 [05:46<00:02,  3.42it/s] 99%|█████████▉| 771/780 [05:47<00:02,  3.43it/s] 99%|█████████▉| 772/780 [05:47<00:02,  3.44it/s] 99%|█████████▉| 773/780 [05:47<00:02,  3.44it/s] 99%|█████████▉| 774/780 [05:47<00:01,  3.45it/s] 99%|█████████▉| 775/780 [05:48<00:01,  3.45it/s] 99%|█████████▉| 776/780 [05:48<00:01,  3.45it/s]100%|█████████▉| 777/780 [05:48<00:00,  3.45it/s]100%|█████████▉| 778/780 [05:49<00:00,  3.45it/s]100%|█████████▉| 779/780 [05:49<00:00,  3.46it/s]100%|██████████| 780/780 [05:49<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 21:33:53,161 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:33:53,162 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:33:53,162 >>   Batch size = 8
{'eval_loss': 0.9769630432128906, 'eval_runtime': 9.4782, 'eval_samples_per_second': 368.108, 'eval_steps_per_second': 46.106, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.38it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.37it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.59it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.79it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.37it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.04it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.90it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.60it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.51it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.50it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.51it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.57it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.62it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.55it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.41it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.37it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.27it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.27it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.34it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.33it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.39it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.34it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.49it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.49it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.44it/s][A
 30%|███       | 133/437 [00:02<00:06, 45.68it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 45.79it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 45.83it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.15it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.11it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.23it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.36it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.41it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.25it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.21it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.27it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.35it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.40it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.30it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.25it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.41it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.48it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.47it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.41it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.23it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.22it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.28it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.35it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.42it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.47it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.27it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.38it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.31it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.39it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.24it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.28it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.19it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.40it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.39it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.41it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.45it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.38it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.27it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.28it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.20it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.30it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.38it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.42it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.24it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.37it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.28it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.31it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.13it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 45.00it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 45.33it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 45.71it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 45.91it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.14it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.14it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.19it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.24it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.04it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.23it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.25it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.29it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.40it/s][A                                                 
                                                 [A100%|██████████| 780/780 [05:59<00:00,  3.41it/s]
100%|██████████| 437/437 [00:09<00:00, 46.40it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:34:02,638 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 21:34:02,677 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:34:07,901 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:34:08,212 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:34:08,265 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:34:22,566 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:34:22,579 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156 (score: 0.9459603428840637).
                                                 100%|██████████| 780/780 [06:26<00:00,  3.41it/s]100%|██████████| 780/780 [06:26<00:00,  2.02it/s]
[INFO|trainer.py:1894] 2023-08-28 21:34:30,172 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 21:34:30,207 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:34:37,509 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:34:37,527 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:34:37,536 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:34:37,737 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:37,737 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:37,738 >>   train_loss               =     0.5184
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:37,738 >>   train_runtime            = 0:06:26.65
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:37,738 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:37,738 >>   train_samples_per_second =    129.301
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:37,738 >>   train_steps_per_second   =      2.017
{'eval_loss': 0.9787880182266235, 'eval_runtime': 9.4349, 'eval_samples_per_second': 369.798, 'eval_steps_per_second': 46.317, 'epoch': 5.0}
{'train_runtime': 386.657, 'train_samples_per_second': 129.301, 'train_steps_per_second': 2.017, 'train_loss': 0.518412604698768, 'epoch': 5.0}
08/28/2023 21:34:37 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:34:37,892 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:34:37,892 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:34:37,892 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 57.92it/s]  3%|▎         | 12/437 [00:00<00:08, 50.96it/s]  4%|▍         | 18/437 [00:00<00:08, 49.20it/s]  5%|▌         | 23/437 [00:00<00:08, 48.51it/s]  6%|▋         | 28/437 [00:00<00:08, 48.12it/s]  8%|▊         | 33/437 [00:00<00:08, 47.82it/s]  9%|▊         | 38/437 [00:00<00:08, 47.54it/s] 10%|▉         | 43/437 [00:00<00:08, 47.30it/s] 11%|█         | 48/437 [00:00<00:08, 46.97it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.02it/s] 13%|█▎        | 58/437 [00:01<00:08, 47.06it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.07it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.01it/s] 17%|█▋        | 73/437 [00:01<00:07, 46.98it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.05it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.12it/s] 20%|██        | 88/437 [00:01<00:07, 47.02it/s] 21%|██▏       | 93/437 [00:01<00:07, 46.97it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.91it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.85it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.81it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.88it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.02it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.02it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.04it/s] 30%|███       | 133/437 [00:02<00:06, 46.90it/s] 32%|███▏      | 138/437 [00:02<00:06, 46.67it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.81it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.89it/s] 35%|███▌      | 153/437 [00:03<00:06, 46.92it/s] 36%|███▌      | 158/437 [00:03<00:05, 46.81it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.87it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.67it/s] 40%|███▉      | 173/437 [00:03<00:05, 46.65it/s] 41%|████      | 178/437 [00:03<00:05, 46.57it/s] 42%|████▏     | 183/437 [00:03<00:05, 46.74it/s] 43%|████▎     | 188/437 [00:03<00:05, 46.55it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.55it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.46it/s] 46%|████▋     | 203/437 [00:04<00:05, 46.64it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.82it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.91it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.79it/s] 51%|█████     | 223/437 [00:04<00:04, 46.75it/s] 52%|█████▏    | 228/437 [00:04<00:04, 46.80it/s] 53%|█████▎    | 233/437 [00:04<00:04, 46.76it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.81it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.86it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.78it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.86it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.92it/s] 60%|██████    | 263/437 [00:05<00:03, 46.89it/s] 61%|██████▏   | 268/437 [00:05<00:03, 46.79it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.89it/s] 64%|██████▎   | 278/437 [00:05<00:03, 46.86it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.82it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.87it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.80it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.76it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.87it/s] 70%|███████   | 308/437 [00:06<00:02, 46.90it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.76it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.71it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.62it/s] 75%|███████▌  | 328/437 [00:06<00:02, 46.65it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.73it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.76it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.74it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.84it/s] 81%|████████  | 353/437 [00:07<00:01, 46.74it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.81it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.96it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.98it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.95it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.97it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.89it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.92it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.97it/s] 91%|█████████ | 398/437 [00:08<00:00, 47.03it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.94it/s] 93%|█████████▎| 408/437 [00:08<00:00, 46.97it/s] 95%|█████████▍| 413/437 [00:08<00:00, 46.71it/s] 96%|█████████▌| 418/437 [00:08<00:00, 46.86it/s] 97%|█████████▋| 423/437 [00:09<00:00, 45.54it/s] 98%|█████████▊| 428/437 [00:09<00:00, 46.06it/s] 99%|█████████▉| 433/437 [00:09<00:00, 46.40it/s]100%|██████████| 437/437 [00:09<00:00, 46.94it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:34:47,223 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:47,224 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:47,224 >>   eval_loss               =      0.946
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:47,224 >>   eval_runtime            = 0:00:09.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:47,224 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:47,224 >>   eval_samples_per_second =    373.902
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:47,224 >>   eval_steps_per_second   =     46.832
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:47,224 >>   perplexity              =     2.5753
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:56,781 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:56,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:56,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:56,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:56,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:34:58,103 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:34:58,104 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:58,867 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:59,885 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:59,885 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:04,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:04,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:04,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:04,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:04,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:35:05,629 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:35:05,630 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:35:06,827 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:35:06,980 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:35:06,980 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.39it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.45it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.44it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:12,  1.47it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:14,  1.46it/s]Extractor Predicting: 22it [00:15,  1.45it/s]Extractor Predicting: 23it [00:15,  1.46it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:17,  1.45it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:19,  1.45it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.49it/s]Extractor Predicting: 31it [00:21,  1.48it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:22,  1.53it/s]Extractor Predicting: 34it [00:23,  1.52it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:24,  1.46it/s]Extractor Predicting: 37it [00:25,  1.43it/s]Extractor Predicting: 38it [00:26,  1.43it/s]Extractor Predicting: 39it [00:26,  1.43it/s]Extractor Predicting: 40it [00:27,  1.45it/s]Extractor Predicting: 41it [00:28,  1.43it/s]Extractor Predicting: 42it [00:28,  1.43it/s]Extractor Predicting: 43it [00:29,  1.43it/s]Extractor Predicting: 44it [00:30,  1.42it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:31,  1.45it/s]Extractor Predicting: 47it [00:32,  1.44it/s]Extractor Predicting: 48it [00:32,  1.43it/s]Extractor Predicting: 49it [00:33,  1.44it/s]Extractor Predicting: 50it [00:34,  1.43it/s]Extractor Predicting: 51it [00:35,  1.43it/s]Extractor Predicting: 52it [00:35,  1.43it/s]Extractor Predicting: 53it [00:36,  1.44it/s]Extractor Predicting: 54it [00:37,  1.43it/s]Extractor Predicting: 55it [00:37,  1.41it/s]Extractor Predicting: 56it [00:38,  1.43it/s]Extractor Predicting: 57it [00:39,  1.42it/s]Extractor Predicting: 58it [00:39,  1.43it/s]Extractor Predicting: 59it [00:40,  1.40it/s]Extractor Predicting: 60it [00:41,  1.41it/s]Extractor Predicting: 61it [00:42,  1.42it/s]Extractor Predicting: 62it [00:42,  1.45it/s]Extractor Predicting: 63it [00:43,  1.47it/s]Extractor Predicting: 64it [00:44,  1.47it/s]Extractor Predicting: 65it [00:44,  1.49it/s]Extractor Predicting: 66it [00:45,  1.48it/s]Extractor Predicting: 67it [00:46,  1.44it/s]Extractor Predicting: 68it [00:46,  1.46it/s]Extractor Predicting: 69it [00:47,  1.50it/s]Extractor Predicting: 70it [00:48,  1.52it/s]Extractor Predicting: 71it [00:48,  1.52it/s]Extractor Predicting: 72it [00:49,  1.50it/s]Extractor Predicting: 73it [00:50,  1.50it/s]Extractor Predicting: 74it [00:50,  1.53it/s]Extractor Predicting: 75it [00:51,  1.48it/s]Extractor Predicting: 76it [00:52,  1.47it/s]Extractor Predicting: 77it [00:52,  1.49it/s]Extractor Predicting: 78it [00:53,  1.52it/s]Extractor Predicting: 79it [00:54,  1.52it/s]Extractor Predicting: 80it [00:54,  1.53it/s]Extractor Predicting: 81it [00:55,  1.53it/s]Extractor Predicting: 82it [00:56,  1.51it/s]Extractor Predicting: 83it [00:56,  1.50it/s]Extractor Predicting: 84it [00:57,  1.49it/s]Extractor Predicting: 85it [00:58,  1.51it/s]Extractor Predicting: 86it [00:58,  1.53it/s]Extractor Predicting: 87it [00:59,  1.53it/s]Extractor Predicting: 88it [01:00,  1.53it/s]Extractor Predicting: 89it [01:00,  1.51it/s]Extractor Predicting: 90it [01:01,  1.52it/s]Extractor Predicting: 91it [01:02,  1.51it/s]Extractor Predicting: 92it [01:02,  1.43it/s]Extractor Predicting: 93it [01:03,  1.45it/s]Extractor Predicting: 94it [01:04,  1.46it/s]Extractor Predicting: 95it [01:04,  1.45it/s]Extractor Predicting: 96it [01:05,  1.33it/s]Extractor Predicting: 97it [01:06,  1.36it/s]Extractor Predicting: 98it [01:07,  1.41it/s]Extractor Predicting: 99it [01:07,  1.41it/s]Extractor Predicting: 100it [01:08,  1.41it/s]Extractor Predicting: 101it [01:09,  1.44it/s]Extractor Predicting: 102it [01:09,  1.42it/s]Extractor Predicting: 103it [01:10,  1.41it/s]Extractor Predicting: 104it [01:11,  1.43it/s]Extractor Predicting: 105it [01:11,  1.43it/s]Extractor Predicting: 106it [01:12,  1.43it/s]Extractor Predicting: 107it [01:13,  1.45it/s]Extractor Predicting: 108it [01:14,  1.46it/s]Extractor Predicting: 109it [01:14,  1.42it/s]Extractor Predicting: 110it [01:15,  1.42it/s]Extractor Predicting: 111it [01:16,  1.42it/s]Extractor Predicting: 112it [01:16,  1.43it/s]Extractor Predicting: 113it [01:17,  1.44it/s]Extractor Predicting: 114it [01:18,  1.44it/s]Extractor Predicting: 115it [01:18,  1.43it/s]Extractor Predicting: 116it [01:19,  1.46it/s]Extractor Predicting: 117it [01:20,  1.45it/s]Extractor Predicting: 118it [01:20,  1.46it/s]Extractor Predicting: 119it [01:21,  1.49it/s]Extractor Predicting: 120it [01:22,  1.54it/s]Extractor Predicting: 121it [01:22,  1.53it/s]Extractor Predicting: 122it [01:23,  1.53it/s]Extractor Predicting: 123it [01:24,  1.50it/s]Extractor Predicting: 124it [01:24,  1.50it/s]Extractor Predicting: 125it [01:25,  1.51it/s]Extractor Predicting: 126it [01:26,  1.51it/s]Extractor Predicting: 127it [01:26,  1.51it/s]Extractor Predicting: 128it [01:27,  1.48it/s]Extractor Predicting: 129it [01:28,  1.52it/s]Extractor Predicting: 130it [01:28,  1.48it/s]Extractor Predicting: 131it [01:29,  1.49it/s]Extractor Predicting: 132it [01:30,  1.49it/s]Extractor Predicting: 133it [01:30,  1.47it/s]Extractor Predicting: 134it [01:31,  1.46it/s]Extractor Predicting: 135it [01:32,  1.49it/s]Extractor Predicting: 136it [01:32,  1.51it/s]Extractor Predicting: 137it [01:33,  1.47it/s]Extractor Predicting: 138it [01:34,  1.46it/s]Extractor Predicting: 139it [01:34,  1.49it/s]Extractor Predicting: 140it [01:35,  1.48it/s]Extractor Predicting: 141it [01:36,  1.49it/s]Extractor Predicting: 142it [01:36,  1.48it/s]Extractor Predicting: 143it [01:37,  1.50it/s]Extractor Predicting: 144it [01:38,  1.50it/s]Extractor Predicting: 145it [01:38,  1.96it/s]Extractor Predicting: 145it [01:38,  1.47it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:57,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:57,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:57,587 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:57,587 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:57,587 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:36:58,811 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:36:58,812 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:36:59,424 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:00,462 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:00,462 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:04,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:04,133 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:04,133 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:04,133 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:04,133 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:37:04,905 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:37:04,906 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:37:05,607 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:05,781 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:05,781 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5420081967213115,
  "recall": 0.15161937517913443,
  "score": 0.23695408734602466,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.45it/s]Extractor Predicting: 10it [00:06,  1.42it/s]Extractor Predicting: 11it [00:07,  1.43it/s]Extractor Predicting: 12it [00:08,  1.44it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:13,  1.49it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.52it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:17,  1.46it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:19,  1.45it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:20,  1.43it/s]Extractor Predicting: 31it [00:21,  1.43it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:22,  1.42it/s]Extractor Predicting: 34it [00:23,  1.43it/s]Extractor Predicting: 35it [00:24,  1.45it/s]Extractor Predicting: 36it [00:24,  1.46it/s]Extractor Predicting: 37it [00:25,  1.51it/s]Extractor Predicting: 38it [00:25,  1.51it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:27,  1.51it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.51it/s]Extractor Predicting: 43it [00:29,  1.48it/s]Extractor Predicting: 44it [00:29,  1.49it/s]Extractor Predicting: 45it [00:30,  1.48it/s]Extractor Predicting: 46it [00:31,  1.49it/s]Extractor Predicting: 47it [00:31,  1.49it/s]Extractor Predicting: 48it [00:32,  1.49it/s]Extractor Predicting: 49it [00:33,  1.49it/s]Extractor Predicting: 50it [00:34,  1.48it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:36,  1.49it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:37,  1.46it/s]Extractor Predicting: 56it [00:38,  1.47it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:39,  1.52it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.59it/s]Extractor Predicting: 61it [00:41,  1.60it/s]Extractor Predicting: 62it [00:41,  1.62it/s]Extractor Predicting: 63it [00:42,  1.62it/s]Extractor Predicting: 64it [00:42,  1.65it/s]Extractor Predicting: 65it [00:43,  1.66it/s]Extractor Predicting: 66it [00:44,  1.65it/s]Extractor Predicting: 67it [00:44,  1.68it/s]Extractor Predicting: 68it [00:45,  1.71it/s]Extractor Predicting: 69it [00:45,  1.73it/s]Extractor Predicting: 70it [00:46,  1.71it/s]Extractor Predicting: 71it [00:47,  1.71it/s]Extractor Predicting: 72it [00:47,  1.71it/s]Extractor Predicting: 73it [00:48,  1.71it/s]Extractor Predicting: 74it [00:48,  1.69it/s]Extractor Predicting: 75it [00:49,  1.67it/s]Extractor Predicting: 76it [00:50,  1.71it/s]Extractor Predicting: 77it [00:50,  1.73it/s]Extractor Predicting: 78it [00:51,  1.71it/s]Extractor Predicting: 79it [00:51,  1.72it/s]Extractor Predicting: 80it [00:52,  1.73it/s]Extractor Predicting: 81it [00:52,  1.70it/s]Extractor Predicting: 82it [00:53,  1.68it/s]Extractor Predicting: 83it [00:54,  1.67it/s]Extractor Predicting: 84it [00:54,  1.65it/s]Extractor Predicting: 85it [00:55,  1.67it/s]Extractor Predicting: 86it [00:56,  1.61it/s]Extractor Predicting: 87it [00:56,  1.57it/s]Extractor Predicting: 88it [00:57,  1.54it/s]Extractor Predicting: 89it [00:58,  1.51it/s]Extractor Predicting: 90it [00:58,  1.47it/s]Extractor Predicting: 91it [00:59,  1.47it/s]Extractor Predicting: 92it [01:00,  1.44it/s]Extractor Predicting: 93it [01:00,  1.48it/s]Extractor Predicting: 94it [01:01,  1.46it/s]Extractor Predicting: 95it [01:02,  1.47it/s]Extractor Predicting: 96it [01:02,  1.45it/s]Extractor Predicting: 97it [01:03,  1.45it/s]Extractor Predicting: 98it [01:04,  1.30it/s]Extractor Predicting: 99it [01:05,  1.35it/s]Extractor Predicting: 100it [01:05,  1.37it/s]Extractor Predicting: 101it [01:06,  1.39it/s]Extractor Predicting: 102it [01:07,  1.39it/s]Extractor Predicting: 103it [01:08,  1.42it/s]Extractor Predicting: 104it [01:08,  1.40it/s]Extractor Predicting: 105it [01:09,  1.41it/s]Extractor Predicting: 106it [01:10,  1.38it/s]Extractor Predicting: 107it [01:10,  1.38it/s]Extractor Predicting: 108it [01:11,  1.39it/s]Extractor Predicting: 109it [01:12,  1.42it/s]Extractor Predicting: 110it [01:13,  1.42it/s]Extractor Predicting: 111it [01:13,  1.40it/s]Extractor Predicting: 112it [01:14,  1.37it/s]Extractor Predicting: 113it [01:15,  1.39it/s]Extractor Predicting: 114it [01:15,  1.43it/s]Extractor Predicting: 115it [01:16,  1.42it/s]Extractor Predicting: 116it [01:17,  1.44it/s]Extractor Predicting: 117it [01:17,  1.46it/s]Extractor Predicting: 118it [01:18,  1.43it/s]Extractor Predicting: 119it [01:19,  1.40it/s]Extractor Predicting: 120it [01:20,  1.41it/s]Extractor Predicting: 121it [01:20,  1.40it/s]Extractor Predicting: 122it [01:21,  1.42it/s]Extractor Predicting: 123it [01:22,  1.43it/s]Extractor Predicting: 124it [01:22,  1.43it/s]Extractor Predicting: 125it [01:23,  1.42it/s]Extractor Predicting: 126it [01:24,  1.38it/s]Extractor Predicting: 127it [01:25,  1.38it/s]Extractor Predicting: 128it [01:25,  1.37it/s]Extractor Predicting: 129it [01:26,  1.38it/s]Extractor Predicting: 130it [01:27,  1.38it/s]Extractor Predicting: 131it [01:27,  1.41it/s]Extractor Predicting: 132it [01:28,  1.41it/s]Extractor Predicting: 133it [01:29,  1.42it/s]Extractor Predicting: 134it [01:30,  1.44it/s]Extractor Predicting: 135it [01:30,  1.39it/s]Extractor Predicting: 136it [01:31,  1.37it/s]Extractor Predicting: 137it [01:32,  1.38it/s]Extractor Predicting: 138it [01:33,  1.38it/s]Extractor Predicting: 139it [01:33,  1.38it/s]Extractor Predicting: 140it [01:34,  1.40it/s]Extractor Predicting: 141it [01:35,  1.40it/s]Extractor Predicting: 142it [01:35,  1.41it/s]Extractor Predicting: 143it [01:36,  1.38it/s]Extractor Predicting: 144it [01:37,  1.38it/s]Extractor Predicting: 145it [01:38,  1.39it/s]Extractor Predicting: 146it [01:38,  1.40it/s]Extractor Predicting: 147it [01:39,  1.43it/s]Extractor Predicting: 148it [01:40,  1.45it/s]Extractor Predicting: 149it [01:40,  1.42it/s]Extractor Predicting: 150it [01:41,  1.42it/s]Extractor Predicting: 151it [01:42,  1.44it/s]Extractor Predicting: 152it [01:42,  1.47it/s]Extractor Predicting: 153it [01:43,  1.48it/s]Extractor Predicting: 154it [01:44,  1.50it/s]Extractor Predicting: 155it [01:44,  1.49it/s]Extractor Predicting: 156it [01:45,  1.52it/s]Extractor Predicting: 157it [01:46,  1.51it/s]Extractor Predicting: 158it [01:46,  1.53it/s]Extractor Predicting: 159it [01:47,  1.57it/s]Extractor Predicting: 160it [01:48,  1.53it/s]Extractor Predicting: 161it [01:48,  1.50it/s]Extractor Predicting: 162it [01:49,  1.47it/s]Extractor Predicting: 163it [01:50,  1.48it/s]Extractor Predicting: 164it [01:50,  1.47it/s]Extractor Predicting: 165it [01:51,  1.47it/s]Extractor Predicting: 166it [01:52,  1.46it/s]Extractor Predicting: 167it [01:52,  1.46it/s]Extractor Predicting: 168it [01:53,  1.46it/s]Extractor Predicting: 169it [01:54,  1.45it/s]Extractor Predicting: 170it [01:54,  1.48it/s]Extractor Predicting: 171it [01:55,  1.47it/s]Extractor Predicting: 172it [01:56,  1.44it/s]Extractor Predicting: 173it [01:57,  1.46it/s]Extractor Predicting: 174it [01:57,  1.46it/s]Extractor Predicting: 175it [01:58,  1.45it/s]Extractor Predicting: 176it [01:59,  1.45it/s]Extractor Predicting: 177it [01:59,  1.50it/s]Extractor Predicting: 178it [02:00,  1.48it/s]Extractor Predicting: 179it [02:01,  1.50it/s]Extractor Predicting: 180it [02:01,  1.51it/s]Extractor Predicting: 181it [02:02,  1.50it/s]Extractor Predicting: 182it [02:02,  1.53it/s]Extractor Predicting: 183it [02:03,  1.54it/s]Extractor Predicting: 184it [02:04,  1.52it/s]Extractor Predicting: 185it [02:04,  1.55it/s]Extractor Predicting: 186it [02:05,  1.54it/s]Extractor Predicting: 187it [02:06,  1.50it/s]Extractor Predicting: 188it [02:06,  1.54it/s]Extractor Predicting: 189it [02:07,  1.56it/s]Extractor Predicting: 190it [02:08,  1.59it/s]Extractor Predicting: 191it [02:08,  1.53it/s]Extractor Predicting: 192it [02:09,  1.55it/s]Extractor Predicting: 193it [02:10,  1.56it/s]Extractor Predicting: 194it [02:10,  1.60it/s]Extractor Predicting: 195it [02:11,  1.55it/s]Extractor Predicting: 196it [02:12,  1.55it/s]Extractor Predicting: 197it [02:12,  1.57it/s]Extractor Predicting: 198it [02:13,  1.55it/s]Extractor Predicting: 199it [02:13,  1.52it/s]Extractor Predicting: 200it [02:14,  1.53it/s]Extractor Predicting: 201it [02:15,  1.53it/s]Extractor Predicting: 202it [02:15,  1.56it/s]Extractor Predicting: 203it [02:16,  1.56it/s]Extractor Predicting: 204it [02:17,  1.58it/s]Extractor Predicting: 205it [02:17,  1.58it/s]Extractor Predicting: 206it [02:18,  1.64it/s]Extractor Predicting: 207it [02:18,  1.61it/s]Extractor Predicting: 208it [02:19,  1.65it/s]Extractor Predicting: 209it [02:20,  1.63it/s]Extractor Predicting: 210it [02:20,  1.63it/s]Extractor Predicting: 211it [02:21,  1.62it/s]Extractor Predicting: 212it [02:22,  1.65it/s]Extractor Predicting: 213it [02:22,  1.66it/s]Extractor Predicting: 214it [02:23,  1.67it/s]Extractor Predicting: 215it [02:23,  1.71it/s]Extractor Predicting: 216it [02:24,  1.70it/s]Extractor Predicting: 217it [02:24,  1.65it/s]Extractor Predicting: 218it [02:25,  1.63it/s]Extractor Predicting: 219it [02:26,  1.65it/s]Extractor Predicting: 220it [02:27,  1.43it/s]Extractor Predicting: 221it [02:27,  1.49it/s]Extractor Predicting: 222it [02:28,  1.57it/s]Extractor Predicting: 223it [02:28,  1.56it/s]Extractor Predicting: 224it [02:29,  1.61it/s]Extractor Predicting: 225it [02:30,  1.60it/s]Extractor Predicting: 226it [02:30,  1.62it/s]Extractor Predicting: 227it [02:31,  1.66it/s]Extractor Predicting: 228it [02:31,  1.64it/s]Extractor Predicting: 229it [02:32,  1.56it/s]Extractor Predicting: 230it [02:33,  1.52it/s]Extractor Predicting: 231it [02:34,  1.46it/s]Extractor Predicting: 232it [02:34,  1.43it/s]Extractor Predicting: 233it [02:35,  1.43it/s]Extractor Predicting: 234it [02:36,  1.42it/s]Extractor Predicting: 235it [02:36,  1.42it/s]Extractor Predicting: 236it [02:37,  1.42it/s]Extractor Predicting: 237it [02:38,  1.39it/s]Extractor Predicting: 238it [02:39,  1.37it/s]Extractor Predicting: 239it [02:39,  1.38it/s]Extractor Predicting: 240it [02:40,  1.38it/s]Extractor Predicting: 241it [02:41,  1.40it/s]Extractor Predicting: 242it [02:41,  1.42it/s]Extractor Predicting: 243it [02:42,  1.37it/s]Extractor Predicting: 244it [02:43,  1.40it/s]Extractor Predicting: 245it [02:44,  1.40it/s]Extractor Predicting: 246it [02:44,  1.42it/s]Extractor Predicting: 247it [02:45,  1.38it/s]Extractor Predicting: 248it [02:46,  1.31it/s]Extractor Predicting: 249it [02:47,  1.30it/s]Extractor Predicting: 250it [02:47,  1.33it/s]Extractor Predicting: 251it [02:48,  1.35it/s]Extractor Predicting: 252it [02:49,  1.37it/s]Extractor Predicting: 253it [02:50,  1.39it/s]Extractor Predicting: 254it [02:50,  1.38it/s]Extractor Predicting: 255it [02:51,  1.38it/s]Extractor Predicting: 256it [02:52,  1.40it/s]Extractor Predicting: 257it [02:52,  1.45it/s]Extractor Predicting: 258it [02:53,  1.46it/s]Extractor Predicting: 259it [02:54,  1.47it/s]Extractor Predicting: 260it [02:54,  1.50it/s]Extractor Predicting: 261it [02:55,  1.50it/s]Extractor Predicting: 262it [02:56,  1.49it/s]Extractor Predicting: 263it [02:56,  1.50it/s]Extractor Predicting: 264it [02:57,  1.51it/s]Extractor Predicting: 265it [02:58,  1.48it/s]Extractor Predicting: 266it [02:58,  1.51it/s]Extractor Predicting: 267it [02:59,  1.45it/s]Extractor Predicting: 268it [03:00,  1.45it/s]Extractor Predicting: 269it [03:00,  1.48it/s]Extractor Predicting: 270it [03:01,  1.47it/s]Extractor Predicting: 271it [03:02,  1.46it/s]Extractor Predicting: 272it [03:02,  1.46it/s]Extractor Predicting: 273it [03:03,  1.46it/s]Extractor Predicting: 274it [03:04,  1.46it/s]Extractor Predicting: 275it [03:04,  1.49it/s]Extractor Predicting: 276it [03:05,  1.49it/s]Extractor Predicting: 277it [03:06,  1.48it/s]Extractor Predicting: 278it [03:07,  1.45it/s]Extractor Predicting: 279it [03:07,  1.46it/s]Extractor Predicting: 280it [03:08,  1.46it/s]Extractor Predicting: 281it [03:09,  1.49it/s]Extractor Predicting: 282it [03:09,  1.48it/s]Extractor Predicting: 283it [03:10,  1.48it/s]Extractor Predicting: 284it [03:11,  1.50it/s]Extractor Predicting: 285it [03:11,  1.47it/s]Extractor Predicting: 286it [03:12,  1.47it/s]Extractor Predicting: 287it [03:13,  1.48it/s]Extractor Predicting: 288it [03:13,  1.49it/s]Extractor Predicting: 289it [03:14,  1.48it/s]Extractor Predicting: 290it [03:15,  1.49it/s]Extractor Predicting: 291it [03:15,  1.48it/s]Extractor Predicting: 292it [03:16,  1.49it/s]Extractor Predicting: 293it [03:17,  1.48it/s]Extractor Predicting: 294it [03:17,  1.48it/s]Extractor Predicting: 295it [03:18,  1.49it/s]Extractor Predicting: 296it [03:19,  1.48it/s]Extractor Predicting: 297it [03:19,  1.50it/s]Extractor Predicting: 298it [03:20,  1.46it/s]Extractor Predicting: 299it [03:21,  1.51it/s]Extractor Predicting: 300it [03:21,  1.50it/s]Extractor Predicting: 301it [03:22,  1.50it/s]Extractor Predicting: 302it [03:23,  1.49it/s]Extractor Predicting: 303it [03:23,  1.53it/s]Extractor Predicting: 304it [03:24,  1.52it/s]Extractor Predicting: 305it [03:25,  1.55it/s]Extractor Predicting: 306it [03:25,  1.53it/s]Extractor Predicting: 307it [03:26,  1.51it/s]Extractor Predicting: 308it [03:27,  1.50it/s]Extractor Predicting: 309it [03:27,  1.52it/s]Extractor Predicting: 310it [03:28,  1.48it/s]Extractor Predicting: 311it [03:29,  1.47it/s]Extractor Predicting: 312it [03:29,  1.48it/s]Extractor Predicting: 313it [03:30,  1.49it/s]Extractor Predicting: 314it [03:31,  1.49it/s]Extractor Predicting: 315it [03:31,  1.52it/s]Extractor Predicting: 316it [03:32,  1.50it/s]Extractor Predicting: 317it [03:33,  1.49it/s]Extractor Predicting: 318it [03:33,  1.53it/s]Extractor Predicting: 319it [03:34,  1.53it/s]Extractor Predicting: 320it [03:35,  1.56it/s]Extractor Predicting: 321it [03:35,  1.53it/s]Extractor Predicting: 322it [03:36,  1.52it/s]Extractor Predicting: 323it [03:37,  1.51it/s]Extractor Predicting: 324it [03:37,  1.50it/s]Extractor Predicting: 325it [03:38,  1.48it/s]Extractor Predicting: 326it [03:39,  1.49it/s]Extractor Predicting: 327it [03:39,  1.47it/s]Extractor Predicting: 328it [03:40,  1.43it/s]Extractor Predicting: 329it [03:41,  1.24it/s]Extractor Predicting: 330it [03:42,  1.30it/s]Extractor Predicting: 331it [03:42,  1.34it/s]Extractor Predicting: 332it [03:43,  1.38it/s]Extractor Predicting: 333it [03:44,  1.41it/s]Extractor Predicting: 334it [03:45,  1.43it/s]Extractor Predicting: 335it [03:45,  1.43it/s]Extractor Predicting: 336it [03:46,  1.44it/s]Extractor Predicting: 337it [03:47,  1.44it/s]Extractor Predicting: 338it [03:47,  1.45it/s]Extractor Predicting: 339it [03:48,  1.45it/s]Extractor Predicting: 340it [03:49,  1.49it/s]Extractor Predicting: 341it [03:49,  1.47it/s]Extractor Predicting: 342it [03:50,  1.50it/s]Extractor Predicting: 343it [03:51,  1.45it/s]Extractor Predicting: 344it [03:51,  1.47it/s]Extractor Predicting: 345it [03:52,  1.45it/s]Extractor Predicting: 346it [03:53,  1.48it/s]Extractor Predicting: 347it [03:53,  1.48it/s]Extractor Predicting: 348it [03:54,  1.50it/s]Extractor Predicting: 349it [03:55,  1.44it/s]Extractor Predicting: 350it [03:55,  1.43it/s]Extractor Predicting: 351it [03:56,  1.42it/s]Extractor Predicting: 352it [03:57,  1.44it/s]Extractor Predicting: 353it [03:58,  1.43it/s]Extractor Predicting: 354it [03:58,  1.45it/s]Extractor Predicting: 355it [03:59,  1.43it/s]Extractor Predicting: 356it [04:00,  1.47it/s]Extractor Predicting: 357it [04:00,  1.45it/s]Extractor Predicting: 358it [04:01,  1.44it/s]Extractor Predicting: 359it [04:02,  1.43it/s]Extractor Predicting: 360it [04:02,  1.40it/s]Extractor Predicting: 361it [04:03,  1.44it/s]Extractor Predicting: 362it [04:04,  1.41it/s]Extractor Predicting: 363it [04:05,  1.42it/s]Extractor Predicting: 364it [04:05,  1.45it/s]Extractor Predicting: 365it [04:06,  1.47it/s]Extractor Predicting: 366it [04:07,  1.45it/s]Extractor Predicting: 367it [04:07,  1.45it/s]Extractor Predicting: 368it [04:08,  1.44it/s]Extractor Predicting: 369it [04:09,  1.45it/s]Extractor Predicting: 370it [04:09,  1.44it/s]Extractor Predicting: 371it [04:10,  1.46it/s]Extractor Predicting: 372it [04:11,  1.47it/s]Extractor Predicting: 373it [04:11,  1.49it/s]Extractor Predicting: 374it [04:12,  1.49it/s]Extractor Predicting: 375it [04:13,  1.48it/s]Extractor Predicting: 376it [04:13,  1.50it/s]Extractor Predicting: 377it [04:14,  1.49it/s]Extractor Predicting: 378it [04:15,  1.51it/s]Extractor Predicting: 379it [04:15,  1.52it/s]Extractor Predicting: 380it [04:16,  1.51it/s]Extractor Predicting: 381it [04:17,  1.51it/s]Extractor Predicting: 382it [04:17,  1.48it/s]Extractor Predicting: 383it [04:18,  1.51it/s]Extractor Predicting: 384it [04:19,  1.52it/s]Extractor Predicting: 385it [04:19,  1.52it/s]Extractor Predicting: 386it [04:20,  1.54it/s]Extractor Predicting: 387it [04:21,  1.55it/s]Extractor Predicting: 388it [04:21,  1.52it/s]Extractor Predicting: 389it [04:22,  1.52it/s]Extractor Predicting: 390it [04:23,  1.52it/s]Extractor Predicting: 391it [04:23,  1.51it/s]Extractor Predicting: 392it [04:24,  1.52it/s]Extractor Predicting: 393it [04:25,  1.52it/s]Extractor Predicting: 394it [04:25,  1.54it/s]Extractor Predicting: 395it [04:26,  1.48it/s]Extractor Predicting: 396it [04:27,  1.50it/s]Extractor Predicting: 397it [04:27,  1.52it/s]Extractor Predicting: 398it [04:28,  1.50it/s]Extractor Predicting: 399it [04:29,  1.50it/s]Extractor Predicting: 400it [04:29,  1.49it/s]Extractor Predicting: 401it [04:30,  1.51it/s]Extractor Predicting: 402it [04:30,  1.55it/s]Extractor Predicting: 403it [04:31,  1.53it/s]Extractor Predicting: 404it [04:32,  1.55it/s]Extractor Predicting: 405it [04:32,  1.54it/s]Extractor Predicting: 406it [04:33,  1.54it/s]Extractor Predicting: 407it [04:34,  1.52it/s]Extractor Predicting: 408it [04:34,  1.54it/s]Extractor Predicting: 409it [04:35,  1.56it/s]Extractor Predicting: 410it [04:36,  1.56it/s]Extractor Predicting: 411it [04:36,  1.52it/s]Extractor Predicting: 412it [04:37,  1.54it/s]Extractor Predicting: 413it [04:38,  1.54it/s]Extractor Predicting: 414it [04:38,  1.55it/s]Extractor Predicting: 415it [04:39,  1.57it/s]Extractor Predicting: 416it [04:40,  1.56it/s]Extractor Predicting: 417it [04:40,  1.54it/s]Extractor Predicting: 418it [04:41,  1.55it/s]Extractor Predicting: 419it [04:41,  1.54it/s]Extractor Predicting: 420it [04:42,  1.54it/s]Extractor Predicting: 421it [04:43,  1.53it/s]Extractor Predicting: 422it [04:43,  1.52it/s]Extractor Predicting: 423it [04:44,  1.52it/s]Extractor Predicting: 424it [04:45,  1.52it/s]Extractor Predicting: 425it [04:45,  1.72it/s]Extractor Predicting: 425it [04:45,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:01,966 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:02,022 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:02,022 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:02,023 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:02,023 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:42:02,790 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:42:02,791 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:42:03,376 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:42:04,429 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:42:04,429 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:08,342 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:08,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:08,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:08,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:08,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:42:09,016 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:42:09,017 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:42:09,597 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:42:09,756 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:42:09,756 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.38830584707646176,
  "recall": 0.10170822697820538,
  "score": 0.16119495876769876,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.33it/s]Extractor Predicting: 4it [00:02,  1.33it/s]Extractor Predicting: 5it [00:03,  1.33it/s]Extractor Predicting: 6it [00:04,  1.37it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 7it [00:04,  1.46it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:42:15,469 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:42:15,470 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:42:15,504 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:42:15,505 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:42:15,513 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:42:21,100 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:42:21,100 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:42:21,109 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:42:21,110 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:42:21,114 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:21,127 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:21,127 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:21,127 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:21,127 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:21,127 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:21,127 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6153846153846154,
  "recall": 0.025477707006369428,
  "score": 0.0489296636085627,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:42:21,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:23,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:24,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:25,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:25,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:26,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:29,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:30,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:32,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:34,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:34,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:35,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:46, 15.09s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:36,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:38,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:39,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:40,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:40,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:41,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:42,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:42,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:43,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:44,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:45,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:46,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:46,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:47,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:48,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:48,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:49,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:50,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:51,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:52,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:53,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:33<05:03, 16.88s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:54,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:55,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:56,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:56,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:57,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:58,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:58,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:59,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:00,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:01,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:01,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:02,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:03,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:03,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:04,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:05,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:06,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:06,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:07,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:08,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:08,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:09,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:48<04:36, 16.29s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:10,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:10,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:11,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:12,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:12,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:13,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:14,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:14,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:15,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:16,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:17,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:17,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:18,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:19,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:19,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:20,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:21,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:22,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:22,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:23,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:02<04:04, 15.31s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:24,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:24,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:25,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:26,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:26,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:27,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:28,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:28,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:29,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:30,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:30,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:31,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:32,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:33,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:33,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:34,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:35,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:35,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:36,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:37,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:37,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:17<03:45, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:38,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:39,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:40,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:40,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:41,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:42,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:43,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:44,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:45,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:45,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:46,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:47,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:48,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:48,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:49,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:50,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:50,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:51,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:52,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:53,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:32<03:32, 15.21s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:54,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:54,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:55,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:56,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:57,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:58,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:58,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:59,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:00,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:00,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:01,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:02,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:02,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:04,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:04,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:06,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:06,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:07,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:07,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:08,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:09,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:10,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:49<03:23, 15.65s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:10,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:11,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:12,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:12,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:13,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:14,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:14,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:15,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:16,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:17,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:17,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:18,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:19,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:19,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:20,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:21,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:22,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:22,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:24,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:24,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:25,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:26,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:05<03:09, 15.83s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:26,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:27,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:28,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:29,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:30,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:30,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:31,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:32,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:33,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:33,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:34,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:35,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:36,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:36,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:37,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:38,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:38,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:39,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:40,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:41,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:42,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:42,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:22<02:57, 16.14s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:43,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:44,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:45,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:46,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:47,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:47,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:48,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:49,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:50,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:50,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:51,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:52,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:53,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:54,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:55,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:56,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:56,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:57,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:58,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:59,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:00,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:39<02:44, 16.41s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:00,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:01,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:02,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:03,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:03,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:04,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:05,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:06,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:06,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:07,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:08,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:08,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:09,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:10,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:11,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:11,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:12,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:13,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:14,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:14,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:15,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:54<02:25, 16.15s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:16,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:17,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:17,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:18,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:19,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:20,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:20,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:21,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:22,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:22,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:23,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:24,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:25,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:25,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:26,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:27,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:28,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:28,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:29,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:30,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:30,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:10<02:07, 15.97s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:31,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:32,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:33,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:33,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:34,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:35,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:35,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:36,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:37,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:37,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:38,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:39,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:40,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:40,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:41,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:42,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:42,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:43,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:44,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:44,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:24<01:46, 15.26s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:45,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:46,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:47,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:48,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:48,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:49,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:50,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:51,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:52,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:53,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:54,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:55,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:56,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:56,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:57,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:58,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:59,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:00,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:00,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:01,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:02,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:41<01:36, 16.04s/it][WARNING|generation_utils.py:914] 2023-08-28 21:46:03,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:04,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:04,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:05,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:06,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:07,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:07,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:08,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:09,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:10,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:11,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:11,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:12,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:13,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:14,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:14,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:15,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:16,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:16,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:17,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:18,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:57<01:19, 15.90s/it][WARNING|generation_utils.py:914] 2023-08-28 21:46:18,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:19,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:20,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:21,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:21,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:22,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:23,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:24,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:24,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:25,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:26,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:27,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:27,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:28,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:29,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:30,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:30,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:31,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:32,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:33,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:33,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:13<01:03, 15.78s/it][WARNING|generation_utils.py:914] 2023-08-28 21:46:34,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:35,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:35,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:36,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:37,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:38,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:38,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:39,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:40,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:40,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:41,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:42,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:42,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:43,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:44,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:44,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:45,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:46,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:47,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:47,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:27<00:45, 15.29s/it][WARNING|generation_utils.py:914] 2023-08-28 21:46:48,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:49,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:50,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:50,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:51,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:52,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:53,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:53,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:54,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:55,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:55,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:56,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:57,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:58,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:59,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:59,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:00,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:01,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:02,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:02,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:42<00:30, 15.16s/it][WARNING|generation_utils.py:914] 2023-08-28 21:47:03,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:04,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:04,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:05,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:06,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:07,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:07,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:08,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:09,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:09,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:10,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:11,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:11,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:12,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:13,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:14,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:14,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:15,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:16,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:16,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:17,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:56<00:15, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-28 21:47:18,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:18,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:19,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:20,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:21,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:21,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:22,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:23,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:24,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:24,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:25,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:26,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:26,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:27,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:28,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:29,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:29,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:30,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:31,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:32,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:11<00:00, 14.92s/it]Generating: 100%|██████████| 20/20 [05:11<00:00, 15.57s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:42,300 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:42,305 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:42,305 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:42,305 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:42,305 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:47:43,211 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:47:43,212 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:47:44,326 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:47:45,397 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:47:45,404 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:49,951 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:49,962 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:49,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:49,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:49,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:47:50,887 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:47:50,888 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:47:51,827 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:47:51,997 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:47:51,997 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : has part .', 'success_rate': 0.959375, 'errors': {''}}
['Relation : location . Context : Later in the year , the museum built a monument to the French colonial leader Louis XIV . Head Entity : Louis XIV , Tail Entity : France .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8274456521739131, 'errors': {'', "('Καστρον', 'location', '', 'Erosion in the Greek cities of Καστρον ( Καστρον - των ) and Κικόμάσων ( τδρόμων - μάσων ) and ποκαύμανομ are also included : Καστρον Καστρον Καστρον .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : platform .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : position held .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.975, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.890625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : country of citizenship . Context : On 31 March 2014 , the United States appointed him a Representative to represent Kentucky in the US Senate . Head Entity : United States Senate , Tail Entity : Kentucky .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : father .', 'success_rate': 0.8877840909090909, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9211309523809523, 'errors': {''}}
['Relation : heritage designation . Context : The site is located in the heart of the city , and offers views of the city from the south , including the downtown area . Head Entity : Downtown , Tail Entity : City of Pittsburgh .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.96875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9196428571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.95625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 407, 'raw': 416}
{'target': 600, 'success': 438, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.975, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 9148
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9248, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.29it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:02,  1.41it/s]Extractor Estimating: 4it [00:02,  1.41it/s]Extractor Estimating: 5it [00:03,  1.44it/s]Extractor Estimating: 6it [00:04,  1.39it/s]Extractor Estimating: 7it [00:04,  1.41it/s]Extractor Estimating: 8it [00:05,  1.43it/s]Extractor Estimating: 9it [00:06,  1.51it/s]Extractor Estimating: 10it [00:06,  1.51it/s]Extractor Estimating: 11it [00:07,  1.53it/s]Extractor Estimating: 12it [00:08,  1.50it/s]Extractor Estimating: 13it [00:08,  1.48it/s]Extractor Estimating: 14it [00:09,  1.39it/s]Extractor Estimating: 15it [00:10,  1.44it/s]Extractor Estimating: 16it [00:11,  1.49it/s]Extractor Estimating: 17it [00:11,  1.48it/s]Extractor Estimating: 18it [00:12,  1.55it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:13,  1.53it/s]Extractor Estimating: 21it [00:14,  1.41it/s]Extractor Estimating: 22it [00:15,  1.37it/s]Extractor Estimating: 23it [00:15,  1.37it/s]Extractor Estimating: 24it [00:16,  1.41it/s]Extractor Estimating: 25it [00:17,  1.44it/s]Extractor Estimating: 26it [00:17,  1.51it/s]Extractor Estimating: 27it [00:18,  1.54it/s]Extractor Estimating: 28it [00:19,  1.56it/s]Extractor Estimating: 29it [00:19,  1.46it/s]Extractor Estimating: 30it [00:20,  1.50it/s]Extractor Estimating: 31it [00:21,  1.54it/s]Extractor Estimating: 32it [00:21,  1.54it/s]Extractor Estimating: 33it [00:22,  1.56it/s]Extractor Estimating: 34it [00:23,  1.47it/s]Extractor Estimating: 35it [00:23,  1.47it/s]Extractor Estimating: 36it [00:24,  1.47it/s]Extractor Estimating: 37it [00:25,  1.55it/s]Extractor Estimating: 38it [00:25,  1.55it/s]Extractor Estimating: 39it [00:26,  1.53it/s]Extractor Estimating: 40it [00:27,  1.53it/s]Extractor Estimating: 41it [00:27,  1.50it/s]Extractor Estimating: 42it [00:28,  1.50it/s]Extractor Estimating: 43it [00:29,  1.55it/s]Extractor Estimating: 44it [00:29,  1.56it/s]Extractor Estimating: 45it [00:30,  1.57it/s]Extractor Estimating: 46it [00:30,  1.61it/s]Extractor Estimating: 47it [00:31,  1.57it/s]Extractor Estimating: 48it [00:32,  1.63it/s]Extractor Estimating: 49it [00:32,  1.58it/s]Extractor Estimating: 50it [00:33,  1.58it/s]Extractor Estimating: 51it [00:34,  1.59it/s]Extractor Estimating: 52it [00:34,  1.60it/s]Extractor Estimating: 53it [00:35,  1.59it/s]Extractor Estimating: 54it [00:35,  1.58it/s]Extractor Estimating: 55it [00:36,  1.58it/s]Extractor Estimating: 56it [00:37,  1.58it/s]Extractor Estimating: 57it [00:37,  1.55it/s]Extractor Estimating: 58it [00:38,  1.63it/s]Extractor Estimating: 59it [00:39,  1.55it/s]Extractor Estimating: 60it [00:39,  1.57it/s]Extractor Estimating: 61it [00:40,  1.64it/s]Extractor Estimating: 62it [00:40,  1.64it/s]Extractor Estimating: 63it [00:41,  1.63it/s]Extractor Estimating: 64it [00:42,  1.59it/s]Extractor Estimating: 65it [00:42,  1.55it/s]Extractor Estimating: 66it [00:43,  1.53it/s]Extractor Estimating: 67it [00:44,  1.52it/s]Extractor Estimating: 68it [00:44,  1.58it/s]Extractor Estimating: 69it [00:45,  1.62it/s]Extractor Estimating: 70it [00:45,  1.64it/s]Extractor Estimating: 71it [00:46,  1.63it/s]Extractor Estimating: 72it [00:47,  1.65it/s]Extractor Estimating: 73it [00:47,  1.66it/s]Extractor Estimating: 74it [00:48,  1.65it/s]Extractor Estimating: 75it [00:49,  1.61it/s]Extractor Estimating: 76it [00:49,  1.64it/s]Extractor Estimating: 77it [00:50,  1.66it/s]Extractor Estimating: 78it [00:50,  1.71it/s]Extractor Estimating: 79it [00:51,  1.69it/s]Extractor Estimating: 80it [00:51,  1.64it/s]Extractor Estimating: 81it [00:52,  1.57it/s]Extractor Estimating: 82it [00:53,  1.61it/s]Extractor Estimating: 83it [00:53,  1.62it/s]Extractor Estimating: 84it [00:54,  1.66it/s]Extractor Estimating: 85it [00:55,  1.52it/s]Extractor Estimating: 86it [00:55,  1.56it/s]Extractor Estimating: 87it [00:56,  1.53it/s]Extractor Estimating: 88it [00:57,  1.56it/s]Extractor Estimating: 89it [00:57,  1.59it/s]Extractor Estimating: 90it [00:58,  1.62it/s]Extractor Estimating: 91it [00:58,  1.60it/s]Extractor Estimating: 92it [00:59,  1.61it/s]Extractor Estimating: 93it [01:00,  1.66it/s]Extractor Estimating: 94it [01:00,  1.64it/s]Extractor Estimating: 95it [01:01,  1.64it/s]Extractor Estimating: 96it [01:02,  1.50it/s]Extractor Estimating: 97it [01:02,  1.47it/s]Extractor Estimating: 98it [01:03,  1.55it/s]Extractor Estimating: 99it [01:04,  1.58it/s]Extractor Estimating: 100it [01:04,  1.60it/s]Extractor Estimating: 101it [01:05,  1.63it/s]Extractor Estimating: 102it [01:05,  1.58it/s]Extractor Estimating: 103it [01:06,  1.59it/s]Extractor Estimating: 104it [01:07,  1.63it/s]Extractor Estimating: 105it [01:07,  1.66it/s]Extractor Estimating: 106it [01:08,  1.66it/s]Extractor Estimating: 107it [01:08,  1.63it/s]Extractor Estimating: 108it [01:09,  1.67it/s]Extractor Estimating: 109it [01:10,  1.53it/s]Extractor Estimating: 110it [01:10,  1.60it/s]Extractor Estimating: 111it [01:11,  1.56it/s]Extractor Estimating: 112it [01:12,  1.64it/s]Extractor Estimating: 113it [01:12,  1.64it/s]Extractor Estimating: 114it [01:13,  1.57it/s]Extractor Estimating: 115it [01:13,  1.62it/s]Extractor Estimating: 116it [01:14,  1.65it/s]Extractor Estimating: 117it [01:15,  1.72it/s]Extractor Estimating: 118it [01:15,  1.70it/s]Extractor Estimating: 119it [01:16,  1.71it/s]Extractor Estimating: 120it [01:16,  1.73it/s]Extractor Estimating: 121it [01:17,  1.73it/s]Extractor Estimating: 122it [01:17,  1.73it/s]Extractor Estimating: 123it [01:18,  1.74it/s]Extractor Estimating: 124it [01:19,  1.73it/s]Extractor Estimating: 125it [01:19,  1.69it/s]Extractor Estimating: 126it [01:20,  1.62it/s]Extractor Estimating: 127it [01:21,  1.58it/s]Extractor Estimating: 128it [01:21,  1.58it/s]Extractor Estimating: 129it [01:22,  1.56it/s]Extractor Estimating: 130it [01:22,  1.59it/s]Extractor Estimating: 131it [01:23,  1.59it/s]Extractor Estimating: 132it [01:24,  1.55it/s]Extractor Estimating: 133it [01:25,  1.46it/s]Extractor Estimating: 134it [01:25,  1.47it/s]Extractor Estimating: 135it [01:26,  1.51it/s]Extractor Estimating: 136it [01:27,  1.43it/s]Extractor Estimating: 137it [01:27,  1.50it/s]Extractor Estimating: 138it [01:28,  1.58it/s]Extractor Estimating: 139it [01:28,  1.56it/s]Extractor Estimating: 140it [01:29,  1.52it/s]Extractor Estimating: 141it [01:30,  1.53it/s]Extractor Estimating: 142it [01:30,  1.55it/s]Extractor Estimating: 143it [01:31,  1.56it/s]Extractor Estimating: 144it [01:32,  1.56it/s]Extractor Estimating: 145it [01:32,  1.58it/s]Extractor Estimating: 146it [01:33,  1.61it/s]Extractor Estimating: 147it [01:34,  1.58it/s]Extractor Estimating: 148it [01:34,  1.45it/s]Extractor Estimating: 149it [01:35,  1.49it/s]Extractor Estimating: 150it [01:36,  1.46it/s]Extractor Estimating: 151it [01:36,  1.55it/s]Extractor Estimating: 152it [01:37,  1.54it/s]Extractor Estimating: 153it [01:37,  1.59it/s]Extractor Estimating: 154it [01:38,  1.59it/s]Extractor Estimating: 155it [01:39,  1.65it/s]Extractor Estimating: 156it [01:39,  1.70it/s]Extractor Estimating: 157it [01:40,  1.78it/s]Extractor Estimating: 158it [01:40,  1.81it/s]Extractor Estimating: 159it [01:41,  1.77it/s]Extractor Estimating: 160it [01:41,  1.77it/s]Extractor Estimating: 161it [01:42,  1.76it/s]Extractor Estimating: 162it [01:43,  1.80it/s]Extractor Estimating: 163it [01:43,  1.70it/s]Extractor Estimating: 164it [01:44,  1.78it/s]Extractor Estimating: 165it [01:44,  1.80it/s]Extractor Estimating: 166it [01:45,  1.75it/s]Extractor Estimating: 167it [01:46,  1.62it/s]Extractor Estimating: 168it [01:46,  1.67it/s]Extractor Estimating: 169it [01:47,  1.76it/s]Extractor Estimating: 170it [01:47,  1.77it/s]Extractor Estimating: 171it [01:48,  1.83it/s]Extractor Estimating: 172it [01:48,  1.84it/s]Extractor Estimating: 173it [01:49,  1.81it/s]Extractor Estimating: 174it [01:49,  1.74it/s]Extractor Estimating: 175it [01:50,  1.75it/s]Extractor Estimating: 176it [01:51,  1.69it/s]Extractor Estimating: 177it [01:51,  1.66it/s]Extractor Estimating: 178it [01:52,  1.67it/s]Extractor Estimating: 179it [01:52,  1.63it/s]Extractor Estimating: 180it [01:53,  1.60it/s]Extractor Estimating: 181it [01:54,  1.63it/s]Extractor Estimating: 182it [01:54,  1.66it/s]Extractor Estimating: 183it [01:55,  1.58it/s]Extractor Estimating: 184it [01:55,  1.70it/s]Extractor Estimating: 185it [01:56,  1.71it/s]Extractor Estimating: 186it [01:57,  1.70it/s]Extractor Estimating: 187it [01:57,  1.70it/s]Extractor Estimating: 188it [01:58,  1.70it/s]Extractor Estimating: 189it [01:58,  1.75it/s]Extractor Estimating: 190it [01:59,  1.77it/s]Extractor Estimating: 191it [01:59,  1.77it/s]Extractor Estimating: 192it [02:00,  1.75it/s]Extractor Estimating: 193it [02:01,  1.78it/s]Extractor Estimating: 194it [02:01,  1.83it/s]Extractor Estimating: 195it [02:02,  1.81it/s]Extractor Estimating: 196it [02:02,  1.80it/s]Extractor Estimating: 197it [02:03,  1.74it/s]Extractor Estimating: 198it [02:03,  1.74it/s]Extractor Estimating: 199it [02:04,  1.77it/s]Extractor Estimating: 200it [02:05,  1.71it/s]Extractor Estimating: 201it [02:05,  1.56it/s]Extractor Estimating: 202it [02:06,  1.60it/s]Extractor Estimating: 203it [02:07,  1.59it/s]Extractor Estimating: 204it [02:07,  1.51it/s]Extractor Estimating: 205it [02:08,  1.58it/s]Extractor Estimating: 206it [02:09,  1.61it/s]Extractor Estimating: 207it [02:09,  1.71it/s]Extractor Estimating: 208it [02:10,  1.71it/s]Extractor Estimating: 209it [02:10,  1.69it/s]Extractor Estimating: 210it [02:11,  1.64it/s]Extractor Estimating: 211it [02:11,  1.68it/s]Extractor Estimating: 212it [02:12,  1.66it/s]Extractor Estimating: 213it [02:13,  1.62it/s]Extractor Estimating: 214it [02:13,  1.54it/s]Extractor Estimating: 215it [02:14,  1.56it/s]Extractor Estimating: 216it [02:15,  1.62it/s]Extractor Estimating: 217it [02:15,  1.64it/s]Extractor Estimating: 218it [02:16,  1.69it/s]Extractor Estimating: 219it [02:16,  1.64it/s]Extractor Estimating: 220it [02:17,  1.67it/s]Extractor Estimating: 221it [02:18,  1.71it/s]Extractor Estimating: 222it [02:18,  1.68it/s]Extractor Estimating: 223it [02:19,  1.68it/s]Extractor Estimating: 224it [02:19,  1.68it/s]Extractor Estimating: 225it [02:20,  1.57it/s]Extractor Estimating: 226it [02:21,  1.59it/s]Extractor Estimating: 227it [02:21,  1.52it/s]Extractor Estimating: 228it [02:22,  1.57it/s]Extractor Estimating: 229it [02:23,  1.62it/s]Extractor Estimating: 230it [02:23,  1.60it/s]Extractor Estimating: 231it [02:24,  1.54it/s]Extractor Estimating: 232it [02:25,  1.46it/s]Extractor Estimating: 233it [02:25,  1.52it/s]Extractor Estimating: 234it [02:26,  1.54it/s]Extractor Estimating: 235it [02:27,  1.50it/s]Extractor Estimating: 236it [02:27,  1.48it/s]Extractor Estimating: 237it [02:28,  1.51it/s]Extractor Estimating: 238it [02:29,  1.49it/s]Extractor Estimating: 239it [02:29,  1.53it/s]Extractor Estimating: 240it [02:30,  1.35it/s]Extractor Estimating: 241it [02:31,  1.37it/s]Extractor Estimating: 242it [02:32,  1.39it/s]Extractor Estimating: 243it [02:32,  1.43it/s]Extractor Estimating: 244it [02:33,  1.45it/s]Extractor Estimating: 245it [02:34,  1.48it/s]Extractor Estimating: 246it [02:34,  1.47it/s]Extractor Estimating: 247it [02:35,  1.45it/s]Extractor Estimating: 248it [02:36,  1.53it/s]Extractor Estimating: 249it [02:36,  1.48it/s]Extractor Estimating: 250it [02:37,  1.53it/s]Extractor Estimating: 251it [02:37,  1.57it/s]Extractor Estimating: 252it [02:38,  1.54it/s]Extractor Estimating: 253it [02:39,  1.57it/s]Extractor Estimating: 254it [02:39,  1.60it/s]Extractor Estimating: 255it [02:40,  1.63it/s]Extractor Estimating: 256it [02:40,  1.65it/s]Extractor Estimating: 257it [02:41,  1.62it/s]Extractor Estimating: 258it [02:42,  1.59it/s]Extractor Estimating: 259it [02:42,  1.63it/s]Extractor Estimating: 260it [02:43,  1.65it/s]Extractor Estimating: 261it [02:43,  1.71it/s]Extractor Estimating: 262it [02:44,  1.69it/s]Extractor Estimating: 263it [02:45,  1.74it/s]Extractor Estimating: 264it [02:45,  1.76it/s]Extractor Estimating: 265it [02:46,  1.73it/s]Extractor Estimating: 266it [02:46,  1.68it/s]Extractor Estimating: 267it [02:47,  1.67it/s]Extractor Estimating: 268it [02:48,  1.65it/s]Extractor Estimating: 269it [02:48,  1.64it/s]Extractor Estimating: 270it [02:49,  1.64it/s]Extractor Estimating: 271it [02:49,  1.63it/s]Extractor Estimating: 272it [02:50,  1.67it/s]Extractor Estimating: 273it [02:51,  1.64it/s]Extractor Estimating: 274it [02:51,  1.59it/s]Extractor Estimating: 275it [02:52,  1.45it/s]Extractor Estimating: 276it [02:53,  1.46it/s]Extractor Estimating: 277it [02:54,  1.46it/s]Extractor Estimating: 278it [02:54,  1.47it/s]Extractor Estimating: 279it [02:55,  1.48it/s]Extractor Estimating: 280it [02:56,  1.51it/s]Extractor Estimating: 281it [02:56,  1.57it/s]Extractor Estimating: 282it [02:57,  1.58it/s]Extractor Estimating: 283it [02:57,  1.62it/s]Extractor Estimating: 284it [02:58,  1.61it/s]Extractor Estimating: 285it [02:59,  1.52it/s]Extractor Estimating: 286it [02:59,  1.53it/s]Extractor Estimating: 287it [03:00,  1.54it/s]Extractor Estimating: 288it [03:01,  1.55it/s]Extractor Estimating: 289it [03:01,  1.58it/s]Extractor Estimating: 290it [03:02,  1.56it/s]Extractor Estimating: 291it [03:02,  1.57it/s]Extractor Estimating: 292it [03:03,  1.59it/s]Extractor Estimating: 293it [03:04,  1.64it/s]Extractor Estimating: 294it [03:04,  1.65it/s]Extractor Estimating: 295it [03:05,  1.62it/s]Extractor Estimating: 296it [03:06,  1.55it/s]Extractor Estimating: 297it [03:06,  1.51it/s]Extractor Estimating: 298it [03:07,  1.50it/s]Extractor Estimating: 299it [03:08,  1.51it/s]Extractor Estimating: 300it [03:08,  1.50it/s]Extractor Estimating: 301it [03:09,  1.57it/s]Extractor Estimating: 302it [03:10,  1.58it/s]Extractor Estimating: 303it [03:10,  1.58it/s]Extractor Estimating: 304it [03:11,  1.61it/s]Extractor Estimating: 305it [03:11,  1.59it/s]Extractor Estimating: 306it [03:12,  1.63it/s]Extractor Estimating: 307it [03:13,  1.64it/s]Extractor Estimating: 308it [03:13,  1.59it/s]Extractor Estimating: 309it [03:14,  1.57it/s]Extractor Estimating: 310it [03:15,  1.54it/s]Extractor Estimating: 311it [03:15,  1.53it/s]Extractor Estimating: 312it [03:16,  1.55it/s]Extractor Estimating: 313it [03:17,  1.53it/s]Extractor Estimating: 314it [03:17,  1.56it/s]Extractor Estimating: 315it [03:18,  1.51it/s]Extractor Estimating: 316it [03:18,  1.55it/s]Extractor Estimating: 317it [03:19,  1.59it/s]Extractor Estimating: 318it [03:20,  1.58it/s]Extractor Estimating: 319it [03:20,  1.60it/s]Extractor Estimating: 320it [03:21,  1.61it/s]Extractor Estimating: 321it [03:22,  1.61it/s]Extractor Estimating: 322it [03:22,  1.56it/s]Extractor Estimating: 323it [03:23,  1.63it/s]Extractor Estimating: 324it [03:23,  1.60it/s]Extractor Estimating: 325it [03:24,  1.61it/s]Extractor Estimating: 326it [03:25,  1.70it/s]Extractor Estimating: 327it [03:25,  1.56it/s]Extractor Estimating: 328it [03:26,  1.60it/s]Extractor Estimating: 329it [03:27,  1.63it/s]Extractor Estimating: 330it [03:27,  1.67it/s]Extractor Estimating: 331it [03:28,  1.68it/s]Extractor Estimating: 332it [03:28,  1.68it/s]Extractor Estimating: 333it [03:29,  1.67it/s]Extractor Estimating: 334it [03:29,  1.66it/s]Extractor Estimating: 335it [03:30,  1.73it/s]Extractor Estimating: 336it [03:31,  1.66it/s]Extractor Estimating: 337it [03:31,  1.71it/s]Extractor Estimating: 338it [03:32,  1.66it/s]Extractor Estimating: 339it [03:32,  1.63it/s]Extractor Estimating: 340it [03:33,  1.69it/s]Extractor Estimating: 341it [03:34,  1.72it/s]Extractor Estimating: 342it [03:34,  1.74it/s]Extractor Estimating: 343it [03:35,  1.72it/s]Extractor Estimating: 344it [03:35,  1.68it/s]Extractor Estimating: 345it [03:36,  1.70it/s]Extractor Estimating: 346it [03:37,  1.69it/s]Extractor Estimating: 347it [03:37,  1.71it/s]Extractor Estimating: 348it [03:38,  1.69it/s]Extractor Estimating: 349it [03:38,  1.72it/s]Extractor Estimating: 350it [03:39,  1.68it/s]Extractor Estimating: 351it [03:40,  1.67it/s]Extractor Estimating: 352it [03:40,  1.68it/s]Extractor Estimating: 353it [03:41,  1.60it/s]Extractor Estimating: 354it [03:41,  1.64it/s]Extractor Estimating: 355it [03:42,  1.66it/s]Extractor Estimating: 356it [03:43,  1.63it/s]Extractor Estimating: 357it [03:43,  1.70it/s]Extractor Estimating: 358it [03:44,  1.64it/s]Extractor Estimating: 359it [03:45,  1.52it/s]Extractor Estimating: 360it [03:45,  1.62it/s]Extractor Estimating: 361it [03:46,  1.70it/s]Extractor Estimating: 362it [03:46,  1.69it/s]Extractor Estimating: 363it [03:47,  1.66it/s]Extractor Estimating: 364it [03:47,  1.64it/s]Extractor Estimating: 365it [03:48,  1.64it/s]Extractor Estimating: 366it [03:49,  1.53it/s]Extractor Estimating: 367it [03:49,  1.55it/s]Extractor Estimating: 368it [03:50,  1.62it/s]Extractor Estimating: 369it [03:51,  1.69it/s]Extractor Estimating: 370it [03:51,  1.70it/s]Extractor Estimating: 371it [03:52,  1.69it/s]Extractor Estimating: 372it [03:52,  1.67it/s]Extractor Estimating: 373it [03:53,  1.67it/s]Extractor Estimating: 374it [03:54,  1.64it/s]Extractor Estimating: 375it [03:54,  1.65it/s]Extractor Estimating: 376it [03:55,  1.65it/s]Extractor Estimating: 377it [03:55,  1.57it/s]Extractor Estimating: 378it [03:56,  1.59it/s]Extractor Estimating: 379it [03:57,  1.60it/s]Extractor Estimating: 380it [03:57,  1.63it/s]Extractor Estimating: 381it [03:58,  1.51it/s]Extractor Estimating: 382it [03:59,  1.57it/s]Extractor Estimating: 383it [03:59,  1.62it/s]Extractor Estimating: 384it [04:00,  1.59it/s]Extractor Estimating: 385it [04:00,  1.62it/s]Extractor Estimating: 386it [04:01,  1.59it/s]Extractor Estimating: 387it [04:02,  1.66it/s]Extractor Estimating: 388it [04:02,  1.59it/s]Extractor Estimating: 389it [04:03,  1.64it/s]Extractor Estimating: 390it [04:03,  1.69it/s]Extractor Estimating: 391it [04:04,  1.74it/s]Extractor Estimating: 392it [04:04,  1.77it/s]Extractor Estimating: 393it [04:05,  1.65it/s]Extractor Estimating: 394it [04:06,  1.68it/s]Extractor Estimating: 395it [04:06,  1.69it/s]Extractor Estimating: 396it [04:07,  1.71it/s]Extractor Estimating: 397it [04:08,  1.65it/s]Extractor Estimating: 398it [04:08,  1.69it/s]Extractor Estimating: 399it [04:09,  1.68it/s]Extractor Estimating: 400it [04:09,  1.75it/s]Extractor Estimating: 401it [04:10,  1.68it/s]Extractor Estimating: 402it [04:11,  1.64it/s]Extractor Estimating: 403it [04:11,  1.63it/s]Extractor Estimating: 404it [04:12,  1.60it/s]Extractor Estimating: 405it [04:12,  1.63it/s]Extractor Estimating: 406it [04:13,  1.63it/s]Extractor Estimating: 407it [04:14,  1.63it/s]Extractor Estimating: 408it [04:14,  1.67it/s]Extractor Estimating: 409it [04:15,  1.65it/s]Extractor Estimating: 410it [04:15,  1.66it/s]Extractor Estimating: 411it [04:16,  1.66it/s]Extractor Estimating: 412it [04:17,  1.71it/s]Extractor Estimating: 413it [04:17,  1.72it/s]Extractor Estimating: 414it [04:18,  1.66it/s]Extractor Estimating: 415it [04:18,  1.63it/s]Extractor Estimating: 416it [04:19,  1.64it/s]Extractor Estimating: 417it [04:20,  1.66it/s]Extractor Estimating: 418it [04:20,  1.66it/s]Extractor Estimating: 419it [04:21,  1.65it/s]Extractor Estimating: 420it [04:21,  1.63it/s]Extractor Estimating: 421it [04:22,  1.64it/s]Extractor Estimating: 422it [04:23,  1.67it/s]Extractor Estimating: 423it [04:23,  1.66it/s]Extractor Estimating: 424it [04:24,  1.64it/s]Extractor Estimating: 425it [04:25,  1.63it/s]Extractor Estimating: 426it [04:25,  1.50it/s]Extractor Estimating: 427it [04:26,  1.54it/s]Extractor Estimating: 428it [04:27,  1.56it/s]Extractor Estimating: 429it [04:27,  1.54it/s]Extractor Estimating: 430it [04:28,  1.56it/s]Extractor Estimating: 431it [04:28,  1.60it/s]Extractor Estimating: 432it [04:29,  1.60it/s]Extractor Estimating: 433it [04:30,  1.58it/s]Extractor Estimating: 434it [04:30,  1.60it/s]Extractor Estimating: 435it [04:31,  1.63it/s]Extractor Estimating: 436it [04:32,  1.58it/s]Extractor Estimating: 437it [04:32,  1.56it/s]Extractor Estimating: 438it [04:33,  1.59it/s]Extractor Estimating: 439it [04:34,  1.53it/s]Extractor Estimating: 440it [04:34,  1.38it/s]Extractor Estimating: 441it [04:35,  1.45it/s]Extractor Estimating: 442it [04:36,  1.47it/s]Extractor Estimating: 443it [04:36,  1.51it/s]Extractor Estimating: 444it [04:37,  1.54it/s]Extractor Estimating: 445it [04:38,  1.54it/s]Extractor Estimating: 446it [04:38,  1.55it/s]Extractor Estimating: 447it [04:39,  1.60it/s]Extractor Estimating: 448it [04:39,  1.66it/s]Extractor Estimating: 449it [04:40,  1.46it/s]Extractor Estimating: 450it [04:41,  1.48it/s]Extractor Estimating: 451it [04:42,  1.50it/s]Extractor Estimating: 452it [04:42,  1.57it/s]Extractor Estimating: 453it [04:43,  1.62it/s]Extractor Estimating: 454it [04:43,  1.69it/s]Extractor Estimating: 455it [04:44,  1.68it/s]Extractor Estimating: 456it [04:44,  1.72it/s]Extractor Estimating: 457it [04:45,  1.71it/s]Extractor Estimating: 458it [04:46,  1.71it/s]Extractor Estimating: 459it [04:46,  1.72it/s]Extractor Estimating: 460it [04:47,  1.76it/s]Extractor Estimating: 461it [04:47,  1.76it/s]Extractor Estimating: 462it [04:48,  1.81it/s]Extractor Estimating: 463it [04:48,  1.77it/s]Extractor Estimating: 464it [04:49,  1.80it/s]Extractor Estimating: 465it [04:49,  1.77it/s]Extractor Estimating: 466it [04:50,  1.79it/s]Extractor Estimating: 467it [04:51,  1.79it/s]Extractor Estimating: 468it [04:51,  1.78it/s]Extractor Estimating: 469it [04:52,  1.78it/s]Extractor Estimating: 470it [04:52,  1.76it/s]Extractor Estimating: 471it [04:53,  1.76it/s]Extractor Estimating: 472it [04:53,  1.81it/s]Extractor Estimating: 473it [04:54,  1.79it/s]Extractor Estimating: 474it [04:55,  1.69it/s]Extractor Estimating: 475it [04:55,  1.74it/s]Extractor Estimating: 476it [04:56,  1.68it/s]Extractor Estimating: 477it [04:56,  1.64it/s]Extractor Estimating: 478it [04:57,  1.65it/s]Extractor Estimating: 479it [04:58,  1.68it/s]Extractor Estimating: 480it [04:58,  1.57it/s]Extractor Estimating: 481it [04:59,  1.60it/s]Extractor Estimating: 482it [04:59,  1.64it/s]Extractor Estimating: 483it [05:00,  1.57it/s]Extractor Estimating: 484it [05:01,  1.58it/s]Extractor Estimating: 485it [05:01,  1.59it/s]Extractor Estimating: 486it [05:02,  1.57it/s]Extractor Estimating: 487it [05:03,  1.59it/s]Extractor Estimating: 488it [05:03,  1.60it/s]Extractor Estimating: 489it [05:04,  1.57it/s]Extractor Estimating: 490it [05:05,  1.57it/s]Extractor Estimating: 491it [05:05,  1.58it/s]Extractor Estimating: 492it [05:06,  1.53it/s]Extractor Estimating: 493it [05:07,  1.46it/s]Extractor Estimating: 494it [05:07,  1.48it/s]Extractor Estimating: 495it [05:08,  1.48it/s]Extractor Estimating: 496it [05:09,  1.49it/s]Extractor Estimating: 497it [05:09,  1.57it/s]Extractor Estimating: 498it [05:10,  1.60it/s]Extractor Estimating: 499it [05:10,  1.65it/s]Extractor Estimating: 500it [05:11,  1.90it/s]Extractor Estimating: 500it [05:11,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:53:16,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:53:16,423 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:53:16,423 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:53:16,423 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:53:16,423 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:53:16,768 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:53:16,770 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:53:17,072 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:53:18,145 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:53:18,145 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:53:19,974 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:53:19,986 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:53:19,986 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:53:19,986 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:53:19,986 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:53:20,363 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:53:20,364 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:53:20,694 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:53:20,874 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:53:20,874 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:06:45,538 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:06:45,571 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9992 mean pseudo reward: 0.9412154276362839
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 16931
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17031, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17031, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.095, loss:714.1982
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.162, loss:671.1914
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.121, loss:651.7909
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.125, loss:622.5159
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.108, loss:609.7364
>> valid entity prec:0.5827, rec:0.5803, f1:0.5815
>> valid relation prec:0.4080, rec:0.1881, f1:0.2575
>> valid relation with NER prec:0.4080, rec:0.1881, f1:0.2575
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.529, loss:618.5778
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.113, loss:628.1224
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.137, loss:625.2051
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.111, loss:571.1468
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.121, loss:614.1092
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5584, rec:0.5125, f1:0.5344
>> valid relation prec:0.4317, rec:0.1577, f1:0.2310
>> valid relation with NER prec:0.4317, rec:0.1577, f1:0.2310
g_step 1100, step 266, avg_time 2.530, loss:625.7592
g_step 1200, step 366, avg_time 1.112, loss:611.4421
g_step 1300, step 49, avg_time 1.127, loss:583.8398
g_step 1400, step 149, avg_time 1.144, loss:550.0801
g_step 1500, step 249, avg_time 1.106, loss:555.8499
>> valid entity prec:0.6055, rec:0.5076, f1:0.5522
>> valid relation prec:0.4005, rec:0.1506, f1:0.2188
>> valid relation with NER prec:0.4005, rec:0.1506, f1:0.2188
g_step 1600, step 349, avg_time 2.517, loss:606.3033
g_step 1700, step 32, avg_time 1.114, loss:545.6362
g_step 1800, step 132, avg_time 1.133, loss:536.1299
g_step 1900, step 232, avg_time 1.121, loss:542.7749
g_step 2000, step 332, avg_time 1.111, loss:558.3393
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5584, rec:0.5440, f1:0.5511
>> valid relation prec:0.3708, rec:0.1666, f1:0.2299
>> valid relation with NER prec:0.3708, rec:0.1666, f1:0.2299
g_step 2100, step 15, avg_time 2.531, loss:539.6094
g_step 2200, step 115, avg_time 1.108, loss:523.1503
g_step 2300, step 215, avg_time 1.136, loss:509.9537
g_step 2400, step 315, avg_time 1.116, loss:527.0578
g_step 2500, step 415, avg_time 1.124, loss:519.6822
>> valid entity prec:0.6056, rec:0.4932, f1:0.5436
>> valid relation prec:0.3765, rec:0.1491, f1:0.2136
>> valid relation with NER prec:0.3765, rec:0.1491, f1:0.2136
g_step 2600, step 98, avg_time 2.542, loss:474.6157
g_step 2700, step 198, avg_time 1.112, loss:506.4140
g_step 2800, step 298, avg_time 1.147, loss:519.7223
g_step 2900, step 398, avg_time 1.110, loss:507.4670
g_step 3000, step 81, avg_time 1.117, loss:490.4525
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5641, rec:0.4983, f1:0.5292
>> valid relation prec:0.3356, rec:0.1554, f1:0.2125
>> valid relation with NER prec:0.3356, rec:0.1554, f1:0.2125
g_step 3100, step 181, avg_time 2.533, loss:466.9562
g_step 3200, step 281, avg_time 1.133, loss:480.9256
g_step 3300, step 381, avg_time 1.112, loss:507.6936
g_step 3400, step 64, avg_time 1.122, loss:449.6893
g_step 3500, step 164, avg_time 1.120, loss:451.8497
>> valid entity prec:0.5581, rec:0.5641, f1:0.5611
>> valid relation prec:0.3872, rec:0.1792, f1:0.2451
>> valid relation with NER prec:0.3872, rec:0.1792, f1:0.2451
g_step 3600, step 264, avg_time 2.523, loss:455.4514
g_step 3700, step 364, avg_time 1.118, loss:485.9292
g_step 3800, step 47, avg_time 1.122, loss:446.1402
g_step 3900, step 147, avg_time 1.130, loss:457.0232
g_step 4000, step 247, avg_time 1.107, loss:458.0311
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5127, rec:0.5594, f1:0.5351
>> valid relation prec:0.3609, rec:0.1566, f1:0.2184
>> valid relation with NER prec:0.3609, rec:0.1566, f1:0.2184
g_step 4100, step 347, avg_time 2.520, loss:443.6707
g_step 4200, step 30, avg_time 1.128, loss:450.8380
g_step 4300, step 130, avg_time 1.100, loss:408.4591
g_step 4400, step 230, avg_time 1.152, loss:437.1862
g_step 4500, step 330, avg_time 1.121, loss:430.6627
>> valid entity prec:0.5514, rec:0.5489, f1:0.5502
>> valid relation prec:0.3173, rec:0.1666, f1:0.2185
>> valid relation with NER prec:0.3173, rec:0.1666, f1:0.2185
g_step 4600, step 13, avg_time 2.507, loss:429.2219
g_step 4700, step 113, avg_time 1.135, loss:389.0293
g_step 4800, step 213, avg_time 1.126, loss:436.3623
g_step 4900, step 313, avg_time 1.128, loss:445.4225
g_step 5000, step 413, avg_time 1.097, loss:434.9397
learning rate was adjusted to 0.0008
>> valid entity prec:0.5335, rec:0.5341, f1:0.5338
>> valid relation prec:0.3395, rec:0.1471, f1:0.2053
>> valid relation with NER prec:0.3395, rec:0.1471, f1:0.2053
g_step 5100, step 96, avg_time 2.547, loss:388.5959
g_step 5200, step 196, avg_time 1.123, loss:395.5828
g_step 5300, step 296, avg_time 1.119, loss:415.5960
g_step 5400, step 396, avg_time 1.100, loss:419.5831
g_step 5500, step 79, avg_time 1.115, loss:366.6981
>> valid entity prec:0.5296, rec:0.5378, f1:0.5336
>> valid relation prec:0.2738, rec:0.1758, f1:0.2141
>> valid relation with NER prec:0.2738, rec:0.1758, f1:0.2141
g_step 5600, step 179, avg_time 2.532, loss:390.3304
g_step 5700, step 279, avg_time 1.131, loss:396.4716
g_step 5800, step 379, avg_time 1.109, loss:397.1031
g_step 5900, step 62, avg_time 1.121, loss:390.1073
g_step 6000, step 162, avg_time 1.127, loss:380.3674
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5315, rec:0.5649, f1:0.5477
>> valid relation prec:0.3133, rec:0.1807, f1:0.2292
>> valid relation with NER prec:0.3133, rec:0.1807, f1:0.2292
g_step 6100, step 262, avg_time 2.522, loss:381.9957
g_step 6200, step 362, avg_time 1.125, loss:379.9969
g_step 6300, step 45, avg_time 1.106, loss:377.0525
g_step 6400, step 145, avg_time 1.127, loss:356.8279
g_step 6500, step 245, avg_time 1.132, loss:368.8266
>> valid entity prec:0.5417, rec:0.5378, f1:0.5397
>> valid relation prec:0.3117, rec:0.1761, f1:0.2250
>> valid relation with NER prec:0.3117, rec:0.1761, f1:0.2250
g_step 6600, step 345, avg_time 2.515, loss:359.1991
g_step 6700, step 28, avg_time 1.117, loss:361.3653
g_step 6800, step 128, avg_time 1.142, loss:343.8769
g_step 6900, step 228, avg_time 1.106, loss:377.4121
g_step 7000, step 328, avg_time 1.106, loss:339.8964
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5471, rec:0.5578, f1:0.5524
>> valid relation prec:0.3274, rec:0.1738, f1:0.2271
>> valid relation with NER prec:0.3274, rec:0.1738, f1:0.2271
g_step 7100, step 11, avg_time 2.533, loss:372.9394
g_step 7200, step 111, avg_time 1.114, loss:350.4546
g_step 7300, step 211, avg_time 1.141, loss:344.8017
g_step 7400, step 311, avg_time 1.111, loss:351.9849
g_step 7500, step 411, avg_time 1.129, loss:354.7225
>> valid entity prec:0.5435, rec:0.5562, f1:0.5498
>> valid relation prec:0.3002, rec:0.1775, f1:0.2231
>> valid relation with NER prec:0.3002, rec:0.1775, f1:0.2231
g_step 7600, step 94, avg_time 2.517, loss:315.5860
g_step 7700, step 194, avg_time 1.122, loss:333.9560
g_step 7800, step 294, avg_time 1.113, loss:346.7600
g_step 7900, step 394, avg_time 1.132, loss:363.1163
g_step 8000, step 77, avg_time 1.143, loss:320.2804
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5228, rec:0.5765, f1:0.5483
>> valid relation prec:0.2782, rec:0.1597, f1:0.2030
>> valid relation with NER prec:0.2782, rec:0.1597, f1:0.2030
g_step 8100, step 177, avg_time 2.528, loss:347.0613
g_step 8200, step 277, avg_time 1.102, loss:328.3216
g_step 8300, step 377, avg_time 1.128, loss:336.6476
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:06:45 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:06:45 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-06-45_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:06:46 - WARNING - datasets.builder -   Using custom data configuration default-743c74ca71ae1f6d
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-743c74ca71ae1f6d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:06:47,871 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:06:47,872 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:06:47,872 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:06:47,873 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:06:47,884 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:06:47,890 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:06:47,890 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:06:47,890 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:06:47,890 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:06:47,891 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:06:47,891 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:06:48,033 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:06:51,323 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:06:51,325 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-743c74ca71ae1f6d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.27ba/s] 20%|██        | 2/10 [00:00<00:01,  4.09ba/s] 30%|███       | 3/10 [00:00<00:01,  4.42ba/s] 40%|████      | 4/10 [00:00<00:01,  4.63ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.74ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.83ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.88ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.93ba/s] 90%|█████████ | 9/10 [00:01<00:00,  4.93ba/s]100%|██████████| 10/10 [00:02<00:00,  4.95ba/s]100%|██████████| 10/10 [00:02<00:00,  4.73ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.05ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.25ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.36ba/s]100%|██████████| 4/4 [00:00<00:00,  5.40ba/s]100%|██████████| 4/4 [00:00<00:00,  4.89ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  6.84ba/s] 30%|███       | 3/10 [00:00<00:00,  9.20ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.71ba/s] 70%|███████   | 7/10 [00:00<00:00,  9.93ba/s] 90%|█████████ | 9/10 [00:00<00:00, 10.05ba/s]100%|██████████| 10/10 [00:01<00:00,  9.80ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.55ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.86ba/s]100%|██████████| 4/4 [00:00<00:00, 11.20ba/s]
[INFO|trainer.py:414] 2023-08-29 01:06:56,040 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:06:56,055 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:06:56,055 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 01:06:56,055 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:06:56,055 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:06:56,055 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:06:56,055 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:06:56,055 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:54,  3.32it/s]  0%|          | 2/780 [00:00<03:48,  3.41it/s]  0%|          | 3/780 [00:00<03:45,  3.44it/s]  1%|          | 4/780 [00:01<03:44,  3.46it/s]  1%|          | 5/780 [00:01<03:43,  3.47it/s]  1%|          | 6/780 [00:01<03:45,  3.44it/s]  1%|          | 7/780 [00:02<03:43,  3.45it/s]  1%|          | 8/780 [00:02<03:43,  3.46it/s]  1%|          | 9/780 [00:02<03:42,  3.46it/s]  1%|▏         | 10/780 [00:02<03:42,  3.47it/s]  1%|▏         | 11/780 [00:03<03:41,  3.47it/s]  2%|▏         | 12/780 [00:03<03:41,  3.47it/s]  2%|▏         | 13/780 [00:03<03:40,  3.48it/s]  2%|▏         | 14/780 [00:04<03:40,  3.48it/s]  2%|▏         | 15/780 [00:04<03:39,  3.48it/s]  2%|▏         | 16/780 [00:04<03:39,  3.48it/s]  2%|▏         | 17/780 [00:04<03:41,  3.45it/s]  2%|▏         | 18/780 [00:05<03:40,  3.46it/s]  2%|▏         | 19/780 [00:05<03:39,  3.46it/s]  3%|▎         | 20/780 [00:05<03:39,  3.46it/s]  3%|▎         | 21/780 [00:06<03:39,  3.47it/s]  3%|▎         | 22/780 [00:06<03:38,  3.47it/s]  3%|▎         | 23/780 [00:06<03:38,  3.47it/s]  3%|▎         | 24/780 [00:06<03:37,  3.47it/s]  3%|▎         | 25/780 [00:07<03:37,  3.47it/s]  3%|▎         | 26/780 [00:07<03:37,  3.47it/s]  3%|▎         | 27/780 [00:07<03:36,  3.47it/s]  4%|▎         | 28/780 [00:08<03:37,  3.46it/s]  4%|▎         | 29/780 [00:08<03:36,  3.46it/s]  4%|▍         | 30/780 [00:08<03:36,  3.47it/s]  4%|▍         | 31/780 [00:08<03:35,  3.47it/s]  4%|▍         | 32/780 [00:09<03:35,  3.47it/s]  4%|▍         | 33/780 [00:09<03:35,  3.47it/s]  4%|▍         | 34/780 [00:09<03:35,  3.47it/s]  4%|▍         | 35/780 [00:10<03:34,  3.47it/s]  5%|▍         | 36/780 [00:10<03:34,  3.47it/s]  5%|▍         | 37/780 [00:10<03:33,  3.47it/s]  5%|▍         | 38/780 [00:10<03:33,  3.47it/s]  5%|▌         | 39/780 [00:11<03:33,  3.46it/s]  5%|▌         | 40/780 [00:11<03:33,  3.47it/s]  5%|▌         | 41/780 [00:11<03:33,  3.47it/s]  5%|▌         | 42/780 [00:12<03:32,  3.47it/s]  6%|▌         | 43/780 [00:12<03:32,  3.47it/s]  6%|▌         | 44/780 [00:12<03:32,  3.47it/s]  6%|▌         | 45/780 [00:12<03:31,  3.47it/s]  6%|▌         | 46/780 [00:13<03:31,  3.47it/s]  6%|▌         | 47/780 [00:13<03:31,  3.47it/s]  6%|▌         | 48/780 [00:13<03:31,  3.47it/s]  6%|▋         | 49/780 [00:14<03:30,  3.47it/s]  6%|▋         | 50/780 [00:14<03:31,  3.45it/s]  7%|▋         | 51/780 [00:14<03:31,  3.45it/s]  7%|▋         | 52/780 [00:15<03:30,  3.45it/s]  7%|▋         | 53/780 [00:15<03:30,  3.46it/s]  7%|▋         | 54/780 [00:15<03:29,  3.46it/s]  7%|▋         | 55/780 [00:15<03:29,  3.46it/s]  7%|▋         | 56/780 [00:16<03:28,  3.46it/s]  7%|▋         | 57/780 [00:16<03:28,  3.46it/s]  7%|▋         | 58/780 [00:16<03:28,  3.47it/s]  8%|▊         | 59/780 [00:17<03:28,  3.46it/s]  8%|▊         | 60/780 [00:17<03:27,  3.46it/s]  8%|▊         | 61/780 [00:17<03:28,  3.45it/s]  8%|▊         | 62/780 [00:17<03:27,  3.46it/s]  8%|▊         | 63/780 [00:18<03:27,  3.46it/s]  8%|▊         | 64/780 [00:18<03:26,  3.46it/s]  8%|▊         | 65/780 [00:18<03:26,  3.46it/s]  8%|▊         | 66/780 [00:19<03:26,  3.46it/s]  9%|▊         | 67/780 [00:19<03:25,  3.46it/s]  9%|▊         | 68/780 [00:19<03:25,  3.46it/s]  9%|▉         | 69/780 [00:19<03:25,  3.46it/s]  9%|▉         | 70/780 [00:20<03:24,  3.46it/s]  9%|▉         | 71/780 [00:20<03:24,  3.46it/s]  9%|▉         | 72/780 [00:20<03:24,  3.45it/s]  9%|▉         | 73/780 [00:21<03:24,  3.46it/s]  9%|▉         | 74/780 [00:21<03:24,  3.46it/s] 10%|▉         | 75/780 [00:21<03:23,  3.46it/s] 10%|▉         | 76/780 [00:21<03:23,  3.46it/s] 10%|▉         | 77/780 [00:22<03:23,  3.46it/s] 10%|█         | 78/780 [00:22<03:22,  3.46it/s] 10%|█         | 79/780 [00:22<03:22,  3.46it/s] 10%|█         | 80/780 [00:23<03:22,  3.47it/s] 10%|█         | 81/780 [00:23<03:21,  3.46it/s] 11%|█         | 82/780 [00:23<03:21,  3.46it/s] 11%|█         | 83/780 [00:23<03:21,  3.46it/s] 11%|█         | 84/780 [00:24<03:20,  3.46it/s] 11%|█         | 85/780 [00:24<03:20,  3.46it/s] 11%|█         | 86/780 [00:24<03:20,  3.46it/s] 11%|█         | 87/780 [00:25<03:20,  3.46it/s] 11%|█▏        | 88/780 [00:25<03:19,  3.46it/s] 11%|█▏        | 89/780 [00:25<03:19,  3.46it/s] 12%|█▏        | 90/780 [00:25<03:19,  3.46it/s] 12%|█▏        | 91/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 92/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 93/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 94/780 [00:27<03:18,  3.46it/s] 12%|█▏        | 95/780 [00:27<03:18,  3.46it/s] 12%|█▏        | 96/780 [00:27<03:17,  3.46it/s] 12%|█▏        | 97/780 [00:28<03:17,  3.46it/s] 13%|█▎        | 98/780 [00:28<03:17,  3.46it/s] 13%|█▎        | 99/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 100/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 101/780 [00:29<03:16,  3.46it/s] 13%|█▎        | 102/780 [00:29<03:15,  3.46it/s] 13%|█▎        | 103/780 [00:29<03:15,  3.46it/s] 13%|█▎        | 104/780 [00:30<03:15,  3.46it/s] 13%|█▎        | 105/780 [00:30<03:15,  3.46it/s] 14%|█▎        | 106/780 [00:30<03:14,  3.46it/s] 14%|█▎        | 107/780 [00:30<03:14,  3.46it/s] 14%|█▍        | 108/780 [00:31<03:14,  3.46it/s] 14%|█▍        | 109/780 [00:31<03:14,  3.46it/s] 14%|█▍        | 110/780 [00:31<03:13,  3.46it/s] 14%|█▍        | 111/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 112/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 113/780 [00:32<03:12,  3.46it/s] 15%|█▍        | 114/780 [00:32<03:12,  3.46it/s] 15%|█▍        | 115/780 [00:33<03:12,  3.46it/s] 15%|█▍        | 116/780 [00:33<03:12,  3.45it/s] 15%|█▌        | 117/780 [00:33<03:11,  3.45it/s] 15%|█▌        | 118/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 119/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 120/780 [00:34<03:11,  3.45it/s] 16%|█▌        | 121/780 [00:34<03:10,  3.46it/s] 16%|█▌        | 122/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 123/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 124/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 125/780 [00:36<03:09,  3.45it/s] 16%|█▌        | 126/780 [00:36<03:10,  3.44it/s] 16%|█▋        | 127/780 [00:36<03:09,  3.44it/s] 16%|█▋        | 128/780 [00:36<03:09,  3.44it/s] 17%|█▋        | 129/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 130/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 131/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 132/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 133/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 134/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 135/780 [00:39<03:06,  3.45it/s] 17%|█▋        | 136/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 137/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 138/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 139/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 140/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 141/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 142/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 143/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 144/780 [00:41<03:04,  3.45it/s] 19%|█▊        | 145/780 [00:41<03:04,  3.45it/s] 19%|█▊        | 146/780 [00:42<03:03,  3.45it/s] 19%|█▉        | 147/780 [00:42<03:03,  3.45it/s] 19%|█▉        | 148/780 [00:42<03:03,  3.45it/s] 19%|█▉        | 149/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 150/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 151/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 152/780 [00:43<03:01,  3.45it/s] 20%|█▉        | 153/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 154/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 155/780 [00:44<03:01,  3.44it/s] 20%|██        | 156/780 [00:45<03:01,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 01:07:41,209 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:07:41,209 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 01:07:41,209 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.04it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.47it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.48it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.86it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.44it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.10it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.95it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.59it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.60it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.60it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.56it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.55it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.63it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.46it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.55it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.41it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.33it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.16it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.18it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.17it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.22it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.35it/s][A
 27%|██▋       | 118/437 [00:02<00:07, 42.85it/s][A
 28%|██▊       | 123/437 [00:02<00:07, 43.87it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 44.71it/s][A
 30%|███       | 133/437 [00:02<00:06, 45.22it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 45.72it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 45.91it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.14it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.33it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.18it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.24it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.21it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.35it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.42it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.50it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.54it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.50it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.53it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.41it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.22it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.34it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.28it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.25it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.24it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.16it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.14it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.12it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.05it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.14it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 45.08it/s][A
 60%|██████    | 263/437 [00:05<00:03, 45.55it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 45.79it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.09it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.21it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.39it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.41it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.43it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.30it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.31it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.32it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.48it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.48it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.48it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.52it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.49it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.57it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.52it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.52it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.40it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.35it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.48it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.49it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.49it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.55it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.53it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.54it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.53it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 41.86it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 43.10it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 43.90it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 44.62it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 45.06it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 45.36it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 45.64it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 45.78it/s][A                                                 
                                                 [A 20%|██        | 156/780 [00:54<03:01,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 45.78it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:07:50,755 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 01:07:50,781 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:07:53,319 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:07:53,349 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:07:53,368 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:03<59:29,  5.73s/it] 20%|██        | 158/780 [01:03<42:29,  4.10s/it] 20%|██        | 159/780 [01:04<30:35,  2.96s/it] 21%|██        | 160/780 [01:04<22:16,  2.16s/it] 21%|██        | 161/780 [01:04<16:27,  1.60s/it] 21%|██        | 162/780 [01:04<12:23,  1.20s/it] 21%|██        | 163/780 [01:05<09:33,  1.08it/s] 21%|██        | 164/780 [01:05<07:33,  1.36it/s] 21%|██        | 165/780 [01:05<06:10,  1.66it/s] 21%|██▏       | 166/780 [01:06<05:12,  1.97it/s] 21%|██▏       | 167/780 [01:06<04:31,  2.26it/s] 22%|██▏       | 168/780 [01:06<04:02,  2.52it/s] 22%|██▏       | 169/780 [01:07<03:43,  2.74it/s] 22%|██▏       | 170/780 [01:07<03:28,  2.92it/s] 22%|██▏       | 171/780 [01:07<03:18,  3.07it/s] 22%|██▏       | 172/780 [01:07<03:11,  3.18it/s] 22%|██▏       | 173/780 [01:08<03:06,  3.26it/s] 22%|██▏       | 174/780 [01:08<03:02,  3.31it/s] 22%|██▏       | 175/780 [01:08<03:00,  3.36it/s] 23%|██▎       | 176/780 [01:09<02:58,  3.39it/s] 23%|██▎       | 177/780 [01:09<02:56,  3.41it/s] 23%|██▎       | 178/780 [01:09<02:55,  3.43it/s] 23%|██▎       | 179/780 [01:09<02:54,  3.44it/s] 23%|██▎       | 180/780 [01:10<02:54,  3.45it/s] 23%|██▎       | 181/780 [01:10<02:53,  3.45it/s] 23%|██▎       | 182/780 [01:10<02:53,  3.45it/s] 23%|██▎       | 183/780 [01:11<02:52,  3.46it/s] 24%|██▎       | 184/780 [01:11<02:52,  3.46it/s] 24%|██▎       | 185/780 [01:11<02:52,  3.46it/s] 24%|██▍       | 186/780 [01:11<02:51,  3.46it/s] 24%|██▍       | 187/780 [01:12<02:52,  3.44it/s] 24%|██▍       | 188/780 [01:12<02:51,  3.45it/s] 24%|██▍       | 189/780 [01:12<02:51,  3.45it/s] 24%|██▍       | 190/780 [01:13<02:50,  3.45it/s] 24%|██▍       | 191/780 [01:13<02:50,  3.46it/s] 25%|██▍       | 192/780 [01:13<02:49,  3.46it/s] 25%|██▍       | 193/780 [01:13<02:49,  3.46it/s] 25%|██▍       | 194/780 [01:14<02:49,  3.46it/s] 25%|██▌       | 195/780 [01:14<02:49,  3.46it/s] 25%|██▌       | 196/780 [01:14<02:49,  3.45it/s] 25%|██▌       | 197/780 [01:15<02:48,  3.45it/s] 25%|██▌       | 198/780 [01:15<02:49,  3.44it/s] 26%|██▌       | 199/780 [01:15<02:48,  3.44it/s] 26%|██▌       | 200/780 [01:15<02:48,  3.45it/s] 26%|██▌       | 201/780 [01:16<02:47,  3.45it/s] 26%|██▌       | 202/780 [01:16<02:47,  3.46it/s] 26%|██▌       | 203/780 [01:16<02:47,  3.45it/s] 26%|██▌       | 204/780 [01:17<02:46,  3.46it/s] 26%|██▋       | 205/780 [01:17<02:46,  3.45it/s] 26%|██▋       | 206/780 [01:17<02:46,  3.46it/s] 27%|██▋       | 207/780 [01:17<02:45,  3.46it/s] 27%|██▋       | 208/780 [01:18<02:45,  3.46it/s] 27%|██▋       | 209/780 [01:18<02:45,  3.44it/s] 27%|██▋       | 210/780 [01:18<02:45,  3.44it/s] 27%|██▋       | 211/780 [01:19<02:45,  3.45it/s] 27%|██▋       | 212/780 [01:19<02:44,  3.45it/s] 27%|██▋       | 213/780 [01:19<02:44,  3.45it/s] 27%|██▋       | 214/780 [01:20<02:43,  3.46it/s] 28%|██▊       | 215/780 [01:20<02:43,  3.45it/s] 28%|██▊       | 216/780 [01:20<02:43,  3.45it/s] 28%|██▊       | 217/780 [01:20<02:43,  3.45it/s] 28%|██▊       | 218/780 [01:21<02:42,  3.45it/s] 28%|██▊       | 219/780 [01:21<02:42,  3.45it/s] 28%|██▊       | 220/780 [01:21<02:42,  3.44it/s] 28%|██▊       | 221/780 [01:22<02:42,  3.44it/s] 28%|██▊       | 222/780 [01:22<02:42,  3.44it/s] 29%|██▊       | 223/780 [01:22<02:41,  3.44it/s] 29%|██▊       | 224/780 [01:22<02:41,  3.45it/s] 29%|██▉       | 225/780 [01:23<02:40,  3.45it/s] 29%|██▉       | 226/780 [01:23<02:40,  3.45it/s] 29%|██▉       | 227/780 [01:23<02:40,  3.45it/s] 29%|██▉       | 228/780 [01:24<02:39,  3.45it/s] 29%|██▉       | 229/780 [01:24<02:39,  3.45it/s] 29%|██▉       | 230/780 [01:24<02:39,  3.45it/s] 30%|██▉       | 231/780 [01:24<02:39,  3.43it/s] 30%|██▉       | 232/780 [01:25<02:39,  3.44it/s] 30%|██▉       | 233/780 [01:25<02:38,  3.44it/s] 30%|███       | 234/780 [01:25<02:38,  3.44it/s] 30%|███       | 235/780 [01:26<02:38,  3.45it/s] 30%|███       | 236/780 [01:26<02:37,  3.45it/s] 30%|███       | 237/780 [01:26<02:37,  3.45it/s] 31%|███       | 238/780 [01:26<02:37,  3.45it/s] 31%|███       | 239/780 [01:27<02:36,  3.45it/s] 31%|███       | 240/780 [01:27<02:36,  3.45it/s] 31%|███       | 241/780 [01:27<02:36,  3.45it/s] 31%|███       | 242/780 [01:28<02:36,  3.44it/s] 31%|███       | 243/780 [01:28<02:36,  3.44it/s] 31%|███▏      | 244/780 [01:28<02:35,  3.44it/s] 31%|███▏      | 245/780 [01:29<02:35,  3.45it/s] 32%|███▏      | 246/780 [01:29<02:34,  3.45it/s] 32%|███▏      | 247/780 [01:29<02:34,  3.45it/s] 32%|███▏      | 248/780 [01:29<02:34,  3.45it/s] 32%|███▏      | 249/780 [01:30<02:34,  3.45it/s] 32%|███▏      | 250/780 [01:30<02:33,  3.45it/s] 32%|███▏      | 251/780 [01:30<02:33,  3.45it/s] 32%|███▏      | 252/780 [01:31<02:33,  3.45it/s] 32%|███▏      | 253/780 [01:31<02:33,  3.44it/s] 33%|███▎      | 254/780 [01:31<02:32,  3.44it/s] 33%|███▎      | 255/780 [01:31<02:32,  3.45it/s] 33%|███▎      | 256/780 [01:32<02:32,  3.44it/s] 33%|███▎      | 257/780 [01:32<02:31,  3.45it/s] 33%|███▎      | 258/780 [01:32<02:31,  3.45it/s] 33%|███▎      | 259/780 [01:33<02:31,  3.45it/s] 33%|███▎      | 260/780 [01:33<02:30,  3.45it/s] 33%|███▎      | 261/780 [01:33<02:30,  3.45it/s] 34%|███▎      | 262/780 [01:33<02:30,  3.45it/s] 34%|███▎      | 263/780 [01:34<02:29,  3.45it/s] 34%|███▍      | 264/780 [01:34<02:36,  3.30it/s] 34%|███▍      | 265/780 [01:34<02:34,  3.33it/s] 34%|███▍      | 266/780 [01:35<02:32,  3.37it/s] 34%|███▍      | 267/780 [01:35<02:31,  3.39it/s] 34%|███▍      | 268/780 [01:35<02:30,  3.41it/s] 34%|███▍      | 269/780 [01:36<02:29,  3.42it/s] 35%|███▍      | 270/780 [01:36<02:28,  3.43it/s] 35%|███▍      | 271/780 [01:36<02:28,  3.43it/s] 35%|███▍      | 272/780 [01:36<02:27,  3.44it/s] 35%|███▌      | 273/780 [01:37<02:27,  3.44it/s] 35%|███▌      | 274/780 [01:37<02:26,  3.44it/s] 35%|███▌      | 275/780 [01:37<02:26,  3.44it/s] 35%|███▌      | 276/780 [01:38<02:26,  3.44it/s] 36%|███▌      | 277/780 [01:38<02:26,  3.44it/s] 36%|███▌      | 278/780 [01:38<02:25,  3.45it/s] 36%|███▌      | 279/780 [01:38<02:25,  3.45it/s] 36%|███▌      | 280/780 [01:39<02:25,  3.45it/s] 36%|███▌      | 281/780 [01:39<02:24,  3.45it/s] 36%|███▌      | 282/780 [01:39<02:24,  3.45it/s] 36%|███▋      | 283/780 [01:40<02:24,  3.45it/s] 36%|███▋      | 284/780 [01:40<02:23,  3.45it/s] 37%|███▋      | 285/780 [01:40<02:23,  3.45it/s] 37%|███▋      | 286/780 [01:40<02:23,  3.45it/s] 37%|███▋      | 287/780 [01:41<02:22,  3.45it/s] 37%|███▋      | 288/780 [01:41<02:22,  3.45it/s] 37%|███▋      | 289/780 [01:41<02:22,  3.45it/s] 37%|███▋      | 290/780 [01:42<02:22,  3.45it/s] 37%|███▋      | 291/780 [01:42<02:21,  3.45it/s] 37%|███▋      | 292/780 [01:42<02:21,  3.44it/s] 38%|███▊      | 293/780 [01:42<02:21,  3.43it/s] 38%|███▊      | 294/780 [01:43<02:21,  3.44it/s] 38%|███▊      | 295/780 [01:43<02:20,  3.44it/s] 38%|███▊      | 296/780 [01:43<02:20,  3.44it/s] 38%|███▊      | 297/780 [01:44<02:20,  3.44it/s] 38%|███▊      | 298/780 [01:44<02:19,  3.44it/s] 38%|███▊      | 299/780 [01:44<02:19,  3.45it/s] 38%|███▊      | 300/780 [01:45<02:19,  3.45it/s] 39%|███▊      | 301/780 [01:45<02:18,  3.45it/s] 39%|███▊      | 302/780 [01:45<02:18,  3.45it/s] 39%|███▉      | 303/780 [01:45<02:18,  3.45it/s] 39%|███▉      | 304/780 [01:46<02:18,  3.45it/s] 39%|███▉      | 305/780 [01:46<02:17,  3.45it/s] 39%|███▉      | 306/780 [01:46<02:17,  3.45it/s] 39%|███▉      | 307/780 [01:47<02:17,  3.45it/s] 39%|███▉      | 308/780 [01:47<02:16,  3.45it/s] 40%|███▉      | 309/780 [01:47<02:16,  3.45it/s] 40%|███▉      | 310/780 [01:47<02:16,  3.45it/s] 40%|███▉      | 311/780 [01:48<02:17,  3.42it/s] 40%|████      | 312/780 [01:48<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 01:08:44,617 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:08:44,618 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 01:08:44,618 >>   Batch size = 8
{'eval_loss': 0.9912354350090027, 'eval_runtime': 9.5136, 'eval_samples_per_second': 366.74, 'eval_steps_per_second': 45.934, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.31it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.42it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.51it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.84it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.30it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.97it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.74it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.36it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.42it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.49it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.56it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.44it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.48it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.45it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.48it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.47it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.25it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.21it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.21it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.31it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.46it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.49it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.41it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.45it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.37it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.32it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.44it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.34it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.34it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.27it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.43it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.40it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.44it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.47it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.37it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.29it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.30it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.28it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.43it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.44it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.43it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.43it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.39it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.34it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.42it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.41it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.34it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.38it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.39it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.36it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.43it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.44it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.39it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.38it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.43it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.27it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.37it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.42it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.34it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.36it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.45it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.36it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.43it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.34it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.27it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.25it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.42it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.33it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.40it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.38it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.30it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.38it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.39it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.20it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.27it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.26it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.34it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.41it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.42it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.37it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.41it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.28it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.28it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.33it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.19it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.34it/s][A                                                 
                                                 [A 40%|████      | 312/780 [01:58<02:16,  3.43it/s]
100%|██████████| 437/437 [00:09<00:00, 46.34it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:08:54,084 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 01:08:54,108 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:08:56,326 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:08:56,345 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:08:56,355 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:05<40:29,  5.20s/it] 40%|████      | 314/780 [02:05<28:58,  3.73s/it] 40%|████      | 315/780 [02:05<20:54,  2.70s/it] 41%|████      | 316/780 [02:06<15:16,  1.98s/it] 41%|████      | 317/780 [02:06<11:20,  1.47s/it] 41%|████      | 318/780 [02:06<08:35,  1.12s/it] 41%|████      | 319/780 [02:06<06:39,  1.15it/s] 41%|████      | 320/780 [02:07<05:19,  1.44it/s] 41%|████      | 321/780 [02:07<04:22,  1.75it/s] 41%|████▏     | 322/780 [02:07<03:43,  2.05it/s] 41%|████▏     | 323/780 [02:08<03:16,  2.33it/s] 42%|████▏     | 324/780 [02:08<02:56,  2.58it/s] 42%|████▏     | 325/780 [02:08<02:43,  2.79it/s] 42%|████▏     | 326/780 [02:08<02:33,  2.96it/s] 42%|████▏     | 327/780 [02:09<02:26,  3.09it/s] 42%|████▏     | 328/780 [02:09<02:21,  3.19it/s] 42%|████▏     | 329/780 [02:09<02:18,  3.27it/s] 42%|████▏     | 330/780 [02:10<02:15,  3.32it/s] 42%|████▏     | 331/780 [02:10<02:13,  3.36it/s] 43%|████▎     | 332/780 [02:10<02:12,  3.39it/s] 43%|████▎     | 333/780 [02:10<02:11,  3.41it/s] 43%|████▎     | 334/780 [02:11<02:10,  3.42it/s] 43%|████▎     | 335/780 [02:11<02:09,  3.43it/s] 43%|████▎     | 336/780 [02:11<02:09,  3.43it/s] 43%|████▎     | 337/780 [02:12<02:08,  3.44it/s] 43%|████▎     | 338/780 [02:12<02:08,  3.44it/s] 43%|████▎     | 339/780 [02:12<02:08,  3.44it/s] 44%|████▎     | 340/780 [02:13<02:07,  3.45it/s] 44%|████▎     | 341/780 [02:13<02:07,  3.44it/s] 44%|████▍     | 342/780 [02:13<02:07,  3.45it/s] 44%|████▍     | 343/780 [02:13<02:06,  3.45it/s] 44%|████▍     | 344/780 [02:14<02:06,  3.45it/s] 44%|████▍     | 345/780 [02:14<02:06,  3.45it/s] 44%|████▍     | 346/780 [02:14<02:05,  3.45it/s] 44%|████▍     | 347/780 [02:15<02:06,  3.42it/s] 45%|████▍     | 348/780 [02:15<02:06,  3.43it/s] 45%|████▍     | 349/780 [02:15<02:05,  3.44it/s] 45%|████▍     | 350/780 [02:15<02:04,  3.44it/s] 45%|████▌     | 351/780 [02:16<02:04,  3.44it/s] 45%|████▌     | 352/780 [02:16<02:04,  3.44it/s] 45%|████▌     | 353/780 [02:16<02:03,  3.45it/s] 45%|████▌     | 354/780 [02:17<02:03,  3.45it/s] 46%|████▌     | 355/780 [02:17<02:03,  3.45it/s] 46%|████▌     | 356/780 [02:17<02:02,  3.45it/s] 46%|████▌     | 357/780 [02:17<02:02,  3.45it/s] 46%|████▌     | 358/780 [02:18<02:02,  3.44it/s] 46%|████▌     | 359/780 [02:18<02:02,  3.44it/s] 46%|████▌     | 360/780 [02:18<02:02,  3.44it/s] 46%|████▋     | 361/780 [02:19<02:01,  3.45it/s] 46%|████▋     | 362/780 [02:19<02:01,  3.45it/s] 47%|████▋     | 363/780 [02:19<02:00,  3.45it/s] 47%|████▋     | 364/780 [02:19<02:00,  3.45it/s] 47%|████▋     | 365/780 [02:20<02:00,  3.45it/s] 47%|████▋     | 366/780 [02:20<02:00,  3.45it/s] 47%|████▋     | 367/780 [02:20<01:59,  3.45it/s] 47%|████▋     | 368/780 [02:21<01:59,  3.45it/s] 47%|████▋     | 369/780 [02:21<02:00,  3.41it/s] 47%|████▋     | 370/780 [02:21<01:59,  3.42it/s] 48%|████▊     | 371/780 [02:22<01:59,  3.43it/s] 48%|████▊     | 372/780 [02:22<01:58,  3.43it/s] 48%|████▊     | 373/780 [02:22<01:58,  3.43it/s] 48%|████▊     | 374/780 [02:22<01:58,  3.44it/s] 48%|████▊     | 375/780 [02:23<01:57,  3.44it/s] 48%|████▊     | 376/780 [02:23<01:57,  3.44it/s] 48%|████▊     | 377/780 [02:23<01:57,  3.44it/s] 48%|████▊     | 378/780 [02:24<01:56,  3.44it/s] 49%|████▊     | 379/780 [02:24<01:56,  3.45it/s] 49%|████▊     | 380/780 [02:24<01:56,  3.44it/s] 49%|████▉     | 381/780 [02:24<01:56,  3.44it/s] 49%|████▉     | 382/780 [02:25<01:55,  3.44it/s] 49%|████▉     | 383/780 [02:25<01:55,  3.44it/s] 49%|████▉     | 384/780 [02:25<01:54,  3.45it/s] 49%|████▉     | 385/780 [02:26<01:54,  3.45it/s] 49%|████▉     | 386/780 [02:26<01:54,  3.45it/s] 50%|████▉     | 387/780 [02:26<01:54,  3.45it/s] 50%|████▉     | 388/780 [02:26<01:53,  3.45it/s] 50%|████▉     | 389/780 [02:27<01:53,  3.45it/s] 50%|█████     | 390/780 [02:27<01:53,  3.45it/s] 50%|█████     | 391/780 [02:27<01:52,  3.45it/s] 50%|█████     | 392/780 [02:28<01:52,  3.45it/s] 50%|█████     | 393/780 [02:28<01:52,  3.45it/s] 51%|█████     | 394/780 [02:28<01:52,  3.45it/s] 51%|█████     | 395/780 [02:28<01:52,  3.42it/s] 51%|█████     | 396/780 [02:29<01:51,  3.43it/s] 51%|█████     | 397/780 [02:29<01:51,  3.43it/s] 51%|█████     | 398/780 [02:29<01:51,  3.44it/s] 51%|█████     | 399/780 [02:30<01:50,  3.44it/s] 51%|█████▏    | 400/780 [02:30<01:50,  3.44it/s] 51%|█████▏    | 401/780 [02:30<01:50,  3.44it/s] 52%|█████▏    | 402/780 [02:31<01:49,  3.44it/s] 52%|█████▏    | 403/780 [02:31<01:49,  3.44it/s] 52%|█████▏    | 404/780 [02:31<01:49,  3.45it/s] 52%|█████▏    | 405/780 [02:31<01:48,  3.45it/s] 52%|█████▏    | 406/780 [02:32<01:48,  3.43it/s] 52%|█████▏    | 407/780 [02:32<01:48,  3.44it/s] 52%|█████▏    | 408/780 [02:32<01:48,  3.44it/s] 52%|█████▏    | 409/780 [02:33<01:47,  3.44it/s] 53%|█████▎    | 410/780 [02:33<01:47,  3.44it/s] 53%|█████▎    | 411/780 [02:33<01:47,  3.45it/s] 53%|█████▎    | 412/780 [02:33<01:46,  3.44it/s] 53%|█████▎    | 413/780 [02:34<01:46,  3.45it/s] 53%|█████▎    | 414/780 [02:34<01:46,  3.45it/s] 53%|█████▎    | 415/780 [02:34<01:49,  3.34it/s] 53%|█████▎    | 416/780 [02:35<01:48,  3.36it/s] 53%|█████▎    | 417/780 [02:35<01:49,  3.32it/s] 54%|█████▎    | 418/780 [02:35<01:47,  3.36it/s] 54%|█████▎    | 419/780 [02:36<01:46,  3.38it/s] 54%|█████▍    | 420/780 [02:36<01:45,  3.40it/s] 54%|█████▍    | 421/780 [02:36<01:45,  3.41it/s] 54%|█████▍    | 422/780 [02:36<01:44,  3.42it/s] 54%|█████▍    | 423/780 [02:37<01:44,  3.43it/s] 54%|█████▍    | 424/780 [02:37<01:43,  3.43it/s] 54%|█████▍    | 425/780 [02:37<01:43,  3.44it/s] 55%|█████▍    | 426/780 [02:38<01:42,  3.44it/s] 55%|█████▍    | 427/780 [02:38<01:42,  3.44it/s] 55%|█████▍    | 428/780 [02:38<01:42,  3.44it/s] 55%|█████▌    | 429/780 [02:38<01:42,  3.44it/s] 55%|█████▌    | 430/780 [02:39<01:41,  3.43it/s] 55%|█████▌    | 431/780 [02:39<01:41,  3.44it/s] 55%|█████▌    | 432/780 [02:39<01:41,  3.44it/s] 56%|█████▌    | 433/780 [02:40<01:40,  3.44it/s] 56%|█████▌    | 434/780 [02:40<01:40,  3.44it/s] 56%|█████▌    | 435/780 [02:40<01:40,  3.45it/s] 56%|█████▌    | 436/780 [02:40<01:39,  3.45it/s] 56%|█████▌    | 437/780 [02:41<01:39,  3.45it/s] 56%|█████▌    | 438/780 [02:41<01:39,  3.45it/s] 56%|█████▋    | 439/780 [02:41<01:38,  3.45it/s] 56%|█████▋    | 440/780 [02:42<01:38,  3.45it/s] 57%|█████▋    | 441/780 [02:42<01:38,  3.45it/s] 57%|█████▋    | 442/780 [02:42<01:38,  3.45it/s] 57%|█████▋    | 443/780 [02:42<01:37,  3.45it/s] 57%|█████▋    | 444/780 [02:43<01:37,  3.45it/s] 57%|█████▋    | 445/780 [02:43<01:37,  3.45it/s] 57%|█████▋    | 446/780 [02:43<01:36,  3.45it/s] 57%|█████▋    | 447/780 [02:44<01:36,  3.45it/s] 57%|█████▋    | 448/780 [02:44<01:36,  3.44it/s] 58%|█████▊    | 449/780 [02:44<01:36,  3.44it/s] 58%|█████▊    | 450/780 [02:45<01:35,  3.44it/s] 58%|█████▊    | 451/780 [02:45<01:35,  3.45it/s] 58%|█████▊    | 452/780 [02:45<01:35,  3.45it/s] 58%|█████▊    | 453/780 [02:45<01:34,  3.45it/s] 58%|█████▊    | 454/780 [02:46<01:34,  3.45it/s] 58%|█████▊    | 455/780 [02:46<01:34,  3.45it/s] 58%|█████▊    | 456/780 [02:46<01:33,  3.45it/s] 59%|█████▊    | 457/780 [02:47<01:33,  3.45it/s] 59%|█████▊    | 458/780 [02:47<01:33,  3.45it/s] 59%|█████▉    | 459/780 [02:47<01:33,  3.45it/s] 59%|█████▉    | 460/780 [02:47<01:32,  3.45it/s] 59%|█████▉    | 461/780 [02:48<01:32,  3.45it/s] 59%|█████▉    | 462/780 [02:48<01:32,  3.45it/s] 59%|█████▉    | 463/780 [02:48<01:31,  3.45it/s] 59%|█████▉    | 464/780 [02:49<01:31,  3.45it/s] 60%|█████▉    | 465/780 [02:49<01:31,  3.45it/s] 60%|█████▉    | 466/780 [02:49<01:31,  3.43it/s] 60%|█████▉    | 467/780 [02:49<01:31,  3.43it/s] 60%|██████    | 468/780 [02:50<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 01:09:46,339 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:09:46,339 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 01:09:46,339 >>   Batch size = 8
{'eval_loss': 1.0015147924423218, 'eval_runtime': 9.441, 'eval_samples_per_second': 369.56, 'eval_steps_per_second': 46.288, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.07it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.13it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.38it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.81it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.23it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.97it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.68it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.40it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.33it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.29it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.37it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.37it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.38it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.38it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.43it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.41it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.26it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.23it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.22it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.26it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.38it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.37it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.46it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.38it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.29it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.27it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.30it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.34it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.31it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.28it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.21it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.43it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.47it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.48it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.30it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.21it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.33it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.39it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.36it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.31it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.37it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.30it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.40it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.32it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.31it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.30it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.39it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.28it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.31it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.33it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.41it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.36it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.38it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.25it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.21it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.35it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.40it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.34it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.30it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.31it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.30it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.35it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.37it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.23it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.28it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.42it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.40it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.22it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.35it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.30it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.35it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.40it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.24it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.29it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.28it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.31it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.37it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.19it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.15it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.29it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.34it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.32it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.37it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.28it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.33it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.35it/s][A                                                 
                                                 [A 60%|██████    | 468/780 [02:59<01:30,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 46.35it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:09:55,806 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 01:09:55,826 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:09:58,142 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:09:58,153 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:09:58,162 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:07<27:20,  5.27s/it] 60%|██████    | 470/780 [03:07<19:31,  3.78s/it] 60%|██████    | 471/780 [03:07<14:04,  2.73s/it] 61%|██████    | 472/780 [03:08<10:15,  2.00s/it] 61%|██████    | 473/780 [03:08<07:36,  1.49s/it] 61%|██████    | 474/780 [03:08<05:45,  1.13s/it] 61%|██████    | 475/780 [03:08<04:27,  1.14it/s] 61%|██████    | 476/780 [03:09<03:32,  1.43it/s] 61%|██████    | 477/780 [03:09<02:54,  1.73it/s] 61%|██████▏   | 478/780 [03:09<02:28,  2.04it/s] 61%|██████▏   | 479/780 [03:10<02:09,  2.32it/s] 62%|██████▏   | 480/780 [03:10<01:56,  2.58it/s] 62%|██████▏   | 481/780 [03:10<01:47,  2.78it/s] 62%|██████▏   | 482/780 [03:10<01:40,  2.96it/s] 62%|██████▏   | 483/780 [03:11<01:36,  3.09it/s] 62%|██████▏   | 484/780 [03:11<01:32,  3.19it/s] 62%|██████▏   | 485/780 [03:11<01:30,  3.27it/s] 62%|██████▏   | 486/780 [03:12<01:28,  3.32it/s] 62%|██████▏   | 487/780 [03:12<01:27,  3.36it/s] 63%|██████▎   | 488/780 [03:12<01:26,  3.39it/s] 63%|██████▎   | 489/780 [03:12<01:25,  3.41it/s] 63%|██████▎   | 490/780 [03:13<01:24,  3.43it/s] 63%|██████▎   | 491/780 [03:13<01:24,  3.43it/s] 63%|██████▎   | 492/780 [03:13<01:23,  3.43it/s] 63%|██████▎   | 493/780 [03:14<01:23,  3.44it/s] 63%|██████▎   | 494/780 [03:14<01:23,  3.45it/s] 63%|██████▎   | 495/780 [03:14<01:22,  3.45it/s] 64%|██████▎   | 496/780 [03:14<01:22,  3.44it/s] 64%|██████▎   | 497/780 [03:15<01:22,  3.44it/s] 64%|██████▍   | 498/780 [03:15<01:21,  3.44it/s] 64%|██████▍   | 499/780 [03:15<01:21,  3.44it/s] 64%|██████▍   | 500/780 [03:16<01:21,  3.45it/s]                                                  64%|██████▍   | 500/780 [03:16<01:21,  3.45it/s] 64%|██████▍   | 501/780 [03:16<01:20,  3.45it/s] 64%|██████▍   | 502/780 [03:16<01:20,  3.45it/s] 64%|██████▍   | 503/780 [03:16<01:20,  3.44it/s] 65%|██████▍   | 504/780 [03:17<01:20,  3.45it/s] 65%|██████▍   | 505/780 [03:17<01:19,  3.44it/s] 65%|██████▍   | 506/780 [03:17<01:19,  3.45it/s] 65%|██████▌   | 507/780 [03:18<01:19,  3.44it/s] 65%|██████▌   | 508/780 [03:18<01:18,  3.45it/s] 65%|██████▌   | 509/780 [03:18<01:18,  3.45it/s] 65%|██████▌   | 510/780 [03:19<01:18,  3.45it/s] 66%|██████▌   | 511/780 [03:19<01:18,  3.45it/s] 66%|██████▌   | 512/780 [03:19<01:17,  3.45it/s] 66%|██████▌   | 513/780 [03:19<01:17,  3.45it/s] 66%|██████▌   | 514/780 [03:20<01:17,  3.44it/s] 66%|██████▌   | 515/780 [03:20<01:16,  3.44it/s] 66%|██████▌   | 516/780 [03:20<01:16,  3.45it/s] 66%|██████▋   | 517/780 [03:21<01:16,  3.45it/s] 66%|██████▋   | 518/780 [03:21<01:16,  3.45it/s] 67%|██████▋   | 519/780 [03:21<01:15,  3.45it/s] 67%|██████▋   | 520/780 [03:21<01:15,  3.45it/s] 67%|██████▋   | 521/780 [03:22<01:15,  3.45it/s] 67%|██████▋   | 522/780 [03:22<01:14,  3.45it/s] 67%|██████▋   | 523/780 [03:22<01:14,  3.45it/s] 67%|██████▋   | 524/780 [03:23<01:14,  3.45it/s] 67%|██████▋   | 525/780 [03:23<01:14,  3.44it/s] 67%|██████▋   | 526/780 [03:23<01:13,  3.44it/s] 68%|██████▊   | 527/780 [03:23<01:13,  3.44it/s] 68%|██████▊   | 528/780 [03:24<01:13,  3.45it/s] 68%|██████▊   | 529/780 [03:24<01:12,  3.45it/s] 68%|██████▊   | 530/780 [03:24<01:12,  3.45it/s] 68%|██████▊   | 531/780 [03:25<01:12,  3.45it/s] 68%|██████▊   | 532/780 [03:25<01:11,  3.45it/s] 68%|██████▊   | 533/780 [03:25<01:11,  3.45it/s] 68%|██████▊   | 534/780 [03:25<01:11,  3.44it/s] 69%|██████▊   | 535/780 [03:26<01:11,  3.45it/s] 69%|██████▊   | 536/780 [03:26<01:11,  3.43it/s] 69%|██████▉   | 537/780 [03:26<01:10,  3.44it/s] 69%|██████▉   | 538/780 [03:27<01:10,  3.44it/s] 69%|██████▉   | 539/780 [03:27<01:09,  3.45it/s] 69%|██████▉   | 540/780 [03:27<01:09,  3.45it/s] 69%|██████▉   | 541/780 [03:28<01:09,  3.45it/s] 69%|██████▉   | 542/780 [03:28<01:09,  3.45it/s] 70%|██████▉   | 543/780 [03:28<01:08,  3.45it/s] 70%|██████▉   | 544/780 [03:28<01:08,  3.45it/s] 70%|██████▉   | 545/780 [03:29<01:08,  3.45it/s] 70%|███████   | 546/780 [03:29<01:07,  3.45it/s] 70%|███████   | 547/780 [03:29<01:07,  3.45it/s] 70%|███████   | 548/780 [03:30<01:07,  3.45it/s] 70%|███████   | 549/780 [03:30<01:06,  3.45it/s] 71%|███████   | 550/780 [03:30<01:06,  3.44it/s] 71%|███████   | 551/780 [03:30<01:06,  3.44it/s] 71%|███████   | 552/780 [03:31<01:06,  3.44it/s] 71%|███████   | 553/780 [03:31<01:05,  3.45it/s] 71%|███████   | 554/780 [03:31<01:05,  3.45it/s] 71%|███████   | 555/780 [03:32<01:05,  3.45it/s] 71%|███████▏  | 556/780 [03:32<01:04,  3.45it/s] 71%|███████▏  | 557/780 [03:32<01:04,  3.45it/s] 72%|███████▏  | 558/780 [03:32<01:04,  3.45it/s] 72%|███████▏  | 559/780 [03:33<01:04,  3.45it/s] 72%|███████▏  | 560/780 [03:33<01:03,  3.45it/s] 72%|███████▏  | 561/780 [03:33<01:03,  3.44it/s] 72%|███████▏  | 562/780 [03:34<01:03,  3.44it/s] 72%|███████▏  | 563/780 [03:34<01:02,  3.44it/s] 72%|███████▏  | 564/780 [03:34<01:02,  3.45it/s] 72%|███████▏  | 565/780 [03:35<01:03,  3.36it/s] 73%|███████▎  | 566/780 [03:35<01:03,  3.39it/s] 73%|███████▎  | 567/780 [03:35<01:02,  3.40it/s] 73%|███████▎  | 568/780 [03:35<01:02,  3.42it/s] 73%|███████▎  | 569/780 [03:36<01:01,  3.42it/s] 73%|███████▎  | 570/780 [03:36<01:01,  3.43it/s] 73%|███████▎  | 571/780 [03:36<01:00,  3.44it/s] 73%|███████▎  | 572/780 [03:37<01:00,  3.43it/s] 73%|███████▎  | 573/780 [03:37<01:00,  3.43it/s] 74%|███████▎  | 574/780 [03:37<00:59,  3.44it/s] 74%|███████▎  | 575/780 [03:37<00:59,  3.44it/s] 74%|███████▍  | 576/780 [03:38<00:59,  3.44it/s] 74%|███████▍  | 577/780 [03:38<00:58,  3.45it/s] 74%|███████▍  | 578/780 [03:38<00:58,  3.45it/s] 74%|███████▍  | 579/780 [03:39<00:58,  3.45it/s] 74%|███████▍  | 580/780 [03:39<00:58,  3.45it/s] 74%|███████▍  | 581/780 [03:39<00:57,  3.44it/s] 75%|███████▍  | 582/780 [03:39<00:57,  3.44it/s] 75%|███████▍  | 583/780 [03:40<00:57,  3.45it/s] 75%|███████▍  | 584/780 [03:40<00:56,  3.45it/s] 75%|███████▌  | 585/780 [03:40<00:56,  3.44it/s] 75%|███████▌  | 586/780 [03:41<00:56,  3.44it/s] 75%|███████▌  | 587/780 [03:41<00:55,  3.45it/s] 75%|███████▌  | 588/780 [03:41<00:55,  3.45it/s] 76%|███████▌  | 589/780 [03:41<00:55,  3.45it/s] 76%|███████▌  | 590/780 [03:42<00:55,  3.45it/s] 76%|███████▌  | 591/780 [03:42<00:54,  3.45it/s] 76%|███████▌  | 592/780 [03:42<00:54,  3.44it/s] 76%|███████▌  | 593/780 [03:43<00:54,  3.44it/s] 76%|███████▌  | 594/780 [03:43<00:54,  3.44it/s] 76%|███████▋  | 595/780 [03:43<00:53,  3.45it/s] 76%|███████▋  | 596/780 [03:44<00:53,  3.44it/s] 77%|███████▋  | 597/780 [03:44<00:53,  3.44it/s] 77%|███████▋  | 598/780 [03:44<00:52,  3.44it/s] 77%|███████▋  | 599/780 [03:44<00:52,  3.44it/s] 77%|███████▋  | 600/780 [03:45<00:52,  3.44it/s] 77%|███████▋  | 601/780 [03:45<00:51,  3.44it/s] 77%|███████▋  | 602/780 [03:45<00:51,  3.43it/s] 77%|███████▋  | 603/780 [03:46<00:51,  3.44it/s] 77%|███████▋  | 604/780 [03:46<00:51,  3.44it/s] 78%|███████▊  | 605/780 [03:46<00:50,  3.44it/s] 78%|███████▊  | 606/780 [03:46<00:50,  3.44it/s] 78%|███████▊  | 607/780 [03:47<00:50,  3.44it/s] 78%|███████▊  | 608/780 [03:47<00:49,  3.44it/s] 78%|███████▊  | 609/780 [03:47<00:49,  3.44it/s] 78%|███████▊  | 610/780 [03:48<00:49,  3.45it/s] 78%|███████▊  | 611/780 [03:48<00:49,  3.44it/s] 78%|███████▊  | 612/780 [03:48<00:48,  3.45it/s] 79%|███████▊  | 613/780 [03:48<00:48,  3.45it/s] 79%|███████▊  | 614/780 [03:49<00:48,  3.45it/s] 79%|███████▉  | 615/780 [03:49<00:47,  3.45it/s] 79%|███████▉  | 616/780 [03:49<00:47,  3.44it/s] 79%|███████▉  | 617/780 [03:50<00:47,  3.44it/s] 79%|███████▉  | 618/780 [03:50<00:47,  3.44it/s] 79%|███████▉  | 619/780 [03:50<00:46,  3.44it/s] 79%|███████▉  | 620/780 [03:50<00:46,  3.43it/s] 80%|███████▉  | 621/780 [03:51<00:46,  3.44it/s] 80%|███████▉  | 622/780 [03:51<00:45,  3.44it/s] 80%|███████▉  | 623/780 [03:51<00:45,  3.44it/s] 80%|████████  | 624/780 [03:52<00:45,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 01:10:48,252 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:10:48,252 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 01:10:48,253 >>   Batch size = 8
{'eval_loss': 1.0275254249572754, 'eval_runtime': 9.4507, 'eval_samples_per_second': 369.18, 'eval_steps_per_second': 46.24, 'epoch': 3.0}
{'loss': 0.4034, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.80it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.34it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.38it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.70it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.30it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.97it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.75it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.32it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.41it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.49it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.58it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.44it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.46it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.37it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.34it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.27it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.26it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.18it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.20it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.29it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.41it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.38it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.43it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.36it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.27it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.13it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.24it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.21it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.29it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.25it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.34it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.38it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.39it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.46it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.51it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.37it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.38it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.36it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.33it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.36it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.39it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.45it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.35it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.35it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.42it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.41it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.34it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.30it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.22it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.26it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.33it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.45it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.44it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.36it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.41it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.39it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.38it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.32it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.41it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.39it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.36it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.38it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.39it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.38it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.34it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.24it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.24it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.29it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.36it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.29it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.32it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.32it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.26it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.35it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.34it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.32it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.34it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.39it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.33it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.37it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.33it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.41it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.46it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.37it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.38it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.22it/s][A                                                 
                                                 [A 80%|████████  | 624/780 [04:01<00:45,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 46.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:10:57,732 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 01:10:57,753 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:11:00,205 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:11:00,223 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:11:00,234 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:09<13:42,  5.31s/it] 80%|████████  | 626/780 [04:09<09:45,  3.80s/it] 80%|████████  | 627/780 [04:09<07:00,  2.75s/it] 81%|████████  | 628/780 [04:10<05:05,  2.01s/it] 81%|████████  | 629/780 [04:10<03:45,  1.49s/it] 81%|████████  | 630/780 [04:10<02:49,  1.13s/it] 81%|████████  | 631/780 [04:10<02:11,  1.14it/s] 81%|████████  | 632/780 [04:11<01:43,  1.42it/s] 81%|████████  | 633/780 [04:11<01:25,  1.73it/s] 81%|████████▏ | 634/780 [04:11<01:11,  2.03it/s] 81%|████████▏ | 635/780 [04:12<01:02,  2.32it/s] 82%|████████▏ | 636/780 [04:12<00:55,  2.57it/s] 82%|████████▏ | 637/780 [04:12<00:51,  2.78it/s] 82%|████████▏ | 638/780 [04:12<00:48,  2.95it/s] 82%|████████▏ | 639/780 [04:13<00:45,  3.09it/s] 82%|████████▏ | 640/780 [04:13<00:43,  3.19it/s] 82%|████████▏ | 641/780 [04:13<00:42,  3.27it/s] 82%|████████▏ | 642/780 [04:14<00:41,  3.32it/s] 82%|████████▏ | 643/780 [04:14<00:40,  3.36it/s] 83%|████████▎ | 644/780 [04:14<00:40,  3.39it/s] 83%|████████▎ | 645/780 [04:14<00:39,  3.41it/s] 83%|████████▎ | 646/780 [04:15<00:39,  3.42it/s] 83%|████████▎ | 647/780 [04:15<00:38,  3.43it/s] 83%|████████▎ | 648/780 [04:15<00:38,  3.43it/s] 83%|████████▎ | 649/780 [04:16<00:38,  3.44it/s] 83%|████████▎ | 650/780 [04:16<00:37,  3.44it/s] 83%|████████▎ | 651/780 [04:16<00:37,  3.45it/s] 84%|████████▎ | 652/780 [04:16<00:37,  3.45it/s] 84%|████████▎ | 653/780 [04:17<00:36,  3.45it/s] 84%|████████▍ | 654/780 [04:17<00:36,  3.45it/s] 84%|████████▍ | 655/780 [04:17<00:36,  3.45it/s] 84%|████████▍ | 656/780 [04:18<00:35,  3.46it/s] 84%|████████▍ | 657/780 [04:18<00:35,  3.46it/s] 84%|████████▍ | 658/780 [04:18<00:35,  3.46it/s] 84%|████████▍ | 659/780 [04:19<00:35,  3.44it/s] 85%|████████▍ | 660/780 [04:19<00:34,  3.44it/s] 85%|████████▍ | 661/780 [04:19<00:34,  3.44it/s] 85%|████████▍ | 662/780 [04:19<00:34,  3.44it/s] 85%|████████▌ | 663/780 [04:20<00:33,  3.45it/s] 85%|████████▌ | 664/780 [04:20<00:33,  3.45it/s] 85%|████████▌ | 665/780 [04:20<00:33,  3.45it/s] 85%|████████▌ | 666/780 [04:21<00:33,  3.44it/s] 86%|████████▌ | 667/780 [04:21<00:32,  3.45it/s] 86%|████████▌ | 668/780 [04:21<00:32,  3.45it/s] 86%|████████▌ | 669/780 [04:21<00:32,  3.45it/s] 86%|████████▌ | 670/780 [04:22<00:32,  3.43it/s] 86%|████████▌ | 671/780 [04:22<00:31,  3.44it/s] 86%|████████▌ | 672/780 [04:22<00:31,  3.44it/s] 86%|████████▋ | 673/780 [04:23<00:31,  3.44it/s] 86%|████████▋ | 674/780 [04:23<00:30,  3.44it/s] 87%|████████▋ | 675/780 [04:23<00:30,  3.45it/s] 87%|████████▋ | 676/780 [04:23<00:30,  3.45it/s] 87%|████████▋ | 677/780 [04:24<00:29,  3.45it/s] 87%|████████▋ | 678/780 [04:24<00:29,  3.45it/s] 87%|████████▋ | 679/780 [04:24<00:29,  3.45it/s] 87%|████████▋ | 680/780 [04:25<00:28,  3.45it/s] 87%|████████▋ | 681/780 [04:25<00:28,  3.44it/s] 87%|████████▋ | 682/780 [04:25<00:28,  3.44it/s] 88%|████████▊ | 683/780 [04:25<00:28,  3.44it/s] 88%|████████▊ | 684/780 [04:26<00:27,  3.44it/s] 88%|████████▊ | 685/780 [04:26<00:27,  3.44it/s] 88%|████████▊ | 686/780 [04:26<00:27,  3.44it/s] 88%|████████▊ | 687/780 [04:27<00:27,  3.44it/s] 88%|████████▊ | 688/780 [04:27<00:26,  3.45it/s] 88%|████████▊ | 689/780 [04:27<00:26,  3.45it/s] 88%|████████▊ | 690/780 [04:28<00:26,  3.45it/s] 89%|████████▊ | 691/780 [04:28<00:25,  3.45it/s] 89%|████████▊ | 692/780 [04:28<00:25,  3.44it/s] 89%|████████▉ | 693/780 [04:28<00:25,  3.44it/s] 89%|████████▉ | 694/780 [04:29<00:24,  3.45it/s] 89%|████████▉ | 695/780 [04:29<00:24,  3.45it/s] 89%|████████▉ | 696/780 [04:29<00:24,  3.45it/s] 89%|████████▉ | 697/780 [04:30<00:24,  3.45it/s] 89%|████████▉ | 698/780 [04:30<00:23,  3.45it/s] 90%|████████▉ | 699/780 [04:30<00:23,  3.45it/s] 90%|████████▉ | 700/780 [04:30<00:23,  3.45it/s] 90%|████████▉ | 701/780 [04:31<00:22,  3.45it/s] 90%|█████████ | 702/780 [04:31<00:22,  3.45it/s] 90%|█████████ | 703/780 [04:31<00:22,  3.45it/s] 90%|█████████ | 704/780 [04:32<00:22,  3.44it/s] 90%|█████████ | 705/780 [04:32<00:21,  3.44it/s] 91%|█████████ | 706/780 [04:32<00:21,  3.44it/s] 91%|█████████ | 707/780 [04:32<00:21,  3.44it/s] 91%|█████████ | 708/780 [04:33<00:20,  3.45it/s] 91%|█████████ | 709/780 [04:33<00:20,  3.45it/s] 91%|█████████ | 710/780 [04:33<00:20,  3.45it/s] 91%|█████████ | 711/780 [04:34<00:20,  3.45it/s] 91%|█████████▏| 712/780 [04:34<00:19,  3.45it/s] 91%|█████████▏| 713/780 [04:34<00:19,  3.45it/s] 92%|█████████▏| 714/780 [04:34<00:19,  3.45it/s] 92%|█████████▏| 715/780 [04:35<00:19,  3.39it/s] 92%|█████████▏| 716/780 [04:35<00:18,  3.41it/s] 92%|█████████▏| 717/780 [04:35<00:18,  3.42it/s] 92%|█████████▏| 718/780 [04:36<00:18,  3.43it/s] 92%|█████████▏| 719/780 [04:36<00:17,  3.43it/s] 92%|█████████▏| 720/780 [04:36<00:17,  3.43it/s] 92%|█████████▏| 721/780 [04:37<00:17,  3.43it/s] 93%|█████████▎| 722/780 [04:37<00:16,  3.44it/s] 93%|█████████▎| 723/780 [04:37<00:16,  3.44it/s] 93%|█████████▎| 724/780 [04:37<00:16,  3.44it/s] 93%|█████████▎| 725/780 [04:38<00:15,  3.44it/s] 93%|█████████▎| 726/780 [04:38<00:15,  3.42it/s] 93%|█████████▎| 727/780 [04:38<00:15,  3.43it/s] 93%|█████████▎| 728/780 [04:39<00:15,  3.43it/s] 93%|█████████▎| 729/780 [04:39<00:14,  3.44it/s] 94%|█████████▎| 730/780 [04:39<00:14,  3.44it/s] 94%|█████████▎| 731/780 [04:39<00:14,  3.44it/s] 94%|█████████▍| 732/780 [04:40<00:13,  3.44it/s] 94%|█████████▍| 733/780 [04:40<00:13,  3.44it/s] 94%|█████████▍| 734/780 [04:40<00:13,  3.44it/s] 94%|█████████▍| 735/780 [04:41<00:13,  3.45it/s] 94%|█████████▍| 736/780 [04:41<00:12,  3.45it/s] 94%|█████████▍| 737/780 [04:41<00:12,  3.43it/s] 95%|█████████▍| 738/780 [04:41<00:12,  3.44it/s] 95%|█████████▍| 739/780 [04:42<00:11,  3.44it/s] 95%|█████████▍| 740/780 [04:42<00:11,  3.44it/s] 95%|█████████▌| 741/780 [04:42<00:11,  3.45it/s] 95%|█████████▌| 742/780 [04:43<00:11,  3.45it/s] 95%|█████████▌| 743/780 [04:43<00:10,  3.45it/s] 95%|█████████▌| 744/780 [04:43<00:10,  3.45it/s] 96%|█████████▌| 745/780 [04:43<00:10,  3.45it/s] 96%|█████████▌| 746/780 [04:44<00:09,  3.45it/s] 96%|█████████▌| 747/780 [04:44<00:09,  3.45it/s] 96%|█████████▌| 748/780 [04:44<00:09,  3.44it/s] 96%|█████████▌| 749/780 [04:45<00:09,  3.44it/s] 96%|█████████▌| 750/780 [04:45<00:08,  3.44it/s] 96%|█████████▋| 751/780 [04:45<00:08,  3.44it/s] 96%|█████████▋| 752/780 [04:46<00:08,  3.44it/s] 97%|█████████▋| 753/780 [04:46<00:07,  3.45it/s] 97%|█████████▋| 754/780 [04:46<00:07,  3.45it/s] 97%|█████████▋| 755/780 [04:46<00:07,  3.45it/s] 97%|█████████▋| 756/780 [04:47<00:06,  3.45it/s] 97%|█████████▋| 757/780 [04:47<00:06,  3.45it/s] 97%|█████████▋| 758/780 [04:47<00:06,  3.45it/s] 97%|█████████▋| 759/780 [04:48<00:06,  3.43it/s] 97%|█████████▋| 760/780 [04:48<00:05,  3.43it/s] 98%|█████████▊| 761/780 [04:48<00:05,  3.44it/s] 98%|█████████▊| 762/780 [04:48<00:05,  3.44it/s] 98%|█████████▊| 763/780 [04:49<00:04,  3.44it/s] 98%|█████████▊| 764/780 [04:49<00:04,  3.44it/s] 98%|█████████▊| 765/780 [04:49<00:04,  3.45it/s] 98%|█████████▊| 766/780 [04:50<00:04,  3.45it/s] 98%|█████████▊| 767/780 [04:50<00:03,  3.45it/s] 98%|█████████▊| 768/780 [04:50<00:03,  3.45it/s] 99%|█████████▊| 769/780 [04:50<00:03,  3.45it/s] 99%|█████████▊| 770/780 [04:51<00:02,  3.44it/s] 99%|█████████▉| 771/780 [04:51<00:02,  3.44it/s] 99%|█████████▉| 772/780 [04:51<00:02,  3.44it/s] 99%|█████████▉| 773/780 [04:52<00:02,  3.44it/s] 99%|█████████▉| 774/780 [04:52<00:01,  3.44it/s] 99%|█████████▉| 775/780 [04:52<00:01,  3.44it/s] 99%|█████████▉| 776/780 [04:52<00:01,  3.44it/s]100%|█████████▉| 777/780 [04:53<00:00,  3.45it/s]100%|█████████▉| 778/780 [04:53<00:00,  3.44it/s]100%|█████████▉| 779/780 [04:53<00:00,  3.44it/s]100%|██████████| 780/780 [04:54<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 01:11:50,220 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:11:50,220 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 01:11:50,220 >>   Batch size = 8
{'eval_loss': 1.0278226137161255, 'eval_runtime': 9.4464, 'eval_samples_per_second': 369.346, 'eval_steps_per_second': 46.261, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.74it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.34it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.45it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.79it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.33it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.97it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.89it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.68it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.50it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.37it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.43it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.50it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.52it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.47it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.45it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.38it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.32it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.33it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.38it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.43it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.33it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.37it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.44it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.44it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.41it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.35it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.35it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.32it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.40it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.46it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.49it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.35it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.36it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.37it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.37it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.39it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.36it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.39it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.35it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.39it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.41it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.38it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.30it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.29it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.36it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.39it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.36it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.37it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.35it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.34it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.35it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.42it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.41it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.48it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.38it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.41it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.40it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.48it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.48it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.44it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.32it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.24it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.43it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.46it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.39it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.34it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.32it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.39it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.44it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.47it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.45it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.37it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.42it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.41it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.41it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.47it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.36it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.38it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.41it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.44it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.42it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.43it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.34it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.37it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.39it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.33it/s][A                                                 
                                                 [A100%|██████████| 780/780 [05:03<00:00,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 46.33it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:11:59,651 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 01:11:59,668 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:12:02,072 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:12:02,092 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:12:02,103 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:12:06,805 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:12:06,807 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156 (score: 0.9912354350090027).
                                                 100%|██████████| 780/780 [05:12<00:00,  3.44it/s]100%|██████████| 780/780 [05:12<00:00,  2.50it/s]
[INFO|trainer.py:1894] 2023-08-29 01:12:08,651 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 01:12:08,668 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:12:10,965 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:12:10,986 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:12:10,998 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:12:11,206 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:11,206 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:11,206 >>   train_loss               =     0.3962
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:11,207 >>   train_runtime            = 0:05:12.59
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:11,207 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:11,207 >>   train_samples_per_second =    159.954
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:11,207 >>   train_steps_per_second   =      2.495
{'eval_loss': 1.0353991985321045, 'eval_runtime': 9.4101, 'eval_samples_per_second': 370.771, 'eval_steps_per_second': 46.439, 'epoch': 5.0}
{'train_runtime': 312.5908, 'train_samples_per_second': 159.954, 'train_steps_per_second': 2.495, 'train_loss': 0.3961841778877454, 'epoch': 5.0}
08/29/2023 01:12:11 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:12:11,241 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:12:11,241 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 01:12:11,241 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 57.46it/s]  3%|▎         | 12/437 [00:00<00:08, 50.42it/s]  4%|▍         | 18/437 [00:00<00:08, 48.85it/s]  5%|▌         | 23/437 [00:00<00:08, 48.17it/s]  6%|▋         | 28/437 [00:00<00:08, 47.66it/s]  8%|▊         | 33/437 [00:00<00:08, 47.40it/s]  9%|▊         | 38/437 [00:00<00:08, 47.13it/s] 10%|▉         | 43/437 [00:00<00:08, 47.05it/s] 11%|█         | 48/437 [00:01<00:08, 46.93it/s] 12%|█▏        | 53/437 [00:01<00:08, 46.87it/s] 13%|█▎        | 58/437 [00:01<00:08, 46.92it/s] 14%|█▍        | 63/437 [00:01<00:07, 46.81it/s] 16%|█▌        | 68/437 [00:01<00:07, 46.70it/s] 17%|█▋        | 73/437 [00:01<00:07, 46.75it/s] 18%|█▊        | 78/437 [00:01<00:07, 46.81it/s] 19%|█▉        | 83/437 [00:01<00:07, 46.80it/s] 20%|██        | 88/437 [00:01<00:07, 46.80it/s] 21%|██▏       | 93/437 [00:01<00:07, 46.76it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.73it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.73it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.69it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.74it/s] 27%|██▋       | 118/437 [00:02<00:06, 46.79it/s] 28%|██▊       | 123/437 [00:02<00:06, 46.75it/s] 29%|██▉       | 128/437 [00:02<00:06, 46.60it/s] 30%|███       | 133/437 [00:02<00:06, 46.48it/s] 32%|███▏      | 138/437 [00:02<00:06, 46.58it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.69it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.69it/s] 35%|███▌      | 153/437 [00:03<00:06, 46.73it/s] 36%|███▌      | 158/437 [00:03<00:05, 46.68it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.66it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.61it/s] 40%|███▉      | 173/437 [00:03<00:05, 46.61it/s] 41%|████      | 178/437 [00:03<00:05, 46.59it/s] 42%|████▏     | 183/437 [00:03<00:05, 46.63it/s] 43%|████▎     | 188/437 [00:04<00:05, 46.68it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.66it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.69it/s] 46%|████▋     | 203/437 [00:04<00:05, 46.75it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.71it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.70it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.66it/s] 51%|█████     | 223/437 [00:04<00:04, 46.59it/s] 52%|█████▏    | 228/437 [00:04<00:04, 46.65it/s] 53%|█████▎    | 233/437 [00:04<00:04, 46.71it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.74it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.66it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.58it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.52it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.54it/s] 60%|██████    | 263/437 [00:05<00:03, 46.61it/s] 61%|██████▏   | 268/437 [00:05<00:03, 46.59it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.66it/s] 64%|██████▎   | 278/437 [00:05<00:03, 46.64it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.67it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.53it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.60it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.57it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.50it/s] 70%|███████   | 308/437 [00:06<00:02, 46.55it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.68it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.72it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.72it/s] 75%|███████▌  | 328/437 [00:07<00:02, 46.70it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.67it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.65it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.65it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.60it/s] 81%|████████  | 353/437 [00:07<00:01, 46.51it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.51it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.66it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.73it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.73it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.75it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.66it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.66it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.68it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.66it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.50it/s] 93%|█████████▎| 408/437 [00:08<00:00, 46.53it/s] 95%|█████████▍| 413/437 [00:08<00:00, 46.51it/s] 96%|█████████▌| 418/437 [00:08<00:00, 46.65it/s] 97%|█████████▋| 423/437 [00:09<00:00, 46.67it/s] 98%|█████████▊| 428/437 [00:09<00:00, 46.72it/s] 99%|█████████▉| 433/437 [00:09<00:00, 46.72it/s]100%|██████████| 437/437 [00:09<00:00, 46.79it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:12:20,602 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:20,603 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:20,603 >>   eval_loss               =     0.9912
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:20,603 >>   eval_runtime            = 0:00:09.36
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:20,603 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:20,603 >>   eval_samples_per_second =    372.715
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:20,603 >>   eval_steps_per_second   =     46.683
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:20,603 >>   perplexity              =     2.6946
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:27,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:27,412 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:27,412 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:27,412 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:27,412 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:12:28,117 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:12:28,118 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:12:28,743 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:12:29,784 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:12:29,787 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:32,816 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:32,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:32,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:32,822 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:32,822 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:12:33,479 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:12:33,480 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:12:34,064 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:12:34,231 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:12:34,231 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:08,  1.51it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.45it/s]Extractor Predicting: 20it [00:13,  1.46it/s]Extractor Predicting: 21it [00:14,  1.47it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:16,  1.45it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:18,  1.49it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.49it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:22,  1.53it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:23,  1.40it/s]Extractor Predicting: 36it [00:24,  1.41it/s]Extractor Predicting: 37it [00:25,  1.41it/s]Extractor Predicting: 38it [00:25,  1.41it/s]Extractor Predicting: 39it [00:26,  1.41it/s]Extractor Predicting: 40it [00:27,  1.43it/s]Extractor Predicting: 41it [00:27,  1.42it/s]Extractor Predicting: 42it [00:28,  1.42it/s]Extractor Predicting: 43it [00:29,  1.43it/s]Extractor Predicting: 44it [00:30,  1.42it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:31,  1.45it/s]Extractor Predicting: 47it [00:32,  1.44it/s]Extractor Predicting: 48it [00:32,  1.42it/s]Extractor Predicting: 49it [00:33,  1.44it/s]Extractor Predicting: 50it [00:34,  1.42it/s]Extractor Predicting: 51it [00:34,  1.42it/s]Extractor Predicting: 52it [00:35,  1.44it/s]Extractor Predicting: 53it [00:36,  1.44it/s]Extractor Predicting: 54it [00:37,  1.43it/s]Extractor Predicting: 55it [00:37,  1.41it/s]Extractor Predicting: 56it [00:38,  1.42it/s]Extractor Predicting: 57it [00:39,  1.42it/s]Extractor Predicting: 58it [00:39,  1.42it/s]Extractor Predicting: 59it [00:40,  1.40it/s]Extractor Predicting: 60it [00:41,  1.40it/s]Extractor Predicting: 61it [00:42,  1.41it/s]Extractor Predicting: 62it [00:42,  1.45it/s]Extractor Predicting: 63it [00:43,  1.46it/s]Extractor Predicting: 64it [00:44,  1.46it/s]Extractor Predicting: 65it [00:44,  1.48it/s]Extractor Predicting: 66it [00:45,  1.47it/s]Extractor Predicting: 67it [00:46,  1.49it/s]Extractor Predicting: 68it [00:46,  1.50it/s]Extractor Predicting: 69it [00:47,  1.52it/s]Extractor Predicting: 70it [00:47,  1.53it/s]Extractor Predicting: 71it [00:48,  1.53it/s]Extractor Predicting: 72it [00:49,  1.52it/s]Extractor Predicting: 73it [00:49,  1.52it/s]Extractor Predicting: 74it [00:50,  1.53it/s]Extractor Predicting: 75it [00:51,  1.49it/s]Extractor Predicting: 76it [00:52,  1.47it/s]Extractor Predicting: 77it [00:52,  1.50it/s]Extractor Predicting: 78it [00:53,  1.52it/s]Extractor Predicting: 79it [00:53,  1.52it/s]Extractor Predicting: 80it [00:54,  1.53it/s]Extractor Predicting: 81it [00:55,  1.52it/s]Extractor Predicting: 82it [00:55,  1.54it/s]Extractor Predicting: 83it [00:56,  1.52it/s]Extractor Predicting: 84it [00:57,  1.49it/s]Extractor Predicting: 85it [00:57,  1.52it/s]Extractor Predicting: 86it [00:58,  1.53it/s]Extractor Predicting: 87it [00:59,  1.54it/s]Extractor Predicting: 88it [00:59,  1.54it/s]Extractor Predicting: 89it [01:00,  1.52it/s]Extractor Predicting: 90it [01:01,  1.52it/s]Extractor Predicting: 91it [01:01,  1.51it/s]Extractor Predicting: 92it [01:02,  1.47it/s]Extractor Predicting: 93it [01:03,  1.48it/s]Extractor Predicting: 94it [01:03,  1.49it/s]Extractor Predicting: 95it [01:04,  1.47it/s]Extractor Predicting: 96it [01:05,  1.47it/s]Extractor Predicting: 97it [01:06,  1.34it/s]Extractor Predicting: 98it [01:06,  1.40it/s]Extractor Predicting: 99it [01:07,  1.41it/s]Extractor Predicting: 100it [01:08,  1.41it/s]Extractor Predicting: 101it [01:08,  1.44it/s]Extractor Predicting: 102it [01:09,  1.43it/s]Extractor Predicting: 103it [01:10,  1.41it/s]Extractor Predicting: 104it [01:10,  1.43it/s]Extractor Predicting: 105it [01:11,  1.43it/s]Extractor Predicting: 106it [01:12,  1.42it/s]Extractor Predicting: 107it [01:13,  1.45it/s]Extractor Predicting: 108it [01:13,  1.46it/s]Extractor Predicting: 109it [01:14,  1.42it/s]Extractor Predicting: 110it [01:15,  1.41it/s]Extractor Predicting: 111it [01:15,  1.42it/s]Extractor Predicting: 112it [01:16,  1.42it/s]Extractor Predicting: 113it [01:17,  1.43it/s]Extractor Predicting: 114it [01:17,  1.43it/s]Extractor Predicting: 115it [01:18,  1.42it/s]Extractor Predicting: 116it [01:19,  1.45it/s]Extractor Predicting: 117it [01:20,  1.45it/s]Extractor Predicting: 118it [01:20,  1.45it/s]Extractor Predicting: 119it [01:21,  1.49it/s]Extractor Predicting: 120it [01:21,  1.52it/s]Extractor Predicting: 121it [01:22,  1.53it/s]Extractor Predicting: 122it [01:23,  1.53it/s]Extractor Predicting: 123it [01:23,  1.50it/s]Extractor Predicting: 124it [01:24,  1.50it/s]Extractor Predicting: 125it [01:25,  1.51it/s]Extractor Predicting: 126it [01:25,  1.50it/s]Extractor Predicting: 127it [01:26,  1.50it/s]Extractor Predicting: 128it [01:27,  1.47it/s]Extractor Predicting: 129it [01:27,  1.51it/s]Extractor Predicting: 130it [01:28,  1.47it/s]Extractor Predicting: 131it [01:29,  1.48it/s]Extractor Predicting: 132it [01:30,  1.48it/s]Extractor Predicting: 133it [01:30,  1.47it/s]Extractor Predicting: 134it [01:31,  1.46it/s]Extractor Predicting: 135it [01:32,  1.49it/s]Extractor Predicting: 136it [01:32,  1.50it/s]Extractor Predicting: 137it [01:33,  1.48it/s]Extractor Predicting: 138it [01:34,  1.47it/s]Extractor Predicting: 139it [01:34,  1.49it/s]Extractor Predicting: 140it [01:35,  1.48it/s]Extractor Predicting: 141it [01:36,  1.48it/s]Extractor Predicting: 142it [01:36,  1.48it/s]Extractor Predicting: 143it [01:37,  1.50it/s]Extractor Predicting: 144it [01:38,  1.50it/s]Extractor Predicting: 145it [01:38,  1.96it/s]Extractor Predicting: 145it [01:38,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:21,219 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:21,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:21,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:21,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:21,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:14:21,532 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:14:21,533 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:14:21,805 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:14:22,868 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:14:22,868 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:25,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:25,517 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:25,517 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:25,517 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:25,517 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:14:26,280 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:14:26,281 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:14:27,146 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:14:27,307 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:14:27,307 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.49451353328456477,
  "recall": 0.1937517913442247,
  "score": 0.2784184514003295,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:20,  1.43it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:21,  1.44it/s]Extractor Predicting: 33it [00:22,  1.44it/s]Extractor Predicting: 34it [00:23,  1.44it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:24,  1.47it/s]Extractor Predicting: 37it [00:25,  1.52it/s]Extractor Predicting: 38it [00:25,  1.51it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:27,  1.51it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.51it/s]Extractor Predicting: 43it [00:29,  1.49it/s]Extractor Predicting: 44it [00:29,  1.50it/s]Extractor Predicting: 45it [00:30,  1.48it/s]Extractor Predicting: 46it [00:31,  1.49it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:32,  1.50it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:33,  1.48it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.50it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:37,  1.47it/s]Extractor Predicting: 56it [00:37,  1.47it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:39,  1.52it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.59it/s]Extractor Predicting: 61it [00:40,  1.61it/s]Extractor Predicting: 62it [00:41,  1.63it/s]Extractor Predicting: 63it [00:42,  1.63it/s]Extractor Predicting: 64it [00:42,  1.65it/s]Extractor Predicting: 65it [00:43,  1.66it/s]Extractor Predicting: 66it [00:43,  1.65it/s]Extractor Predicting: 67it [00:44,  1.67it/s]Extractor Predicting: 68it [00:45,  1.71it/s]Extractor Predicting: 69it [00:45,  1.74it/s]Extractor Predicting: 70it [00:46,  1.72it/s]Extractor Predicting: 71it [00:46,  1.71it/s]Extractor Predicting: 72it [00:47,  1.72it/s]Extractor Predicting: 73it [00:48,  1.72it/s]Extractor Predicting: 74it [00:48,  1.69it/s]Extractor Predicting: 75it [00:49,  1.68it/s]Extractor Predicting: 76it [00:49,  1.71it/s]Extractor Predicting: 77it [00:50,  1.73it/s]Extractor Predicting: 78it [00:50,  1.71it/s]Extractor Predicting: 79it [00:51,  1.73it/s]Extractor Predicting: 80it [00:52,  1.73it/s]Extractor Predicting: 81it [00:52,  1.72it/s]Extractor Predicting: 82it [00:53,  1.70it/s]Extractor Predicting: 83it [00:53,  1.68it/s]Extractor Predicting: 84it [00:54,  1.48it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.53it/s]Extractor Predicting: 87it [00:56,  1.51it/s]Extractor Predicting: 88it [00:57,  1.49it/s]Extractor Predicting: 89it [00:58,  1.47it/s]Extractor Predicting: 90it [00:58,  1.43it/s]Extractor Predicting: 91it [00:59,  1.44it/s]Extractor Predicting: 92it [01:00,  1.42it/s]Extractor Predicting: 93it [01:00,  1.46it/s]Extractor Predicting: 94it [01:01,  1.45it/s]Extractor Predicting: 95it [01:02,  1.45it/s]Extractor Predicting: 96it [01:02,  1.44it/s]Extractor Predicting: 97it [01:03,  1.44it/s]Extractor Predicting: 98it [01:04,  1.44it/s]Extractor Predicting: 99it [01:05,  1.45it/s]Extractor Predicting: 100it [01:05,  1.44it/s]Extractor Predicting: 101it [01:06,  1.44it/s]Extractor Predicting: 102it [01:07,  1.43it/s]Extractor Predicting: 103it [01:07,  1.46it/s]Extractor Predicting: 104it [01:08,  1.42it/s]Extractor Predicting: 105it [01:09,  1.43it/s]Extractor Predicting: 106it [01:09,  1.40it/s]Extractor Predicting: 107it [01:10,  1.39it/s]Extractor Predicting: 108it [01:11,  1.39it/s]Extractor Predicting: 109it [01:12,  1.42it/s]Extractor Predicting: 110it [01:12,  1.42it/s]Extractor Predicting: 111it [01:13,  1.41it/s]Extractor Predicting: 112it [01:14,  1.38it/s]Extractor Predicting: 113it [01:14,  1.39it/s]Extractor Predicting: 114it [01:15,  1.43it/s]Extractor Predicting: 115it [01:16,  1.42it/s]Extractor Predicting: 116it [01:17,  1.44it/s]Extractor Predicting: 117it [01:17,  1.47it/s]Extractor Predicting: 118it [01:18,  1.42it/s]Extractor Predicting: 119it [01:19,  1.40it/s]Extractor Predicting: 120it [01:19,  1.41it/s]Extractor Predicting: 121it [01:20,  1.40it/s]Extractor Predicting: 122it [01:21,  1.43it/s]Extractor Predicting: 123it [01:21,  1.44it/s]Extractor Predicting: 124it [01:22,  1.43it/s]Extractor Predicting: 125it [01:23,  1.42it/s]Extractor Predicting: 126it [01:24,  1.39it/s]Extractor Predicting: 127it [01:24,  1.40it/s]Extractor Predicting: 128it [01:25,  1.37it/s]Extractor Predicting: 129it [01:26,  1.38it/s]Extractor Predicting: 130it [01:27,  1.37it/s]Extractor Predicting: 131it [01:27,  1.40it/s]Extractor Predicting: 132it [01:28,  1.40it/s]Extractor Predicting: 133it [01:29,  1.41it/s]Extractor Predicting: 134it [01:29,  1.43it/s]Extractor Predicting: 135it [01:30,  1.42it/s]Extractor Predicting: 136it [01:31,  1.39it/s]Extractor Predicting: 137it [01:31,  1.40it/s]Extractor Predicting: 138it [01:32,  1.39it/s]Extractor Predicting: 139it [01:33,  1.38it/s]Extractor Predicting: 140it [01:34,  1.40it/s]Extractor Predicting: 141it [01:34,  1.41it/s]Extractor Predicting: 142it [01:35,  1.41it/s]Extractor Predicting: 143it [01:36,  1.38it/s]Extractor Predicting: 144it [01:37,  1.38it/s]Extractor Predicting: 145it [01:37,  1.39it/s]Extractor Predicting: 146it [01:38,  1.40it/s]Extractor Predicting: 147it [01:39,  1.43it/s]Extractor Predicting: 148it [01:39,  1.45it/s]Extractor Predicting: 149it [01:40,  1.41it/s]Extractor Predicting: 150it [01:41,  1.41it/s]Extractor Predicting: 151it [01:41,  1.44it/s]Extractor Predicting: 152it [01:42,  1.47it/s]Extractor Predicting: 153it [01:43,  1.47it/s]Extractor Predicting: 154it [01:43,  1.50it/s]Extractor Predicting: 155it [01:44,  1.51it/s]Extractor Predicting: 156it [01:45,  1.53it/s]Extractor Predicting: 157it [01:45,  1.51it/s]Extractor Predicting: 158it [01:46,  1.53it/s]Extractor Predicting: 159it [01:47,  1.57it/s]Extractor Predicting: 160it [01:47,  1.53it/s]Extractor Predicting: 161it [01:48,  1.51it/s]Extractor Predicting: 162it [01:49,  1.48it/s]Extractor Predicting: 163it [01:49,  1.49it/s]Extractor Predicting: 164it [01:50,  1.47it/s]Extractor Predicting: 165it [01:51,  1.47it/s]Extractor Predicting: 166it [01:51,  1.47it/s]Extractor Predicting: 167it [01:52,  1.47it/s]Extractor Predicting: 168it [01:53,  1.46it/s]Extractor Predicting: 169it [01:53,  1.45it/s]Extractor Predicting: 170it [01:54,  1.48it/s]Extractor Predicting: 171it [01:55,  1.47it/s]Extractor Predicting: 172it [01:56,  1.45it/s]Extractor Predicting: 173it [01:56,  1.46it/s]Extractor Predicting: 174it [01:57,  1.46it/s]Extractor Predicting: 175it [01:58,  1.45it/s]Extractor Predicting: 176it [01:58,  1.46it/s]Extractor Predicting: 177it [01:59,  1.50it/s]Extractor Predicting: 178it [02:00,  1.49it/s]Extractor Predicting: 179it [02:00,  1.50it/s]Extractor Predicting: 180it [02:01,  1.51it/s]Extractor Predicting: 181it [02:02,  1.49it/s]Extractor Predicting: 182it [02:02,  1.54it/s]Extractor Predicting: 183it [02:03,  1.54it/s]Extractor Predicting: 184it [02:03,  1.52it/s]Extractor Predicting: 185it [02:04,  1.55it/s]Extractor Predicting: 186it [02:05,  1.36it/s]Extractor Predicting: 187it [02:06,  1.37it/s]Extractor Predicting: 188it [02:06,  1.44it/s]Extractor Predicting: 189it [02:07,  1.48it/s]Extractor Predicting: 190it [02:08,  1.53it/s]Extractor Predicting: 191it [02:08,  1.48it/s]Extractor Predicting: 192it [02:09,  1.51it/s]Extractor Predicting: 193it [02:10,  1.53it/s]Extractor Predicting: 194it [02:10,  1.57it/s]Extractor Predicting: 195it [02:11,  1.53it/s]Extractor Predicting: 196it [02:12,  1.53it/s]Extractor Predicting: 197it [02:12,  1.54it/s]Extractor Predicting: 198it [02:13,  1.52it/s]Extractor Predicting: 199it [02:14,  1.49it/s]Extractor Predicting: 200it [02:14,  1.50it/s]Extractor Predicting: 201it [02:15,  1.51it/s]Extractor Predicting: 202it [02:15,  1.54it/s]Extractor Predicting: 203it [02:16,  1.56it/s]Extractor Predicting: 204it [02:17,  1.58it/s]Extractor Predicting: 205it [02:17,  1.57it/s]Extractor Predicting: 206it [02:18,  1.62it/s]Extractor Predicting: 207it [02:19,  1.59it/s]Extractor Predicting: 208it [02:19,  1.63it/s]Extractor Predicting: 209it [02:20,  1.61it/s]Extractor Predicting: 210it [02:20,  1.60it/s]Extractor Predicting: 211it [02:21,  1.59it/s]Extractor Predicting: 212it [02:22,  1.62it/s]Extractor Predicting: 213it [02:22,  1.64it/s]Extractor Predicting: 214it [02:23,  1.64it/s]Extractor Predicting: 215it [02:23,  1.67it/s]Extractor Predicting: 216it [02:24,  1.67it/s]Extractor Predicting: 217it [02:25,  1.62it/s]Extractor Predicting: 218it [02:25,  1.60it/s]Extractor Predicting: 219it [02:26,  1.62it/s]Extractor Predicting: 220it [02:27,  1.63it/s]Extractor Predicting: 221it [02:27,  1.64it/s]Extractor Predicting: 222it [02:28,  1.68it/s]Extractor Predicting: 223it [02:28,  1.63it/s]Extractor Predicting: 224it [02:29,  1.66it/s]Extractor Predicting: 225it [02:30,  1.65it/s]Extractor Predicting: 226it [02:30,  1.64it/s]Extractor Predicting: 227it [02:31,  1.67it/s]Extractor Predicting: 228it [02:31,  1.66it/s]Extractor Predicting: 229it [02:32,  1.57it/s]Extractor Predicting: 230it [02:33,  1.53it/s]Extractor Predicting: 231it [02:34,  1.46it/s]Extractor Predicting: 232it [02:34,  1.43it/s]Extractor Predicting: 233it [02:35,  1.43it/s]Extractor Predicting: 234it [02:36,  1.41it/s]Extractor Predicting: 235it [02:36,  1.42it/s]Extractor Predicting: 236it [02:37,  1.42it/s]Extractor Predicting: 237it [02:38,  1.38it/s]Extractor Predicting: 238it [02:39,  1.37it/s]Extractor Predicting: 239it [02:39,  1.38it/s]Extractor Predicting: 240it [02:40,  1.39it/s]Extractor Predicting: 241it [02:41,  1.37it/s]Extractor Predicting: 242it [02:41,  1.40it/s]Extractor Predicting: 243it [02:42,  1.37it/s]Extractor Predicting: 244it [02:43,  1.39it/s]Extractor Predicting: 245it [02:44,  1.40it/s]Extractor Predicting: 246it [02:44,  1.42it/s]Extractor Predicting: 247it [02:45,  1.38it/s]Extractor Predicting: 248it [02:46,  1.38it/s]Extractor Predicting: 249it [02:47,  1.35it/s]Extractor Predicting: 250it [02:47,  1.36it/s]Extractor Predicting: 251it [02:48,  1.37it/s]Extractor Predicting: 252it [02:49,  1.39it/s]Extractor Predicting: 253it [02:49,  1.40it/s]Extractor Predicting: 254it [02:50,  1.39it/s]Extractor Predicting: 255it [02:51,  1.39it/s]Extractor Predicting: 256it [02:52,  1.41it/s]Extractor Predicting: 257it [02:52,  1.46it/s]Extractor Predicting: 258it [02:53,  1.47it/s]Extractor Predicting: 259it [02:54,  1.48it/s]Extractor Predicting: 260it [02:54,  1.50it/s]Extractor Predicting: 261it [02:55,  1.50it/s]Extractor Predicting: 262it [02:55,  1.49it/s]Extractor Predicting: 263it [02:56,  1.51it/s]Extractor Predicting: 264it [02:57,  1.52it/s]Extractor Predicting: 265it [02:58,  1.48it/s]Extractor Predicting: 266it [02:58,  1.51it/s]Extractor Predicting: 267it [02:59,  1.53it/s]Extractor Predicting: 268it [02:59,  1.51it/s]Extractor Predicting: 269it [03:00,  1.52it/s]Extractor Predicting: 270it [03:01,  1.50it/s]Extractor Predicting: 271it [03:01,  1.49it/s]Extractor Predicting: 272it [03:02,  1.48it/s]Extractor Predicting: 273it [03:03,  1.48it/s]Extractor Predicting: 274it [03:04,  1.47it/s]Extractor Predicting: 275it [03:04,  1.50it/s]Extractor Predicting: 276it [03:05,  1.49it/s]Extractor Predicting: 277it [03:06,  1.48it/s]Extractor Predicting: 278it [03:06,  1.45it/s]Extractor Predicting: 279it [03:07,  1.47it/s]Extractor Predicting: 280it [03:08,  1.46it/s]Extractor Predicting: 281it [03:08,  1.49it/s]Extractor Predicting: 282it [03:09,  1.48it/s]Extractor Predicting: 283it [03:10,  1.48it/s]Extractor Predicting: 284it [03:10,  1.49it/s]Extractor Predicting: 285it [03:11,  1.46it/s]Extractor Predicting: 286it [03:12,  1.46it/s]Extractor Predicting: 287it [03:12,  1.48it/s]Extractor Predicting: 288it [03:13,  1.48it/s]Extractor Predicting: 289it [03:14,  1.47it/s]Extractor Predicting: 290it [03:14,  1.48it/s]Extractor Predicting: 291it [03:15,  1.48it/s]Extractor Predicting: 292it [03:16,  1.48it/s]Extractor Predicting: 293it [03:16,  1.47it/s]Extractor Predicting: 294it [03:17,  1.48it/s]Extractor Predicting: 295it [03:18,  1.48it/s]Extractor Predicting: 296it [03:18,  1.48it/s]Extractor Predicting: 297it [03:19,  1.49it/s]Extractor Predicting: 298it [03:20,  1.46it/s]Extractor Predicting: 299it [03:20,  1.50it/s]Extractor Predicting: 300it [03:21,  1.32it/s]Extractor Predicting: 301it [03:22,  1.37it/s]Extractor Predicting: 302it [03:23,  1.39it/s]Extractor Predicting: 303it [03:23,  1.45it/s]Extractor Predicting: 304it [03:24,  1.46it/s]Extractor Predicting: 305it [03:25,  1.50it/s]Extractor Predicting: 306it [03:25,  1.48it/s]Extractor Predicting: 307it [03:26,  1.47it/s]Extractor Predicting: 308it [03:27,  1.47it/s]Extractor Predicting: 309it [03:27,  1.49it/s]Extractor Predicting: 310it [03:28,  1.45it/s]Extractor Predicting: 311it [03:29,  1.44it/s]Extractor Predicting: 312it [03:30,  1.45it/s]Extractor Predicting: 313it [03:30,  1.47it/s]Extractor Predicting: 314it [03:31,  1.46it/s]Extractor Predicting: 315it [03:31,  1.49it/s]Extractor Predicting: 316it [03:32,  1.48it/s]Extractor Predicting: 317it [03:33,  1.47it/s]Extractor Predicting: 318it [03:33,  1.51it/s]Extractor Predicting: 319it [03:34,  1.51it/s]Extractor Predicting: 320it [03:35,  1.54it/s]Extractor Predicting: 321it [03:35,  1.51it/s]Extractor Predicting: 322it [03:36,  1.50it/s]Extractor Predicting: 323it [03:37,  1.50it/s]Extractor Predicting: 324it [03:38,  1.48it/s]Extractor Predicting: 325it [03:38,  1.47it/s]Extractor Predicting: 326it [03:39,  1.47it/s]Extractor Predicting: 327it [03:40,  1.46it/s]Extractor Predicting: 328it [03:40,  1.46it/s]Extractor Predicting: 329it [03:41,  1.42it/s]Extractor Predicting: 330it [03:42,  1.44it/s]Extractor Predicting: 331it [03:42,  1.45it/s]Extractor Predicting: 332it [03:43,  1.48it/s]Extractor Predicting: 333it [03:44,  1.49it/s]Extractor Predicting: 334it [03:44,  1.49it/s]Extractor Predicting: 335it [03:45,  1.48it/s]Extractor Predicting: 336it [03:46,  1.48it/s]Extractor Predicting: 337it [03:46,  1.47it/s]Extractor Predicting: 338it [03:47,  1.48it/s]Extractor Predicting: 339it [03:48,  1.47it/s]Extractor Predicting: 340it [03:48,  1.51it/s]Extractor Predicting: 341it [03:49,  1.49it/s]Extractor Predicting: 342it [03:50,  1.51it/s]Extractor Predicting: 343it [03:50,  1.46it/s]Extractor Predicting: 344it [03:51,  1.48it/s]Extractor Predicting: 345it [03:52,  1.46it/s]Extractor Predicting: 346it [03:52,  1.48it/s]Extractor Predicting: 347it [03:53,  1.49it/s]Extractor Predicting: 348it [03:54,  1.50it/s]Extractor Predicting: 349it [03:55,  1.44it/s]Extractor Predicting: 350it [03:55,  1.43it/s]Extractor Predicting: 351it [03:56,  1.42it/s]Extractor Predicting: 352it [03:57,  1.44it/s]Extractor Predicting: 353it [03:57,  1.43it/s]Extractor Predicting: 354it [03:58,  1.45it/s]Extractor Predicting: 355it [03:59,  1.43it/s]Extractor Predicting: 356it [03:59,  1.47it/s]Extractor Predicting: 357it [04:00,  1.45it/s]Extractor Predicting: 358it [04:01,  1.44it/s]Extractor Predicting: 359it [04:01,  1.43it/s]Extractor Predicting: 360it [04:02,  1.41it/s]Extractor Predicting: 361it [04:03,  1.45it/s]Extractor Predicting: 362it [04:04,  1.47it/s]Extractor Predicting: 363it [04:04,  1.46it/s]Extractor Predicting: 364it [04:05,  1.47it/s]Extractor Predicting: 365it [04:06,  1.48it/s]Extractor Predicting: 366it [04:06,  1.46it/s]Extractor Predicting: 367it [04:07,  1.46it/s]Extractor Predicting: 368it [04:08,  1.45it/s]Extractor Predicting: 369it [04:08,  1.47it/s]Extractor Predicting: 370it [04:09,  1.44it/s]Extractor Predicting: 371it [04:10,  1.47it/s]Extractor Predicting: 372it [04:10,  1.48it/s]Extractor Predicting: 373it [04:11,  1.50it/s]Extractor Predicting: 374it [04:12,  1.50it/s]Extractor Predicting: 375it [04:12,  1.48it/s]Extractor Predicting: 376it [04:13,  1.49it/s]Extractor Predicting: 377it [04:14,  1.50it/s]Extractor Predicting: 378it [04:14,  1.50it/s]Extractor Predicting: 379it [04:15,  1.52it/s]Extractor Predicting: 380it [04:16,  1.51it/s]Extractor Predicting: 381it [04:16,  1.51it/s]Extractor Predicting: 382it [04:17,  1.48it/s]Extractor Predicting: 383it [04:18,  1.51it/s]Extractor Predicting: 384it [04:18,  1.52it/s]Extractor Predicting: 385it [04:19,  1.52it/s]Extractor Predicting: 386it [04:20,  1.54it/s]Extractor Predicting: 387it [04:20,  1.56it/s]Extractor Predicting: 388it [04:21,  1.34it/s]Extractor Predicting: 389it [04:22,  1.39it/s]Extractor Predicting: 390it [04:23,  1.42it/s]Extractor Predicting: 391it [04:23,  1.44it/s]Extractor Predicting: 392it [04:24,  1.46it/s]Extractor Predicting: 393it [04:25,  1.48it/s]Extractor Predicting: 394it [04:25,  1.50it/s]Extractor Predicting: 395it [04:26,  1.45it/s]Extractor Predicting: 396it [04:27,  1.48it/s]Extractor Predicting: 397it [04:27,  1.50it/s]Extractor Predicting: 398it [04:28,  1.47it/s]Extractor Predicting: 399it [04:29,  1.48it/s]Extractor Predicting: 400it [04:29,  1.47it/s]Extractor Predicting: 401it [04:30,  1.49it/s]Extractor Predicting: 402it [04:31,  1.53it/s]Extractor Predicting: 403it [04:31,  1.51it/s]Extractor Predicting: 404it [04:32,  1.52it/s]Extractor Predicting: 405it [04:33,  1.52it/s]Extractor Predicting: 406it [04:33,  1.52it/s]Extractor Predicting: 407it [04:34,  1.51it/s]Extractor Predicting: 408it [04:34,  1.53it/s]Extractor Predicting: 409it [04:35,  1.55it/s]Extractor Predicting: 410it [04:36,  1.54it/s]Extractor Predicting: 411it [04:36,  1.51it/s]Extractor Predicting: 412it [04:37,  1.53it/s]Extractor Predicting: 413it [04:38,  1.53it/s]Extractor Predicting: 414it [04:38,  1.53it/s]Extractor Predicting: 415it [04:39,  1.56it/s]Extractor Predicting: 416it [04:40,  1.56it/s]Extractor Predicting: 417it [04:40,  1.53it/s]Extractor Predicting: 418it [04:41,  1.55it/s]Extractor Predicting: 419it [04:42,  1.53it/s]Extractor Predicting: 420it [04:42,  1.53it/s]Extractor Predicting: 421it [04:43,  1.52it/s]Extractor Predicting: 422it [04:44,  1.51it/s]Extractor Predicting: 423it [04:44,  1.51it/s]Extractor Predicting: 424it [04:45,  1.52it/s]Extractor Predicting: 425it [04:45,  1.70it/s]Extractor Predicting: 425it [04:45,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:19:21,509 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:19:21,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:19:21,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:19:21,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:19:21,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:19:21,812 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:19:21,814 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:19:22,085 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:19:23,125 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:19:23,125 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:19:24,848 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:19:24,852 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:19:24,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:19:24,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:19:24,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:19:25,172 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:19:25,174 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:19:25,439 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:19:25,586 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:19:25,586 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3062225664859282,
  "recall": 0.11643432161790693,
  "score": 0.16871754747848353,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 4it [00:02,  1.33it/s]Extractor Predicting: 5it [00:03,  1.33it/s]Extractor Predicting: 6it [00:04,  1.37it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 7it [00:04,  1.45it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.65,
  "recall": 0.041401273885350316,
  "score": 0.07784431137724551,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_15_seed_3', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:14, 14.51s/it]Extractor Predicting: 2it [00:15,  6.66s/it]Extractor Predicting: 3it [00:16,  3.91s/it]Extractor Predicting: 4it [00:16,  2.58s/it]Extractor Predicting: 5it [00:17,  1.85s/it]Extractor Predicting: 6it [00:17,  1.41s/it]Extractor Predicting: 7it [00:18,  1.15s/it]Extractor Predicting: 8it [00:19,  1.15s/it]Extractor Predicting: 9it [00:20,  1.03it/s]Extractor Predicting: 10it [00:20,  1.20it/s]Extractor Predicting: 11it [00:21,  1.31it/s]Extractor Predicting: 12it [00:21,  1.44it/s]Extractor Predicting: 13it [00:22,  1.52it/s]Extractor Predicting: 14it [00:23,  1.56it/s]Extractor Predicting: 15it [00:23,  1.65it/s]Extractor Predicting: 16it [00:24,  1.67it/s]Extractor Predicting: 17it [00:24,  1.68it/s]Extractor Predicting: 18it [00:25,  1.69it/s]Extractor Predicting: 19it [00:26,  1.68it/s]Extractor Predicting: 20it [00:26,  1.75it/s]Extractor Predicting: 21it [00:27,  1.63it/s]Extractor Predicting: 22it [00:27,  1.64it/s]Extractor Predicting: 23it [00:28,  1.60it/s]Extractor Predicting: 24it [00:29,  1.56it/s]Extractor Predicting: 25it [00:29,  1.52it/s]Extractor Predicting: 26it [00:31,  1.21it/s]Extractor Predicting: 27it [00:31,  1.31it/s]Extractor Predicting: 28it [00:32,  1.38it/s]Extractor Predicting: 29it [00:32,  1.46it/s]Extractor Predicting: 30it [00:33,  1.52it/s]Extractor Predicting: 31it [00:34,  1.55it/s]Extractor Predicting: 32it [00:34,  1.59it/s]Extractor Predicting: 33it [00:35,  1.61it/s]Extractor Predicting: 34it [00:35,  1.62it/s]Extractor Predicting: 35it [00:36,  1.60it/s]Extractor Predicting: 36it [00:37,  1.62it/s]Extractor Predicting: 37it [00:37,  1.60it/s]Extractor Predicting: 38it [00:38,  1.61it/s]Extractor Predicting: 39it [00:39,  1.59it/s]Extractor Predicting: 40it [00:39,  1.60it/s]Extractor Predicting: 41it [00:40,  1.62it/s]Extractor Predicting: 42it [00:40,  1.60it/s]Extractor Predicting: 43it [00:41,  1.59it/s]Extractor Predicting: 44it [00:42,  1.63it/s]Extractor Predicting: 45it [00:42,  1.61it/s]Extractor Predicting: 46it [00:43,  1.62it/s]Extractor Predicting: 47it [00:44,  1.62it/s]Extractor Predicting: 48it [00:44,  1.62it/s]Extractor Predicting: 49it [00:45,  1.60it/s]Extractor Predicting: 50it [00:45,  1.60it/s]Extractor Predicting: 51it [00:46,  1.59it/s]Extractor Predicting: 52it [00:47,  1.59it/s]Extractor Predicting: 53it [00:47,  1.58it/s]Extractor Predicting: 54it [00:48,  1.63it/s]Extractor Predicting: 55it [00:49,  1.64it/s]Extractor Predicting: 56it [00:49,  1.62it/s]Extractor Predicting: 57it [00:50,  1.61it/s]Extractor Predicting: 58it [00:50,  1.60it/s]Extractor Predicting: 59it [00:51,  1.61it/s]Extractor Predicting: 60it [00:52,  1.59it/s]Extractor Predicting: 61it [00:52,  1.54it/s]Extractor Predicting: 62it [00:53,  1.55it/s]Extractor Predicting: 63it [00:54,  1.56it/s]Extractor Predicting: 64it [00:54,  1.59it/s]Extractor Predicting: 65it [00:55,  1.58it/s]Extractor Predicting: 66it [00:56,  1.60it/s]Extractor Predicting: 67it [00:56,  1.61it/s]Extractor Predicting: 68it [00:57,  1.56it/s]Extractor Predicting: 69it [00:57,  1.56it/s]Extractor Predicting: 70it [00:58,  1.59it/s]Extractor Predicting: 71it [00:59,  1.64it/s]Extractor Predicting: 72it [00:59,  1.63it/s]Extractor Predicting: 73it [01:00,  1.64it/s]Extractor Predicting: 74it [01:00,  1.62it/s]Extractor Predicting: 75it [01:01,  1.63it/s]Extractor Predicting: 76it [01:02,  1.60it/s]Extractor Predicting: 77it [01:02,  1.60it/s]Extractor Predicting: 78it [01:03,  1.63it/s]Extractor Predicting: 79it [01:04,  1.62it/s]Extractor Predicting: 80it [01:04,  1.65it/s]Extractor Predicting: 81it [01:05,  1.62it/s]Extractor Predicting: 82it [01:05,  1.61it/s]Extractor Predicting: 83it [01:06,  1.59it/s]Extractor Predicting: 84it [01:07,  1.61it/s]Extractor Predicting: 85it [01:07,  1.60it/s]Extractor Predicting: 86it [01:08,  1.59it/s]Extractor Predicting: 87it [01:09,  1.62it/s]Extractor Predicting: 88it [01:09,  1.60it/s]Extractor Predicting: 89it [01:10,  1.22it/s]Extractor Predicting: 90it [01:11,  1.33it/s]Extractor Predicting: 91it [01:12,  1.41it/s]Extractor Predicting: 92it [01:12,  1.47it/s]Extractor Predicting: 93it [01:13,  1.53it/s]Extractor Predicting: 94it [01:13,  1.55it/s]Extractor Predicting: 95it [01:14,  1.51it/s]Extractor Predicting: 96it [01:15,  1.54it/s]Extractor Predicting: 97it [01:15,  1.56it/s]Extractor Predicting: 98it [01:16,  1.44it/s]Extractor Predicting: 99it [01:17,  1.50it/s]Extractor Predicting: 100it [01:17,  1.52it/s]Extractor Predicting: 101it [01:18,  1.55it/s]Extractor Predicting: 102it [01:19,  1.59it/s]Extractor Predicting: 103it [01:19,  1.63it/s]Extractor Predicting: 104it [01:20,  1.62it/s]Extractor Predicting: 105it [01:21,  1.60it/s]Extractor Predicting: 106it [01:21,  1.60it/s]Extractor Predicting: 107it [01:22,  1.61it/s]Extractor Predicting: 108it [01:22,  1.64it/s]Extractor Predicting: 109it [01:23,  1.60it/s]Extractor Predicting: 110it [01:24,  1.57it/s]Extractor Predicting: 111it [01:24,  1.60it/s]Extractor Predicting: 112it [01:25,  1.60it/s]Extractor Predicting: 113it [01:26,  1.61it/s]Extractor Predicting: 114it [01:26,  1.65it/s]Extractor Predicting: 115it [01:27,  1.71it/s]Extractor Predicting: 116it [01:27,  1.70it/s]Extractor Predicting: 117it [01:28,  1.69it/s]Extractor Predicting: 118it [01:28,  1.72it/s]Extractor Predicting: 119it [01:29,  1.68it/s]Extractor Predicting: 120it [01:30,  1.70it/s]Extractor Predicting: 121it [01:30,  1.67it/s]Extractor Predicting: 122it [01:31,  1.60it/s]Extractor Predicting: 123it [01:32,  1.58it/s]Extractor Predicting: 124it [01:32,  1.54it/s]Extractor Predicting: 125it [01:33,  1.56it/s]Extractor Predicting: 126it [01:33,  1.59it/s]Extractor Predicting: 127it [01:34,  1.59it/s]Extractor Predicting: 128it [01:35,  1.23it/s]Extractor Predicting: 129it [01:36,  1.33it/s]Extractor Predicting: 130it [01:37,  1.41it/s]Extractor Predicting: 131it [01:37,  1.50it/s]Extractor Predicting: 132it [01:38,  1.48it/s]Extractor Predicting: 133it [01:38,  1.49it/s]Extractor Predicting: 134it [01:39,  1.53it/s]Extractor Predicting: 135it [01:40,  1.57it/s]Extractor Predicting: 136it [01:40,  1.59it/s]Extractor Predicting: 137it [01:41,  1.61it/s]Extractor Predicting: 138it [01:42,  1.60it/s]Extractor Predicting: 139it [01:42,  1.60it/s]Extractor Predicting: 140it [01:43,  1.59it/s]Extractor Predicting: 141it [01:43,  1.63it/s]Extractor Predicting: 142it [01:44,  1.58it/s]Extractor Predicting: 143it [01:45,  1.62it/s]Extractor Predicting: 144it [01:45,  1.63it/s]Extractor Predicting: 145it [01:46,  1.64it/s]Extractor Predicting: 146it [01:47,  1.58it/s]Extractor Predicting: 147it [01:47,  1.55it/s]Extractor Predicting: 148it [01:48,  1.56it/s]Extractor Predicting: 149it [01:48,  1.56it/s]Extractor Predicting: 150it [01:49,  1.58it/s]Extractor Predicting: 151it [01:50,  1.57it/s]Extractor Predicting: 152it [01:50,  1.61it/s]Extractor Predicting: 153it [01:51,  1.59it/s]Extractor Predicting: 154it [01:52,  1.59it/s]Extractor Predicting: 155it [01:52,  1.61it/s]Extractor Predicting: 156it [01:53,  1.63it/s]Extractor Predicting: 157it [01:53,  1.63it/s]Extractor Predicting: 158it [01:54,  1.65it/s]Extractor Predicting: 159it [01:55,  1.62it/s]Extractor Predicting: 160it [01:55,  1.63it/s]Extractor Predicting: 161it [01:56,  1.61it/s]Extractor Predicting: 162it [01:57,  1.59it/s]Extractor Predicting: 163it [01:57,  1.60it/s]Extractor Predicting: 164it [01:58,  1.59it/s]Extractor Predicting: 165it [01:58,  1.61it/s]Extractor Predicting: 166it [01:59,  1.58it/s]Extractor Predicting: 167it [02:00,  1.57it/s]Extractor Predicting: 168it [02:00,  1.55it/s]Extractor Predicting: 169it [02:01,  1.52it/s]Extractor Predicting: 170it [02:02,  1.53it/s]Extractor Predicting: 171it [02:02,  1.52it/s]Extractor Predicting: 172it [02:03,  1.55it/s]Extractor Predicting: 173it [02:04,  1.52it/s]Extractor Predicting: 174it [02:04,  1.49it/s]Extractor Predicting: 175it [02:05,  1.54it/s]Extractor Predicting: 176it [02:06,  1.52it/s]Extractor Predicting: 177it [02:06,  1.49it/s]Extractor Predicting: 178it [02:07,  1.48it/s]Extractor Predicting: 179it [02:08,  1.46it/s]Extractor Predicting: 180it [02:08,  1.46it/s]Extractor Predicting: 181it [02:09,  1.49it/s]Extractor Predicting: 182it [02:10,  1.48it/s]Extractor Predicting: 183it [02:10,  1.48it/s]Extractor Predicting: 184it [02:11,  1.49it/s]Extractor Predicting: 185it [02:12,  1.39it/s]Extractor Predicting: 186it [02:13,  1.42it/s]Extractor Predicting: 187it [02:13,  1.56it/s]Extractor Predicting: 187it [02:13,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6272040302267002,
  "recall": 0.051192434210526314,
  "score": 0.09465881011214598,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:11,  1.58it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.53it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.56it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.47it/s]Extractor Predicting: 37it [00:23,  1.49it/s]Extractor Predicting: 38it [00:24,  1.50it/s]Extractor Predicting: 39it [00:24,  1.55it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:25,  1.60it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:27,  1.62it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:28,  1.62it/s]Extractor Predicting: 46it [00:28,  1.61it/s]Extractor Predicting: 47it [00:29,  1.56it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:30,  1.53it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:32,  1.54it/s]Extractor Predicting: 52it [00:32,  1.55it/s]Extractor Predicting: 53it [00:33,  1.60it/s]Extractor Predicting: 54it [00:34,  1.61it/s]Extractor Predicting: 55it [00:34,  1.62it/s]Extractor Predicting: 56it [00:35,  1.62it/s]Extractor Predicting: 57it [00:36,  1.59it/s]Extractor Predicting: 58it [00:36,  1.61it/s]Extractor Predicting: 59it [00:37,  1.62it/s]Extractor Predicting: 60it [00:37,  1.64it/s]Extractor Predicting: 61it [00:38,  1.61it/s]Extractor Predicting: 62it [00:39,  1.61it/s]Extractor Predicting: 63it [00:39,  1.59it/s]Extractor Predicting: 64it [00:40,  1.58it/s]Extractor Predicting: 65it [00:40,  1.59it/s]Extractor Predicting: 66it [00:41,  1.61it/s]Extractor Predicting: 67it [00:42,  1.62it/s]Extractor Predicting: 68it [00:43,  1.22it/s]Extractor Predicting: 69it [00:44,  1.33it/s]Extractor Predicting: 70it [00:44,  1.35it/s]Extractor Predicting: 71it [00:45,  1.43it/s]Extractor Predicting: 72it [00:46,  1.47it/s]Extractor Predicting: 73it [00:46,  1.55it/s]Extractor Predicting: 74it [00:47,  1.58it/s]Extractor Predicting: 75it [00:47,  1.60it/s]Extractor Predicting: 76it [00:48,  1.54it/s]Extractor Predicting: 77it [00:49,  1.58it/s]Extractor Predicting: 78it [00:49,  1.62it/s]Extractor Predicting: 79it [00:50,  1.61it/s]Extractor Predicting: 80it [00:50,  1.63it/s]Extractor Predicting: 81it [00:51,  1.64it/s]Extractor Predicting: 82it [00:52,  1.61it/s]Extractor Predicting: 83it [00:52,  1.61it/s]Extractor Predicting: 84it [00:53,  1.55it/s]Extractor Predicting: 85it [00:54,  1.59it/s]Extractor Predicting: 86it [00:54,  1.59it/s]Extractor Predicting: 87it [00:55,  1.64it/s]Extractor Predicting: 88it [00:55,  1.68it/s]Extractor Predicting: 89it [00:56,  1.66it/s]Extractor Predicting: 90it [00:57,  1.60it/s]Extractor Predicting: 91it [00:57,  1.66it/s]Extractor Predicting: 92it [00:58,  1.69it/s]Extractor Predicting: 93it [00:58,  1.66it/s]Extractor Predicting: 94it [00:59,  1.68it/s]Extractor Predicting: 95it [01:00,  1.68it/s]Extractor Predicting: 96it [01:00,  1.70it/s]Extractor Predicting: 97it [01:01,  1.67it/s]Extractor Predicting: 98it [01:01,  1.66it/s]Extractor Predicting: 99it [01:02,  1.62it/s]Extractor Predicting: 100it [01:03,  1.65it/s]Extractor Predicting: 101it [01:03,  1.62it/s]Extractor Predicting: 102it [01:04,  1.63it/s]Extractor Predicting: 103it [01:04,  1.61it/s]Extractor Predicting: 104it [01:05,  1.62it/s]Extractor Predicting: 105it [01:06,  1.57it/s]Extractor Predicting: 106it [01:06,  1.58it/s]Extractor Predicting: 107it [01:07,  1.60it/s]Extractor Predicting: 108it [01:08,  1.61it/s]Extractor Predicting: 109it [01:08,  1.58it/s]Extractor Predicting: 110it [01:09,  1.58it/s]Extractor Predicting: 111it [01:10,  1.56it/s]Extractor Predicting: 112it [01:10,  1.60it/s]Extractor Predicting: 113it [01:11,  1.61it/s]Extractor Predicting: 114it [01:11,  1.62it/s]Extractor Predicting: 115it [01:12,  1.60it/s]Extractor Predicting: 116it [01:13,  1.60it/s]Extractor Predicting: 117it [01:13,  1.60it/s]Extractor Predicting: 118it [01:14,  1.57it/s]Extractor Predicting: 119it [01:15,  1.59it/s]Extractor Predicting: 120it [01:15,  1.58it/s]Extractor Predicting: 121it [01:16,  1.60it/s]Extractor Predicting: 122it [01:16,  1.61it/s]Extractor Predicting: 123it [01:17,  1.61it/s]Extractor Predicting: 124it [01:18,  1.62it/s]Extractor Predicting: 125it [01:18,  1.61it/s]Extractor Predicting: 126it [01:19,  1.60it/s]Extractor Predicting: 127it [01:19,  1.62it/s]Extractor Predicting: 128it [01:20,  1.58it/s]Extractor Predicting: 129it [01:21,  1.55it/s]Extractor Predicting: 130it [01:21,  1.60it/s]Extractor Predicting: 131it [01:22,  1.60it/s]Extractor Predicting: 132it [01:23,  1.58it/s]Extractor Predicting: 133it [01:23,  1.61it/s]Extractor Predicting: 134it [01:24,  1.57it/s]Extractor Predicting: 135it [01:25,  1.58it/s]Extractor Predicting: 136it [01:25,  1.58it/s]Extractor Predicting: 137it [01:26,  1.55it/s]Extractor Predicting: 138it [01:27,  1.55it/s]Extractor Predicting: 139it [01:27,  1.57it/s]Extractor Predicting: 140it [01:28,  1.57it/s]Extractor Predicting: 141it [01:28,  1.56it/s]Extractor Predicting: 142it [01:29,  1.54it/s]Extractor Predicting: 143it [01:30,  1.55it/s]Extractor Predicting: 144it [01:30,  1.56it/s]Extractor Predicting: 145it [01:31,  1.55it/s]Extractor Predicting: 146it [01:32,  1.55it/s]Extractor Predicting: 147it [01:32,  1.55it/s]Extractor Predicting: 148it [01:33,  1.58it/s]Extractor Predicting: 149it [01:34,  1.57it/s]Extractor Predicting: 150it [01:34,  1.53it/s]Extractor Predicting: 151it [01:35,  1.51it/s]Extractor Predicting: 152it [01:36,  1.54it/s]Extractor Predicting: 153it [01:36,  1.59it/s]Extractor Predicting: 154it [01:37,  1.58it/s]Extractor Predicting: 155it [01:37,  1.61it/s]Extractor Predicting: 156it [01:38,  1.57it/s]Extractor Predicting: 157it [01:39,  1.58it/s]Extractor Predicting: 158it [01:39,  1.58it/s]Extractor Predicting: 159it [01:40,  1.57it/s]Extractor Predicting: 160it [01:41,  1.57it/s]Extractor Predicting: 161it [01:41,  1.58it/s]Extractor Predicting: 162it [01:42,  1.57it/s]Extractor Predicting: 163it [01:42,  1.59it/s]Extractor Predicting: 164it [01:43,  1.57it/s]Extractor Predicting: 165it [01:44,  1.58it/s]Extractor Predicting: 166it [01:44,  1.59it/s]Extractor Predicting: 167it [01:45,  1.40it/s]Extractor Predicting: 168it [01:46,  1.44it/s]Extractor Predicting: 169it [01:47,  1.47it/s]Extractor Predicting: 170it [01:47,  1.53it/s]Extractor Predicting: 171it [01:48,  1.60it/s]Extractor Predicting: 172it [01:48,  1.61it/s]Extractor Predicting: 173it [01:49,  1.60it/s]Extractor Predicting: 174it [01:50,  1.59it/s]Extractor Predicting: 175it [01:50,  1.54it/s]Extractor Predicting: 176it [01:51,  1.53it/s]Extractor Predicting: 177it [01:52,  1.56it/s]Extractor Predicting: 178it [01:52,  1.55it/s]Extractor Predicting: 179it [01:53,  1.56it/s]Extractor Predicting: 180it [01:54,  1.53it/s]Extractor Predicting: 181it [01:54,  1.53it/s]Extractor Predicting: 182it [01:55,  1.54it/s]Extractor Predicting: 183it [01:56,  1.52it/s]Extractor Predicting: 184it [01:56,  1.52it/s]Extractor Predicting: 185it [01:57,  1.53it/s]Extractor Predicting: 186it [01:57,  1.54it/s]Extractor Predicting: 187it [01:58,  1.52it/s]Extractor Predicting: 188it [01:59,  1.55it/s]Extractor Predicting: 189it [01:59,  1.56it/s]Extractor Predicting: 190it [02:00,  1.55it/s]Extractor Predicting: 191it [02:01,  1.57it/s]Extractor Predicting: 192it [02:01,  1.56it/s]Extractor Predicting: 193it [02:02,  1.56it/s]Extractor Predicting: 194it [02:03,  1.57it/s]Extractor Predicting: 195it [02:03,  1.61it/s]Extractor Predicting: 196it [02:04,  1.63it/s]Extractor Predicting: 197it [02:04,  1.62it/s]Extractor Predicting: 198it [02:05,  1.62it/s]Extractor Predicting: 199it [02:06,  1.61it/s]Extractor Predicting: 200it [02:06,  1.62it/s]Extractor Predicting: 201it [02:07,  1.60it/s]Extractor Predicting: 202it [02:08,  1.60it/s]Extractor Predicting: 203it [02:08,  1.60it/s]Extractor Predicting: 204it [02:09,  1.59it/s]Extractor Predicting: 205it [02:09,  1.58it/s]Extractor Predicting: 206it [02:10,  1.60it/s]Extractor Predicting: 207it [02:11,  1.59it/s]Extractor Predicting: 208it [02:11,  1.63it/s]Extractor Predicting: 209it [02:12,  1.64it/s]Extractor Predicting: 210it [02:12,  1.61it/s]Extractor Predicting: 211it [02:13,  1.63it/s]Extractor Predicting: 212it [02:14,  1.63it/s]Extractor Predicting: 213it [02:14,  1.60it/s]Extractor Predicting: 214it [02:15,  1.57it/s]Extractor Predicting: 215it [02:16,  1.58it/s]Extractor Predicting: 216it [02:16,  1.57it/s]Extractor Predicting: 217it [02:17,  1.61it/s]Extractor Predicting: 218it [02:18,  1.58it/s]Extractor Predicting: 219it [02:18,  1.56it/s]Extractor Predicting: 220it [02:19,  1.60it/s]Extractor Predicting: 221it [02:19,  1.56it/s]Extractor Predicting: 222it [02:20,  1.56it/s]Extractor Predicting: 223it [02:21,  1.57it/s]Extractor Predicting: 224it [02:21,  1.59it/s]Extractor Predicting: 225it [02:22,  1.63it/s]Extractor Predicting: 226it [02:23,  1.63it/s]Extractor Predicting: 227it [02:23,  1.64it/s]Extractor Predicting: 228it [02:24,  1.64it/s]Extractor Predicting: 229it [02:24,  1.63it/s]Extractor Predicting: 230it [02:25,  1.65it/s]Extractor Predicting: 231it [02:26,  1.65it/s]Extractor Predicting: 232it [02:26,  1.65it/s]Extractor Predicting: 233it [02:27,  1.64it/s]Extractor Predicting: 234it [02:27,  1.60it/s]Extractor Predicting: 235it [02:28,  1.59it/s]Extractor Predicting: 236it [02:29,  1.60it/s]Extractor Predicting: 237it [02:29,  1.58it/s]Extractor Predicting: 238it [02:30,  1.60it/s]Extractor Predicting: 239it [02:31,  1.59it/s]Extractor Predicting: 240it [02:31,  1.61it/s]Extractor Predicting: 241it [02:32,  1.64it/s]Extractor Predicting: 242it [02:32,  1.62it/s]Extractor Predicting: 243it [02:33,  1.65it/s]Extractor Predicting: 244it [02:34,  1.65it/s]Extractor Predicting: 245it [02:34,  1.61it/s]Extractor Predicting: 246it [02:35,  1.64it/s]Extractor Predicting: 247it [02:35,  1.62it/s]Extractor Predicting: 248it [02:36,  1.64it/s]Extractor Predicting: 249it [02:37,  1.63it/s]Extractor Predicting: 250it [02:37,  1.68it/s]Extractor Predicting: 251it [02:38,  1.65it/s]Extractor Predicting: 252it [02:38,  1.67it/s]Extractor Predicting: 253it [02:39,  1.71it/s]Extractor Predicting: 254it [02:40,  1.66it/s]Extractor Predicting: 255it [02:40,  1.66it/s]Extractor Predicting: 256it [02:41,  1.65it/s]Extractor Predicting: 257it [02:41,  1.65it/s]Extractor Predicting: 258it [02:42,  1.65it/s]Extractor Predicting: 259it [02:43,  1.64it/s]Extractor Predicting: 260it [02:43,  1.64it/s]Extractor Predicting: 261it [02:44,  1.60it/s]Extractor Predicting: 262it [02:45,  1.61it/s]Extractor Predicting: 263it [02:45,  1.56it/s]Extractor Predicting: 264it [02:46,  1.59it/s]Extractor Predicting: 265it [02:47,  1.58it/s]Extractor Predicting: 266it [02:47,  1.56it/s]Extractor Predicting: 267it [02:48,  1.56it/s]Extractor Predicting: 268it [02:48,  1.56it/s]Extractor Predicting: 269it [02:49,  1.53it/s]Extractor Predicting: 270it [02:50,  1.55it/s]Extractor Predicting: 271it [02:50,  1.56it/s]Extractor Predicting: 272it [02:51,  1.59it/s]Extractor Predicting: 273it [02:52,  1.58it/s]Extractor Predicting: 274it [02:52,  1.56it/s]Extractor Predicting: 275it [02:53,  1.56it/s]Extractor Predicting: 276it [02:54,  1.59it/s]Extractor Predicting: 277it [02:54,  1.58it/s]Extractor Predicting: 278it [02:55,  1.58it/s]Extractor Predicting: 279it [02:55,  1.57it/s]Extractor Predicting: 280it [02:56,  1.54it/s]Extractor Predicting: 281it [02:57,  1.55it/s]Extractor Predicting: 282it [02:57,  1.55it/s]Extractor Predicting: 283it [02:58,  1.56it/s]Extractor Predicting: 284it [02:59,  1.58it/s]Extractor Predicting: 285it [02:59,  1.60it/s]Extractor Predicting: 286it [03:00,  1.60it/s]Extractor Predicting: 287it [03:01,  1.57it/s]Extractor Predicting: 288it [03:01,  1.58it/s]Extractor Predicting: 289it [03:02,  1.56it/s]Extractor Predicting: 290it [03:03,  1.54it/s]Extractor Predicting: 291it [03:03,  1.55it/s]Extractor Predicting: 292it [03:04,  1.38it/s]Extractor Predicting: 293it [03:05,  1.42it/s]Extractor Predicting: 294it [03:05,  1.43it/s]Extractor Predicting: 295it [03:06,  1.48it/s]Extractor Predicting: 296it [03:07,  1.50it/s]Extractor Predicting: 297it [03:07,  1.51it/s]Extractor Predicting: 298it [03:08,  1.53it/s]Extractor Predicting: 299it [03:09,  1.53it/s]Extractor Predicting: 300it [03:09,  1.48it/s]Extractor Predicting: 301it [03:10,  1.45it/s]Extractor Predicting: 302it [03:11,  1.43it/s]Extractor Predicting: 303it [03:11,  1.47it/s]Extractor Predicting: 304it [03:12,  1.43it/s]Extractor Predicting: 305it [03:13,  1.40it/s]Extractor Predicting: 306it [03:14,  1.47it/s]Extractor Predicting: 307it [03:14,  1.52it/s]Extractor Predicting: 308it [03:15,  1.54it/s]Extractor Predicting: 309it [03:15,  1.61it/s]Extractor Predicting: 310it [03:16,  1.59it/s]Extractor Predicting: 311it [03:17,  1.54it/s]Extractor Predicting: 312it [03:17,  1.56it/s]Extractor Predicting: 313it [03:18,  1.57it/s]Extractor Predicting: 314it [03:19,  1.55it/s]Extractor Predicting: 315it [03:19,  1.62it/s]Extractor Predicting: 316it [03:20,  1.63it/s]Extractor Predicting: 317it [03:20,  1.62it/s]Extractor Predicting: 318it [03:21,  1.60it/s]Extractor Predicting: 319it [03:22,  1.58it/s]Extractor Predicting: 320it [03:22,  1.60it/s]Extractor Predicting: 321it [03:23,  1.62it/s]Extractor Predicting: 322it [03:23,  1.65it/s]Extractor Predicting: 323it [03:24,  1.65it/s]Extractor Predicting: 324it [03:25,  1.65it/s]Extractor Predicting: 325it [03:25,  1.64it/s]Extractor Predicting: 326it [03:26,  1.66it/s]Extractor Predicting: 327it [03:26,  1.66it/s]Extractor Predicting: 328it [03:27,  1.64it/s]Extractor Predicting: 329it [03:28,  1.62it/s]Extractor Predicting: 330it [03:28,  1.60it/s]Extractor Predicting: 331it [03:29,  1.60it/s]Extractor Predicting: 332it [03:30,  1.62it/s]Extractor Predicting: 333it [03:30,  1.60it/s]Extractor Predicting: 334it [03:31,  1.60it/s]Extractor Predicting: 335it [03:31,  1.60it/s]Extractor Predicting: 336it [03:32,  1.63it/s]Extractor Predicting: 337it [03:33,  1.62it/s]Extractor Predicting: 338it [03:33,  1.63it/s]Extractor Predicting: 339it [03:34,  1.62it/s]Extractor Predicting: 340it [03:35,  1.63it/s]Extractor Predicting: 341it [03:35,  1.63it/s]Extractor Predicting: 342it [03:36,  1.60it/s]Extractor Predicting: 343it [03:36,  1.60it/s]Extractor Predicting: 344it [03:37,  1.64it/s]Extractor Predicting: 345it [03:38,  1.66it/s]Extractor Predicting: 346it [03:38,  1.68it/s]Extractor Predicting: 347it [03:39,  1.72it/s]Extractor Predicting: 348it [03:39,  1.69it/s]Extractor Predicting: 349it [03:40,  1.70it/s]Extractor Predicting: 350it [03:41,  1.69it/s]Extractor Predicting: 351it [03:41,  1.69it/s]Extractor Predicting: 352it [03:42,  1.69it/s]Extractor Predicting: 353it [03:42,  1.70it/s]Extractor Predicting: 354it [03:43,  1.70it/s]Extractor Predicting: 355it [03:43,  1.69it/s]Extractor Predicting: 356it [03:44,  1.72it/s]Extractor Predicting: 357it [03:45,  1.69it/s]Extractor Predicting: 358it [03:45,  1.69it/s]Extractor Predicting: 359it [03:46,  1.67it/s]Extractor Predicting: 360it [03:46,  1.66it/s]Extractor Predicting: 361it [03:47,  1.66it/s]Extractor Predicting: 362it [03:48,  1.67it/s]Extractor Predicting: 363it [03:48,  1.69it/s]Extractor Predicting: 364it [03:49,  1.72it/s]Extractor Predicting: 365it [03:49,  1.66it/s]Extractor Predicting: 366it [03:50,  1.62it/s]Extractor Predicting: 367it [03:51,  1.56it/s]Extractor Predicting: 368it [03:51,  1.54it/s]Extractor Predicting: 369it [03:52,  1.60it/s]Extractor Predicting: 370it [03:53,  1.56it/s]Extractor Predicting: 371it [03:53,  1.59it/s]Extractor Predicting: 372it [03:54,  1.61it/s]Extractor Predicting: 373it [03:54,  1.62it/s]Extractor Predicting: 374it [03:55,  1.62it/s]Extractor Predicting: 375it [03:56,  1.60it/s]Extractor Predicting: 376it [03:56,  1.59it/s]Extractor Predicting: 377it [03:57,  1.57it/s]Extractor Predicting: 378it [03:58,  1.57it/s]Extractor Predicting: 379it [03:58,  1.57it/s]Extractor Predicting: 380it [03:59,  1.56it/s]Extractor Predicting: 381it [04:00,  1.60it/s]Extractor Predicting: 382it [04:00,  1.63it/s]Extractor Predicting: 383it [04:01,  1.63it/s]Extractor Predicting: 384it [04:01,  1.64it/s]Extractor Predicting: 385it [04:02,  1.62it/s]Extractor Predicting: 386it [04:03,  1.65it/s]Extractor Predicting: 387it [04:03,  1.65it/s]Extractor Predicting: 388it [04:04,  1.68it/s]Extractor Predicting: 389it [04:04,  1.63it/s]Extractor Predicting: 390it [04:05,  1.60it/s]Extractor Predicting: 391it [04:06,  1.62it/s]Extractor Predicting: 392it [04:06,  1.68it/s]Extractor Predicting: 393it [04:07,  1.70it/s]Extractor Predicting: 394it [04:07,  1.71it/s]Extractor Predicting: 395it [04:08,  1.72it/s]Extractor Predicting: 396it [04:08,  1.74it/s]Extractor Predicting: 397it [04:09,  1.70it/s]Extractor Predicting: 398it [04:10,  1.69it/s]Extractor Predicting: 399it [04:10,  1.69it/s]Extractor Predicting: 400it [04:11,  1.66it/s]Extractor Predicting: 401it [04:11,  1.69it/s]Extractor Predicting: 402it [04:12,  1.71it/s]Extractor Predicting: 403it [04:13,  1.70it/s]Extractor Predicting: 404it [04:13,  1.70it/s]Extractor Predicting: 405it [04:14,  1.66it/s]Extractor Predicting: 406it [04:14,  1.67it/s]Extractor Predicting: 407it [04:15,  1.67it/s]Extractor Predicting: 408it [04:16,  1.69it/s]Extractor Predicting: 409it [04:17,  1.19it/s]Extractor Predicting: 410it [04:18,  1.30it/s]Extractor Predicting: 411it [04:18,  1.37it/s]Extractor Predicting: 412it [04:19,  1.43it/s]Extractor Predicting: 413it [04:20,  1.43it/s]Extractor Predicting: 414it [04:20,  1.42it/s]Extractor Predicting: 415it [04:21,  1.41it/s]Extractor Predicting: 416it [04:22,  1.42it/s]Extractor Predicting: 417it [04:22,  1.46it/s]Extractor Predicting: 418it [04:23,  1.50it/s]Extractor Predicting: 419it [04:24,  1.51it/s]Extractor Predicting: 420it [04:24,  1.48it/s]Extractor Predicting: 421it [04:25,  1.50it/s]Extractor Predicting: 422it [04:26,  1.50it/s]Extractor Predicting: 423it [04:26,  1.54it/s]Extractor Predicting: 424it [04:27,  1.52it/s]Extractor Predicting: 425it [04:28,  1.51it/s]Extractor Predicting: 426it [04:29,  1.33it/s]Extractor Predicting: 427it [04:29,  1.40it/s]Extractor Predicting: 428it [04:30,  1.44it/s]Extractor Predicting: 429it [04:31,  1.49it/s]Extractor Predicting: 430it [04:31,  1.52it/s]Extractor Predicting: 431it [04:32,  1.54it/s]Extractor Predicting: 432it [04:32,  1.54it/s]Extractor Predicting: 433it [04:33,  1.54it/s]Extractor Predicting: 434it [04:34,  1.55it/s]Extractor Predicting: 435it [04:34,  1.57it/s]Extractor Predicting: 436it [04:35,  1.61it/s]Extractor Predicting: 437it [04:36,  1.58it/s]Extractor Predicting: 438it [04:36,  1.60it/s]Extractor Predicting: 439it [04:37,  1.59it/s]Extractor Predicting: 440it [04:37,  1.59it/s]Extractor Predicting: 441it [04:38,  1.56it/s]Extractor Predicting: 442it [04:39,  1.57it/s]Extractor Predicting: 443it [04:39,  1.55it/s]Extractor Predicting: 444it [04:40,  1.56it/s]Extractor Predicting: 445it [04:41,  1.54it/s]Extractor Predicting: 446it [04:41,  1.55it/s]Extractor Predicting: 447it [04:42,  1.56it/s]Extractor Predicting: 448it [04:43,  1.54it/s]Extractor Predicting: 449it [04:43,  1.58it/s]Extractor Predicting: 450it [04:44,  1.61it/s]Extractor Predicting: 451it [04:44,  1.59it/s]Extractor Predicting: 452it [04:45,  1.57it/s]Extractor Predicting: 453it [04:46,  1.58it/s]Extractor Predicting: 454it [04:46,  1.57it/s]Extractor Predicting: 455it [04:47,  1.54it/s]Extractor Predicting: 456it [04:48,  1.56it/s]Extractor Predicting: 457it [04:48,  1.53it/s]Extractor Predicting: 458it [04:49,  1.55it/s]Extractor Predicting: 459it [04:50,  1.55it/s]Extractor Predicting: 460it [04:50,  1.53it/s]Extractor Predicting: 461it [04:51,  1.56it/s]Extractor Predicting: 462it [04:52,  1.56it/s]Extractor Predicting: 463it [04:52,  1.52it/s]Extractor Predicting: 464it [04:53,  1.46it/s]Extractor Predicting: 465it [04:54,  1.47it/s]Extractor Predicting: 466it [04:54,  1.50it/s]Extractor Predicting: 467it [04:55,  1.55it/s]Extractor Predicting: 468it [04:56,  1.57it/s]Extractor Predicting: 469it [04:56,  1.62it/s]Extractor Predicting: 470it [04:57,  1.60it/s]Extractor Predicting: 471it [04:57,  1.57it/s]Extractor Predicting: 472it [04:58,  1.58it/s]Extractor Predicting: 473it [04:59,  1.55it/s]Extractor Predicting: 474it [04:59,  1.54it/s]Extractor Predicting: 475it [05:00,  1.54it/s]Extractor Predicting: 476it [05:01,  1.56it/s]Extractor Predicting: 477it [05:01,  1.56it/s]Extractor Predicting: 478it [05:02,  1.56it/s]Extractor Predicting: 479it [05:03,  1.50it/s]Extractor Predicting: 480it [05:03,  1.47it/s]Extractor Predicting: 481it [05:04,  1.46it/s]Extractor Predicting: 482it [05:05,  1.46it/s]Extractor Predicting: 483it [05:05,  1.44it/s]Extractor Predicting: 484it [05:06,  1.43it/s]Extractor Predicting: 485it [05:07,  1.43it/s]Extractor Predicting: 486it [05:08,  1.47it/s]Extractor Predicting: 487it [05:08,  1.50it/s]Extractor Predicting: 488it [05:09,  1.47it/s]Extractor Predicting: 489it [05:10,  1.49it/s]Extractor Predicting: 490it [05:10,  1.45it/s]Extractor Predicting: 491it [05:11,  1.45it/s]Extractor Predicting: 492it [05:12,  1.45it/s]Extractor Predicting: 493it [05:12,  1.47it/s]Extractor Predicting: 494it [05:13,  1.46it/s]Extractor Predicting: 495it [05:14,  1.45it/s]Extractor Predicting: 496it [05:14,  1.46it/s]Extractor Predicting: 497it [05:15,  1.45it/s]Extractor Predicting: 498it [05:16,  1.47it/s]Extractor Predicting: 499it [05:16,  1.43it/s]Extractor Predicting: 500it [05:17,  1.45it/s]Extractor Predicting: 501it [05:18,  1.47it/s]Extractor Predicting: 502it [05:18,  1.44it/s]Extractor Predicting: 503it [05:19,  1.43it/s]Extractor Predicting: 504it [05:20,  1.40it/s]Extractor Predicting: 505it [05:21,  1.40it/s]Extractor Predicting: 506it [05:21,  1.39it/s]Extractor Predicting: 507it [05:22,  1.42it/s]Extractor Predicting: 508it [05:23,  1.41it/s]Extractor Predicting: 508it [05:23,  1.57it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.27560795873249816,
  "recall": 0.030703554716361547,
  "score": 0.05525188358694046,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.50it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.58it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.63it/s]Extractor Predicting: 34it [00:21,  1.52it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:23,  1.57it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:24,  1.59it/s]Extractor Predicting: 39it [00:25,  1.59it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:26,  1.61it/s]Extractor Predicting: 42it [00:26,  1.65it/s]Extractor Predicting: 43it [00:27,  1.64it/s]Extractor Predicting: 44it [00:28,  1.68it/s]Extractor Predicting: 45it [00:28,  1.64it/s]Extractor Predicting: 46it [00:29,  1.64it/s]Extractor Predicting: 47it [00:29,  1.63it/s]Extractor Predicting: 48it [00:30,  1.61it/s]Extractor Predicting: 49it [00:31,  1.60it/s]Extractor Predicting: 50it [00:31,  1.59it/s]Extractor Predicting: 51it [00:32,  1.57it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:33,  1.57it/s]Extractor Predicting: 54it [00:34,  1.53it/s]Extractor Predicting: 55it [00:35,  1.55it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.58it/s]Extractor Predicting: 58it [00:36,  1.58it/s]Extractor Predicting: 59it [00:37,  1.59it/s]Extractor Predicting: 60it [00:38,  1.57it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:39,  1.59it/s]Extractor Predicting: 63it [00:40,  1.59it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:41,  1.55it/s]Extractor Predicting: 66it [00:42,  1.58it/s]Extractor Predicting: 67it [00:42,  1.53it/s]Extractor Predicting: 68it [00:43,  1.52it/s]Extractor Predicting: 69it [00:44,  1.49it/s]Extractor Predicting: 70it [00:44,  1.49it/s]Extractor Predicting: 71it [00:45,  1.49it/s]Extractor Predicting: 72it [00:46,  1.50it/s]Extractor Predicting: 73it [00:46,  1.51it/s]Extractor Predicting: 74it [00:47,  1.48it/s]Extractor Predicting: 75it [00:47,  1.68it/s]Extractor Predicting: 75it [00:47,  1.57it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6204081632653061,
  "recall": 0.03828715365239295,
  "score": 0.07212336892052194,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:29, 14.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:31<04:50, 16.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:50<04:54, 17.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:06<04:32, 17.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:24<04:17, 17.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:41<03:58, 17.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:56<03:36, 16.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:12<03:16, 16.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:29<03:01, 16.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:46<02:44, 16.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:03<02:30, 16.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:19<02:11, 16.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:35<01:55, 16.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:51<01:37, 16.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:05<01:18, 15.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:26<01:08, 17.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:40<00:49, 16.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:54<00:31, 15.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:10<00:15, 15.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:28<00:00, 16.38s/it]Generating: 100%|██████████| 20/20 [05:28<00:00, 16.43s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('soap', 'product or material produced', '', 'The first production is still an early and successful creation to the Danish market , although later products were marketed as an alternative to soap in the late 1970s .')"}}
['Relation : said to be the same as . Context : The Cistercian Library contains a number of Latin volumes and compendiums of books belonging to the Church of Rome from Antiqua to the 1st Century AD , dating back to the reign of Constantine I . Head Entity : Roman , Tail Entity : Church of Rome .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 626, 'raw': 800}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7825, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : student .', 'success_rate': 0.8536931818181818, 'errors': {''}}
['Relation : winner . Context : Later in 2008 , the winner of the 2010 FIFA World Cup squad match against Australia at Wembley was awarded the Victoria Cross . Head Entity : 2010 FIFA World Cup , Tail Entity : 2010 Victoria Cross .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 568, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 623, 'raw': 800}
{'prompt': 'Relation : winner .', 'success_rate': 0.77875, 'errors': {'', "('Brooklyn Dodgers', 'winner', '', 'During the 1950s , the Yankees beat the White Sox and the Brooklyn Dodgers , but lost to the Boston Red Sox on the road .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 559, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7903645833333334, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8355978260869565, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8721590909090909, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8274456521739131, 'errors': {''}}
['Relation : given name . Context : The name Möbius comes from the Greek word for son of God ( παὶ ) . Head Entity : μ , Tail Entity : Möbius .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8233695652173914, 'errors': {'', "('Army Reserve', 'given name', '', 'He entered the Army as a sergeant in 1863 and then went into the Royal Artillery to join the British Army Reserve .')", 'too many values to unpack (expected 2)', "('Communist International', 'given name', '', 'He would later serve as a diplomat , and was elected as a member of the Communist International committee for the Second International in Budapest , Hungary .')", "('Miss Universe', 'given name', '', 'She won the inaugural Miss Universe s 2015 competition at the 2015 Miss Universe Brazil 2014 Miss Universe Brasil on August 30 , 2015 .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Cleveland Cavaliers', 'lyrics by', '', 'After a brief bout in February with the Detroit Pistons , Lautner agreed to a trade from the Cleveland Cavaliers to the Los Angeles Clippers before free agency took place in November 2012 .')"}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : movement .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 364, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 412, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 480, 'raw': 640}
{'target': 600, 'success': 503, 'raw': 672}
{'target': 600, 'success': 525, 'raw': 704}
{'target': 600, 'success': 549, 'raw': 736}
{'target': 600, 'success': 570, 'raw': 768}
{'target': 600, 'success': 595, 'raw': 800}
{'target': 600, 'success': 620, 'raw': 832}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.7451923076923077, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.8464673913043478, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 15622
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15722, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.32it/s]Extractor Estimating: 2it [00:01,  1.41it/s]Extractor Estimating: 3it [00:02,  1.40it/s]Extractor Estimating: 4it [00:02,  1.41it/s]Extractor Estimating: 5it [00:03,  1.44it/s]Extractor Estimating: 6it [00:04,  1.44it/s]Extractor Estimating: 7it [00:04,  1.46it/s]Extractor Estimating: 8it [00:05,  1.47it/s]Extractor Estimating: 9it [00:06,  1.47it/s]Extractor Estimating: 10it [00:06,  1.50it/s]Extractor Estimating: 11it [00:07,  1.48it/s]Extractor Estimating: 12it [00:08,  1.44it/s]Extractor Estimating: 13it [00:08,  1.45it/s]Extractor Estimating: 14it [00:09,  1.48it/s]Extractor Estimating: 15it [00:10,  1.48it/s]Extractor Estimating: 16it [00:10,  1.49it/s]Extractor Estimating: 17it [00:11,  1.48it/s]Extractor Estimating: 18it [00:12,  1.52it/s]Extractor Estimating: 19it [00:12,  1.51it/s]Extractor Estimating: 20it [00:13,  1.48it/s]Extractor Estimating: 21it [00:14,  1.45it/s]Extractor Estimating: 22it [00:15,  1.41it/s]Extractor Estimating: 23it [00:15,  1.43it/s]Extractor Estimating: 24it [00:16,  1.45it/s]Extractor Estimating: 25it [00:17,  1.46it/s]Extractor Estimating: 26it [00:17,  1.55it/s]Extractor Estimating: 27it [00:18,  1.52it/s]Extractor Estimating: 28it [00:18,  1.61it/s]Extractor Estimating: 29it [00:19,  1.61it/s]Extractor Estimating: 30it [00:20,  1.62it/s]Extractor Estimating: 31it [00:20,  1.56it/s]Extractor Estimating: 32it [00:21,  1.56it/s]Extractor Estimating: 33it [00:22,  1.57it/s]Extractor Estimating: 34it [00:22,  1.56it/s]Extractor Estimating: 35it [00:23,  1.59it/s]Extractor Estimating: 36it [00:23,  1.59it/s]Extractor Estimating: 37it [00:24,  1.65it/s]Extractor Estimating: 38it [00:25,  1.69it/s]Extractor Estimating: 39it [00:25,  1.68it/s]Extractor Estimating: 40it [00:26,  1.67it/s]Extractor Estimating: 41it [00:26,  1.63it/s]Extractor Estimating: 42it [00:27,  1.61it/s]Extractor Estimating: 43it [00:28,  1.68it/s]Extractor Estimating: 44it [00:28,  1.72it/s]Extractor Estimating: 45it [00:29,  1.70it/s]Extractor Estimating: 46it [00:29,  1.71it/s]Extractor Estimating: 47it [00:30,  1.61it/s]Extractor Estimating: 48it [00:31,  1.63it/s]Extractor Estimating: 49it [00:31,  1.67it/s]Extractor Estimating: 50it [00:32,  1.63it/s]Extractor Estimating: 51it [00:33,  1.61it/s]Extractor Estimating: 52it [00:33,  1.61it/s]Extractor Estimating: 53it [00:34,  1.62it/s]Extractor Estimating: 54it [00:34,  1.60it/s]Extractor Estimating: 55it [00:35,  1.56it/s]Extractor Estimating: 56it [00:36,  1.54it/s]Extractor Estimating: 57it [00:36,  1.54it/s]Extractor Estimating: 58it [00:37,  1.58it/s]Extractor Estimating: 59it [00:38,  1.58it/s]Extractor Estimating: 60it [00:38,  1.61it/s]Extractor Estimating: 61it [00:39,  1.62it/s]Extractor Estimating: 62it [00:39,  1.62it/s]Extractor Estimating: 63it [00:40,  1.62it/s]Extractor Estimating: 64it [00:41,  1.64it/s]Extractor Estimating: 65it [00:41,  1.59it/s]Extractor Estimating: 66it [00:42,  1.60it/s]Extractor Estimating: 67it [00:43,  1.62it/s]Extractor Estimating: 68it [00:43,  1.57it/s]Extractor Estimating: 69it [00:44,  1.58it/s]Extractor Estimating: 70it [00:44,  1.61it/s]Extractor Estimating: 71it [00:45,  1.67it/s]Extractor Estimating: 72it [00:46,  1.63it/s]Extractor Estimating: 73it [00:46,  1.62it/s]Extractor Estimating: 74it [00:47,  1.62it/s]Extractor Estimating: 75it [00:47,  1.62it/s]Extractor Estimating: 76it [00:48,  1.60it/s]Extractor Estimating: 77it [00:49,  1.58it/s]Extractor Estimating: 78it [00:49,  1.60it/s]Extractor Estimating: 79it [00:50,  1.58it/s]Extractor Estimating: 80it [00:51,  1.61it/s]Extractor Estimating: 81it [00:51,  1.59it/s]Extractor Estimating: 82it [00:52,  1.63it/s]Extractor Estimating: 83it [00:52,  1.60it/s]Extractor Estimating: 84it [00:53,  1.57it/s]Extractor Estimating: 85it [00:54,  1.53it/s]Extractor Estimating: 86it [00:54,  1.55it/s]Extractor Estimating: 87it [00:55,  1.57it/s]Extractor Estimating: 88it [00:56,  1.40it/s]Extractor Estimating: 89it [00:57,  1.42it/s]Extractor Estimating: 90it [00:57,  1.46it/s]Extractor Estimating: 91it [00:58,  1.51it/s]Extractor Estimating: 92it [00:59,  1.50it/s]Extractor Estimating: 93it [00:59,  1.52it/s]Extractor Estimating: 94it [01:00,  1.51it/s]Extractor Estimating: 95it [01:01,  1.56it/s]Extractor Estimating: 96it [01:01,  1.51it/s]Extractor Estimating: 97it [01:02,  1.51it/s]Extractor Estimating: 98it [01:02,  1.57it/s]Extractor Estimating: 99it [01:03,  1.60it/s]Extractor Estimating: 100it [01:04,  1.58it/s]Extractor Estimating: 101it [01:04,  1.62it/s]Extractor Estimating: 102it [01:05,  1.55it/s]Extractor Estimating: 103it [01:06,  1.53it/s]Extractor Estimating: 104it [01:06,  1.59it/s]Extractor Estimating: 105it [01:07,  1.60it/s]Extractor Estimating: 106it [01:08,  1.56it/s]Extractor Estimating: 107it [01:08,  1.58it/s]Extractor Estimating: 108it [01:09,  1.59it/s]Extractor Estimating: 109it [01:09,  1.63it/s]Extractor Estimating: 110it [01:10,  1.63it/s]Extractor Estimating: 111it [01:11,  1.63it/s]Extractor Estimating: 112it [01:11,  1.63it/s]Extractor Estimating: 113it [01:12,  1.59it/s]Extractor Estimating: 114it [01:12,  1.62it/s]Extractor Estimating: 115it [01:13,  1.64it/s]Extractor Estimating: 116it [01:14,  1.65it/s]Extractor Estimating: 117it [01:14,  1.58it/s]Extractor Estimating: 118it [01:15,  1.59it/s]Extractor Estimating: 119it [01:16,  1.57it/s]Extractor Estimating: 120it [01:16,  1.56it/s]Extractor Estimating: 121it [01:17,  1.59it/s]Extractor Estimating: 122it [01:17,  1.59it/s]Extractor Estimating: 123it [01:18,  1.61it/s]Extractor Estimating: 124it [01:19,  1.62it/s]Extractor Estimating: 125it [01:19,  1.58it/s]Extractor Estimating: 126it [01:20,  1.59it/s]Extractor Estimating: 127it [01:21,  1.57it/s]Extractor Estimating: 128it [01:21,  1.61it/s]Extractor Estimating: 129it [01:22,  1.57it/s]Extractor Estimating: 130it [01:22,  1.63it/s]Extractor Estimating: 131it [01:23,  1.63it/s]Extractor Estimating: 132it [01:24,  1.65it/s]Extractor Estimating: 133it [01:24,  1.64it/s]Extractor Estimating: 134it [01:25,  1.64it/s]Extractor Estimating: 135it [01:25,  1.64it/s]Extractor Estimating: 136it [01:26,  1.59it/s]Extractor Estimating: 137it [01:27,  1.58it/s]Extractor Estimating: 138it [01:28,  1.23it/s]Extractor Estimating: 139it [01:29,  1.31it/s]Extractor Estimating: 140it [01:29,  1.41it/s]Extractor Estimating: 141it [01:30,  1.46it/s]Extractor Estimating: 142it [01:30,  1.54it/s]Extractor Estimating: 143it [01:31,  1.58it/s]Extractor Estimating: 144it [01:32,  1.55it/s]Extractor Estimating: 145it [01:32,  1.57it/s]Extractor Estimating: 146it [01:33,  1.57it/s]Extractor Estimating: 147it [01:34,  1.62it/s]Extractor Estimating: 148it [01:34,  1.63it/s]Extractor Estimating: 149it [01:35,  1.63it/s]Extractor Estimating: 150it [01:35,  1.60it/s]Extractor Estimating: 151it [01:36,  1.59it/s]Extractor Estimating: 152it [01:37,  1.64it/s]Extractor Estimating: 153it [01:37,  1.71it/s]Extractor Estimating: 154it [01:38,  1.76it/s]Extractor Estimating: 155it [01:38,  1.77it/s]Extractor Estimating: 156it [01:39,  1.77it/s]Extractor Estimating: 157it [01:39,  1.77it/s]Extractor Estimating: 158it [01:40,  1.82it/s]Extractor Estimating: 159it [01:40,  1.86it/s]Extractor Estimating: 160it [01:41,  1.86it/s]Extractor Estimating: 161it [01:41,  1.85it/s]Extractor Estimating: 162it [01:42,  1.86it/s]Extractor Estimating: 163it [01:43,  1.82it/s]Extractor Estimating: 164it [01:43,  1.73it/s]Extractor Estimating: 165it [01:44,  1.70it/s]Extractor Estimating: 166it [01:44,  1.78it/s]Extractor Estimating: 167it [01:45,  1.73it/s]Extractor Estimating: 168it [01:45,  1.79it/s]Extractor Estimating: 169it [01:46,  1.80it/s]Extractor Estimating: 170it [01:47,  1.69it/s]Extractor Estimating: 171it [01:47,  1.70it/s]Extractor Estimating: 172it [01:48,  1.69it/s]Extractor Estimating: 173it [01:48,  1.70it/s]Extractor Estimating: 174it [01:49,  1.72it/s]Extractor Estimating: 175it [01:50,  1.77it/s]Extractor Estimating: 176it [01:50,  1.66it/s]Extractor Estimating: 177it [01:51,  1.63it/s]Extractor Estimating: 178it [01:52,  1.60it/s]Extractor Estimating: 179it [01:52,  1.56it/s]Extractor Estimating: 180it [01:53,  1.56it/s]Extractor Estimating: 181it [01:54,  1.52it/s]Extractor Estimating: 182it [01:54,  1.40it/s]Extractor Estimating: 183it [01:55,  1.43it/s]Extractor Estimating: 184it [01:56,  1.44it/s]Extractor Estimating: 185it [01:56,  1.46it/s]Extractor Estimating: 186it [01:57,  1.49it/s]Extractor Estimating: 187it [01:58,  1.50it/s]Extractor Estimating: 188it [01:58,  1.51it/s]Extractor Estimating: 189it [01:59,  1.49it/s]Extractor Estimating: 190it [02:00,  1.49it/s]Extractor Estimating: 191it [02:00,  1.49it/s]Extractor Estimating: 192it [02:01,  1.48it/s]Extractor Estimating: 193it [02:02,  1.51it/s]Extractor Estimating: 194it [02:02,  1.51it/s]Extractor Estimating: 195it [02:03,  1.50it/s]Extractor Estimating: 196it [02:04,  1.48it/s]Extractor Estimating: 197it [02:04,  1.45it/s]Extractor Estimating: 198it [02:05,  1.46it/s]Extractor Estimating: 199it [02:06,  1.49it/s]Extractor Estimating: 200it [02:06,  1.48it/s]Extractor Estimating: 201it [02:07,  1.49it/s]Extractor Estimating: 202it [02:08,  1.51it/s]Extractor Estimating: 203it [02:08,  1.52it/s]Extractor Estimating: 204it [02:09,  1.47it/s]Extractor Estimating: 205it [02:10,  1.47it/s]Extractor Estimating: 206it [02:10,  1.52it/s]Extractor Estimating: 207it [02:11,  1.53it/s]Extractor Estimating: 208it [02:12,  1.55it/s]Extractor Estimating: 209it [02:12,  1.52it/s]Extractor Estimating: 210it [02:13,  1.57it/s]Extractor Estimating: 211it [02:14,  1.60it/s]Extractor Estimating: 212it [02:14,  1.57it/s]Extractor Estimating: 213it [02:15,  1.61it/s]Extractor Estimating: 214it [02:15,  1.57it/s]Extractor Estimating: 215it [02:16,  1.57it/s]Extractor Estimating: 216it [02:17,  1.54it/s]Extractor Estimating: 217it [02:18,  1.50it/s]Extractor Estimating: 218it [02:18,  1.49it/s]Extractor Estimating: 219it [02:19,  1.46it/s]Extractor Estimating: 220it [02:20,  1.51it/s]Extractor Estimating: 221it [02:20,  1.48it/s]Extractor Estimating: 222it [02:21,  1.53it/s]Extractor Estimating: 223it [02:21,  1.53it/s]Extractor Estimating: 224it [02:22,  1.54it/s]Extractor Estimating: 225it [02:23,  1.51it/s]Extractor Estimating: 226it [02:24,  1.49it/s]Extractor Estimating: 227it [02:24,  1.49it/s]Extractor Estimating: 228it [02:25,  1.53it/s]Extractor Estimating: 229it [02:25,  1.56it/s]Extractor Estimating: 230it [02:26,  1.48it/s]Extractor Estimating: 231it [02:27,  1.44it/s]Extractor Estimating: 232it [02:28,  1.49it/s]Extractor Estimating: 233it [02:28,  1.52it/s]Extractor Estimating: 234it [02:29,  1.54it/s]Extractor Estimating: 235it [02:29,  1.53it/s]Extractor Estimating: 236it [02:30,  1.52it/s]Extractor Estimating: 237it [02:31,  1.56it/s]Extractor Estimating: 238it [02:31,  1.58it/s]Extractor Estimating: 239it [02:32,  1.60it/s]Extractor Estimating: 240it [02:33,  1.57it/s]Extractor Estimating: 241it [02:33,  1.54it/s]Extractor Estimating: 242it [02:34,  1.56it/s]Extractor Estimating: 243it [02:35,  1.51it/s]Extractor Estimating: 244it [02:35,  1.53it/s]Extractor Estimating: 245it [02:36,  1.50it/s]Extractor Estimating: 246it [02:37,  1.56it/s]Extractor Estimating: 247it [02:37,  1.58it/s]Extractor Estimating: 248it [02:38,  1.62it/s]Extractor Estimating: 249it [02:38,  1.59it/s]Extractor Estimating: 250it [02:39,  1.59it/s]Extractor Estimating: 251it [02:40,  1.61it/s]Extractor Estimating: 252it [02:40,  1.57it/s]Extractor Estimating: 253it [02:41,  1.56it/s]Extractor Estimating: 254it [02:42,  1.54it/s]Extractor Estimating: 255it [02:42,  1.56it/s]Extractor Estimating: 256it [02:43,  1.56it/s]Extractor Estimating: 257it [02:44,  1.53it/s]Extractor Estimating: 258it [02:44,  1.53it/s]Extractor Estimating: 259it [02:45,  1.57it/s]Extractor Estimating: 260it [02:45,  1.55it/s]Extractor Estimating: 261it [02:46,  1.51it/s]Extractor Estimating: 262it [02:47,  1.48it/s]Extractor Estimating: 263it [02:48,  1.48it/s]Extractor Estimating: 264it [02:48,  1.48it/s]Extractor Estimating: 265it [02:49,  1.35it/s]Extractor Estimating: 266it [02:50,  1.43it/s]Extractor Estimating: 267it [02:50,  1.46it/s]Extractor Estimating: 268it [02:51,  1.51it/s]Extractor Estimating: 269it [02:52,  1.46it/s]Extractor Estimating: 270it [02:52,  1.46it/s]Extractor Estimating: 271it [02:53,  1.48it/s]Extractor Estimating: 272it [02:54,  1.51it/s]Extractor Estimating: 273it [02:54,  1.53it/s]Extractor Estimating: 274it [02:55,  1.49it/s]Extractor Estimating: 275it [02:56,  1.47it/s]Extractor Estimating: 276it [02:56,  1.45it/s]Extractor Estimating: 277it [02:57,  1.46it/s]Extractor Estimating: 278it [02:58,  1.56it/s]Extractor Estimating: 279it [02:58,  1.56it/s]Extractor Estimating: 280it [02:59,  1.50it/s]Extractor Estimating: 281it [03:00,  1.49it/s]Extractor Estimating: 282it [03:00,  1.47it/s]Extractor Estimating: 283it [03:01,  1.48it/s]Extractor Estimating: 284it [03:02,  1.50it/s]Extractor Estimating: 285it [03:02,  1.48it/s]Extractor Estimating: 286it [03:03,  1.50it/s]Extractor Estimating: 287it [03:04,  1.48it/s]Extractor Estimating: 288it [03:06,  1.21s/it]Extractor Estimating: 289it [03:07,  1.03s/it]Extractor Estimating: 290it [03:07,  1.09it/s]Extractor Estimating: 291it [03:08,  1.18it/s]Extractor Estimating: 292it [03:09,  1.27it/s]Extractor Estimating: 293it [03:10,  1.30it/s]Extractor Estimating: 294it [03:10,  1.30it/s]Extractor Estimating: 295it [03:11,  1.34it/s]Extractor Estimating: 296it [03:12,  1.38it/s]Extractor Estimating: 297it [03:12,  1.37it/s]Extractor Estimating: 298it [03:13,  1.37it/s]Extractor Estimating: 299it [03:14,  1.44it/s]Extractor Estimating: 300it [03:14,  1.46it/s]Extractor Estimating: 301it [03:15,  1.46it/s]Extractor Estimating: 302it [03:16,  1.48it/s]Extractor Estimating: 303it [03:16,  1.47it/s]Extractor Estimating: 304it [03:17,  1.53it/s]Extractor Estimating: 305it [03:18,  1.55it/s]Extractor Estimating: 306it [03:18,  1.55it/s]Extractor Estimating: 307it [03:19,  1.55it/s]Extractor Estimating: 308it [03:20,  1.54it/s]Extractor Estimating: 309it [03:20,  1.55it/s]Extractor Estimating: 310it [03:21,  1.57it/s]Extractor Estimating: 311it [03:21,  1.59it/s]Extractor Estimating: 312it [03:22,  1.57it/s]Extractor Estimating: 313it [03:23,  1.58it/s]Extractor Estimating: 314it [03:24,  1.50it/s]Extractor Estimating: 315it [03:24,  1.52it/s]Extractor Estimating: 316it [03:25,  1.57it/s]Extractor Estimating: 317it [03:25,  1.62it/s]Extractor Estimating: 318it [03:26,  1.63it/s]Extractor Estimating: 319it [03:27,  1.64it/s]Extractor Estimating: 320it [03:27,  1.61it/s]Extractor Estimating: 321it [03:28,  1.63it/s]Extractor Estimating: 322it [03:28,  1.65it/s]Extractor Estimating: 323it [03:29,  1.62it/s]Extractor Estimating: 324it [03:30,  1.59it/s]Extractor Estimating: 325it [03:30,  1.59it/s]Extractor Estimating: 326it [03:31,  1.57it/s]Extractor Estimating: 327it [03:32,  1.59it/s]Extractor Estimating: 328it [03:32,  1.64it/s]Extractor Estimating: 329it [03:33,  1.64it/s]Extractor Estimating: 330it [03:33,  1.59it/s]Extractor Estimating: 331it [03:34,  1.60it/s]Extractor Estimating: 332it [03:35,  1.62it/s]Extractor Estimating: 333it [03:35,  1.57it/s]Extractor Estimating: 334it [03:36,  1.57it/s]Extractor Estimating: 335it [03:37,  1.53it/s]Extractor Estimating: 336it [03:37,  1.57it/s]Extractor Estimating: 337it [03:38,  1.55it/s]Extractor Estimating: 338it [03:38,  1.59it/s]Extractor Estimating: 339it [03:39,  1.60it/s]Extractor Estimating: 340it [03:40,  1.53it/s]Extractor Estimating: 341it [03:40,  1.55it/s]Extractor Estimating: 342it [03:41,  1.55it/s]Extractor Estimating: 343it [03:42,  1.37it/s]Extractor Estimating: 344it [03:43,  1.45it/s]Extractor Estimating: 345it [03:43,  1.47it/s]Extractor Estimating: 346it [03:44,  1.54it/s]Extractor Estimating: 347it [03:44,  1.58it/s]Extractor Estimating: 348it [03:45,  1.56it/s]Extractor Estimating: 349it [03:46,  1.58it/s]Extractor Estimating: 350it [03:46,  1.53it/s]Extractor Estimating: 351it [03:47,  1.52it/s]Extractor Estimating: 352it [03:48,  1.55it/s]Extractor Estimating: 353it [03:48,  1.55it/s]Extractor Estimating: 354it [03:49,  1.61it/s]Extractor Estimating: 355it [03:50,  1.58it/s]Extractor Estimating: 356it [03:50,  1.55it/s]Extractor Estimating: 357it [03:51,  1.58it/s]Extractor Estimating: 358it [03:51,  1.59it/s]Extractor Estimating: 359it [03:52,  1.57it/s]Extractor Estimating: 360it [03:53,  1.57it/s]Extractor Estimating: 361it [03:53,  1.55it/s]Extractor Estimating: 362it [03:54,  1.50it/s]Extractor Estimating: 363it [03:55,  1.53it/s]Extractor Estimating: 364it [03:55,  1.52it/s]Extractor Estimating: 365it [03:56,  1.53it/s]Extractor Estimating: 366it [03:57,  1.51it/s]Extractor Estimating: 367it [03:57,  1.49it/s]Extractor Estimating: 368it [03:58,  1.50it/s]Extractor Estimating: 369it [03:59,  1.51it/s]Extractor Estimating: 370it [04:00,  1.45it/s]Extractor Estimating: 371it [04:00,  1.45it/s]Extractor Estimating: 372it [04:01,  1.48it/s]Extractor Estimating: 373it [04:01,  1.52it/s]Extractor Estimating: 374it [04:02,  1.54it/s]Extractor Estimating: 375it [04:03,  1.52it/s]Extractor Estimating: 376it [04:03,  1.48it/s]Extractor Estimating: 377it [04:04,  1.50it/s]Extractor Estimating: 378it [04:05,  1.54it/s]Extractor Estimating: 379it [04:05,  1.56it/s]Extractor Estimating: 380it [04:06,  1.54it/s]Extractor Estimating: 381it [04:07,  1.55it/s]Extractor Estimating: 382it [04:07,  1.54it/s]Extractor Estimating: 383it [04:08,  1.50it/s]Extractor Estimating: 384it [04:09,  1.47it/s]Extractor Estimating: 385it [04:09,  1.48it/s]Extractor Estimating: 386it [04:10,  1.43it/s]Extractor Estimating: 387it [04:11,  1.49it/s]Extractor Estimating: 388it [04:11,  1.52it/s]Extractor Estimating: 389it [04:12,  1.51it/s]Extractor Estimating: 390it [04:13,  1.48it/s]Extractor Estimating: 391it [04:13,  1.49it/s]Extractor Estimating: 392it [04:14,  1.51it/s]Extractor Estimating: 393it [04:15,  1.48it/s]Extractor Estimating: 394it [04:16,  1.44it/s]Extractor Estimating: 395it [04:16,  1.46it/s]Extractor Estimating: 396it [04:17,  1.47it/s]Extractor Estimating: 397it [04:18,  1.45it/s]Extractor Estimating: 398it [04:18,  1.39it/s]Extractor Estimating: 399it [04:19,  1.39it/s]Extractor Estimating: 400it [04:20,  1.45it/s]Extractor Estimating: 401it [04:20,  1.49it/s]Extractor Estimating: 402it [04:21,  1.45it/s]Extractor Estimating: 403it [04:22,  1.48it/s]Extractor Estimating: 404it [04:22,  1.54it/s]Extractor Estimating: 405it [04:23,  1.57it/s]Extractor Estimating: 406it [04:24,  1.57it/s]Extractor Estimating: 407it [04:24,  1.58it/s]Extractor Estimating: 408it [04:25,  1.55it/s]Extractor Estimating: 409it [04:25,  1.56it/s]Extractor Estimating: 410it [04:26,  1.59it/s]Extractor Estimating: 411it [04:27,  1.56it/s]Extractor Estimating: 412it [04:27,  1.55it/s]Extractor Estimating: 413it [04:28,  1.56it/s]Extractor Estimating: 414it [04:29,  1.56it/s]Extractor Estimating: 415it [04:29,  1.59it/s]Extractor Estimating: 416it [04:30,  1.59it/s]Extractor Estimating: 417it [04:31,  1.56it/s]Extractor Estimating: 418it [04:31,  1.57it/s]Extractor Estimating: 419it [04:32,  1.55it/s]Extractor Estimating: 420it [04:32,  1.56it/s]Extractor Estimating: 421it [04:33,  1.55it/s]Extractor Estimating: 422it [04:34,  1.54it/s]Extractor Estimating: 423it [04:34,  1.57it/s]Extractor Estimating: 424it [04:35,  1.42it/s]Extractor Estimating: 425it [04:36,  1.48it/s]Extractor Estimating: 426it [04:37,  1.48it/s]Extractor Estimating: 427it [04:37,  1.50it/s]Extractor Estimating: 428it [04:38,  1.54it/s]Extractor Estimating: 429it [04:38,  1.56it/s]Extractor Estimating: 430it [04:39,  1.58it/s]Extractor Estimating: 431it [04:40,  1.58it/s]Extractor Estimating: 432it [04:40,  1.58it/s]Extractor Estimating: 433it [04:41,  1.59it/s]Extractor Estimating: 434it [04:42,  1.59it/s]Extractor Estimating: 435it [04:42,  1.59it/s]Extractor Estimating: 436it [04:43,  1.59it/s]Extractor Estimating: 437it [04:43,  1.55it/s]Extractor Estimating: 438it [04:44,  1.51it/s]Extractor Estimating: 439it [04:45,  1.56it/s]Extractor Estimating: 440it [04:45,  1.59it/s]Extractor Estimating: 441it [04:46,  1.52it/s]Extractor Estimating: 442it [04:47,  1.50it/s]Extractor Estimating: 443it [04:47,  1.49it/s]Extractor Estimating: 444it [04:48,  1.53it/s]Extractor Estimating: 445it [04:49,  1.54it/s]Extractor Estimating: 446it [04:49,  1.52it/s]Extractor Estimating: 447it [04:50,  1.53it/s]Extractor Estimating: 448it [04:51,  1.54it/s]Extractor Estimating: 449it [04:51,  1.56it/s]Extractor Estimating: 450it [04:52,  1.48it/s]Extractor Estimating: 451it [04:53,  1.48it/s]Extractor Estimating: 452it [04:53,  1.45it/s]Extractor Estimating: 453it [04:54,  1.42it/s]Extractor Estimating: 454it [04:55,  1.40it/s]Extractor Estimating: 455it [04:56,  1.39it/s]Extractor Estimating: 456it [04:56,  1.42it/s]Extractor Estimating: 457it [04:57,  1.48it/s]Extractor Estimating: 458it [04:58,  1.48it/s]Extractor Estimating: 459it [04:58,  1.48it/s]Extractor Estimating: 460it [04:59,  1.49it/s]Extractor Estimating: 461it [05:00,  1.51it/s]Extractor Estimating: 462it [05:00,  1.50it/s]Extractor Estimating: 463it [05:01,  1.48it/s]Extractor Estimating: 464it [05:02,  1.48it/s]Extractor Estimating: 465it [05:02,  1.50it/s]Extractor Estimating: 466it [05:03,  1.49it/s]Extractor Estimating: 467it [05:04,  1.50it/s]Extractor Estimating: 468it [05:04,  1.49it/s]Extractor Estimating: 469it [05:05,  1.47it/s]Extractor Estimating: 470it [05:06,  1.51it/s]Extractor Estimating: 471it [05:06,  1.42it/s]Extractor Estimating: 472it [05:07,  1.46it/s]Extractor Estimating: 473it [05:08,  1.49it/s]Extractor Estimating: 474it [05:08,  1.55it/s]Extractor Estimating: 475it [05:09,  1.53it/s]Extractor Estimating: 476it [05:10,  1.54it/s]Extractor Estimating: 477it [05:10,  1.53it/s]Extractor Estimating: 478it [05:11,  1.55it/s]Extractor Estimating: 479it [05:11,  1.57it/s]Extractor Estimating: 480it [05:12,  1.54it/s]Extractor Estimating: 481it [05:13,  1.57it/s]Extractor Estimating: 482it [05:13,  1.52it/s]Extractor Estimating: 483it [05:14,  1.51it/s]Extractor Estimating: 484it [05:15,  1.55it/s]Extractor Estimating: 485it [05:15,  1.55it/s]Extractor Estimating: 486it [05:16,  1.57it/s]Extractor Estimating: 487it [05:17,  1.58it/s]Extractor Estimating: 488it [05:17,  1.55it/s]Extractor Estimating: 489it [05:18,  1.58it/s]Extractor Estimating: 490it [05:19,  1.61it/s]Extractor Estimating: 491it [05:19,  1.59it/s]Extractor Estimating: 492it [05:20,  1.58it/s]Extractor Estimating: 493it [05:20,  1.55it/s]Extractor Estimating: 494it [05:21,  1.46it/s]Extractor Estimating: 495it [05:22,  1.48it/s]Extractor Estimating: 496it [05:23,  1.51it/s]Extractor Estimating: 497it [05:23,  1.49it/s]Extractor Estimating: 498it [05:24,  1.49it/s]Extractor Estimating: 499it [05:25,  1.52it/s]Extractor Estimating: 500it [05:25,  1.60it/s]Extractor Estimating: 500it [05:25,  1.54it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9905 mean pseudo reward: 0.9456768819782909
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 29452
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 29552, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=29552, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.342, loss:718.4890
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.171, loss:698.8321
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.092, loss:680.8299
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.090, loss:713.8970
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 87, avg_time 1.101, loss:652.9805
>> valid entity prec:0.4693, rec:0.4168, f1:0.4415
>> valid relation prec:0.1402, rec:0.0305, f1:0.0501
>> valid relation with NER prec:0.1402, rec:0.0305, f1:0.0501
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 187, avg_time 2.837, loss:669.0873
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 287, avg_time 1.082, loss:677.0345
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 387, avg_time 1.086, loss:725.0754
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 74, avg_time 1.090, loss:685.3972
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 174, avg_time 1.074, loss:677.1635
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4337, rec:0.4838, f1:0.4574
>> valid relation prec:0.1746, rec:0.0264, f1:0.0458
>> valid relation with NER prec:0.1746, rec:0.0264, f1:0.0458
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 274, avg_time 2.838, loss:701.4756
g_step 1200, step 374, avg_time 1.107, loss:748.7204
g_step 1300, step 61, avg_time 1.109, loss:646.1516
g_step 1400, step 161, avg_time 1.084, loss:684.8149
g_step 1500, step 261, avg_time 1.092, loss:690.6220
>> valid entity prec:0.4685, rec:0.3560, f1:0.4046
>> valid relation prec:0.0864, rec:0.0134, f1:0.0232
>> valid relation with NER prec:0.0864, rec:0.0134, f1:0.0232
g_step 1600, step 361, avg_time 2.826, loss:697.2788
g_step 1700, step 48, avg_time 1.083, loss:677.7171
g_step 1800, step 148, avg_time 1.092, loss:648.9307
g_step 1900, step 248, avg_time 1.078, loss:671.3549
g_step 2000, step 348, avg_time 1.091, loss:663.3176
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5054, rec:0.4777, f1:0.4911
>> valid relation prec:0.1484, rec:0.0299, f1:0.0497
>> valid relation with NER prec:0.1484, rec:0.0299, f1:0.0497
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 35, avg_time 2.833, loss:629.7327
g_step 2200, step 135, avg_time 1.089, loss:615.9826
g_step 2300, step 235, avg_time 1.096, loss:652.5836
g_step 2400, step 335, avg_time 1.082, loss:635.7590
g_step 2500, step 22, avg_time 1.089, loss:618.7427
>> valid entity prec:0.5364, rec:0.4014, f1:0.4592
>> valid relation prec:0.0806, rec:0.0169, f1:0.0279
>> valid relation with NER prec:0.0806, rec:0.0169, f1:0.0279
g_step 2600, step 122, avg_time 2.818, loss:570.3840
g_step 2700, step 222, avg_time 1.090, loss:595.7812
g_step 2800, step 322, avg_time 1.093, loss:620.1889
g_step 2900, step 9, avg_time 1.082, loss:646.2271
g_step 3000, step 109, avg_time 1.085, loss:582.2056
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5087, rec:0.4039, f1:0.4503
>> valid relation prec:0.0722, rec:0.0169, f1:0.0274
>> valid relation with NER prec:0.0722, rec:0.0169, f1:0.0274
g_step 3100, step 209, avg_time 2.833, loss:589.8257
g_step 3200, step 309, avg_time 1.084, loss:579.3660
g_step 3300, step 409, avg_time 1.082, loss:593.3357
g_step 3400, step 96, avg_time 1.087, loss:546.9922
g_step 3500, step 196, avg_time 1.078, loss:526.8519
>> valid entity prec:0.4774, rec:0.4698, f1:0.4736
>> valid relation prec:0.0934, rec:0.0243, f1:0.0386
>> valid relation with NER prec:0.0934, rec:0.0243, f1:0.0386
g_step 3600, step 296, avg_time 2.844, loss:584.0484
g_step 3700, step 396, avg_time 1.093, loss:605.4681
g_step 3800, step 83, avg_time 1.081, loss:542.8216
g_step 3900, step 183, avg_time 1.097, loss:541.4488
g_step 4000, step 283, avg_time 1.094, loss:570.8859
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4913, rec:0.4288, f1:0.4579
>> valid relation prec:0.0564, rec:0.0132, f1:0.0214
>> valid relation with NER prec:0.0564, rec:0.0132, f1:0.0214
g_step 4100, step 383, avg_time 2.842, loss:555.6135
g_step 4200, step 70, avg_time 1.096, loss:512.0906
g_step 4300, step 170, avg_time 1.090, loss:540.2763
g_step 4400, step 270, avg_time 1.102, loss:538.8471
g_step 4500, step 370, avg_time 1.076, loss:543.5724
>> valid entity prec:0.4840, rec:0.4759, f1:0.4799
>> valid relation prec:0.0685, rec:0.0142, f1:0.0235
>> valid relation with NER prec:0.0685, rec:0.0142, f1:0.0235
g_step 4600, step 57, avg_time 2.832, loss:512.9355
g_step 4700, step 157, avg_time 1.099, loss:510.3853
g_step 4800, step 257, avg_time 1.084, loss:527.1457
g_step 4900, step 357, avg_time 1.087, loss:539.3067
g_step 5000, step 44, avg_time 1.096, loss:506.1316
learning rate was adjusted to 0.0008
>> valid entity prec:0.4901, rec:0.4099, f1:0.4464
>> valid relation prec:0.0840, rec:0.0179, f1:0.0295
>> valid relation with NER prec:0.0840, rec:0.0179, f1:0.0295
g_step 5100, step 144, avg_time 2.825, loss:479.9801
g_step 5200, step 244, avg_time 1.094, loss:503.6776
g_step 5300, step 344, avg_time 1.092, loss:508.6377
g_step 5400, step 31, avg_time 1.083, loss:496.6869
g_step 5500, step 131, avg_time 1.075, loss:464.6006
>> valid entity prec:0.4956, rec:0.4848, f1:0.4902
>> valid relation prec:0.0872, rec:0.0196, f1:0.0320
>> valid relation with NER prec:0.0872, rec:0.0196, f1:0.0320
g_step 5600, step 231, avg_time 2.830, loss:482.6840
g_step 5700, step 331, avg_time 1.094, loss:487.7231
g_step 5800, step 18, avg_time 1.105, loss:494.4981
g_step 5900, step 118, avg_time 1.084, loss:457.1195
g_step 6000, step 218, avg_time 1.077, loss:464.7756
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4693, rec:0.4509, f1:0.4599
>> valid relation prec:0.0560, rec:0.0165, f1:0.0255
>> valid relation with NER prec:0.0560, rec:0.0165, f1:0.0255
g_step 6100, step 318, avg_time 2.840, loss:482.1476
g_step 6200, step 5, avg_time 1.093, loss:483.3056
g_step 6300, step 105, avg_time 1.071, loss:428.0107
g_step 6400, step 205, avg_time 1.083, loss:454.7989
g_step 6500, step 305, avg_time 1.100, loss:448.3566
>> valid entity prec:0.4843, rec:0.4057, f1:0.4416
>> valid relation prec:0.0893, rec:0.0233, f1:0.0369
>> valid relation with NER prec:0.0893, rec:0.0233, f1:0.0369
g_step 6600, step 405, avg_time 2.833, loss:472.5084
g_step 6700, step 92, avg_time 1.072, loss:421.1994
g_step 6800, step 192, avg_time 1.088, loss:428.8916
g_step 6900, step 292, avg_time 1.088, loss:457.7447
g_step 7000, step 392, avg_time 1.094, loss:472.6667
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5243, rec:0.3805, f1:0.4410
>> valid relation prec:0.0533, rec:0.0122, f1:0.0198
>> valid relation with NER prec:0.0533, rec:0.0122, f1:0.0198
g_step 7100, step 79, avg_time 2.806, loss:414.4696
g_step 7200, step 179, avg_time 1.086, loss:425.3756
g_step 7300, step 279, avg_time 1.098, loss:438.1175
g_step 7400, step 379, avg_time 1.080, loss:447.3435
g_step 7500, step 66, avg_time 1.097, loss:399.5551
>> valid entity prec:0.4664, rec:0.4716, f1:0.4690
>> valid relation prec:0.0614, rec:0.0220, f1:0.0324
>> valid relation with NER prec:0.0614, rec:0.0220, f1:0.0324
g_step 7600, step 166, avg_time 2.842, loss:412.7584
g_step 7700, step 266, avg_time 1.088, loss:402.6673
g_step 7800, step 366, avg_time 1.077, loss:440.0606
g_step 7900, step 53, avg_time 1.077, loss:398.7847
g_step 8000, step 153, avg_time 1.099, loss:398.5569
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5073, rec:0.4426, f1:0.4728
>> valid relation prec:0.0890, rec:0.0239, f1:0.0377
>> valid relation with NER prec:0.0890, rec:0.0239, f1:0.0377
g_step 8100, step 253, avg_time 2.809, loss:398.8254
g_step 8200, step 353, avg_time 1.082, loss:415.3937
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 04:57:47 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 04:57:47 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_04-57-47_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 04:57:48 - WARNING - datasets.builder -   Using custom data configuration default-9ecf2c3229061d44
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-9ecf2c3229061d44/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 04:57:48,405 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:57:48,406 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:57:48,407 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:57:48,408 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:57:48,417 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:57:48,421 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:57:48,421 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:57:48,421 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:57:48,421 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:57:48,421 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:57:48,421 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 04:57:48,603 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:57:51,712 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 04:57:51,721 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-9ecf2c3229061d44/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 04:57:51 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x149f9de24f80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.88ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.74ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.12ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.31ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.42ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.49ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.54ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.57ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  3.81ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.04ba/s]100%|██████████| 11/11 [00:02<00:00,  4.52ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.08ba/s] 40%|████      | 2/5 [00:00<00:00,  4.31ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.41ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.42ba/s]100%|██████████| 5/5 [00:01<00:00,  4.65ba/s]100%|██████████| 5/5 [00:01<00:00,  4.51ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  7.76ba/s] 27%|██▋       | 3/11 [00:00<00:00,  9.35ba/s] 45%|████▌     | 5/11 [00:00<00:00,  9.69ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.89ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.03ba/s]100%|██████████| 11/11 [00:01<00:00, 12.05ba/s]100%|██████████| 11/11 [00:01<00:00, 10.76ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.37ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.23ba/s]100%|██████████| 5/5 [00:00<00:00, 10.00ba/s]100%|██████████| 5/5 [00:00<00:00,  9.66ba/s]
[INFO|trainer.py:414] 2023-08-29 04:57:57,230 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 04:57:57,243 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 04:57:57,243 >>   Num examples = 10029
[INFO|trainer.py:1149] 2023-08-29 04:57:57,243 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 04:57:57,243 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 04:57:57,243 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 04:57:57,243 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 04:57:57,243 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<04:00,  3.26it/s]  0%|          | 2/785 [00:00<03:51,  3.38it/s]  0%|          | 3/785 [00:00<03:48,  3.43it/s]  1%|          | 4/785 [00:01<03:46,  3.44it/s]  1%|          | 5/785 [00:01<03:45,  3.45it/s]  1%|          | 6/785 [00:01<03:44,  3.46it/s]  1%|          | 7/785 [00:02<03:44,  3.47it/s]  1%|          | 8/785 [00:02<03:43,  3.47it/s]  1%|          | 9/785 [00:02<03:43,  3.47it/s]  1%|▏         | 10/785 [00:02<03:42,  3.48it/s]  1%|▏         | 11/785 [00:03<03:42,  3.48it/s]  2%|▏         | 12/785 [00:03<03:42,  3.48it/s]  2%|▏         | 13/785 [00:03<03:41,  3.48it/s]  2%|▏         | 14/785 [00:04<03:41,  3.48it/s]  2%|▏         | 15/785 [00:04<03:42,  3.46it/s]  2%|▏         | 16/785 [00:04<03:41,  3.47it/s]  2%|▏         | 17/785 [00:04<03:41,  3.47it/s]  2%|▏         | 18/785 [00:05<03:40,  3.47it/s]  2%|▏         | 19/785 [00:05<03:40,  3.47it/s]  3%|▎         | 20/785 [00:05<03:40,  3.48it/s]  3%|▎         | 21/785 [00:06<03:39,  3.47it/s]  3%|▎         | 22/785 [00:06<03:39,  3.48it/s]  3%|▎         | 23/785 [00:06<03:39,  3.48it/s]  3%|▎         | 24/785 [00:06<03:38,  3.48it/s]  3%|▎         | 25/785 [00:07<03:38,  3.48it/s]  3%|▎         | 26/785 [00:07<03:38,  3.47it/s]  3%|▎         | 27/785 [00:07<03:38,  3.47it/s]  4%|▎         | 28/785 [00:08<03:37,  3.47it/s]  4%|▎         | 29/785 [00:08<03:37,  3.48it/s]  4%|▍         | 30/785 [00:08<03:37,  3.48it/s]  4%|▍         | 31/785 [00:08<03:37,  3.47it/s]  4%|▍         | 32/785 [00:09<03:36,  3.47it/s]  4%|▍         | 33/785 [00:09<03:36,  3.47it/s]  4%|▍         | 34/785 [00:09<03:35,  3.48it/s]  4%|▍         | 35/785 [00:10<03:35,  3.48it/s]  5%|▍         | 36/785 [00:10<03:35,  3.48it/s]  5%|▍         | 37/785 [00:10<03:35,  3.47it/s]  5%|▍         | 38/785 [00:10<03:35,  3.47it/s]  5%|▍         | 39/785 [00:11<03:34,  3.47it/s]  5%|▌         | 40/785 [00:11<03:34,  3.47it/s]  5%|▌         | 41/785 [00:11<03:34,  3.47it/s]  5%|▌         | 42/785 [00:12<03:34,  3.47it/s]  5%|▌         | 43/785 [00:12<03:33,  3.47it/s]  6%|▌         | 44/785 [00:12<03:33,  3.47it/s]  6%|▌         | 45/785 [00:12<03:33,  3.47it/s]  6%|▌         | 46/785 [00:13<03:32,  3.47it/s]  6%|▌         | 47/785 [00:13<03:32,  3.47it/s]  6%|▌         | 48/785 [00:13<03:33,  3.46it/s]  6%|▌         | 49/785 [00:14<03:32,  3.46it/s]  6%|▋         | 50/785 [00:14<03:32,  3.46it/s]  6%|▋         | 51/785 [00:14<03:31,  3.47it/s]  7%|▋         | 52/785 [00:14<03:31,  3.46it/s]  7%|▋         | 53/785 [00:15<03:31,  3.47it/s]  7%|▋         | 54/785 [00:15<03:30,  3.47it/s]  7%|▋         | 55/785 [00:15<03:30,  3.47it/s]  7%|▋         | 56/785 [00:16<03:30,  3.47it/s]  7%|▋         | 57/785 [00:16<03:29,  3.47it/s]  7%|▋         | 58/785 [00:16<03:29,  3.47it/s]  8%|▊         | 59/785 [00:17<03:30,  3.46it/s]  8%|▊         | 60/785 [00:17<03:29,  3.46it/s]  8%|▊         | 61/785 [00:17<03:29,  3.46it/s]  8%|▊         | 62/785 [00:17<03:28,  3.46it/s]  8%|▊         | 63/785 [00:18<03:28,  3.47it/s]  8%|▊         | 64/785 [00:18<03:28,  3.47it/s]  8%|▊         | 65/785 [00:18<03:27,  3.47it/s]  8%|▊         | 66/785 [00:19<03:27,  3.47it/s]  9%|▊         | 67/785 [00:19<03:26,  3.47it/s]  9%|▊         | 68/785 [00:19<03:26,  3.47it/s]  9%|▉         | 69/785 [00:19<03:26,  3.47it/s]  9%|▉         | 70/785 [00:20<03:26,  3.46it/s]  9%|▉         | 71/785 [00:20<03:26,  3.46it/s]  9%|▉         | 72/785 [00:20<03:25,  3.47it/s]  9%|▉         | 73/785 [00:21<03:25,  3.47it/s]  9%|▉         | 74/785 [00:21<03:25,  3.47it/s] 10%|▉         | 75/785 [00:21<03:24,  3.47it/s] 10%|▉         | 76/785 [00:21<03:24,  3.46it/s] 10%|▉         | 77/785 [00:22<03:24,  3.46it/s] 10%|▉         | 78/785 [00:22<03:24,  3.46it/s] 10%|█         | 79/785 [00:22<03:24,  3.46it/s] 10%|█         | 80/785 [00:23<03:23,  3.46it/s] 10%|█         | 81/785 [00:23<03:32,  3.32it/s] 10%|█         | 82/785 [00:23<03:29,  3.36it/s] 11%|█         | 83/785 [00:23<03:27,  3.39it/s] 11%|█         | 84/785 [00:24<03:25,  3.41it/s] 11%|█         | 85/785 [00:24<03:24,  3.42it/s] 11%|█         | 86/785 [00:24<03:23,  3.43it/s] 11%|█         | 87/785 [00:25<03:23,  3.44it/s] 11%|█         | 88/785 [00:25<03:22,  3.44it/s] 11%|█▏        | 89/785 [00:25<03:21,  3.45it/s] 11%|█▏        | 90/785 [00:26<03:21,  3.45it/s] 12%|█▏        | 91/785 [00:26<03:20,  3.45it/s] 12%|█▏        | 92/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 93/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 94/785 [00:27<03:19,  3.46it/s] 12%|█▏        | 95/785 [00:27<03:19,  3.46it/s] 12%|█▏        | 96/785 [00:27<03:18,  3.46it/s] 12%|█▏        | 97/785 [00:28<03:18,  3.46it/s] 12%|█▏        | 98/785 [00:28<03:18,  3.46it/s] 13%|█▎        | 99/785 [00:28<03:18,  3.46it/s] 13%|█▎        | 100/785 [00:28<03:18,  3.46it/s] 13%|█▎        | 101/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 102/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 103/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 104/785 [00:30<03:17,  3.45it/s] 13%|█▎        | 105/785 [00:30<03:16,  3.45it/s] 14%|█▎        | 106/785 [00:30<03:17,  3.44it/s] 14%|█▎        | 107/785 [00:30<03:16,  3.45it/s] 14%|█▍        | 108/785 [00:31<03:16,  3.45it/s] 14%|█▍        | 109/785 [00:31<03:23,  3.33it/s] 14%|█▍        | 110/785 [00:31<03:20,  3.36it/s] 14%|█▍        | 111/785 [00:32<03:18,  3.39it/s] 14%|█▍        | 112/785 [00:32<03:17,  3.41it/s] 14%|█▍        | 113/785 [00:32<03:16,  3.43it/s] 15%|█▍        | 114/785 [00:32<03:15,  3.44it/s] 15%|█▍        | 115/785 [00:33<03:14,  3.44it/s] 15%|█▍        | 116/785 [00:33<03:13,  3.45it/s] 15%|█▍        | 117/785 [00:33<03:13,  3.45it/s] 15%|█▌        | 118/785 [00:34<03:13,  3.45it/s] 15%|█▌        | 119/785 [00:34<03:13,  3.45it/s] 15%|█▌        | 120/785 [00:34<03:13,  3.43it/s] 15%|█▌        | 121/785 [00:35<03:13,  3.44it/s] 16%|█▌        | 122/785 [00:35<03:12,  3.44it/s] 16%|█▌        | 123/785 [00:35<03:11,  3.45it/s] 16%|█▌        | 124/785 [00:35<03:11,  3.45it/s] 16%|█▌        | 125/785 [00:36<03:11,  3.45it/s] 16%|█▌        | 126/785 [00:36<03:10,  3.46it/s] 16%|█▌        | 127/785 [00:36<03:10,  3.46it/s] 16%|█▋        | 128/785 [00:37<03:10,  3.46it/s] 16%|█▋        | 129/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 130/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 131/785 [00:37<03:10,  3.44it/s] 17%|█▋        | 132/785 [00:38<03:09,  3.45it/s] 17%|█▋        | 133/785 [00:38<03:09,  3.45it/s] 17%|█▋        | 134/785 [00:38<03:08,  3.45it/s] 17%|█▋        | 135/785 [00:39<03:08,  3.45it/s] 17%|█▋        | 136/785 [00:39<03:07,  3.46it/s] 17%|█▋        | 137/785 [00:39<03:07,  3.46it/s] 18%|█▊        | 138/785 [00:39<03:07,  3.46it/s] 18%|█▊        | 139/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 140/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 141/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 142/785 [00:41<03:06,  3.44it/s] 18%|█▊        | 143/785 [00:41<03:06,  3.44it/s] 18%|█▊        | 144/785 [00:41<03:05,  3.45it/s] 18%|█▊        | 145/785 [00:41<03:05,  3.45it/s] 19%|█▊        | 146/785 [00:42<03:05,  3.45it/s] 19%|█▊        | 147/785 [00:42<03:04,  3.45it/s] 19%|█▉        | 148/785 [00:42<03:04,  3.46it/s] 19%|█▉        | 149/785 [00:43<03:04,  3.46it/s] 19%|█▉        | 150/785 [00:43<03:03,  3.46it/s] 19%|█▉        | 151/785 [00:43<03:03,  3.46it/s] 19%|█▉        | 152/785 [00:43<03:03,  3.46it/s] 19%|█▉        | 153/785 [00:44<03:03,  3.45it/s] 20%|█▉        | 154/785 [00:44<03:02,  3.45it/s] 20%|█▉        | 155/785 [00:44<03:02,  3.45it/s] 20%|█▉        | 156/785 [00:45<03:02,  3.45it/s] 20%|██        | 157/785 [00:45<02:47,  3.74it/s][INFO|trainer.py:2140] 2023-08-29 04:58:42,615 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:58:42,615 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 04:58:42,615 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.32it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.63it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.76it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.03it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.49it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.24it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.04it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.73it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.70it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.68it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.70it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.77it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.69it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.69it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.67it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.48it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.40it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.44it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.40it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.52it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.65it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.60it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.59it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.62it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.58it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.43it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.45it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.51it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.48it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.60it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.63it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.64it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.64it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.58it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.50it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.42it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.37it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.50it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.49it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.52it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.57it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.63it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.59it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.41it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.45it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.37it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.48it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.40it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.60it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.67it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.55it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.63it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.64it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.55it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.55it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.49it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.54it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.50it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.60it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.64it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.60it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.63it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.54it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.55it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.49it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.45it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.47it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.47it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.50it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 44.43it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.15it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 45.64it/s][A
 61%|██████    | 368/608 [00:07<00:05, 45.96it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.14it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.29it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.36it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.37it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.25it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.26it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.28it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.42it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.53it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.52it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.50it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.54it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.38it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.31it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.34it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.37it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.46it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.46it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.53it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.64it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.62it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.57it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.46it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.46it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.40it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.43it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.48it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.51it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.58it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.65it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.62it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.54it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.40it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.49it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.58it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.56it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.56it/s][A
 92%|█████████▏| 558/608 [00:11<00:01, 46.55it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.61it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.55it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.55it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.40it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.47it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.48it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.54it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.58it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.53it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.61it/s][A                                                 
                                                 [A 20%|██        | 157/785 [00:58<02:47,  3.74it/s]
100%|██████████| 608/608 [00:13<00:00, 46.61it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:58:55,725 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-29 04:58:55,745 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:58:58,065 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:58:58,083 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:58:58,091 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:05<1:04:31,  6.17s/it] 20%|██        | 159/785 [01:05<46:00,  4.41s/it]   20%|██        | 160/785 [01:05<33:03,  3.17s/it] 21%|██        | 161/785 [01:06<24:00,  2.31s/it] 21%|██        | 162/785 [01:06<17:40,  1.70s/it] 21%|██        | 163/785 [01:06<13:15,  1.28s/it] 21%|██        | 164/785 [01:07<10:09,  1.02it/s] 21%|██        | 165/785 [01:07<07:59,  1.29it/s] 21%|██        | 166/785 [01:07<06:28,  1.59it/s] 21%|██▏       | 167/785 [01:07<05:25,  1.90it/s] 21%|██▏       | 168/785 [01:08<04:40,  2.20it/s] 22%|██▏       | 169/785 [01:08<04:09,  2.47it/s] 22%|██▏       | 170/785 [01:08<03:48,  2.69it/s] 22%|██▏       | 171/785 [01:09<03:32,  2.89it/s] 22%|██▏       | 172/785 [01:09<03:21,  3.04it/s] 22%|██▏       | 173/785 [01:09<03:14,  3.15it/s] 22%|██▏       | 174/785 [01:09<03:08,  3.24it/s] 22%|██▏       | 175/785 [01:10<03:04,  3.30it/s] 22%|██▏       | 176/785 [01:10<03:01,  3.35it/s] 23%|██▎       | 177/785 [01:10<02:59,  3.38it/s] 23%|██▎       | 178/785 [01:11<02:58,  3.40it/s] 23%|██▎       | 179/785 [01:11<02:57,  3.42it/s] 23%|██▎       | 180/785 [01:11<02:56,  3.43it/s] 23%|██▎       | 181/785 [01:11<02:56,  3.43it/s] 23%|██▎       | 182/785 [01:12<02:55,  3.44it/s] 23%|██▎       | 183/785 [01:12<02:54,  3.44it/s] 23%|██▎       | 184/785 [01:12<02:54,  3.45it/s] 24%|██▎       | 185/785 [01:13<02:53,  3.45it/s] 24%|██▎       | 186/785 [01:13<02:53,  3.46it/s] 24%|██▍       | 187/785 [01:13<02:53,  3.46it/s] 24%|██▍       | 188/785 [01:14<02:52,  3.46it/s] 24%|██▍       | 189/785 [01:14<02:52,  3.46it/s] 24%|██▍       | 190/785 [01:14<02:52,  3.46it/s] 24%|██▍       | 191/785 [01:14<02:51,  3.46it/s] 24%|██▍       | 192/785 [01:15<02:52,  3.45it/s] 25%|██▍       | 193/785 [01:15<02:51,  3.45it/s] 25%|██▍       | 194/785 [01:15<02:51,  3.45it/s] 25%|██▍       | 195/785 [01:16<02:50,  3.45it/s] 25%|██▍       | 196/785 [01:16<02:50,  3.46it/s] 25%|██▌       | 197/785 [01:16<02:50,  3.46it/s] 25%|██▌       | 198/785 [01:16<02:49,  3.46it/s] 25%|██▌       | 199/785 [01:17<02:49,  3.46it/s] 25%|██▌       | 200/785 [01:17<02:49,  3.46it/s] 26%|██▌       | 201/785 [01:17<02:48,  3.46it/s] 26%|██▌       | 202/785 [01:18<02:48,  3.46it/s] 26%|██▌       | 203/785 [01:18<02:48,  3.44it/s] 26%|██▌       | 204/785 [01:18<02:48,  3.45it/s] 26%|██▌       | 205/785 [01:18<02:47,  3.45it/s] 26%|██▌       | 206/785 [01:19<02:47,  3.45it/s] 26%|██▋       | 207/785 [01:19<02:47,  3.46it/s] 26%|██▋       | 208/785 [01:19<02:47,  3.45it/s] 27%|██▋       | 209/785 [01:20<02:46,  3.46it/s] 27%|██▋       | 210/785 [01:20<02:46,  3.45it/s] 27%|██▋       | 211/785 [01:20<02:46,  3.46it/s] 27%|██▋       | 212/785 [01:20<02:45,  3.46it/s] 27%|██▋       | 213/785 [01:21<02:45,  3.45it/s] 27%|██▋       | 214/785 [01:21<02:45,  3.45it/s] 27%|██▋       | 215/785 [01:21<02:45,  3.45it/s] 28%|██▊       | 216/785 [01:22<02:44,  3.45it/s] 28%|██▊       | 217/785 [01:22<02:44,  3.46it/s] 28%|██▊       | 218/785 [01:22<02:44,  3.46it/s] 28%|██▊       | 219/785 [01:22<02:43,  3.45it/s] 28%|██▊       | 220/785 [01:23<02:43,  3.45it/s] 28%|██▊       | 221/785 [01:23<02:43,  3.45it/s] 28%|██▊       | 222/785 [01:23<02:43,  3.45it/s] 28%|██▊       | 223/785 [01:24<02:42,  3.45it/s] 29%|██▊       | 224/785 [01:24<02:42,  3.45it/s] 29%|██▊       | 225/785 [01:24<02:43,  3.42it/s] 29%|██▉       | 226/785 [01:25<02:43,  3.43it/s] 29%|██▉       | 227/785 [01:25<02:42,  3.44it/s] 29%|██▉       | 228/785 [01:25<02:41,  3.45it/s] 29%|██▉       | 229/785 [01:25<02:41,  3.45it/s] 29%|██▉       | 230/785 [01:26<02:40,  3.45it/s] 29%|██▉       | 231/785 [01:26<02:40,  3.45it/s] 30%|██▉       | 232/785 [01:26<02:40,  3.45it/s] 30%|██▉       | 233/785 [01:27<02:39,  3.45it/s] 30%|██▉       | 234/785 [01:27<02:39,  3.45it/s] 30%|██▉       | 235/785 [01:27<02:39,  3.45it/s] 30%|███       | 236/785 [01:27<02:38,  3.45it/s] 30%|███       | 237/785 [01:28<02:38,  3.45it/s] 30%|███       | 238/785 [01:28<02:38,  3.45it/s] 30%|███       | 239/785 [01:28<02:38,  3.45it/s] 31%|███       | 240/785 [01:29<02:37,  3.45it/s] 31%|███       | 241/785 [01:29<02:37,  3.45it/s] 31%|███       | 242/785 [01:29<02:37,  3.45it/s] 31%|███       | 243/785 [01:29<02:37,  3.45it/s] 31%|███       | 244/785 [01:30<02:36,  3.45it/s] 31%|███       | 245/785 [01:30<02:36,  3.45it/s] 31%|███▏      | 246/785 [01:30<02:36,  3.45it/s] 31%|███▏      | 247/785 [01:31<02:35,  3.45it/s] 32%|███▏      | 248/785 [01:31<02:35,  3.45it/s] 32%|███▏      | 249/785 [01:31<02:35,  3.45it/s] 32%|███▏      | 250/785 [01:31<02:34,  3.45it/s] 32%|███▏      | 251/785 [01:32<02:34,  3.45it/s] 32%|███▏      | 252/785 [01:32<02:34,  3.46it/s] 32%|███▏      | 253/785 [01:32<02:34,  3.45it/s] 32%|███▏      | 254/785 [01:33<02:34,  3.44it/s] 32%|███▏      | 255/785 [01:33<02:33,  3.45it/s] 33%|███▎      | 256/785 [01:33<02:33,  3.45it/s] 33%|███▎      | 257/785 [01:33<02:32,  3.45it/s] 33%|███▎      | 258/785 [01:34<02:32,  3.45it/s] 33%|███▎      | 259/785 [01:34<02:32,  3.45it/s] 33%|███▎      | 260/785 [01:34<02:32,  3.45it/s] 33%|███▎      | 261/785 [01:35<02:31,  3.45it/s] 33%|███▎      | 262/785 [01:35<02:31,  3.45it/s] 34%|███▎      | 263/785 [01:35<02:31,  3.45it/s] 34%|███▎      | 264/785 [01:36<02:30,  3.45it/s] 34%|███▍      | 265/785 [01:36<02:30,  3.44it/s] 34%|███▍      | 266/785 [01:36<02:30,  3.45it/s] 34%|███▍      | 267/785 [01:36<02:30,  3.45it/s] 34%|███▍      | 268/785 [01:37<02:29,  3.45it/s] 34%|███▍      | 269/785 [01:37<02:29,  3.45it/s] 34%|███▍      | 270/785 [01:37<02:29,  3.45it/s] 35%|███▍      | 271/785 [01:38<02:28,  3.45it/s] 35%|███▍      | 272/785 [01:38<02:28,  3.45it/s] 35%|███▍      | 273/785 [01:38<02:28,  3.45it/s] 35%|███▍      | 274/785 [01:38<02:28,  3.45it/s] 35%|███▌      | 275/785 [01:39<02:27,  3.45it/s] 35%|███▌      | 276/785 [01:39<02:27,  3.44it/s] 35%|███▌      | 277/785 [01:39<02:27,  3.44it/s] 35%|███▌      | 278/785 [01:40<02:27,  3.45it/s] 36%|███▌      | 279/785 [01:40<02:26,  3.45it/s] 36%|███▌      | 280/785 [01:40<02:26,  3.45it/s] 36%|███▌      | 281/785 [01:40<02:26,  3.45it/s] 36%|███▌      | 282/785 [01:41<02:25,  3.45it/s] 36%|███▌      | 283/785 [01:41<02:25,  3.45it/s] 36%|███▌      | 284/785 [01:41<02:25,  3.45it/s] 36%|███▋      | 285/785 [01:42<02:24,  3.45it/s] 36%|███▋      | 286/785 [01:42<02:24,  3.45it/s] 37%|███▋      | 287/785 [01:42<02:24,  3.44it/s] 37%|███▋      | 288/785 [01:42<02:24,  3.45it/s] 37%|███▋      | 289/785 [01:43<02:23,  3.45it/s] 37%|███▋      | 290/785 [01:43<02:23,  3.45it/s] 37%|███▋      | 291/785 [01:43<02:23,  3.45it/s] 37%|███▋      | 292/785 [01:44<02:22,  3.45it/s] 37%|███▋      | 293/785 [01:44<02:22,  3.45it/s] 37%|███▋      | 294/785 [01:44<02:22,  3.45it/s] 38%|███▊      | 295/785 [01:45<02:22,  3.45it/s] 38%|███▊      | 296/785 [01:45<02:21,  3.45it/s] 38%|███▊      | 297/785 [01:45<02:21,  3.45it/s] 38%|███▊      | 298/785 [01:45<02:21,  3.43it/s] 38%|███▊      | 299/785 [01:46<02:21,  3.44it/s] 38%|███▊      | 300/785 [01:46<02:20,  3.44it/s] 38%|███▊      | 301/785 [01:46<02:20,  3.45it/s] 38%|███▊      | 302/785 [01:47<02:20,  3.45it/s] 39%|███▊      | 303/785 [01:47<02:19,  3.45it/s] 39%|███▊      | 304/785 [01:47<02:19,  3.45it/s] 39%|███▉      | 305/785 [01:47<02:19,  3.45it/s] 39%|███▉      | 306/785 [01:48<02:18,  3.45it/s] 39%|███▉      | 307/785 [01:48<02:18,  3.45it/s] 39%|███▉      | 308/785 [01:48<02:18,  3.45it/s] 39%|███▉      | 309/785 [01:49<02:18,  3.43it/s] 39%|███▉      | 310/785 [01:49<02:18,  3.44it/s] 40%|███▉      | 311/785 [01:49<02:17,  3.44it/s] 40%|███▉      | 312/785 [01:49<02:17,  3.44it/s] 40%|███▉      | 313/785 [01:50<02:16,  3.45it/s] 40%|████      | 314/785 [01:50<02:06,  3.73it/s][INFO|trainer.py:2140] 2023-08-29 04:59:47,703 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:59:47,703 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 04:59:47,703 >>   Batch size = 8
{'eval_loss': 0.9234098196029663, 'eval_runtime': 13.083, 'eval_samples_per_second': 371.781, 'eval_steps_per_second': 46.473, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.98it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.34it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.65it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.01it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.61it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.99it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.73it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.33it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.40it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.41it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.44it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.50it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.61it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.64it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.45it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.14it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.12it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.26it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.36it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.48it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.43it/s][A
 19%|█▊        | 113/608 [00:02<00:11, 44.92it/s][A
 20%|█▉        | 119/608 [00:02<00:10, 46.43it/s][A
 20%|██        | 124/608 [00:02<00:10, 46.42it/s][A
 21%|██        | 129/608 [00:02<00:10, 46.27it/s][A
 22%|██▏       | 134/608 [00:02<00:10, 46.16it/s][A
 23%|██▎       | 139/608 [00:02<00:10, 46.22it/s][A
 24%|██▎       | 144/608 [00:03<00:10, 46.30it/s][A
 25%|██▍       | 149/608 [00:03<00:09, 46.46it/s][A
 25%|██▌       | 154/608 [00:03<00:09, 46.54it/s][A
 26%|██▌       | 159/608 [00:03<00:09, 46.48it/s][A
 27%|██▋       | 164/608 [00:03<00:09, 46.56it/s][A
 28%|██▊       | 169/608 [00:03<00:09, 46.45it/s][A
 29%|██▊       | 174/608 [00:03<00:09, 46.32it/s][A
 29%|██▉       | 179/608 [00:03<00:09, 46.31it/s][A
 30%|███       | 184/608 [00:03<00:09, 46.27it/s][A
 31%|███       | 189/608 [00:04<00:09, 46.39it/s][A
 32%|███▏      | 194/608 [00:04<00:08, 46.48it/s][A
 33%|███▎      | 199/608 [00:04<00:08, 46.33it/s][A
 34%|███▎      | 204/608 [00:04<00:08, 46.48it/s][A
 34%|███▍      | 209/608 [00:04<00:08, 46.50it/s][A
 35%|███▌      | 214/608 [00:04<00:08, 46.36it/s][A
 36%|███▌      | 219/608 [00:04<00:08, 46.33it/s][A
 37%|███▋      | 224/608 [00:04<00:08, 46.29it/s][A
 38%|███▊      | 229/608 [00:04<00:08, 46.30it/s][A
 38%|███▊      | 234/608 [00:05<00:08, 46.39it/s][A
 39%|███▉      | 239/608 [00:05<00:07, 46.33it/s][A
 40%|████      | 244/608 [00:05<00:07, 46.42it/s][A
 41%|████      | 249/608 [00:05<00:07, 46.46it/s][A
 42%|████▏     | 254/608 [00:05<00:07, 46.51it/s][A
 43%|████▎     | 259/608 [00:05<00:07, 46.39it/s][A
 43%|████▎     | 264/608 [00:05<00:07, 46.40it/s][A
 44%|████▍     | 269/608 [00:05<00:07, 46.33it/s][A
 45%|████▌     | 274/608 [00:05<00:07, 46.29it/s][A
 46%|████▌     | 279/608 [00:05<00:07, 46.37it/s][A
 47%|████▋     | 284/608 [00:06<00:06, 46.37it/s][A
 48%|████▊     | 289/608 [00:06<00:06, 46.34it/s][A
 48%|████▊     | 294/608 [00:06<00:06, 46.43it/s][A
 49%|████▉     | 299/608 [00:06<00:06, 46.41it/s][A
 50%|█████     | 304/608 [00:06<00:06, 46.45it/s][A
 51%|█████     | 309/608 [00:06<00:06, 46.37it/s][A
 52%|█████▏    | 314/608 [00:06<00:06, 46.30it/s][A
 52%|█████▏    | 319/608 [00:06<00:06, 46.28it/s][A
 53%|█████▎    | 324/608 [00:06<00:06, 46.29it/s][A
 54%|█████▍    | 329/608 [00:07<00:06, 46.36it/s][A
 55%|█████▍    | 334/608 [00:07<00:05, 46.39it/s][A
 56%|█████▌    | 339/608 [00:07<00:05, 46.32it/s][A
 57%|█████▋    | 344/608 [00:07<00:05, 46.41it/s][A
 57%|█████▋    | 349/608 [00:07<00:05, 46.28it/s][A
 58%|█████▊    | 354/608 [00:07<00:05, 46.27it/s][A
 59%|█████▉    | 359/608 [00:07<00:05, 46.29it/s][A
 60%|█████▉    | 364/608 [00:07<00:05, 46.30it/s][A
 61%|██████    | 369/608 [00:07<00:05, 46.31it/s][A
 62%|██████▏   | 374/608 [00:08<00:05, 46.41it/s][A
 62%|██████▏   | 379/608 [00:08<00:04, 46.34it/s][A
 63%|██████▎   | 384/608 [00:08<00:04, 46.42it/s][A
 64%|██████▍   | 389/608 [00:08<00:04, 46.28it/s][A
 65%|██████▍   | 394/608 [00:08<00:04, 46.25it/s][A
 66%|██████▌   | 399/608 [00:08<00:04, 46.26it/s][A
 66%|██████▋   | 404/608 [00:08<00:04, 46.31it/s][A
 67%|██████▋   | 409/608 [00:08<00:04, 46.27it/s][A
 68%|██████▊   | 414/608 [00:08<00:04, 46.20it/s][A
 69%|██████▉   | 419/608 [00:09<00:04, 46.35it/s][A
 70%|██████▉   | 424/608 [00:09<00:03, 46.33it/s][A
 71%|███████   | 429/608 [00:09<00:03, 46.39it/s][A
 71%|███████▏  | 434/608 [00:09<00:03, 46.43it/s][A
 72%|███████▏  | 439/608 [00:09<00:03, 46.34it/s][A
 73%|███████▎  | 444/608 [00:09<00:03, 46.30it/s][A
 74%|███████▍  | 449/608 [00:09<00:03, 46.14it/s][A
 75%|███████▍  | 454/608 [00:09<00:03, 46.21it/s][A
 75%|███████▌  | 459/608 [00:09<00:03, 46.23it/s][A
 76%|███████▋  | 464/608 [00:09<00:03, 46.30it/s][A
 77%|███████▋  | 469/608 [00:10<00:02, 46.34it/s][A
 78%|███████▊  | 474/608 [00:10<00:02, 46.42it/s][A
 79%|███████▉  | 479/608 [00:10<00:02, 46.32it/s][A
 80%|███████▉  | 484/608 [00:10<00:02, 46.38it/s][A
 80%|████████  | 489/608 [00:10<00:02, 46.37it/s][A
 81%|████████▏ | 494/608 [00:10<00:02, 46.32it/s][A
 82%|████████▏ | 499/608 [00:10<00:02, 46.22it/s][A
 83%|████████▎ | 504/608 [00:10<00:02, 46.29it/s][A
 84%|████████▎ | 509/608 [00:10<00:02, 46.19it/s][A
 85%|████████▍ | 514/608 [00:11<00:02, 46.24it/s][A
 85%|████████▌ | 519/608 [00:11<00:01, 46.35it/s][A
 86%|████████▌ | 524/608 [00:11<00:01, 46.35it/s][A
 87%|████████▋ | 529/608 [00:11<00:01, 46.40it/s][A
 88%|████████▊ | 534/608 [00:11<00:01, 46.35it/s][A
 89%|████████▊ | 539/608 [00:11<00:01, 46.29it/s][A
 89%|████████▉ | 544/608 [00:11<00:01, 46.24it/s][A
 90%|█████████ | 549/608 [00:11<00:01, 46.19it/s][A
 91%|█████████ | 554/608 [00:11<00:01, 46.24it/s][A
 92%|█████████▏| 559/608 [00:12<00:01, 46.20it/s][A
 93%|█████████▎| 564/608 [00:12<00:00, 46.28it/s][A
 94%|█████████▎| 569/608 [00:12<00:00, 46.34it/s][A
 94%|█████████▍| 574/608 [00:12<00:00, 46.35it/s][A
 95%|█████████▌| 579/608 [00:12<00:00, 46.29it/s][A
 96%|█████████▌| 584/608 [00:12<00:00, 46.28it/s][A
 97%|█████████▋| 589/608 [00:12<00:00, 46.31it/s][A
 98%|█████████▊| 594/608 [00:12<00:00, 46.23it/s][A
 99%|█████████▊| 599/608 [00:12<00:00, 46.25it/s][A
 99%|█████████▉| 604/608 [00:13<00:00, 46.28it/s][A                                                 
                                                 [A 40%|████      | 314/785 [02:03<02:06,  3.73it/s]
100%|██████████| 608/608 [00:13<00:00, 46.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:00:00,855 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-29 05:00:00,873 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:00:03,107 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:00:03,122 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:00:03,131 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:10<48:43,  6.22s/it] 40%|████      | 316/785 [02:10<34:43,  4.44s/it] 40%|████      | 317/785 [02:11<24:55,  3.20s/it] 41%|████      | 318/785 [02:11<18:05,  2.32s/it] 41%|████      | 319/785 [02:11<13:18,  1.71s/it] 41%|████      | 320/785 [02:12<09:57,  1.29s/it] 41%|████      | 321/785 [02:12<07:37,  1.01it/s] 41%|████      | 322/785 [02:12<06:00,  1.29it/s] 41%|████      | 323/785 [02:12<04:51,  1.58it/s] 41%|████▏     | 324/785 [02:13<04:03,  1.89it/s] 41%|████▏     | 325/785 [02:13<03:30,  2.19it/s] 42%|████▏     | 326/785 [02:13<03:06,  2.46it/s] 42%|████▏     | 327/785 [02:14<02:50,  2.69it/s] 42%|████▏     | 328/785 [02:14<02:38,  2.88it/s] 42%|████▏     | 329/785 [02:14<02:30,  3.03it/s] 42%|████▏     | 330/785 [02:14<02:24,  3.15it/s] 42%|████▏     | 331/785 [02:15<02:20,  3.24it/s] 42%|████▏     | 332/785 [02:15<02:17,  3.30it/s] 42%|████▏     | 333/785 [02:15<02:15,  3.34it/s] 43%|████▎     | 334/785 [02:16<02:13,  3.38it/s] 43%|████▎     | 335/785 [02:16<02:12,  3.40it/s] 43%|████▎     | 336/785 [02:16<02:11,  3.41it/s] 43%|████▎     | 337/785 [02:16<02:10,  3.43it/s] 43%|████▎     | 338/785 [02:17<02:10,  3.43it/s] 43%|████▎     | 339/785 [02:17<02:09,  3.44it/s] 43%|████▎     | 340/785 [02:17<02:09,  3.44it/s] 43%|████▎     | 341/785 [02:18<02:08,  3.45it/s] 44%|████▎     | 342/785 [02:18<02:08,  3.45it/s] 44%|████▎     | 343/785 [02:18<02:08,  3.45it/s] 44%|████▍     | 344/785 [02:18<02:07,  3.45it/s] 44%|████▍     | 345/785 [02:19<02:07,  3.45it/s] 44%|████▍     | 346/785 [02:19<02:07,  3.45it/s] 44%|████▍     | 347/785 [02:19<02:06,  3.46it/s] 44%|████▍     | 348/785 [02:20<02:06,  3.46it/s] 44%|████▍     | 349/785 [02:20<02:06,  3.45it/s] 45%|████▍     | 350/785 [02:20<02:06,  3.45it/s] 45%|████▍     | 351/785 [02:20<02:05,  3.45it/s] 45%|████▍     | 352/785 [02:21<02:05,  3.45it/s] 45%|████▍     | 353/785 [02:21<02:05,  3.45it/s] 45%|████▌     | 354/785 [02:21<02:04,  3.45it/s] 45%|████▌     | 355/785 [02:22<02:04,  3.45it/s] 45%|████▌     | 356/785 [02:22<02:04,  3.45it/s] 45%|████▌     | 357/785 [02:22<02:04,  3.45it/s] 46%|████▌     | 358/785 [02:23<02:03,  3.45it/s] 46%|████▌     | 359/785 [02:23<02:03,  3.45it/s] 46%|████▌     | 360/785 [02:23<02:03,  3.44it/s] 46%|████▌     | 361/785 [02:23<02:03,  3.45it/s] 46%|████▌     | 362/785 [02:24<02:02,  3.45it/s] 46%|████▌     | 363/785 [02:24<02:02,  3.45it/s] 46%|████▋     | 364/785 [02:24<02:02,  3.45it/s] 46%|████▋     | 365/785 [02:25<02:01,  3.45it/s] 47%|████▋     | 366/785 [02:25<02:01,  3.45it/s] 47%|████▋     | 367/785 [02:25<02:01,  3.45it/s] 47%|████▋     | 368/785 [02:25<02:00,  3.45it/s] 47%|████▋     | 369/785 [02:26<02:00,  3.45it/s] 47%|████▋     | 370/785 [02:26<02:00,  3.45it/s] 47%|████▋     | 371/785 [02:26<02:00,  3.44it/s] 47%|████▋     | 372/785 [02:27<01:59,  3.44it/s] 48%|████▊     | 373/785 [02:27<01:59,  3.45it/s] 48%|████▊     | 374/785 [02:27<01:59,  3.45it/s] 48%|████▊     | 375/785 [02:27<01:58,  3.45it/s] 48%|████▊     | 376/785 [02:28<01:58,  3.45it/s] 48%|████▊     | 377/785 [02:28<01:58,  3.45it/s] 48%|████▊     | 378/785 [02:28<01:57,  3.45it/s] 48%|████▊     | 379/785 [02:29<01:57,  3.45it/s] 48%|████▊     | 380/785 [02:29<01:57,  3.45it/s] 49%|████▊     | 381/785 [02:29<01:56,  3.45it/s] 49%|████▊     | 382/785 [02:29<01:56,  3.45it/s] 49%|████▉     | 383/785 [02:30<01:56,  3.45it/s] 49%|████▉     | 384/785 [02:30<01:56,  3.45it/s] 49%|████▉     | 385/785 [02:30<01:55,  3.45it/s] 49%|████▉     | 386/785 [02:31<01:55,  3.45it/s] 49%|████▉     | 387/785 [02:31<01:55,  3.44it/s] 49%|████▉     | 388/785 [02:31<01:55,  3.44it/s] 50%|████▉     | 389/785 [02:32<01:54,  3.45it/s] 50%|████▉     | 390/785 [02:32<01:54,  3.45it/s] 50%|████▉     | 391/785 [02:32<01:54,  3.45it/s] 50%|████▉     | 392/785 [02:32<01:53,  3.45it/s] 50%|█████     | 393/785 [02:33<01:53,  3.45it/s] 50%|█████     | 394/785 [02:33<01:53,  3.45it/s] 50%|█████     | 395/785 [02:33<01:52,  3.45it/s] 50%|█████     | 396/785 [02:34<01:52,  3.45it/s] 51%|█████     | 397/785 [02:34<01:52,  3.45it/s] 51%|█████     | 398/785 [02:34<01:52,  3.45it/s] 51%|█████     | 399/785 [02:34<01:51,  3.45it/s] 51%|█████     | 400/785 [02:35<01:51,  3.45it/s] 51%|█████     | 401/785 [02:35<01:51,  3.45it/s] 51%|█████     | 402/785 [02:35<01:51,  3.45it/s] 51%|█████▏    | 403/785 [02:36<01:50,  3.45it/s] 51%|█████▏    | 404/785 [02:36<01:50,  3.45it/s] 52%|█████▏    | 405/785 [02:36<01:50,  3.45it/s] 52%|█████▏    | 406/785 [02:36<01:49,  3.45it/s] 52%|█████▏    | 407/785 [02:37<01:49,  3.45it/s] 52%|█████▏    | 408/785 [02:37<01:49,  3.45it/s] 52%|█████▏    | 409/785 [02:37<01:49,  3.44it/s] 52%|█████▏    | 410/785 [02:38<01:48,  3.45it/s] 52%|█████▏    | 411/785 [02:38<01:48,  3.45it/s] 52%|█████▏    | 412/785 [02:38<01:48,  3.45it/s] 53%|█████▎    | 413/785 [02:38<01:47,  3.45it/s] 53%|█████▎    | 414/785 [02:39<01:47,  3.45it/s] 53%|█████▎    | 415/785 [02:39<01:47,  3.45it/s] 53%|█████▎    | 416/785 [02:39<01:46,  3.45it/s] 53%|█████▎    | 417/785 [02:40<01:46,  3.45it/s] 53%|█████▎    | 418/785 [02:40<01:46,  3.45it/s] 53%|█████▎    | 419/785 [02:40<01:46,  3.45it/s] 54%|█████▎    | 420/785 [02:40<01:46,  3.44it/s] 54%|█████▎    | 421/785 [02:41<01:45,  3.44it/s] 54%|█████▍    | 422/785 [02:41<01:45,  3.45it/s] 54%|█████▍    | 423/785 [02:41<01:44,  3.45it/s] 54%|█████▍    | 424/785 [02:42<01:44,  3.45it/s] 54%|█████▍    | 425/785 [02:42<01:44,  3.45it/s] 54%|█████▍    | 426/785 [02:42<01:44,  3.45it/s] 54%|█████▍    | 427/785 [02:43<01:43,  3.45it/s] 55%|█████▍    | 428/785 [02:43<01:43,  3.45it/s] 55%|█████▍    | 429/785 [02:43<01:43,  3.45it/s] 55%|█████▍    | 430/785 [02:43<01:42,  3.45it/s] 55%|█████▍    | 431/785 [02:44<01:42,  3.44it/s] 55%|█████▌    | 432/785 [02:44<01:42,  3.44it/s] 55%|█████▌    | 433/785 [02:44<01:42,  3.45it/s] 55%|█████▌    | 434/785 [02:45<01:41,  3.44it/s] 55%|█████▌    | 435/785 [02:45<01:41,  3.45it/s] 56%|█████▌    | 436/785 [02:45<01:41,  3.45it/s] 56%|█████▌    | 437/785 [02:45<01:40,  3.45it/s] 56%|█████▌    | 438/785 [02:46<01:40,  3.45it/s] 56%|█████▌    | 439/785 [02:46<01:40,  3.45it/s] 56%|█████▌    | 440/785 [02:46<01:40,  3.45it/s] 56%|█████▌    | 441/785 [02:47<01:39,  3.45it/s] 56%|█████▋    | 442/785 [02:47<01:39,  3.44it/s] 56%|█████▋    | 443/785 [02:47<01:39,  3.45it/s] 57%|█████▋    | 444/785 [02:47<01:38,  3.44it/s] 57%|█████▋    | 445/785 [02:48<01:38,  3.45it/s] 57%|█████▋    | 446/785 [02:48<01:38,  3.45it/s] 57%|█████▋    | 447/785 [02:48<01:38,  3.45it/s] 57%|█████▋    | 448/785 [02:49<01:37,  3.45it/s] 57%|█████▋    | 449/785 [02:49<01:37,  3.45it/s] 57%|█████▋    | 450/785 [02:49<01:37,  3.45it/s] 57%|█████▋    | 451/785 [02:49<01:36,  3.45it/s] 58%|█████▊    | 452/785 [02:50<01:36,  3.45it/s] 58%|█████▊    | 453/785 [02:50<01:36,  3.44it/s] 58%|█████▊    | 454/785 [02:50<01:36,  3.45it/s] 58%|█████▊    | 455/785 [02:51<01:35,  3.45it/s] 58%|█████▊    | 456/785 [02:51<01:35,  3.44it/s] 58%|█████▊    | 457/785 [02:51<01:35,  3.44it/s] 58%|█████▊    | 458/785 [02:52<01:34,  3.44it/s] 58%|█████▊    | 459/785 [02:52<01:34,  3.45it/s] 59%|█████▊    | 460/785 [02:52<01:34,  3.44it/s] 59%|█████▊    | 461/785 [02:52<01:37,  3.32it/s] 59%|█████▉    | 462/785 [02:53<01:36,  3.35it/s] 59%|█████▉    | 463/785 [02:53<01:35,  3.38it/s] 59%|█████▉    | 464/785 [02:53<01:34,  3.38it/s] 59%|█████▉    | 465/785 [02:54<01:34,  3.40it/s] 59%|█████▉    | 466/785 [02:54<01:33,  3.41it/s] 59%|█████▉    | 467/785 [02:54<01:32,  3.42it/s] 60%|█████▉    | 468/785 [02:54<01:32,  3.43it/s] 60%|█████▉    | 469/785 [02:55<01:32,  3.43it/s] 60%|█████▉    | 470/785 [02:55<01:31,  3.44it/s] 60%|██████    | 471/785 [02:55<01:24,  3.72it/s][INFO|trainer.py:2140] 2023-08-29 05:00:53,005 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:00:53,005 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 05:00:53,005 >>   Batch size = 8
{'eval_loss': 0.93743896484375, 'eval_runtime': 13.1251, 'eval_samples_per_second': 370.589, 'eval_steps_per_second': 46.324, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.47it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.20it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.55it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.90it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.22it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.88it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.71it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.40it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.39it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.37it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.39it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.49it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.55it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.52it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.26it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.28it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.13it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.22it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.24it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.33it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.39it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.45it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.40it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.33it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.27it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.23it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.24it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.23it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.28it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.26it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.30it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.34it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.35it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.22it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.26it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.16it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.20it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 46.03it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.27it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.24it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.30it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.35it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.22it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.18it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.25it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.20it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.26it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.28it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.21it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.23it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.24it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.18it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.15it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.18it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.21it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.20it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.17it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.18it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.24it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.21it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.27it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.25it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.24it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.27it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.15it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.28it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.24it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.15it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.32it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.36it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.36it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.32it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.40it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.32it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.27it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.24it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.26it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.25it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.36it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.33it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.31it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.22it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.26it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.91it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.28it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.30it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.23it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.17it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.28it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.35it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.35it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.26it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.29it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.29it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.28it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.27it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.11it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.24it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.23it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.31it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.25it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.24it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.24it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.25it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.22it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.13it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.08it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.01it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.06it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.13it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.13it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.17it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.18it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.16it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.22it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.18it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.26it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.35it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.30it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.35it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.19it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [03:08<01:24,  3.72it/s]
100%|██████████| 608/608 [00:13<00:00, 46.19it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:01:06,163 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-29 05:01:06,185 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:01:08,554 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:01:08,592 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:01:08,602 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:16<33:04,  6.34s/it] 60%|██████    | 473/785 [03:16<23:32,  4.53s/it] 60%|██████    | 474/785 [03:16<16:52,  3.25s/it] 61%|██████    | 475/785 [03:17<12:13,  2.37s/it] 61%|██████    | 476/785 [03:17<08:58,  1.74s/it] 61%|██████    | 477/785 [03:17<06:42,  1.31s/it] 61%|██████    | 478/785 [03:18<05:07,  1.00s/it] 61%|██████    | 479/785 [03:18<04:01,  1.27it/s] 61%|██████    | 480/785 [03:18<03:14,  1.57it/s] 61%|██████▏   | 481/785 [03:18<02:42,  1.87it/s] 61%|██████▏   | 482/785 [03:19<02:19,  2.17it/s] 62%|██████▏   | 483/785 [03:19<02:03,  2.45it/s] 62%|██████▏   | 484/785 [03:19<01:52,  2.67it/s] 62%|██████▏   | 485/785 [03:20<01:44,  2.87it/s] 62%|██████▏   | 486/785 [03:20<01:38,  3.02it/s] 62%|██████▏   | 487/785 [03:20<01:34,  3.14it/s] 62%|██████▏   | 488/785 [03:20<01:31,  3.23it/s] 62%|██████▏   | 489/785 [03:21<01:29,  3.29it/s] 62%|██████▏   | 490/785 [03:21<01:28,  3.34it/s] 63%|██████▎   | 491/785 [03:21<01:27,  3.37it/s] 63%|██████▎   | 492/785 [03:22<01:26,  3.40it/s] 63%|██████▎   | 493/785 [03:22<01:25,  3.42it/s] 63%|██████▎   | 494/785 [03:22<01:24,  3.43it/s] 63%|██████▎   | 495/785 [03:22<01:24,  3.43it/s] 63%|██████▎   | 496/785 [03:23<01:24,  3.44it/s] 63%|██████▎   | 497/785 [03:23<01:23,  3.45it/s] 63%|██████▎   | 498/785 [03:23<01:23,  3.45it/s] 64%|██████▎   | 499/785 [03:24<01:22,  3.45it/s] 64%|██████▎   | 500/785 [03:24<01:22,  3.45it/s]                                                  64%|██████▎   | 500/785 [03:24<01:22,  3.45it/s] 64%|██████▍   | 501/785 [03:24<01:22,  3.45it/s] 64%|██████▍   | 502/785 [03:24<01:22,  3.45it/s] 64%|██████▍   | 503/785 [03:25<01:21,  3.45it/s] 64%|██████▍   | 504/785 [03:25<01:21,  3.45it/s] 64%|██████▍   | 505/785 [03:25<01:21,  3.45it/s] 64%|██████▍   | 506/785 [03:26<01:20,  3.45it/s] 65%|██████▍   | 507/785 [03:26<01:20,  3.45it/s] 65%|██████▍   | 508/785 [03:26<01:20,  3.45it/s] 65%|██████▍   | 509/785 [03:26<01:19,  3.45it/s] 65%|██████▍   | 510/785 [03:27<01:19,  3.45it/s] 65%|██████▌   | 511/785 [03:27<01:19,  3.46it/s] 65%|██████▌   | 512/785 [03:27<01:18,  3.46it/s] 65%|██████▌   | 513/785 [03:28<01:18,  3.46it/s] 65%|██████▌   | 514/785 [03:28<01:18,  3.46it/s] 66%|██████▌   | 515/785 [03:28<01:18,  3.46it/s] 66%|██████▌   | 516/785 [03:29<01:17,  3.46it/s] 66%|██████▌   | 517/785 [03:29<01:17,  3.45it/s] 66%|██████▌   | 518/785 [03:29<01:17,  3.45it/s] 66%|██████▌   | 519/785 [03:29<01:17,  3.45it/s] 66%|██████▌   | 520/785 [03:30<01:16,  3.46it/s] 66%|██████▋   | 521/785 [03:30<01:16,  3.45it/s] 66%|██████▋   | 522/785 [03:30<01:16,  3.46it/s] 67%|██████▋   | 523/785 [03:31<01:15,  3.45it/s] 67%|██████▋   | 524/785 [03:31<01:15,  3.46it/s] 67%|██████▋   | 525/785 [03:31<01:15,  3.45it/s] 67%|██████▋   | 526/785 [03:31<01:15,  3.45it/s] 67%|██████▋   | 527/785 [03:32<01:14,  3.45it/s] 67%|██████▋   | 528/785 [03:32<01:14,  3.45it/s] 67%|██████▋   | 529/785 [03:32<01:14,  3.45it/s] 68%|██████▊   | 530/785 [03:33<01:13,  3.45it/s] 68%|██████▊   | 531/785 [03:33<01:13,  3.45it/s] 68%|██████▊   | 532/785 [03:33<01:13,  3.46it/s] 68%|██████▊   | 533/785 [03:33<01:12,  3.46it/s] 68%|██████▊   | 534/785 [03:34<01:12,  3.45it/s] 68%|██████▊   | 535/785 [03:34<01:12,  3.46it/s] 68%|██████▊   | 536/785 [03:34<01:12,  3.46it/s] 68%|██████▊   | 537/785 [03:35<01:11,  3.46it/s] 69%|██████▊   | 538/785 [03:35<01:11,  3.45it/s] 69%|██████▊   | 539/785 [03:35<01:11,  3.45it/s] 69%|██████▉   | 540/785 [03:35<01:11,  3.44it/s] 69%|██████▉   | 541/785 [03:36<01:10,  3.45it/s] 69%|██████▉   | 542/785 [03:36<01:10,  3.45it/s] 69%|██████▉   | 543/785 [03:36<01:10,  3.45it/s] 69%|██████▉   | 544/785 [03:37<01:09,  3.45it/s] 69%|██████▉   | 545/785 [03:37<01:09,  3.45it/s] 70%|██████▉   | 546/785 [03:37<01:09,  3.45it/s] 70%|██████▉   | 547/785 [03:37<01:08,  3.45it/s] 70%|██████▉   | 548/785 [03:38<01:08,  3.45it/s] 70%|██████▉   | 549/785 [03:38<01:08,  3.46it/s] 70%|███████   | 550/785 [03:38<01:08,  3.45it/s] 70%|███████   | 551/785 [03:39<01:07,  3.44it/s] 70%|███████   | 552/785 [03:39<01:07,  3.44it/s] 70%|███████   | 553/785 [03:39<01:07,  3.45it/s] 71%|███████   | 554/785 [03:40<01:06,  3.45it/s] 71%|███████   | 555/785 [03:40<01:06,  3.45it/s] 71%|███████   | 556/785 [03:40<01:06,  3.45it/s] 71%|███████   | 557/785 [03:40<01:06,  3.45it/s] 71%|███████   | 558/785 [03:41<01:05,  3.45it/s] 71%|███████   | 559/785 [03:41<01:05,  3.46it/s] 71%|███████▏  | 560/785 [03:41<01:05,  3.45it/s] 71%|███████▏  | 561/785 [03:42<01:04,  3.45it/s] 72%|███████▏  | 562/785 [03:42<01:04,  3.44it/s] 72%|███████▏  | 563/785 [03:42<01:04,  3.45it/s] 72%|███████▏  | 564/785 [03:42<01:04,  3.45it/s] 72%|███████▏  | 565/785 [03:43<01:03,  3.45it/s] 72%|███████▏  | 566/785 [03:43<01:03,  3.45it/s] 72%|███████▏  | 567/785 [03:43<01:03,  3.45it/s] 72%|███████▏  | 568/785 [03:44<01:02,  3.45it/s] 72%|███████▏  | 569/785 [03:44<01:02,  3.45it/s] 73%|███████▎  | 570/785 [03:44<01:02,  3.45it/s] 73%|███████▎  | 571/785 [03:44<01:01,  3.45it/s] 73%|███████▎  | 572/785 [03:45<01:01,  3.45it/s] 73%|███████▎  | 573/785 [03:45<01:01,  3.44it/s] 73%|███████▎  | 574/785 [03:45<01:01,  3.44it/s] 73%|███████▎  | 575/785 [03:46<01:00,  3.45it/s] 73%|███████▎  | 576/785 [03:46<01:00,  3.45it/s] 74%|███████▎  | 577/785 [03:46<01:00,  3.45it/s] 74%|███████▎  | 578/785 [03:46<00:59,  3.45it/s] 74%|███████▍  | 579/785 [03:47<00:59,  3.45it/s] 74%|███████▍  | 580/785 [03:47<00:59,  3.45it/s] 74%|███████▍  | 581/785 [03:47<00:59,  3.45it/s] 74%|███████▍  | 582/785 [03:48<00:58,  3.46it/s] 74%|███████▍  | 583/785 [03:48<00:58,  3.46it/s] 74%|███████▍  | 584/785 [03:48<00:58,  3.45it/s] 75%|███████▍  | 585/785 [03:49<00:57,  3.45it/s] 75%|███████▍  | 586/785 [03:49<00:57,  3.45it/s] 75%|███████▍  | 587/785 [03:49<00:57,  3.45it/s] 75%|███████▍  | 588/785 [03:49<00:57,  3.45it/s] 75%|███████▌  | 589/785 [03:50<00:56,  3.45it/s] 75%|███████▌  | 590/785 [03:50<00:56,  3.45it/s] 75%|███████▌  | 591/785 [03:50<00:56,  3.45it/s] 75%|███████▌  | 592/785 [03:51<00:55,  3.45it/s] 76%|███████▌  | 593/785 [03:51<00:55,  3.45it/s] 76%|███████▌  | 594/785 [03:51<00:55,  3.45it/s] 76%|███████▌  | 595/785 [03:51<00:55,  3.44it/s] 76%|███████▌  | 596/785 [03:52<00:54,  3.44it/s] 76%|███████▌  | 597/785 [03:52<00:54,  3.45it/s] 76%|███████▌  | 598/785 [03:52<00:54,  3.45it/s] 76%|███████▋  | 599/785 [03:53<00:55,  3.36it/s] 76%|███████▋  | 600/785 [03:53<00:54,  3.38it/s] 77%|███████▋  | 601/785 [03:53<00:54,  3.40it/s] 77%|███████▋  | 602/785 [03:53<00:53,  3.42it/s] 77%|███████▋  | 603/785 [03:54<00:53,  3.43it/s] 77%|███████▋  | 604/785 [03:54<00:52,  3.43it/s] 77%|███████▋  | 605/785 [03:54<00:52,  3.44it/s] 77%|███████▋  | 606/785 [03:55<00:52,  3.44it/s] 77%|███████▋  | 607/785 [03:55<00:51,  3.44it/s] 77%|███████▋  | 608/785 [03:55<00:51,  3.44it/s] 78%|███████▊  | 609/785 [03:55<00:51,  3.44it/s] 78%|███████▊  | 610/785 [03:56<00:50,  3.45it/s] 78%|███████▊  | 611/785 [03:56<00:50,  3.45it/s] 78%|███████▊  | 612/785 [03:56<00:50,  3.45it/s] 78%|███████▊  | 613/785 [03:57<00:49,  3.45it/s] 78%|███████▊  | 614/785 [03:57<00:49,  3.45it/s] 78%|███████▊  | 615/785 [03:57<00:49,  3.45it/s] 78%|███████▊  | 616/785 [03:58<00:49,  3.45it/s] 79%|███████▊  | 617/785 [03:58<00:49,  3.42it/s] 79%|███████▊  | 618/785 [03:58<00:48,  3.43it/s] 79%|███████▉  | 619/785 [03:58<00:48,  3.43it/s] 79%|███████▉  | 620/785 [03:59<00:47,  3.44it/s] 79%|███████▉  | 621/785 [03:59<00:47,  3.44it/s] 79%|███████▉  | 622/785 [03:59<00:47,  3.44it/s] 79%|███████▉  | 623/785 [04:00<00:47,  3.44it/s] 79%|███████▉  | 624/785 [04:00<00:46,  3.44it/s] 80%|███████▉  | 625/785 [04:00<00:46,  3.44it/s] 80%|███████▉  | 626/785 [04:00<00:46,  3.44it/s] 80%|███████▉  | 627/785 [04:01<00:45,  3.45it/s] 80%|████████  | 628/785 [04:01<00:42,  3.73it/s][INFO|trainer.py:2140] 2023-08-29 05:01:58,680 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:01:58,680 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 05:01:58,680 >>   Batch size = 8
{'eval_loss': 0.9417624473571777, 'eval_runtime': 13.1464, 'eval_samples_per_second': 369.988, 'eval_steps_per_second': 46.249, 'epoch': 3.0}
{'loss': 0.6693, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.60it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.99it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.30it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.60it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.22it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.94it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.82it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.46it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.51it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.38it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.44it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.41it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.41it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.34it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.39it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.42it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.67it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 45.89it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.11it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.13it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.14it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.19it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.24it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.31it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.31it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.31it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.31it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.30it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.35it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.31it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.34it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.40it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.30it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.31it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.18it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.29it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.34it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.35it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.26it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.30it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.36it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.36it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.33it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.33it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 45.91it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.08it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 46.22it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.18it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.31it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.30it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.31it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.32it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.29it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.30it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.26it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.34it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.30it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.35it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.39it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.26it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.34it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.32it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.19it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.29it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.25it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.37it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.38it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.01it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.12it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.19it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.28it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.24it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.28it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.30it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.23it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.33it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.41it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.27it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.32it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.32it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.27it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.31it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.31it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.26it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.19it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.28it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.36it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.29it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.30it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.19it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 45.99it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.16it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.27it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.26it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.23it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.24it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.31it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.26it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.30it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.29it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.34it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.27it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.31it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.36it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.37it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.41it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.25it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.29it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.22it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.26it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.31it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.30it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.32it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.43it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.28it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.35it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.25it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.29it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.30it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.33it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.24it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [04:14<00:42,  3.73it/s]
100%|██████████| 608/608 [00:13<00:00, 46.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:02:11,840 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-29 05:02:11,858 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:02:14,234 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:02:14,250 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:02:14,264 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:23<17:44,  6.83s/it] 80%|████████  | 630/785 [04:23<12:34,  4.87s/it] 80%|████████  | 631/785 [04:24<08:58,  3.49s/it] 81%|████████  | 632/785 [04:24<06:27,  2.53s/it] 81%|████████  | 633/785 [04:24<04:42,  1.86s/it] 81%|████████  | 634/785 [04:25<03:29,  1.39s/it] 81%|████████  | 635/785 [04:25<02:38,  1.06s/it] 81%|████████  | 636/785 [04:25<02:03,  1.21it/s] 81%|████████  | 637/785 [04:25<01:38,  1.50it/s] 81%|████████▏ | 638/785 [04:26<01:21,  1.81it/s] 81%|████████▏ | 639/785 [04:26<01:09,  2.11it/s] 82%|████████▏ | 640/785 [04:26<01:00,  2.39it/s] 82%|████████▏ | 641/785 [04:27<00:56,  2.57it/s] 82%|████████▏ | 642/785 [04:27<00:51,  2.78it/s] 82%|████████▏ | 643/785 [04:27<00:48,  2.96it/s] 82%|████████▏ | 644/785 [04:27<00:45,  3.09it/s] 82%|████████▏ | 645/785 [04:28<00:43,  3.19it/s] 82%|████████▏ | 646/785 [04:28<00:42,  3.27it/s] 82%|████████▏ | 647/785 [04:28<00:41,  3.32it/s] 83%|████████▎ | 648/785 [04:29<00:40,  3.36it/s] 83%|████████▎ | 649/785 [04:29<00:40,  3.39it/s] 83%|████████▎ | 650/785 [04:29<00:39,  3.41it/s] 83%|████████▎ | 651/785 [04:29<00:39,  3.42it/s] 83%|████████▎ | 652/785 [04:30<00:38,  3.43it/s] 83%|████████▎ | 653/785 [04:30<00:38,  3.44it/s] 83%|████████▎ | 654/785 [04:30<00:38,  3.44it/s] 83%|████████▎ | 655/785 [04:31<00:37,  3.45it/s] 84%|████████▎ | 656/785 [04:31<00:37,  3.45it/s] 84%|████████▎ | 657/785 [04:31<00:37,  3.45it/s] 84%|████████▍ | 658/785 [04:31<00:36,  3.45it/s] 84%|████████▍ | 659/785 [04:32<00:36,  3.45it/s] 84%|████████▍ | 660/785 [04:32<00:36,  3.45it/s] 84%|████████▍ | 661/785 [04:32<00:35,  3.45it/s] 84%|████████▍ | 662/785 [04:33<00:35,  3.45it/s] 84%|████████▍ | 663/785 [04:33<00:35,  3.45it/s] 85%|████████▍ | 664/785 [04:33<00:35,  3.45it/s] 85%|████████▍ | 665/785 [04:34<00:34,  3.45it/s] 85%|████████▍ | 666/785 [04:34<00:34,  3.45it/s] 85%|████████▍ | 667/785 [04:34<00:34,  3.45it/s] 85%|████████▌ | 668/785 [04:34<00:33,  3.45it/s] 85%|████████▌ | 669/785 [04:35<00:33,  3.45it/s] 85%|████████▌ | 670/785 [04:35<00:33,  3.46it/s] 85%|████████▌ | 671/785 [04:35<00:33,  3.45it/s] 86%|████████▌ | 672/785 [04:36<00:32,  3.45it/s] 86%|████████▌ | 673/785 [04:36<00:32,  3.45it/s] 86%|████████▌ | 674/785 [04:36<00:32,  3.45it/s] 86%|████████▌ | 675/785 [04:36<00:31,  3.45it/s] 86%|████████▌ | 676/785 [04:37<00:31,  3.45it/s] 86%|████████▌ | 677/785 [04:37<00:31,  3.45it/s] 86%|████████▋ | 678/785 [04:37<00:31,  3.45it/s] 86%|████████▋ | 679/785 [04:38<00:30,  3.45it/s] 87%|████████▋ | 680/785 [04:38<00:30,  3.45it/s] 87%|████████▋ | 681/785 [04:38<00:30,  3.45it/s] 87%|████████▋ | 682/785 [04:38<00:29,  3.45it/s] 87%|████████▋ | 683/785 [04:39<00:29,  3.45it/s] 87%|████████▋ | 684/785 [04:39<00:29,  3.45it/s] 87%|████████▋ | 685/785 [04:39<00:29,  3.45it/s] 87%|████████▋ | 686/785 [04:40<00:28,  3.45it/s] 88%|████████▊ | 687/785 [04:40<00:28,  3.45it/s] 88%|████████▊ | 688/785 [04:40<00:28,  3.43it/s] 88%|████████▊ | 689/785 [04:40<00:27,  3.44it/s] 88%|████████▊ | 690/785 [04:41<00:27,  3.44it/s] 88%|████████▊ | 691/785 [04:41<00:27,  3.44it/s] 88%|████████▊ | 692/785 [04:41<00:26,  3.45it/s] 88%|████████▊ | 693/785 [04:42<00:26,  3.45it/s] 88%|████████▊ | 694/785 [04:42<00:26,  3.45it/s] 89%|████████▊ | 695/785 [04:42<00:26,  3.45it/s] 89%|████████▊ | 696/785 [04:43<00:25,  3.45it/s] 89%|████████▉ | 697/785 [04:43<00:25,  3.45it/s] 89%|████████▉ | 698/785 [04:43<00:25,  3.45it/s] 89%|████████▉ | 699/785 [04:43<00:25,  3.44it/s] 89%|████████▉ | 700/785 [04:44<00:24,  3.44it/s] 89%|████████▉ | 701/785 [04:44<00:24,  3.45it/s] 89%|████████▉ | 702/785 [04:44<00:24,  3.45it/s] 90%|████████▉ | 703/785 [04:45<00:23,  3.45it/s] 90%|████████▉ | 704/785 [04:45<00:23,  3.45it/s] 90%|████████▉ | 705/785 [04:45<00:23,  3.45it/s] 90%|████████▉ | 706/785 [04:45<00:22,  3.45it/s] 90%|█████████ | 707/785 [04:46<00:22,  3.45it/s] 90%|█████████ | 708/785 [04:46<00:22,  3.45it/s] 90%|█████████ | 709/785 [04:46<00:22,  3.45it/s] 90%|█████████ | 710/785 [04:47<00:21,  3.43it/s] 91%|█████████ | 711/785 [04:47<00:21,  3.44it/s] 91%|█████████ | 712/785 [04:47<00:21,  3.44it/s] 91%|█████████ | 713/785 [04:47<00:20,  3.45it/s] 91%|█████████ | 714/785 [04:48<00:20,  3.45it/s] 91%|█████████ | 715/785 [04:48<00:20,  3.45it/s] 91%|█████████ | 716/785 [04:48<00:20,  3.45it/s] 91%|█████████▏| 717/785 [04:49<00:19,  3.45it/s] 91%|█████████▏| 718/785 [04:49<00:19,  3.45it/s] 92%|█████████▏| 719/785 [04:49<00:19,  3.45it/s] 92%|█████████▏| 720/785 [04:49<00:18,  3.45it/s] 92%|█████████▏| 721/785 [04:50<00:18,  3.44it/s] 92%|█████████▏| 722/785 [04:50<00:18,  3.45it/s] 92%|█████████▏| 723/785 [04:50<00:17,  3.45it/s] 92%|█████████▏| 724/785 [04:51<00:17,  3.45it/s] 92%|█████████▏| 725/785 [04:51<00:17,  3.45it/s] 92%|█████████▏| 726/785 [04:51<00:17,  3.44it/s] 93%|█████████▎| 727/785 [04:52<00:16,  3.44it/s] 93%|█████████▎| 728/785 [04:52<00:16,  3.45it/s] 93%|█████████▎| 729/785 [04:52<00:16,  3.45it/s] 93%|█████████▎| 730/785 [04:52<00:15,  3.45it/s] 93%|█████████▎| 731/785 [04:53<00:15,  3.39it/s] 93%|█████████▎| 732/785 [04:53<00:15,  3.40it/s] 93%|█████████▎| 733/785 [04:53<00:15,  3.41it/s] 94%|█████████▎| 734/785 [04:54<00:14,  3.42it/s] 94%|█████████▎| 735/785 [04:54<00:14,  3.43it/s] 94%|█████████▍| 736/785 [04:54<00:14,  3.44it/s] 94%|█████████▍| 737/785 [04:54<00:13,  3.44it/s] 94%|█████████▍| 738/785 [04:55<00:13,  3.44it/s] 94%|█████████▍| 739/785 [04:55<00:13,  3.44it/s] 94%|█████████▍| 740/785 [04:55<00:13,  3.45it/s] 94%|█████████▍| 741/785 [04:56<00:12,  3.45it/s] 95%|█████████▍| 742/785 [04:56<00:12,  3.45it/s] 95%|█████████▍| 743/785 [04:56<00:12,  3.43it/s] 95%|█████████▍| 744/785 [04:56<00:11,  3.44it/s] 95%|█████████▍| 745/785 [04:57<00:11,  3.44it/s] 95%|█████████▌| 746/785 [04:57<00:11,  3.44it/s] 95%|█████████▌| 747/785 [04:57<00:11,  3.44it/s] 95%|█████████▌| 748/785 [04:58<00:10,  3.45it/s] 95%|█████████▌| 749/785 [04:58<00:10,  3.44it/s] 96%|█████████▌| 750/785 [04:58<00:10,  3.45it/s] 96%|█████████▌| 751/785 [04:58<00:09,  3.44it/s] 96%|█████████▌| 752/785 [04:59<00:09,  3.45it/s] 96%|█████████▌| 753/785 [04:59<00:09,  3.45it/s] 96%|█████████▌| 754/785 [04:59<00:09,  3.43it/s] 96%|█████████▌| 755/785 [05:00<00:08,  3.44it/s] 96%|█████████▋| 756/785 [05:00<00:08,  3.44it/s] 96%|█████████▋| 757/785 [05:00<00:08,  3.44it/s] 97%|█████████▋| 758/785 [05:01<00:07,  3.45it/s] 97%|█████████▋| 759/785 [05:01<00:07,  3.45it/s] 97%|█████████▋| 760/785 [05:01<00:07,  3.45it/s] 97%|█████████▋| 761/785 [05:01<00:06,  3.45it/s] 97%|█████████▋| 762/785 [05:02<00:06,  3.45it/s] 97%|█████████▋| 763/785 [05:02<00:06,  3.45it/s] 97%|█████████▋| 764/785 [05:02<00:06,  3.45it/s] 97%|█████████▋| 765/785 [05:03<00:05,  3.44it/s] 98%|█████████▊| 766/785 [05:03<00:05,  3.44it/s] 98%|█████████▊| 767/785 [05:03<00:05,  3.44it/s] 98%|█████████▊| 768/785 [05:03<00:04,  3.44it/s] 98%|█████████▊| 769/785 [05:04<00:04,  3.44it/s] 98%|█████████▊| 770/785 [05:04<00:04,  3.44it/s] 98%|█████████▊| 771/785 [05:04<00:04,  3.43it/s] 98%|█████████▊| 772/785 [05:05<00:03,  3.43it/s] 98%|█████████▊| 773/785 [05:05<00:03,  3.44it/s] 99%|█████████▊| 774/785 [05:05<00:03,  3.43it/s] 99%|█████████▊| 775/785 [05:05<00:02,  3.44it/s] 99%|█████████▉| 776/785 [05:06<00:02,  3.44it/s] 99%|█████████▉| 777/785 [05:06<00:02,  3.44it/s] 99%|█████████▉| 778/785 [05:06<00:02,  3.44it/s] 99%|█████████▉| 779/785 [05:07<00:01,  3.44it/s] 99%|█████████▉| 780/785 [05:07<00:01,  3.44it/s] 99%|█████████▉| 781/785 [05:07<00:01,  3.44it/s]100%|█████████▉| 782/785 [05:07<00:00,  3.43it/s]100%|█████████▉| 783/785 [05:08<00:00,  3.43it/s]100%|█████████▉| 784/785 [05:08<00:00,  3.44it/s]100%|██████████| 785/785 [05:08<00:00,  3.73it/s][INFO|trainer.py:2140] 2023-08-29 05:03:06,042 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:03:06,042 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 05:03:06,042 >>   Batch size = 8
{'eval_loss': 0.9499561190605164, 'eval_runtime': 13.1422, 'eval_samples_per_second': 370.104, 'eval_steps_per_second': 46.263, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.92it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.94it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.31it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.62it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.25it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.87it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.76it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.43it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.48it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.44it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.44it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.42it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.41it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.31it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.35it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.27it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.34it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.33it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.34it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.30it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.30it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.30it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.32it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.36it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.35it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.22it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.30it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.22it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.30it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.40it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.36it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.30it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.27it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.31it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.33it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.31it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.28it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.24it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.27it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.28it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.30it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.35it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.32it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.39it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.26it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.20it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 46.20it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.25it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.34it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.38it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.27it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.18it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.26it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.31it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.34it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.27it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.23it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.25it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.26it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.31it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.35it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.32it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.22it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.35it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.35it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.27it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.25it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.28it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.23it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.34it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.33it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.31it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.31it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.37it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.06it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.27it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.25it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.25it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.35it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.33it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.32it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.32it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.23it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.32it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.23it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.32it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.22it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.22it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.28it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.31it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.33it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.30it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.31it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.29it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.24it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.30it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.21it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.25it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.32it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.20it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.28it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.30it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.34it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.29it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.25it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.20it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.24it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.25it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.31it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.28it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.31it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.20it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.28it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.28it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.27it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.32it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.33it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.25it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.24it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.28it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.37it/s][A                                                 
                                                 [A100%|██████████| 785/785 [05:21<00:00,  3.73it/s]
100%|██████████| 608/608 [00:13<00:00, 46.37it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:03:19,204 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-29 05:03:19,233 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:03:21,561 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:03:21,580 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:03:21,589 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 05:03:26,004 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 05:03:26,008 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157 (score: 0.9234098196029663).
                                                 100%|██████████| 785/785 [05:30<00:00,  3.73it/s]100%|██████████| 785/785 [05:30<00:00,  2.38it/s]
[INFO|trainer.py:1894] 2023-08-29 05:03:27,654 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 05:03:27,668 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:03:30,281 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:03:30,434 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:03:30,536 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:03:31,100 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:31,100 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:31,100 >>   train_loss               =     0.6587
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:31,100 >>   train_runtime            = 0:05:30.40
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:31,100 >>   train_samples            =      10029
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:31,100 >>   train_samples_per_second =    151.768
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:31,100 >>   train_steps_per_second   =      2.376
{'eval_loss': 0.9540205597877502, 'eval_runtime': 13.1366, 'eval_samples_per_second': 370.265, 'eval_steps_per_second': 46.283, 'epoch': 5.0}
{'train_runtime': 330.406, 'train_samples_per_second': 151.768, 'train_steps_per_second': 2.376, 'train_loss': 0.658722312587082, 'epoch': 5.0}
08/29/2023 05:03:31 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 05:03:31,324 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:03:31,324 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 05:03:31,324 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 58.01it/s]  2%|▏         | 12/608 [00:00<00:11, 50.85it/s]  3%|▎         | 18/608 [00:00<00:12, 48.98it/s]  4%|▍         | 23/608 [00:00<00:12, 48.05it/s]  5%|▍         | 28/608 [00:00<00:12, 47.58it/s]  5%|▌         | 33/608 [00:00<00:12, 47.26it/s]  6%|▋         | 38/608 [00:00<00:12, 46.93it/s]  7%|▋         | 43/608 [00:00<00:12, 46.75it/s]  8%|▊         | 48/608 [00:01<00:12, 46.60it/s]  9%|▊         | 53/608 [00:01<00:11, 46.51it/s] 10%|▉         | 58/608 [00:01<00:11, 46.52it/s] 10%|█         | 63/608 [00:01<00:11, 46.51it/s] 11%|█         | 68/608 [00:01<00:11, 46.39it/s] 12%|█▏        | 73/608 [00:01<00:11, 46.50it/s] 13%|█▎        | 78/608 [00:01<00:11, 46.55it/s] 14%|█▎        | 83/608 [00:01<00:11, 46.60it/s] 14%|█▍        | 88/608 [00:01<00:11, 46.61it/s] 15%|█▌        | 93/608 [00:01<00:11, 46.69it/s] 16%|█▌        | 98/608 [00:02<00:10, 46.70it/s] 17%|█▋        | 103/608 [00:02<00:10, 46.72it/s] 18%|█▊        | 108/608 [00:02<00:10, 46.73it/s] 19%|█▊        | 113/608 [00:02<00:10, 46.71it/s] 19%|█▉        | 118/608 [00:02<00:10, 46.68it/s] 20%|██        | 123/608 [00:02<00:10, 46.74it/s] 21%|██        | 128/608 [00:02<00:10, 46.64it/s] 22%|██▏       | 133/608 [00:02<00:10, 46.65it/s] 23%|██▎       | 138/608 [00:02<00:10, 46.70it/s] 24%|██▎       | 143/608 [00:03<00:09, 46.63it/s] 24%|██▍       | 148/608 [00:03<00:09, 46.69it/s] 25%|██▌       | 153/608 [00:03<00:09, 46.75it/s] 26%|██▌       | 158/608 [00:03<00:09, 46.76it/s] 27%|██▋       | 163/608 [00:03<00:09, 46.71it/s] 28%|██▊       | 168/608 [00:03<00:09, 46.69it/s] 28%|██▊       | 173/608 [00:03<00:09, 46.72it/s] 29%|██▉       | 178/608 [00:03<00:09, 46.63it/s] 30%|███       | 183/608 [00:03<00:09, 46.61it/s] 31%|███       | 188/608 [00:04<00:08, 46.73it/s] 32%|███▏      | 193/608 [00:04<00:08, 46.68it/s] 33%|███▎      | 198/608 [00:04<00:09, 44.00it/s] 33%|███▎      | 203/608 [00:04<00:09, 44.77it/s] 34%|███▍      | 208/608 [00:04<00:08, 45.24it/s] 35%|███▌      | 213/608 [00:04<00:08, 45.70it/s] 36%|███▌      | 218/608 [00:04<00:08, 45.96it/s] 37%|███▋      | 223/608 [00:04<00:08, 46.19it/s] 38%|███▊      | 228/608 [00:04<00:08, 46.25it/s] 38%|███▊      | 233/608 [00:04<00:08, 46.43it/s] 39%|███▉      | 238/608 [00:05<00:07, 46.42it/s] 40%|███▉      | 243/608 [00:05<00:07, 46.51it/s] 41%|████      | 248/608 [00:05<00:07, 46.51it/s] 42%|████▏     | 253/608 [00:05<00:07, 46.51it/s] 42%|████▏     | 258/608 [00:05<00:07, 46.58it/s] 43%|████▎     | 263/608 [00:05<00:07, 46.64it/s] 44%|████▍     | 268/608 [00:05<00:07, 46.62it/s] 45%|████▍     | 273/608 [00:05<00:07, 46.59it/s] 46%|████▌     | 278/608 [00:05<00:07, 46.62it/s] 47%|████▋     | 283/608 [00:06<00:06, 46.72it/s] 47%|████▋     | 288/608 [00:06<00:06, 46.70it/s] 48%|████▊     | 293/608 [00:06<00:06, 46.72it/s] 49%|████▉     | 298/608 [00:06<00:06, 46.69it/s] 50%|████▉     | 303/608 [00:06<00:06, 46.61it/s] 51%|█████     | 308/608 [00:06<00:06, 46.63it/s] 51%|█████▏    | 313/608 [00:06<00:06, 46.60it/s] 52%|█████▏    | 318/608 [00:06<00:06, 46.67it/s] 53%|█████▎    | 323/608 [00:06<00:06, 46.61it/s] 54%|█████▍    | 328/608 [00:07<00:06, 46.51it/s] 55%|█████▍    | 333/608 [00:07<00:05, 46.66it/s] 56%|█████▌    | 338/608 [00:07<00:05, 46.73it/s] 56%|█████▋    | 343/608 [00:07<00:05, 46.72it/s] 57%|█████▋    | 348/608 [00:07<00:05, 46.69it/s] 58%|█████▊    | 353/608 [00:07<00:05, 46.73it/s] 59%|█████▉    | 358/608 [00:07<00:05, 46.57it/s] 60%|█████▉    | 363/608 [00:07<00:05, 46.69it/s] 61%|██████    | 368/608 [00:07<00:05, 46.59it/s] 61%|██████▏   | 373/608 [00:07<00:05, 46.61it/s] 62%|██████▏   | 378/608 [00:08<00:04, 46.56it/s] 63%|██████▎   | 383/608 [00:08<00:04, 46.63it/s] 64%|██████▍   | 388/608 [00:08<00:04, 46.61it/s] 65%|██████▍   | 393/608 [00:08<00:04, 46.63it/s] 65%|██████▌   | 398/608 [00:08<00:04, 46.64it/s] 66%|██████▋   | 403/608 [00:08<00:04, 46.64it/s] 67%|██████▋   | 408/608 [00:08<00:04, 46.64it/s] 68%|██████▊   | 413/608 [00:08<00:04, 46.69it/s] 69%|██████▉   | 418/608 [00:08<00:04, 46.55it/s] 70%|██████▉   | 423/608 [00:09<00:03, 46.63it/s] 70%|███████   | 428/608 [00:09<00:03, 46.62it/s] 71%|███████   | 433/608 [00:09<00:03, 46.59it/s] 72%|███████▏  | 438/608 [00:09<00:03, 46.63it/s] 73%|███████▎  | 443/608 [00:09<00:03, 46.66it/s] 74%|███████▎  | 448/608 [00:09<00:03, 46.65it/s] 75%|███████▍  | 453/608 [00:09<00:03, 46.57it/s] 75%|███████▌  | 458/608 [00:09<00:03, 46.60it/s] 76%|███████▌  | 463/608 [00:09<00:03, 46.65it/s] 77%|███████▋  | 468/608 [00:10<00:03, 46.58it/s] 78%|███████▊  | 473/608 [00:10<00:02, 46.62it/s] 79%|███████▊  | 478/608 [00:10<00:02, 46.49it/s] 79%|███████▉  | 483/608 [00:10<00:02, 46.59it/s] 80%|████████  | 488/608 [00:10<00:02, 46.59it/s] 81%|████████  | 493/608 [00:10<00:02, 46.60it/s] 82%|████████▏ | 498/608 [00:10<00:02, 46.58it/s] 83%|████████▎ | 503/608 [00:10<00:02, 46.61it/s] 84%|████████▎ | 508/608 [00:10<00:02, 46.53it/s] 84%|████████▍ | 513/608 [00:10<00:02, 46.59it/s] 85%|████████▌ | 518/608 [00:11<00:01, 46.58it/s] 86%|████████▌ | 523/608 [00:11<00:01, 46.63it/s] 87%|████████▋ | 528/608 [00:11<00:01, 46.62it/s] 88%|████████▊ | 533/608 [00:11<00:01, 46.62it/s] 88%|████████▊ | 538/608 [00:11<00:01, 46.49it/s] 89%|████████▉ | 543/608 [00:11<00:01, 46.59it/s] 90%|█████████ | 548/608 [00:11<00:01, 46.54it/s] 91%|█████████ | 553/608 [00:11<00:01, 46.61it/s] 92%|█████████▏| 558/608 [00:11<00:01, 46.64it/s] 93%|█████████▎| 563/608 [00:12<00:00, 46.65it/s] 93%|█████████▎| 568/608 [00:12<00:00, 46.66it/s] 94%|█████████▍| 573/608 [00:12<00:00, 46.55it/s] 95%|█████████▌| 578/608 [00:12<00:00, 46.50it/s] 96%|█████████▌| 583/608 [00:12<00:00, 46.61it/s] 97%|█████████▋| 588/608 [00:12<00:00, 46.57it/s] 98%|█████████▊| 593/608 [00:12<00:00, 46.61it/s] 98%|█████████▊| 598/608 [00:12<00:00, 46.63it/s] 99%|█████████▉| 603/608 [00:12<00:00, 46.53it/s]100%|██████████| 608/608 [00:13<00:00, 46.61it/s]100%|██████████| 608/608 [00:13<00:00, 46.59it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:03:44,395 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:44,395 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:44,395 >>   eval_loss               =     0.9234
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:44,395 >>   eval_runtime            = 0:00:13.07
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:44,395 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:44,395 >>   eval_samples_per_second =    372.129
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:44,395 >>   eval_steps_per_second   =     46.516
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:03:44,395 >>   perplexity              =     2.5179
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:03:50,161 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:03:50,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:03:50,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:03:50,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:03:50,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:03:50,884 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:03:50,885 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:03:51,146 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:03:52,193 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:03:52,193 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:03:55,072 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:03:55,076 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:03:55,076 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:03:55,076 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:03:55,076 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:03:55,715 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:03:55,717 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:03:56,277 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:03:56,445 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:03:56,445 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.70it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.68it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.70it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.70it/s]Extractor Predicting: 16it [00:09,  1.69it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.68it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.58it/s]Extractor Predicting: 24it [00:14,  1.52it/s]Extractor Predicting: 25it [00:15,  1.48it/s]Extractor Predicting: 26it [00:16,  1.48it/s]Extractor Predicting: 27it [00:16,  1.50it/s]Extractor Predicting: 28it [00:17,  1.50it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:18,  1.56it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:21,  1.55it/s]Extractor Predicting: 36it [00:22,  1.55it/s]Extractor Predicting: 37it [00:23,  1.53it/s]Extractor Predicting: 38it [00:23,  1.54it/s]Extractor Predicting: 39it [00:24,  1.52it/s]Extractor Predicting: 40it [00:25,  1.52it/s]Extractor Predicting: 41it [00:25,  1.54it/s]Extractor Predicting: 42it [00:26,  1.53it/s]Extractor Predicting: 43it [00:27,  1.42it/s]Extractor Predicting: 44it [00:27,  1.49it/s]Extractor Predicting: 45it [00:28,  1.49it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:29,  1.52it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:31,  1.52it/s]Extractor Predicting: 50it [00:31,  1.53it/s]Extractor Predicting: 51it [00:32,  1.52it/s]Extractor Predicting: 52it [00:33,  1.52it/s]Extractor Predicting: 53it [00:33,  1.51it/s]Extractor Predicting: 54it [00:34,  1.57it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.55it/s]Extractor Predicting: 58it [00:36,  1.54it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:38,  1.53it/s]Extractor Predicting: 61it [00:38,  1.49it/s]Extractor Predicting: 62it [00:39,  1.50it/s]Extractor Predicting: 63it [00:40,  1.51it/s]Extractor Predicting: 64it [00:40,  1.53it/s]Extractor Predicting: 65it [00:41,  1.52it/s]Extractor Predicting: 66it [00:42,  1.53it/s]Extractor Predicting: 67it [00:42,  1.54it/s]Extractor Predicting: 68it [00:43,  1.50it/s]Extractor Predicting: 69it [00:44,  1.50it/s]Extractor Predicting: 70it [00:44,  1.53it/s]Extractor Predicting: 71it [00:45,  1.57it/s]Extractor Predicting: 72it [00:46,  1.56it/s]Extractor Predicting: 73it [00:46,  1.56it/s]Extractor Predicting: 74it [00:47,  1.55it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:48,  1.55it/s]Extractor Predicting: 77it [00:49,  1.55it/s]Extractor Predicting: 78it [00:49,  1.57it/s]Extractor Predicting: 79it [00:50,  1.57it/s]Extractor Predicting: 80it [00:51,  1.59it/s]Extractor Predicting: 81it [00:51,  1.56it/s]Extractor Predicting: 82it [00:52,  1.55it/s]Extractor Predicting: 83it [00:53,  1.53it/s]Extractor Predicting: 84it [00:53,  1.55it/s]Extractor Predicting: 85it [00:54,  1.55it/s]Extractor Predicting: 86it [00:55,  1.53it/s]Extractor Predicting: 87it [00:55,  1.56it/s]Extractor Predicting: 88it [00:56,  1.54it/s]Extractor Predicting: 89it [00:57,  1.48it/s]Extractor Predicting: 90it [00:57,  1.52it/s]Extractor Predicting: 91it [00:58,  1.55it/s]Extractor Predicting: 92it [00:58,  1.56it/s]Extractor Predicting: 93it [00:59,  1.58it/s]Extractor Predicting: 94it [01:00,  1.56it/s]Extractor Predicting: 95it [01:00,  1.51it/s]Extractor Predicting: 96it [01:01,  1.53it/s]Extractor Predicting: 97it [01:02,  1.53it/s]Extractor Predicting: 98it [01:02,  1.54it/s]Extractor Predicting: 99it [01:03,  1.56it/s]Extractor Predicting: 100it [01:04,  1.55it/s]Extractor Predicting: 101it [01:04,  1.55it/s]Extractor Predicting: 102it [01:05,  1.57it/s]Extractor Predicting: 103it [01:06,  1.59it/s]Extractor Predicting: 104it [01:06,  1.58it/s]Extractor Predicting: 105it [01:07,  1.54it/s]Extractor Predicting: 106it [01:08,  1.54it/s]Extractor Predicting: 107it [01:08,  1.55it/s]Extractor Predicting: 108it [01:09,  1.58it/s]Extractor Predicting: 109it [01:09,  1.54it/s]Extractor Predicting: 110it [01:10,  1.50it/s]Extractor Predicting: 111it [01:11,  1.53it/s]Extractor Predicting: 112it [01:11,  1.53it/s]Extractor Predicting: 113it [01:12,  1.55it/s]Extractor Predicting: 114it [01:13,  1.58it/s]Extractor Predicting: 115it [01:13,  1.63it/s]Extractor Predicting: 116it [01:14,  1.62it/s]Extractor Predicting: 117it [01:14,  1.62it/s]Extractor Predicting: 118it [01:15,  1.64it/s]Extractor Predicting: 119it [01:16,  1.60it/s]Extractor Predicting: 120it [01:16,  1.63it/s]Extractor Predicting: 121it [01:17,  1.59it/s]Extractor Predicting: 122it [01:18,  1.54it/s]Extractor Predicting: 123it [01:18,  1.50it/s]Extractor Predicting: 124it [01:19,  1.47it/s]Extractor Predicting: 125it [01:20,  1.49it/s]Extractor Predicting: 126it [01:21,  1.38it/s]Extractor Predicting: 127it [01:21,  1.42it/s]Extractor Predicting: 128it [01:22,  1.42it/s]Extractor Predicting: 129it [01:23,  1.46it/s]Extractor Predicting: 130it [01:23,  1.50it/s]Extractor Predicting: 131it [01:24,  1.55it/s]Extractor Predicting: 132it [01:25,  1.51it/s]Extractor Predicting: 133it [01:25,  1.51it/s]Extractor Predicting: 134it [01:26,  1.53it/s]Extractor Predicting: 135it [01:26,  1.56it/s]Extractor Predicting: 136it [01:27,  1.56it/s]Extractor Predicting: 137it [01:28,  1.57it/s]Extractor Predicting: 138it [01:28,  1.55it/s]Extractor Predicting: 139it [01:29,  1.55it/s]Extractor Predicting: 140it [01:30,  1.53it/s]Extractor Predicting: 141it [01:30,  1.57it/s]Extractor Predicting: 142it [01:31,  1.54it/s]Extractor Predicting: 143it [01:32,  1.57it/s]Extractor Predicting: 144it [01:32,  1.57it/s]Extractor Predicting: 145it [01:33,  1.57it/s]Extractor Predicting: 146it [01:34,  1.52it/s]Extractor Predicting: 147it [01:34,  1.50it/s]Extractor Predicting: 148it [01:35,  1.51it/s]Extractor Predicting: 149it [01:36,  1.50it/s]Extractor Predicting: 150it [01:36,  1.53it/s]Extractor Predicting: 151it [01:37,  1.51it/s]Extractor Predicting: 152it [01:37,  1.55it/s]Extractor Predicting: 153it [01:38,  1.53it/s]Extractor Predicting: 154it [01:39,  1.53it/s]Extractor Predicting: 155it [01:39,  1.55it/s]Extractor Predicting: 156it [01:40,  1.58it/s]Extractor Predicting: 157it [01:41,  1.57it/s]Extractor Predicting: 158it [01:41,  1.59it/s]Extractor Predicting: 159it [01:42,  1.56it/s]Extractor Predicting: 160it [01:43,  1.57it/s]Extractor Predicting: 161it [01:43,  1.55it/s]Extractor Predicting: 162it [01:44,  1.53it/s]Extractor Predicting: 163it [01:45,  1.54it/s]Extractor Predicting: 164it [01:45,  1.53it/s]Extractor Predicting: 165it [01:46,  1.56it/s]Extractor Predicting: 166it [01:47,  1.52it/s]Extractor Predicting: 167it [01:47,  1.51it/s]Extractor Predicting: 168it [01:48,  1.48it/s]Extractor Predicting: 169it [01:49,  1.46it/s]Extractor Predicting: 170it [01:49,  1.47it/s]Extractor Predicting: 171it [01:50,  1.47it/s]Extractor Predicting: 172it [01:51,  1.48it/s]Extractor Predicting: 173it [01:51,  1.46it/s]Extractor Predicting: 174it [01:52,  1.43it/s]Extractor Predicting: 175it [01:53,  1.48it/s]Extractor Predicting: 176it [01:53,  1.46it/s]Extractor Predicting: 177it [01:54,  1.44it/s]Extractor Predicting: 178it [01:55,  1.40it/s]Extractor Predicting: 179it [01:56,  1.38it/s]Extractor Predicting: 180it [01:56,  1.39it/s]Extractor Predicting: 181it [01:57,  1.42it/s]Extractor Predicting: 182it [01:58,  1.41it/s]Extractor Predicting: 183it [01:58,  1.41it/s]Extractor Predicting: 184it [01:59,  1.42it/s]Extractor Predicting: 185it [02:00,  1.46it/s]Extractor Predicting: 186it [02:00,  1.45it/s]Extractor Predicting: 187it [02:01,  1.56it/s]Extractor Predicting: 187it [02:01,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:08,236 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:08,241 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:08,241 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:08,241 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:08,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:06:08,863 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:06:08,864 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:06:09,491 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:06:10,533 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:06:10,533 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:13,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:13,462 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:13,462 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:13,462 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:13,462 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:06:14,125 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:06:14,126 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:06:14,684 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:06:14,849 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:06:14,849 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5498154981549815,
  "recall": 0.030633223684210526,
  "score": 0.05803310613437196,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.57it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.49it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.56it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:23,  1.53it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:25,  1.53it/s]Extractor Predicting: 40it [00:25,  1.53it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:29,  1.55it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:31,  1.47it/s]Extractor Predicting: 50it [00:32,  1.49it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:33,  1.49it/s]Extractor Predicting: 53it [00:34,  1.55it/s]Extractor Predicting: 54it [00:35,  1.55it/s]Extractor Predicting: 55it [00:35,  1.56it/s]Extractor Predicting: 56it [00:36,  1.56it/s]Extractor Predicting: 57it [00:37,  1.53it/s]Extractor Predicting: 58it [00:37,  1.55it/s]Extractor Predicting: 59it [00:38,  1.55it/s]Extractor Predicting: 60it [00:38,  1.57it/s]Extractor Predicting: 61it [00:39,  1.54it/s]Extractor Predicting: 62it [00:40,  1.54it/s]Extractor Predicting: 63it [00:40,  1.54it/s]Extractor Predicting: 64it [00:41,  1.53it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:43,  1.56it/s]Extractor Predicting: 68it [00:44,  1.47it/s]Extractor Predicting: 69it [00:44,  1.51it/s]Extractor Predicting: 70it [00:45,  1.45it/s]Extractor Predicting: 71it [00:46,  1.49it/s]Extractor Predicting: 72it [00:46,  1.49it/s]Extractor Predicting: 73it [00:47,  1.55it/s]Extractor Predicting: 74it [00:48,  1.55it/s]Extractor Predicting: 75it [00:48,  1.56it/s]Extractor Predicting: 76it [00:49,  1.53it/s]Extractor Predicting: 77it [00:50,  1.55it/s]Extractor Predicting: 78it [00:50,  1.59it/s]Extractor Predicting: 79it [00:51,  1.55it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:52,  1.58it/s]Extractor Predicting: 82it [00:53,  1.57it/s]Extractor Predicting: 83it [00:53,  1.57it/s]Extractor Predicting: 84it [00:54,  1.52it/s]Extractor Predicting: 85it [00:55,  1.39it/s]Extractor Predicting: 86it [00:56,  1.45it/s]Extractor Predicting: 87it [00:56,  1.52it/s]Extractor Predicting: 88it [00:57,  1.57it/s]Extractor Predicting: 89it [00:57,  1.58it/s]Extractor Predicting: 90it [00:58,  1.55it/s]Extractor Predicting: 91it [00:59,  1.60it/s]Extractor Predicting: 92it [00:59,  1.63it/s]Extractor Predicting: 93it [01:00,  1.61it/s]Extractor Predicting: 94it [01:00,  1.63it/s]Extractor Predicting: 95it [01:01,  1.63it/s]Extractor Predicting: 96it [01:02,  1.64it/s]Extractor Predicting: 97it [01:02,  1.62it/s]Extractor Predicting: 98it [01:03,  1.60it/s]Extractor Predicting: 99it [01:04,  1.58it/s]Extractor Predicting: 100it [01:04,  1.61it/s]Extractor Predicting: 101it [01:05,  1.60it/s]Extractor Predicting: 102it [01:05,  1.60it/s]Extractor Predicting: 103it [01:06,  1.58it/s]Extractor Predicting: 104it [01:07,  1.57it/s]Extractor Predicting: 105it [01:07,  1.55it/s]Extractor Predicting: 106it [01:08,  1.54it/s]Extractor Predicting: 107it [01:09,  1.56it/s]Extractor Predicting: 108it [01:09,  1.57it/s]Extractor Predicting: 109it [01:10,  1.53it/s]Extractor Predicting: 110it [01:11,  1.53it/s]Extractor Predicting: 111it [01:11,  1.52it/s]Extractor Predicting: 112it [01:12,  1.55it/s]Extractor Predicting: 113it [01:13,  1.56it/s]Extractor Predicting: 114it [01:13,  1.57it/s]Extractor Predicting: 115it [01:14,  1.55it/s]Extractor Predicting: 116it [01:15,  1.54it/s]Extractor Predicting: 117it [01:15,  1.55it/s]Extractor Predicting: 118it [01:16,  1.54it/s]Extractor Predicting: 119it [01:16,  1.55it/s]Extractor Predicting: 120it [01:17,  1.53it/s]Extractor Predicting: 121it [01:18,  1.55it/s]Extractor Predicting: 122it [01:18,  1.56it/s]Extractor Predicting: 123it [01:19,  1.57it/s]Extractor Predicting: 124it [01:20,  1.58it/s]Extractor Predicting: 125it [01:20,  1.56it/s]Extractor Predicting: 126it [01:21,  1.55it/s]Extractor Predicting: 127it [01:22,  1.57it/s]Extractor Predicting: 128it [01:22,  1.53it/s]Extractor Predicting: 129it [01:23,  1.51it/s]Extractor Predicting: 130it [01:24,  1.55it/s]Extractor Predicting: 131it [01:24,  1.55it/s]Extractor Predicting: 132it [01:25,  1.53it/s]Extractor Predicting: 133it [01:26,  1.56it/s]Extractor Predicting: 134it [01:26,  1.54it/s]Extractor Predicting: 135it [01:27,  1.55it/s]Extractor Predicting: 136it [01:27,  1.55it/s]Extractor Predicting: 137it [01:28,  1.53it/s]Extractor Predicting: 138it [01:29,  1.51it/s]Extractor Predicting: 139it [01:29,  1.53it/s]Extractor Predicting: 140it [01:30,  1.53it/s]Extractor Predicting: 141it [01:31,  1.52it/s]Extractor Predicting: 142it [01:31,  1.51it/s]Extractor Predicting: 143it [01:32,  1.53it/s]Extractor Predicting: 144it [01:33,  1.52it/s]Extractor Predicting: 145it [01:33,  1.51it/s]Extractor Predicting: 146it [01:34,  1.51it/s]Extractor Predicting: 147it [01:35,  1.51it/s]Extractor Predicting: 148it [01:35,  1.53it/s]Extractor Predicting: 149it [01:36,  1.52it/s]Extractor Predicting: 150it [01:37,  1.48it/s]Extractor Predicting: 151it [01:37,  1.48it/s]Extractor Predicting: 152it [01:38,  1.49it/s]Extractor Predicting: 153it [01:39,  1.54it/s]Extractor Predicting: 154it [01:39,  1.55it/s]Extractor Predicting: 155it [01:40,  1.57it/s]Extractor Predicting: 156it [01:41,  1.54it/s]Extractor Predicting: 157it [01:41,  1.55it/s]Extractor Predicting: 158it [01:42,  1.54it/s]Extractor Predicting: 159it [01:43,  1.53it/s]Extractor Predicting: 160it [01:43,  1.52it/s]Extractor Predicting: 161it [01:44,  1.53it/s]Extractor Predicting: 162it [01:45,  1.52it/s]Extractor Predicting: 163it [01:45,  1.54it/s]Extractor Predicting: 164it [01:46,  1.53it/s]Extractor Predicting: 165it [01:46,  1.55it/s]Extractor Predicting: 166it [01:47,  1.55it/s]Extractor Predicting: 167it [01:48,  1.55it/s]Extractor Predicting: 168it [01:48,  1.53it/s]Extractor Predicting: 169it [01:49,  1.52it/s]Extractor Predicting: 170it [01:50,  1.55it/s]Extractor Predicting: 171it [01:50,  1.60it/s]Extractor Predicting: 172it [01:51,  1.59it/s]Extractor Predicting: 173it [01:52,  1.56it/s]Extractor Predicting: 174it [01:52,  1.54it/s]Extractor Predicting: 175it [01:53,  1.49it/s]Extractor Predicting: 176it [01:54,  1.48it/s]Extractor Predicting: 177it [01:54,  1.51it/s]Extractor Predicting: 178it [01:55,  1.49it/s]Extractor Predicting: 179it [01:56,  1.50it/s]Extractor Predicting: 180it [01:56,  1.48it/s]Extractor Predicting: 181it [01:57,  1.48it/s]Extractor Predicting: 182it [01:58,  1.48it/s]Extractor Predicting: 183it [01:58,  1.47it/s]Extractor Predicting: 184it [01:59,  1.47it/s]Extractor Predicting: 185it [02:00,  1.48it/s]Extractor Predicting: 186it [02:00,  1.49it/s]Extractor Predicting: 187it [02:01,  1.47it/s]Extractor Predicting: 188it [02:02,  1.50it/s]Extractor Predicting: 189it [02:02,  1.50it/s]Extractor Predicting: 190it [02:03,  1.49it/s]Extractor Predicting: 191it [02:04,  1.51it/s]Extractor Predicting: 192it [02:04,  1.50it/s]Extractor Predicting: 193it [02:05,  1.50it/s]Extractor Predicting: 194it [02:06,  1.51it/s]Extractor Predicting: 195it [02:06,  1.55it/s]Extractor Predicting: 196it [02:07,  1.57it/s]Extractor Predicting: 197it [02:08,  1.56it/s]Extractor Predicting: 198it [02:08,  1.56it/s]Extractor Predicting: 199it [02:09,  1.55it/s]Extractor Predicting: 200it [02:10,  1.57it/s]Extractor Predicting: 201it [02:10,  1.55it/s]Extractor Predicting: 202it [02:11,  1.54it/s]Extractor Predicting: 203it [02:12,  1.54it/s]Extractor Predicting: 204it [02:12,  1.53it/s]Extractor Predicting: 205it [02:13,  1.53it/s]Extractor Predicting: 206it [02:13,  1.55it/s]Extractor Predicting: 207it [02:14,  1.54it/s]Extractor Predicting: 208it [02:15,  1.57it/s]Extractor Predicting: 209it [02:15,  1.58it/s]Extractor Predicting: 210it [02:16,  1.55it/s]Extractor Predicting: 211it [02:17,  1.57it/s]Extractor Predicting: 212it [02:18,  1.38it/s]Extractor Predicting: 213it [02:18,  1.40it/s]Extractor Predicting: 214it [02:19,  1.41it/s]Extractor Predicting: 215it [02:20,  1.44it/s]Extractor Predicting: 216it [02:20,  1.47it/s]Extractor Predicting: 217it [02:21,  1.52it/s]Extractor Predicting: 218it [02:22,  1.50it/s]Extractor Predicting: 219it [02:22,  1.49it/s]Extractor Predicting: 220it [02:23,  1.54it/s]Extractor Predicting: 221it [02:24,  1.50it/s]Extractor Predicting: 222it [02:24,  1.49it/s]Extractor Predicting: 223it [02:25,  1.50it/s]Extractor Predicting: 224it [02:25,  1.53it/s]Extractor Predicting: 225it [02:26,  1.55it/s]Extractor Predicting: 226it [02:27,  1.56it/s]Extractor Predicting: 227it [02:27,  1.56it/s]Extractor Predicting: 228it [02:28,  1.56it/s]Extractor Predicting: 229it [02:29,  1.55it/s]Extractor Predicting: 230it [02:29,  1.58it/s]Extractor Predicting: 231it [02:30,  1.57it/s]Extractor Predicting: 232it [02:31,  1.57it/s]Extractor Predicting: 233it [02:31,  1.57it/s]Extractor Predicting: 234it [02:32,  1.53it/s]Extractor Predicting: 235it [02:33,  1.52it/s]Extractor Predicting: 236it [02:33,  1.53it/s]Extractor Predicting: 237it [02:34,  1.51it/s]Extractor Predicting: 238it [02:35,  1.53it/s]Extractor Predicting: 239it [02:35,  1.52it/s]Extractor Predicting: 240it [02:36,  1.54it/s]Extractor Predicting: 241it [02:36,  1.57it/s]Extractor Predicting: 242it [02:37,  1.54it/s]Extractor Predicting: 243it [02:38,  1.58it/s]Extractor Predicting: 244it [02:38,  1.58it/s]Extractor Predicting: 245it [02:39,  1.55it/s]Extractor Predicting: 246it [02:40,  1.57it/s]Extractor Predicting: 247it [02:40,  1.55it/s]Extractor Predicting: 248it [02:41,  1.57it/s]Extractor Predicting: 249it [02:42,  1.56it/s]Extractor Predicting: 250it [02:42,  1.61it/s]Extractor Predicting: 251it [02:43,  1.58it/s]Extractor Predicting: 252it [02:43,  1.59it/s]Extractor Predicting: 253it [02:44,  1.64it/s]Extractor Predicting: 254it [02:45,  1.59it/s]Extractor Predicting: 255it [02:45,  1.59it/s]Extractor Predicting: 256it [02:46,  1.58it/s]Extractor Predicting: 257it [02:47,  1.58it/s]Extractor Predicting: 258it [02:47,  1.59it/s]Extractor Predicting: 259it [02:48,  1.58it/s]Extractor Predicting: 260it [02:48,  1.57it/s]Extractor Predicting: 261it [02:49,  1.54it/s]Extractor Predicting: 262it [02:50,  1.55it/s]Extractor Predicting: 263it [02:51,  1.50it/s]Extractor Predicting: 264it [02:51,  1.53it/s]Extractor Predicting: 265it [02:52,  1.52it/s]Extractor Predicting: 266it [02:52,  1.51it/s]Extractor Predicting: 267it [02:53,  1.51it/s]Extractor Predicting: 268it [02:54,  1.51it/s]Extractor Predicting: 269it [02:54,  1.48it/s]Extractor Predicting: 270it [02:55,  1.50it/s]Extractor Predicting: 271it [02:56,  1.50it/s]Extractor Predicting: 272it [02:56,  1.55it/s]Extractor Predicting: 273it [02:57,  1.53it/s]Extractor Predicting: 274it [02:58,  1.51it/s]Extractor Predicting: 275it [02:58,  1.50it/s]Extractor Predicting: 276it [02:59,  1.54it/s]Extractor Predicting: 277it [03:00,  1.52it/s]Extractor Predicting: 278it [03:00,  1.53it/s]Extractor Predicting: 279it [03:01,  1.51it/s]Extractor Predicting: 280it [03:02,  1.51it/s]Extractor Predicting: 281it [03:02,  1.52it/s]Extractor Predicting: 282it [03:03,  1.52it/s]Extractor Predicting: 283it [03:04,  1.52it/s]Extractor Predicting: 284it [03:04,  1.54it/s]Extractor Predicting: 285it [03:05,  1.55it/s]Extractor Predicting: 286it [03:06,  1.55it/s]Extractor Predicting: 287it [03:06,  1.52it/s]Extractor Predicting: 288it [03:07,  1.52it/s]Extractor Predicting: 289it [03:08,  1.51it/s]Extractor Predicting: 290it [03:08,  1.49it/s]Extractor Predicting: 291it [03:09,  1.50it/s]Extractor Predicting: 292it [03:10,  1.52it/s]Extractor Predicting: 293it [03:10,  1.50it/s]Extractor Predicting: 294it [03:11,  1.47it/s]Extractor Predicting: 295it [03:12,  1.48it/s]Extractor Predicting: 296it [03:12,  1.49it/s]Extractor Predicting: 297it [03:13,  1.48it/s]Extractor Predicting: 298it [03:14,  1.49it/s]Extractor Predicting: 299it [03:14,  1.48it/s]Extractor Predicting: 300it [03:15,  1.44it/s]Extractor Predicting: 301it [03:16,  1.40it/s]Extractor Predicting: 302it [03:17,  1.39it/s]Extractor Predicting: 303it [03:17,  1.42it/s]Extractor Predicting: 304it [03:18,  1.38it/s]Extractor Predicting: 305it [03:19,  1.36it/s]Extractor Predicting: 306it [03:19,  1.42it/s]Extractor Predicting: 307it [03:20,  1.46it/s]Extractor Predicting: 308it [03:21,  1.48it/s]Extractor Predicting: 309it [03:21,  1.54it/s]Extractor Predicting: 310it [03:22,  1.52it/s]Extractor Predicting: 311it [03:23,  1.48it/s]Extractor Predicting: 312it [03:23,  1.49it/s]Extractor Predicting: 313it [03:24,  1.50it/s]Extractor Predicting: 314it [03:25,  1.49it/s]Extractor Predicting: 315it [03:25,  1.56it/s]Extractor Predicting: 316it [03:26,  1.56it/s]Extractor Predicting: 317it [03:27,  1.55it/s]Extractor Predicting: 318it [03:27,  1.54it/s]Extractor Predicting: 319it [03:28,  1.52it/s]Extractor Predicting: 320it [03:29,  1.53it/s]Extractor Predicting: 321it [03:29,  1.55it/s]Extractor Predicting: 322it [03:30,  1.58it/s]Extractor Predicting: 323it [03:30,  1.58it/s]Extractor Predicting: 324it [03:31,  1.58it/s]Extractor Predicting: 325it [03:32,  1.57it/s]Extractor Predicting: 326it [03:32,  1.58it/s]Extractor Predicting: 327it [03:33,  1.58it/s]Extractor Predicting: 328it [03:34,  1.56it/s]Extractor Predicting: 329it [03:34,  1.55it/s]Extractor Predicting: 330it [03:35,  1.54it/s]Extractor Predicting: 331it [03:36,  1.53it/s]Extractor Predicting: 332it [03:36,  1.56it/s]Extractor Predicting: 333it [03:37,  1.54it/s]Extractor Predicting: 334it [03:37,  1.54it/s]Extractor Predicting: 335it [03:38,  1.55it/s]Extractor Predicting: 336it [03:39,  1.56it/s]Extractor Predicting: 337it [03:39,  1.56it/s]Extractor Predicting: 338it [03:40,  1.56it/s]Extractor Predicting: 339it [03:41,  1.56it/s]Extractor Predicting: 340it [03:41,  1.57it/s]Extractor Predicting: 341it [03:42,  1.57it/s]Extractor Predicting: 342it [03:43,  1.54it/s]Extractor Predicting: 343it [03:43,  1.54it/s]Extractor Predicting: 344it [03:44,  1.57it/s]Extractor Predicting: 345it [03:45,  1.40it/s]Extractor Predicting: 346it [03:45,  1.48it/s]Extractor Predicting: 347it [03:46,  1.55it/s]Extractor Predicting: 348it [03:47,  1.55it/s]Extractor Predicting: 349it [03:47,  1.58it/s]Extractor Predicting: 350it [03:48,  1.59it/s]Extractor Predicting: 351it [03:48,  1.60it/s]Extractor Predicting: 352it [03:49,  1.60it/s]Extractor Predicting: 353it [03:50,  1.61it/s]Extractor Predicting: 354it [03:50,  1.61it/s]Extractor Predicting: 355it [03:51,  1.62it/s]Extractor Predicting: 356it [03:51,  1.65it/s]Extractor Predicting: 357it [03:52,  1.62it/s]Extractor Predicting: 358it [03:53,  1.61it/s]Extractor Predicting: 359it [03:53,  1.61it/s]Extractor Predicting: 360it [03:54,  1.60it/s]Extractor Predicting: 361it [03:55,  1.60it/s]Extractor Predicting: 362it [03:55,  1.61it/s]Extractor Predicting: 363it [03:56,  1.63it/s]Extractor Predicting: 364it [03:56,  1.66it/s]Extractor Predicting: 365it [03:57,  1.60it/s]Extractor Predicting: 366it [03:58,  1.56it/s]Extractor Predicting: 367it [03:58,  1.51it/s]Extractor Predicting: 368it [03:59,  1.49it/s]Extractor Predicting: 369it [04:00,  1.54it/s]Extractor Predicting: 370it [04:00,  1.51it/s]Extractor Predicting: 371it [04:01,  1.53it/s]Extractor Predicting: 372it [04:02,  1.54it/s]Extractor Predicting: 373it [04:02,  1.55it/s]Extractor Predicting: 374it [04:03,  1.55it/s]Extractor Predicting: 375it [04:04,  1.53it/s]Extractor Predicting: 376it [04:04,  1.52it/s]Extractor Predicting: 377it [04:05,  1.51it/s]Extractor Predicting: 378it [04:06,  1.51it/s]Extractor Predicting: 379it [04:06,  1.51it/s]Extractor Predicting: 380it [04:07,  1.51it/s]Extractor Predicting: 381it [04:08,  1.54it/s]Extractor Predicting: 382it [04:08,  1.56it/s]Extractor Predicting: 383it [04:09,  1.56it/s]Extractor Predicting: 384it [04:10,  1.58it/s]Extractor Predicting: 385it [04:10,  1.56it/s]Extractor Predicting: 386it [04:11,  1.58it/s]Extractor Predicting: 387it [04:11,  1.58it/s]Extractor Predicting: 388it [04:12,  1.61it/s]Extractor Predicting: 389it [04:13,  1.56it/s]Extractor Predicting: 390it [04:13,  1.53it/s]Extractor Predicting: 391it [04:14,  1.55it/s]Extractor Predicting: 392it [04:15,  1.61it/s]Extractor Predicting: 393it [04:15,  1.63it/s]Extractor Predicting: 394it [04:16,  1.63it/s]Extractor Predicting: 395it [04:16,  1.64it/s]Extractor Predicting: 396it [04:17,  1.66it/s]Extractor Predicting: 397it [04:18,  1.63it/s]Extractor Predicting: 398it [04:18,  1.61it/s]Extractor Predicting: 399it [04:19,  1.61it/s]Extractor Predicting: 400it [04:20,  1.59it/s]Extractor Predicting: 401it [04:20,  1.61it/s]Extractor Predicting: 402it [04:21,  1.63it/s]Extractor Predicting: 403it [04:21,  1.63it/s]Extractor Predicting: 404it [04:22,  1.63it/s]Extractor Predicting: 405it [04:23,  1.59it/s]Extractor Predicting: 406it [04:23,  1.60it/s]Extractor Predicting: 407it [04:24,  1.60it/s]Extractor Predicting: 408it [04:24,  1.61it/s]Extractor Predicting: 409it [04:25,  1.46it/s]Extractor Predicting: 410it [04:26,  1.49it/s]Extractor Predicting: 411it [04:27,  1.50it/s]Extractor Predicting: 412it [04:27,  1.51it/s]Extractor Predicting: 413it [04:28,  1.46it/s]Extractor Predicting: 414it [04:29,  1.43it/s]Extractor Predicting: 415it [04:29,  1.40it/s]Extractor Predicting: 416it [04:30,  1.41it/s]Extractor Predicting: 417it [04:31,  1.43it/s]Extractor Predicting: 418it [04:31,  1.46it/s]Extractor Predicting: 419it [04:32,  1.47it/s]Extractor Predicting: 420it [04:33,  1.45it/s]Extractor Predicting: 421it [04:34,  1.45it/s]Extractor Predicting: 422it [04:34,  1.46it/s]Extractor Predicting: 423it [04:35,  1.48it/s]Extractor Predicting: 424it [04:36,  1.47it/s]Extractor Predicting: 425it [04:36,  1.46it/s]Extractor Predicting: 426it [04:37,  1.45it/s]Extractor Predicting: 427it [04:38,  1.47it/s]Extractor Predicting: 428it [04:38,  1.48it/s]Extractor Predicting: 429it [04:39,  1.50it/s]Extractor Predicting: 430it [04:40,  1.51it/s]Extractor Predicting: 431it [04:40,  1.51it/s]Extractor Predicting: 432it [04:41,  1.50it/s]Extractor Predicting: 433it [04:42,  1.50it/s]Extractor Predicting: 434it [04:42,  1.50it/s]Extractor Predicting: 435it [04:43,  1.51it/s]Extractor Predicting: 436it [04:44,  1.54it/s]Extractor Predicting: 437it [04:44,  1.53it/s]Extractor Predicting: 438it [04:45,  1.54it/s]Extractor Predicting: 439it [04:45,  1.53it/s]Extractor Predicting: 440it [04:46,  1.52it/s]Extractor Predicting: 441it [04:47,  1.50it/s]Extractor Predicting: 442it [04:47,  1.51it/s]Extractor Predicting: 443it [04:48,  1.49it/s]Extractor Predicting: 444it [04:49,  1.49it/s]Extractor Predicting: 445it [04:50,  1.48it/s]Extractor Predicting: 446it [04:50,  1.48it/s]Extractor Predicting: 447it [04:51,  1.49it/s]Extractor Predicting: 448it [04:52,  1.48it/s]Extractor Predicting: 449it [04:52,  1.51it/s]Extractor Predicting: 450it [04:53,  1.53it/s]Extractor Predicting: 451it [04:53,  1.52it/s]Extractor Predicting: 452it [04:54,  1.50it/s]Extractor Predicting: 453it [04:55,  1.50it/s]Extractor Predicting: 454it [04:56,  1.50it/s]Extractor Predicting: 455it [04:56,  1.48it/s]Extractor Predicting: 456it [04:57,  1.49it/s]Extractor Predicting: 457it [04:58,  1.46it/s]Extractor Predicting: 458it [04:58,  1.48it/s]Extractor Predicting: 459it [04:59,  1.47it/s]Extractor Predicting: 460it [05:00,  1.46it/s]Extractor Predicting: 461it [05:00,  1.49it/s]Extractor Predicting: 462it [05:01,  1.50it/s]Extractor Predicting: 463it [05:02,  1.46it/s]Extractor Predicting: 464it [05:02,  1.43it/s]Extractor Predicting: 465it [05:03,  1.27it/s]Extractor Predicting: 466it [05:04,  1.33it/s]Extractor Predicting: 467it [05:05,  1.41it/s]Extractor Predicting: 468it [05:05,  1.45it/s]Extractor Predicting: 469it [05:06,  1.51it/s]Extractor Predicting: 470it [05:07,  1.50it/s]Extractor Predicting: 471it [05:07,  1.48it/s]Extractor Predicting: 472it [05:08,  1.50it/s]Extractor Predicting: 473it [05:09,  1.48it/s]Extractor Predicting: 474it [05:09,  1.48it/s]Extractor Predicting: 475it [05:10,  1.47it/s]Extractor Predicting: 476it [05:11,  1.49it/s]Extractor Predicting: 477it [05:11,  1.49it/s]Extractor Predicting: 478it [05:12,  1.49it/s]Extractor Predicting: 479it [05:13,  1.44it/s]Extractor Predicting: 480it [05:13,  1.40it/s]Extractor Predicting: 481it [05:14,  1.39it/s]Extractor Predicting: 482it [05:15,  1.39it/s]Extractor Predicting: 483it [05:16,  1.37it/s]Extractor Predicting: 484it [05:16,  1.36it/s]Extractor Predicting: 485it [05:17,  1.37it/s]Extractor Predicting: 486it [05:18,  1.40it/s]Extractor Predicting: 487it [05:18,  1.43it/s]Extractor Predicting: 488it [05:19,  1.40it/s]Extractor Predicting: 489it [05:20,  1.43it/s]Extractor Predicting: 490it [05:21,  1.40it/s]Extractor Predicting: 491it [05:21,  1.39it/s]Extractor Predicting: 492it [05:22,  1.39it/s]Extractor Predicting: 493it [05:23,  1.40it/s]Extractor Predicting: 494it [05:24,  1.40it/s]Extractor Predicting: 495it [05:24,  1.39it/s]Extractor Predicting: 496it [05:25,  1.40it/s]Extractor Predicting: 497it [05:26,  1.40it/s]Extractor Predicting: 498it [05:26,  1.41it/s]Extractor Predicting: 499it [05:27,  1.38it/s]Extractor Predicting: 500it [05:28,  1.39it/s]Extractor Predicting: 501it [05:29,  1.41it/s]Extractor Predicting: 502it [05:29,  1.39it/s]Extractor Predicting: 503it [05:30,  1.38it/s]Extractor Predicting: 504it [05:31,  1.35it/s]Extractor Predicting: 505it [05:32,  1.34it/s]Extractor Predicting: 506it [05:32,  1.33it/s]Extractor Predicting: 507it [05:33,  1.37it/s]Extractor Predicting: 508it [05:33,  1.53it/s]Extractor Predicting: 508it [05:33,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:11:58,164 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:11:58,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:11:58,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:11:58,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:11:58,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:11:58,453 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:11:58,454 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:11:58,712 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:11:59,779 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:11:59,779 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:12:01,905 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:12:01,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:12:01,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:12:01,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:12:01,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:12:02,710 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:12:02,711 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:12:03,392 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:12:03,550 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:12:03,550 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2760027192386132,
  "recall": 0.033330596831130446,
  "score": 0.05947846469381776,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.46it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.51it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.46it/s]Extractor Predicting: 22it [00:14,  1.45it/s]Extractor Predicting: 23it [00:15,  1.43it/s]Extractor Predicting: 24it [00:16,  1.46it/s]Extractor Predicting: 25it [00:16,  1.49it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:19,  1.36it/s]Extractor Predicting: 30it [00:20,  1.41it/s]Extractor Predicting: 31it [00:21,  1.47it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:22,  1.53it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:24,  1.55it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.54it/s]Extractor Predicting: 40it [00:26,  1.56it/s]Extractor Predicting: 41it [00:27,  1.55it/s]Extractor Predicting: 42it [00:28,  1.57it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:29,  1.61it/s]Extractor Predicting: 45it [00:30,  1.57it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:31,  1.56it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.54it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:34,  1.50it/s]Extractor Predicting: 52it [00:34,  1.50it/s]Extractor Predicting: 53it [00:35,  1.51it/s]Extractor Predicting: 54it [00:36,  1.45it/s]Extractor Predicting: 55it [00:36,  1.48it/s]Extractor Predicting: 56it [00:37,  1.50it/s]Extractor Predicting: 57it [00:38,  1.51it/s]Extractor Predicting: 58it [00:38,  1.51it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.50it/s]Extractor Predicting: 61it [00:40,  1.52it/s]Extractor Predicting: 62it [00:41,  1.52it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:42,  1.53it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:44,  1.48it/s]Extractor Predicting: 68it [00:45,  1.46it/s]Extractor Predicting: 69it [00:46,  1.43it/s]Extractor Predicting: 70it [00:46,  1.43it/s]Extractor Predicting: 71it [00:47,  1.43it/s]Extractor Predicting: 72it [00:48,  1.44it/s]Extractor Predicting: 73it [00:48,  1.45it/s]Extractor Predicting: 74it [00:49,  1.41it/s]Extractor Predicting: 75it [00:50,  1.60it/s]Extractor Predicting: 75it [00:50,  1.50it/s]
[INFO|configuration_utils.py:515] 2023-08-29 05:12:54,919 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:12:54,920 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:12:54,923 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:12:54,924 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 05:12:54,926 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:12:57,907 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 05:12:57,910 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 05:12:57,921 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:12:57,921 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:12:57,931 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:12:57,935 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:12:57,935 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:12:57,935 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:12:57,935 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:12:57,935 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:12:57,935 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5738255033557047,
  "recall": 0.043073047858942067,
  "score": 0.08013120899718838,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 05:12:58,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:58,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:59,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:00,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:00,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:01,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:02,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:02,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:03,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:04,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:05,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:06,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:06,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:07,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:08,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:08,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:09,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:10,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:11,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:11,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:30, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-29 05:13:12,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:13,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:13,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:14,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:15,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:16,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:16,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:17,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:18,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:18,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:19,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:20,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:21,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:21,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:22,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:23,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:24,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:24,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:25,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:26,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:27,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:27,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:28,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:31<04:44, 15.81s/it][WARNING|generation_utils.py:914] 2023-08-29 05:13:29,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:30,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:31,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:31,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:32,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:33,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:34,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:34,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:35,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:36,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:37,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:38,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:38,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:39,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:40,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:40,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:41,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:42,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:43,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:44,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:45,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:45,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:46,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:49<04:48, 16.96s/it][WARNING|generation_utils.py:914] 2023-08-29 05:13:47,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:48,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:49,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:49,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:50,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:51,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:52,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:52,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:53,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:54,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:55,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:55,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:56,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:57,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:58,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:59,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:59,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:00,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:01,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:02,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:02,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:03,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:06<04:28, 16.80s/it][WARNING|generation_utils.py:914] 2023-08-29 05:14:04,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:04,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:05,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:06,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:06,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:07,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:08,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:08,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:09,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:10,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:11,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:11,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:12,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:13,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:13,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:14,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:15,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:16,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:16,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:17,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:18,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:18,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:19,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:20,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:22<04:10, 16.72s/it][WARNING|generation_utils.py:914] 2023-08-29 05:14:20,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:21,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:22,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:22,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:23,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:24,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:24,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:25,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:26,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:27,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:27,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:28,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:29,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:29,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:30,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:31,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:32,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:32,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:33,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:34,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:34,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:35,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:38<03:47, 16.28s/it][WARNING|generation_utils.py:914] 2023-08-29 05:14:36,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:36,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:37,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:38,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:39,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:39,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:40,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:41,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:41,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:42,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:43,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:44,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:44,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:45,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:46,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:46,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:47,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:48,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:48,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:49,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:50,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:50,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:53<03:27, 15.99s/it][WARNING|generation_utils.py:914] 2023-08-29 05:14:51,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:52,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:53,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:53,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:54,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:55,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:56,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:56,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:57,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:58,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:59,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:59,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:00,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:01,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:01,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:02,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:03,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:04,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:05,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:05,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:06,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:09<03:11, 15.94s/it][WARNING|generation_utils.py:914] 2023-08-29 05:15:07,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:08,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:08,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:09,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:10,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:11,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:12,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:12,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:13,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:14,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:15,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:15,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:16,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:17,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:18,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:18,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:19,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:20,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:21,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:21,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:22,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:25<02:55, 15.95s/it][WARNING|generation_utils.py:914] 2023-08-29 05:15:23,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:24,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:24,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:25,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:26,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:26,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:27,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:28,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:28,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:29,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:30,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:31,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:31,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:32,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:33,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:34,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:34,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:35,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:36,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:37,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:37,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:38,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:41<02:39, 15.92s/it][WARNING|generation_utils.py:914] 2023-08-29 05:15:39,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:40,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:40,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:41,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:42,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:42,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:43,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:44,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:45,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:46,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:46,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:47,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:48,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:49,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:50,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:50,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:51,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:52,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:53,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:54,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:54,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:55,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:58<02:26, 16.26s/it][WARNING|generation_utils.py:914] 2023-08-29 05:15:56,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:56,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:57,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:58,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:59,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:59,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:00,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:01,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:02,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:02,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:03,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:04,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:05,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:05,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:06,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:07,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:08,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:08,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:09,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:10,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:10,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:13<02:07, 15.92s/it][WARNING|generation_utils.py:914] 2023-08-29 05:16:11,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:12,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:12,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:13,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:14,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:15,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:15,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:16,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:17,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:17,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:18,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:19,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:19,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:20,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:21,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:21,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:22,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:23,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:23,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:24,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:25,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:25,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:26,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:29<01:51, 15.98s/it][WARNING|generation_utils.py:914] 2023-08-29 05:16:27,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:28,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:28,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:29,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:30,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:30,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:31,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:32,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:33,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:33,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:34,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:34,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:35,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:36,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:37,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:37,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:38,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:39,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:39,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:40,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:41,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:43<01:33, 15.55s/it][WARNING|generation_utils.py:914] 2023-08-29 05:16:42,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:42,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:43,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:44,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:45,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:45,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:46,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:47,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:47,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:48,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:49,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:50,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:50,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:51,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:52,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:52,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:53,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:54,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:55,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:56,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:58<01:16, 15.30s/it][WARNING|generation_utils.py:914] 2023-08-29 05:16:56,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:57,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:58,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:59,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:59,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:00,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:01,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:02,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:02,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:03,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:04,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:05,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:05,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:06,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:07,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:08,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:08,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:09,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:10,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:11,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:11,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:12,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:13,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:13,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:16<01:04, 16.09s/it][WARNING|generation_utils.py:914] 2023-08-29 05:17:14,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:15,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:16,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:16,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:17,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:18,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:18,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:19,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:20,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:20,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:21,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:22,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:23,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:23,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:24,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:25,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:25,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:26,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:27,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:27,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:30<00:46, 15.41s/it][WARNING|generation_utils.py:914] 2023-08-29 05:17:28,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:29,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:30,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:30,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:31,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:32,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:32,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:33,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:34,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:34,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:35,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:36,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:36,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:37,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:38,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:38,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:39,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:40,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:41,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:41,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:42,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:44<00:30, 15.15s/it][WARNING|generation_utils.py:914] 2023-08-29 05:17:43,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:43,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:44,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:45,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:45,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:46,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:47,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:48,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:49,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:49,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:50,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:51,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:52,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:52,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:53,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:54,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:54,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:55,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:56,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:57,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:59<00:14, 14.99s/it][WARNING|generation_utils.py:914] 2023-08-29 05:17:57,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:59,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:59,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:00,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:01,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:02,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:03,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:04,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:05,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:05,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:06,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:07,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:07,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:08,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:09,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:10,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:10,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:11,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:12,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:13,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:13,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:14,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:17<00:00, 15.85s/it]Generating: 100%|██████████| 20/20 [05:17<00:00, 15.87s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:22,393 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:22,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:22,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:22,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:22,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:18:22,710 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:18:22,711 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:18:23,377 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:18:24,455 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:18:24,455 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:27,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:27,055 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:27,055 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:27,055 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:27,055 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:18:27,378 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:18:27,379 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:18:28,046 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:18:28,212 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:18:28,212 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.971875, 'errors': {''}}
['Relation : product or material produced . Context : Later in the year , the company built a vehicle called the Maserati Huracan and a series of car s based on the Huracan , a prototype of the Huracan produced by Toyota . Head Entity : the Maserati , Tail Entity : the automobile .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.8464673913043478, 'errors': {'', "('Ford Fiesta', 'product or material produced', '', 'In the late 1970s and early 1980s , the company produced a variety of racing mopeds to rival the Ford Fiesta , Ford Ranger , and Toyota Highlander .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Nausicaas', 'said to be the same as', '', 'Her first name , however , came from the Middle English surname Cymru , the name of a fictional character in the Greek myth of Nausicaas .')"}}
['Relation : student . Context : Later in college he studied at the University of Michigan under the direction of Michael J. White and John R. R. Martin . Head Entity : John R. Ration Martin , Tail Entity : University of Michigan .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : student .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : winner .', 'success_rate': 0.80859375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : conflict .', 'success_rate': 0.8536931818181818, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : continent .', 'success_rate': 0.8693181818181818, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8821022727272727, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : given name .', 'success_rate': 0.8551136363636364, 'errors': {'', "('Cersei Lannister', 'given name', '', 'In May 2015 , she announced that she would go on to become the series lead in the HBO comedy series Game of Thrones , and to play Cersei Lannister s mother in the new series .')", "('Isaac Haleves', 'given name', '', 'He was a student of the famous English poet Sir Isaac Haleves ( 1521 1594 ) and spent considerable time in Holland and Newbury Park on his side .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9122023809523809, 'errors': {'', "('My Little Pony', 'lyrics by', '', 'After the group was reformed in 1995 , they released their debut single , My Little Pony , in 2007 for the Sony PlayStation .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.842391304347826, 'errors': {'', "('Governor of Arkansas', 'movement', '', 'He was the 14th Governor of Nebraska , in 1891 was elected the 18th Governor of Alabama , in 1902 was elected as Governor of Arkansas , and was elected in 1903 as Governor of Colorado .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.95, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : producer .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8988095238095238, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9390625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8693181818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('Mälle I Týkke', 'replaces', '', 'The current bishop of Vämma is Mälle I Kocsi , the new bishop of Mälle I Týkke ( of Svartan , Vämma ) is Mälle A Káhönen ( of Täleberg , Vämma ) , and the new bishop of Týkke ( of Vämma ) is Mälle Y Värdolf ( of Svartan ) .')", "('Richard J. Durbin', 'replaces', '', 'His replacement as interim president was James M. Durbin of Illinois , who had been the acting head of the department under President Richard J. Durbin .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 14144
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14244, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.50it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:02,  1.47it/s]Extractor Estimating: 4it [00:02,  1.51it/s]Extractor Estimating: 5it [00:03,  1.49it/s]Extractor Estimating: 6it [00:04,  1.51it/s]Extractor Estimating: 7it [00:04,  1.39it/s]Extractor Estimating: 8it [00:05,  1.43it/s]Extractor Estimating: 9it [00:06,  1.44it/s]Extractor Estimating: 10it [00:06,  1.43it/s]Extractor Estimating: 11it [00:07,  1.45it/s]Extractor Estimating: 12it [00:08,  1.45it/s]Extractor Estimating: 13it [00:09,  1.40it/s]Extractor Estimating: 14it [00:09,  1.41it/s]Extractor Estimating: 15it [00:10,  1.44it/s]Extractor Estimating: 16it [00:11,  1.45it/s]Extractor Estimating: 17it [00:11,  1.44it/s]Extractor Estimating: 18it [00:12,  1.47it/s]Extractor Estimating: 19it [00:13,  1.45it/s]Extractor Estimating: 20it [00:13,  1.45it/s]Extractor Estimating: 21it [00:14,  1.46it/s]Extractor Estimating: 22it [00:15,  1.48it/s]Extractor Estimating: 23it [00:15,  1.47it/s]Extractor Estimating: 24it [00:16,  1.51it/s]Extractor Estimating: 25it [00:17,  1.50it/s]Extractor Estimating: 26it [00:17,  1.59it/s]Extractor Estimating: 27it [00:18,  1.60it/s]Extractor Estimating: 28it [00:18,  1.57it/s]Extractor Estimating: 29it [00:19,  1.60it/s]Extractor Estimating: 30it [00:20,  1.59it/s]Extractor Estimating: 31it [00:20,  1.63it/s]Extractor Estimating: 32it [00:21,  1.64it/s]Extractor Estimating: 33it [00:21,  1.64it/s]Extractor Estimating: 34it [00:22,  1.67it/s]Extractor Estimating: 35it [00:23,  1.67it/s]Extractor Estimating: 36it [00:23,  1.69it/s]Extractor Estimating: 37it [00:24,  1.66it/s]Extractor Estimating: 38it [00:25,  1.61it/s]Extractor Estimating: 39it [00:25,  1.61it/s]Extractor Estimating: 40it [00:26,  1.63it/s]Extractor Estimating: 41it [00:26,  1.66it/s]Extractor Estimating: 42it [00:27,  1.59it/s]Extractor Estimating: 43it [00:28,  1.62it/s]Extractor Estimating: 44it [00:28,  1.64it/s]Extractor Estimating: 45it [00:29,  1.63it/s]Extractor Estimating: 46it [00:29,  1.67it/s]Extractor Estimating: 47it [00:30,  1.66it/s]Extractor Estimating: 48it [00:31,  1.62it/s]Extractor Estimating: 49it [00:31,  1.65it/s]Extractor Estimating: 50it [00:32,  1.61it/s]Extractor Estimating: 51it [00:33,  1.58it/s]Extractor Estimating: 52it [00:33,  1.59it/s]Extractor Estimating: 53it [00:34,  1.61it/s]Extractor Estimating: 54it [00:34,  1.59it/s]Extractor Estimating: 55it [00:35,  1.65it/s]Extractor Estimating: 56it [00:36,  1.66it/s]Extractor Estimating: 57it [00:36,  1.70it/s]Extractor Estimating: 58it [00:37,  1.63it/s]Extractor Estimating: 59it [00:37,  1.56it/s]Extractor Estimating: 60it [00:38,  1.59it/s]Extractor Estimating: 61it [00:39,  1.60it/s]Extractor Estimating: 62it [00:39,  1.61it/s]Extractor Estimating: 63it [00:40,  1.62it/s]Extractor Estimating: 64it [00:41,  1.60it/s]Extractor Estimating: 65it [00:41,  1.62it/s]Extractor Estimating: 66it [00:42,  1.59it/s]Extractor Estimating: 67it [00:43,  1.54it/s]Extractor Estimating: 68it [00:43,  1.59it/s]Extractor Estimating: 69it [00:44,  1.60it/s]Extractor Estimating: 70it [00:44,  1.62it/s]Extractor Estimating: 71it [00:45,  1.66it/s]Extractor Estimating: 72it [00:46,  1.62it/s]Extractor Estimating: 73it [00:46,  1.60it/s]Extractor Estimating: 74it [00:47,  1.64it/s]Extractor Estimating: 75it [00:47,  1.68it/s]Extractor Estimating: 76it [00:48,  1.68it/s]Extractor Estimating: 77it [00:48,  1.68it/s]Extractor Estimating: 78it [00:49,  1.67it/s]Extractor Estimating: 79it [00:50,  1.63it/s]Extractor Estimating: 80it [00:50,  1.64it/s]Extractor Estimating: 81it [00:51,  1.63it/s]Extractor Estimating: 82it [00:52,  1.65it/s]Extractor Estimating: 83it [00:52,  1.68it/s]Extractor Estimating: 84it [00:53,  1.68it/s]Extractor Estimating: 85it [00:53,  1.68it/s]Extractor Estimating: 86it [00:54,  1.61it/s]Extractor Estimating: 87it [00:55,  1.62it/s]Extractor Estimating: 88it [00:55,  1.64it/s]Extractor Estimating: 89it [00:56,  1.45it/s]Extractor Estimating: 90it [00:57,  1.48it/s]Extractor Estimating: 91it [00:57,  1.49it/s]Extractor Estimating: 92it [00:58,  1.56it/s]Extractor Estimating: 93it [00:59,  1.58it/s]Extractor Estimating: 94it [00:59,  1.62it/s]Extractor Estimating: 95it [01:00,  1.51it/s]Extractor Estimating: 96it [01:01,  1.54it/s]Extractor Estimating: 97it [01:01,  1.58it/s]Extractor Estimating: 98it [01:02,  1.60it/s]Extractor Estimating: 99it [01:02,  1.64it/s]Extractor Estimating: 100it [01:03,  1.61it/s]Extractor Estimating: 101it [01:04,  1.63it/s]Extractor Estimating: 102it [01:04,  1.62it/s]Extractor Estimating: 103it [01:05,  1.66it/s]Extractor Estimating: 104it [01:05,  1.63it/s]Extractor Estimating: 105it [01:06,  1.64it/s]Extractor Estimating: 106it [01:07,  1.58it/s]Extractor Estimating: 107it [01:07,  1.63it/s]Extractor Estimating: 108it [01:08,  1.64it/s]Extractor Estimating: 109it [01:08,  1.62it/s]Extractor Estimating: 110it [01:09,  1.67it/s]Extractor Estimating: 111it [01:10,  1.66it/s]Extractor Estimating: 112it [01:10,  1.62it/s]Extractor Estimating: 113it [01:11,  1.59it/s]Extractor Estimating: 114it [01:12,  1.58it/s]Extractor Estimating: 115it [01:12,  1.60it/s]Extractor Estimating: 116it [01:13,  1.61it/s]Extractor Estimating: 117it [01:13,  1.63it/s]Extractor Estimating: 118it [01:14,  1.63it/s]Extractor Estimating: 119it [01:15,  1.58it/s]Extractor Estimating: 120it [01:15,  1.60it/s]Extractor Estimating: 121it [01:16,  1.59it/s]Extractor Estimating: 122it [01:17,  1.60it/s]Extractor Estimating: 123it [01:17,  1.60it/s]Extractor Estimating: 124it [01:18,  1.65it/s]Extractor Estimating: 125it [01:18,  1.64it/s]Extractor Estimating: 126it [01:19,  1.67it/s]Extractor Estimating: 127it [01:20,  1.68it/s]Extractor Estimating: 128it [01:20,  1.70it/s]Extractor Estimating: 129it [01:21,  1.66it/s]Extractor Estimating: 130it [01:21,  1.67it/s]Extractor Estimating: 131it [01:22,  1.65it/s]Extractor Estimating: 132it [01:23,  1.62it/s]Extractor Estimating: 133it [01:23,  1.60it/s]Extractor Estimating: 134it [01:24,  1.60it/s]Extractor Estimating: 135it [01:25,  1.57it/s]Extractor Estimating: 136it [01:25,  1.60it/s]Extractor Estimating: 137it [01:26,  1.59it/s]Extractor Estimating: 138it [01:26,  1.63it/s]Extractor Estimating: 139it [01:27,  1.64it/s]Extractor Estimating: 140it [01:28,  1.65it/s]Extractor Estimating: 141it [01:28,  1.60it/s]Extractor Estimating: 142it [01:29,  1.62it/s]Extractor Estimating: 143it [01:29,  1.65it/s]Extractor Estimating: 144it [01:30,  1.63it/s]Extractor Estimating: 145it [01:31,  1.67it/s]Extractor Estimating: 146it [01:31,  1.67it/s]Extractor Estimating: 147it [01:32,  1.63it/s]Extractor Estimating: 148it [01:32,  1.61it/s]Extractor Estimating: 149it [01:33,  1.60it/s]Extractor Estimating: 150it [01:34,  1.63it/s]Extractor Estimating: 151it [01:34,  1.69it/s]Extractor Estimating: 152it [01:35,  1.71it/s]Extractor Estimating: 153it [01:35,  1.68it/s]Extractor Estimating: 154it [01:36,  1.67it/s]Extractor Estimating: 155it [01:37,  1.70it/s]Extractor Estimating: 156it [01:37,  1.70it/s]Extractor Estimating: 157it [01:38,  1.76it/s]Extractor Estimating: 158it [01:38,  1.79it/s]Extractor Estimating: 159it [01:39,  1.82it/s]Extractor Estimating: 160it [01:39,  1.84it/s]Extractor Estimating: 161it [01:40,  1.73it/s]Extractor Estimating: 162it [01:41,  1.74it/s]Extractor Estimating: 163it [01:41,  1.75it/s]Extractor Estimating: 164it [01:42,  1.76it/s]Extractor Estimating: 165it [01:42,  1.81it/s]Extractor Estimating: 166it [01:43,  1.79it/s]Extractor Estimating: 167it [01:43,  1.85it/s]Extractor Estimating: 168it [01:44,  1.82it/s]Extractor Estimating: 169it [01:44,  1.84it/s]Extractor Estimating: 170it [01:45,  1.92it/s]Extractor Estimating: 171it [01:45,  1.82it/s]Extractor Estimating: 172it [01:46,  1.79it/s]Extractor Estimating: 173it [01:47,  1.82it/s]Extractor Estimating: 174it [01:47,  1.85it/s]Extractor Estimating: 175it [01:48,  1.86it/s]Extractor Estimating: 176it [01:48,  1.80it/s]Extractor Estimating: 177it [01:49,  1.52it/s]Extractor Estimating: 178it [01:50,  1.50it/s]Extractor Estimating: 179it [01:50,  1.51it/s]Extractor Estimating: 180it [01:51,  1.48it/s]Extractor Estimating: 181it [01:52,  1.46it/s]Extractor Estimating: 182it [01:53,  1.40it/s]Extractor Estimating: 183it [01:53,  1.42it/s]Extractor Estimating: 184it [01:54,  1.44it/s]Extractor Estimating: 185it [01:55,  1.46it/s]Extractor Estimating: 186it [01:55,  1.48it/s]Extractor Estimating: 187it [01:56,  1.47it/s]Extractor Estimating: 188it [01:57,  1.42it/s]Extractor Estimating: 189it [01:57,  1.42it/s]Extractor Estimating: 190it [01:58,  1.47it/s]Extractor Estimating: 191it [01:59,  1.48it/s]Extractor Estimating: 192it [01:59,  1.48it/s]Extractor Estimating: 193it [02:00,  1.52it/s]Extractor Estimating: 194it [02:01,  1.49it/s]Extractor Estimating: 195it [02:01,  1.43it/s]Extractor Estimating: 196it [02:02,  1.50it/s]Extractor Estimating: 197it [02:03,  1.49it/s]Extractor Estimating: 198it [02:03,  1.50it/s]Extractor Estimating: 199it [02:04,  1.49it/s]Extractor Estimating: 200it [02:05,  1.45it/s]Extractor Estimating: 201it [02:05,  1.48it/s]Extractor Estimating: 202it [02:06,  1.49it/s]Extractor Estimating: 203it [02:07,  1.52it/s]Extractor Estimating: 204it [02:07,  1.54it/s]Extractor Estimating: 205it [02:08,  1.57it/s]Extractor Estimating: 206it [02:09,  1.55it/s]Extractor Estimating: 207it [02:09,  1.47it/s]Extractor Estimating: 208it [02:10,  1.51it/s]Extractor Estimating: 209it [02:11,  1.55it/s]Extractor Estimating: 210it [02:11,  1.56it/s]Extractor Estimating: 211it [02:12,  1.58it/s]Extractor Estimating: 212it [02:12,  1.61it/s]Extractor Estimating: 213it [02:13,  1.63it/s]Extractor Estimating: 214it [02:14,  1.59it/s]Extractor Estimating: 215it [02:14,  1.58it/s]Extractor Estimating: 216it [02:15,  1.56it/s]Extractor Estimating: 217it [02:16,  1.58it/s]Extractor Estimating: 218it [02:16,  1.60it/s]Extractor Estimating: 219it [02:17,  1.61it/s]Extractor Estimating: 220it [02:17,  1.61it/s]Extractor Estimating: 221it [02:18,  1.55it/s]Extractor Estimating: 222it [02:19,  1.53it/s]Extractor Estimating: 223it [02:20,  1.54it/s]Extractor Estimating: 224it [02:20,  1.61it/s]Extractor Estimating: 225it [02:21,  1.59it/s]Extractor Estimating: 226it [02:21,  1.62it/s]Extractor Estimating: 227it [02:22,  1.62it/s]Extractor Estimating: 228it [02:23,  1.61it/s]Extractor Estimating: 229it [02:23,  1.63it/s]Extractor Estimating: 230it [02:24,  1.66it/s]Extractor Estimating: 231it [02:24,  1.62it/s]Extractor Estimating: 232it [02:25,  1.65it/s]Extractor Estimating: 233it [02:26,  1.62it/s]Extractor Estimating: 234it [02:26,  1.61it/s]Extractor Estimating: 235it [02:27,  1.60it/s]Extractor Estimating: 236it [02:27,  1.61it/s]Extractor Estimating: 237it [02:28,  1.65it/s]Extractor Estimating: 238it [02:29,  1.65it/s]Extractor Estimating: 239it [02:29,  1.65it/s]Extractor Estimating: 240it [02:30,  1.64it/s]Extractor Estimating: 241it [02:31,  1.59it/s]Extractor Estimating: 242it [02:31,  1.59it/s]Extractor Estimating: 243it [02:32,  1.61it/s]Extractor Estimating: 244it [02:32,  1.61it/s]Extractor Estimating: 245it [02:33,  1.65it/s]Extractor Estimating: 246it [02:34,  1.61it/s]Extractor Estimating: 247it [02:34,  1.55it/s]Extractor Estimating: 248it [02:35,  1.59it/s]Extractor Estimating: 249it [02:35,  1.63it/s]Extractor Estimating: 250it [02:36,  1.68it/s]Extractor Estimating: 251it [02:37,  1.64it/s]Extractor Estimating: 252it [02:37,  1.54it/s]Extractor Estimating: 253it [02:38,  1.56it/s]Extractor Estimating: 254it [02:39,  1.57it/s]Extractor Estimating: 255it [02:39,  1.57it/s]Extractor Estimating: 256it [02:40,  1.58it/s]Extractor Estimating: 257it [02:41,  1.55it/s]Extractor Estimating: 258it [02:41,  1.52it/s]Extractor Estimating: 259it [02:42,  1.45it/s]Extractor Estimating: 260it [02:43,  1.49it/s]Extractor Estimating: 261it [02:43,  1.51it/s]Extractor Estimating: 262it [02:44,  1.50it/s]Extractor Estimating: 263it [02:45,  1.53it/s]Extractor Estimating: 264it [02:45,  1.55it/s]Extractor Estimating: 265it [02:46,  1.51it/s]Extractor Estimating: 266it [02:47,  1.32it/s]Extractor Estimating: 267it [02:48,  1.37it/s]Extractor Estimating: 268it [02:48,  1.39it/s]Extractor Estimating: 269it [02:49,  1.47it/s]Extractor Estimating: 270it [02:50,  1.47it/s]Extractor Estimating: 271it [02:50,  1.43it/s]Extractor Estimating: 272it [02:51,  1.48it/s]Extractor Estimating: 273it [02:52,  1.46it/s]Extractor Estimating: 274it [02:52,  1.44it/s]Extractor Estimating: 275it [02:53,  1.46it/s]Extractor Estimating: 276it [02:54,  1.49it/s]Extractor Estimating: 277it [02:54,  1.48it/s]Extractor Estimating: 278it [02:55,  1.49it/s]Extractor Estimating: 279it [02:56,  1.55it/s]Extractor Estimating: 280it [02:56,  1.55it/s]Extractor Estimating: 281it [02:57,  1.50it/s]Extractor Estimating: 282it [02:58,  1.50it/s]Extractor Estimating: 283it [02:58,  1.53it/s]Extractor Estimating: 284it [02:59,  1.55it/s]Extractor Estimating: 285it [03:00,  1.51it/s]Extractor Estimating: 286it [03:00,  1.50it/s]Extractor Estimating: 287it [03:01,  1.52it/s]Extractor Estimating: 288it [03:02,  1.54it/s]Extractor Estimating: 289it [03:02,  1.50it/s]Extractor Estimating: 290it [03:03,  1.48it/s]Extractor Estimating: 291it [03:04,  1.47it/s]Extractor Estimating: 292it [03:04,  1.50it/s]Extractor Estimating: 293it [03:05,  1.52it/s]Extractor Estimating: 294it [03:06,  1.52it/s]Extractor Estimating: 295it [03:06,  1.48it/s]Extractor Estimating: 296it [03:07,  1.53it/s]Extractor Estimating: 297it [03:08,  1.52it/s]Extractor Estimating: 298it [03:08,  1.51it/s]Extractor Estimating: 299it [03:09,  1.51it/s]Extractor Estimating: 300it [03:09,  1.57it/s]Extractor Estimating: 301it [03:10,  1.57it/s]Extractor Estimating: 302it [03:11,  1.58it/s]Extractor Estimating: 303it [03:11,  1.63it/s]Extractor Estimating: 304it [03:12,  1.65it/s]Extractor Estimating: 305it [03:12,  1.67it/s]Extractor Estimating: 306it [03:13,  1.60it/s]Extractor Estimating: 307it [03:14,  1.63it/s]Extractor Estimating: 308it [03:14,  1.66it/s]Extractor Estimating: 309it [03:15,  1.68it/s]Extractor Estimating: 310it [03:16,  1.63it/s]Extractor Estimating: 311it [03:16,  1.64it/s]Extractor Estimating: 312it [03:17,  1.61it/s]Extractor Estimating: 313it [03:17,  1.60it/s]Extractor Estimating: 314it [03:18,  1.59it/s]Extractor Estimating: 315it [03:19,  1.59it/s]Extractor Estimating: 316it [03:19,  1.56it/s]Extractor Estimating: 317it [03:20,  1.58it/s]Extractor Estimating: 318it [03:21,  1.60it/s]Extractor Estimating: 319it [03:21,  1.62it/s]Extractor Estimating: 320it [03:22,  1.64it/s]Extractor Estimating: 321it [03:22,  1.62it/s]Extractor Estimating: 322it [03:23,  1.62it/s]Extractor Estimating: 323it [03:24,  1.59it/s]Extractor Estimating: 324it [03:24,  1.58it/s]Extractor Estimating: 325it [03:25,  1.60it/s]Extractor Estimating: 326it [03:25,  1.66it/s]Extractor Estimating: 327it [03:26,  1.67it/s]Extractor Estimating: 328it [03:27,  1.70it/s]Extractor Estimating: 329it [03:27,  1.71it/s]Extractor Estimating: 330it [03:28,  1.69it/s]Extractor Estimating: 331it [03:28,  1.68it/s]Extractor Estimating: 332it [03:29,  1.66it/s]Extractor Estimating: 333it [03:30,  1.60it/s]Extractor Estimating: 334it [03:30,  1.67it/s]Extractor Estimating: 335it [03:31,  1.65it/s]Extractor Estimating: 336it [03:31,  1.64it/s]Extractor Estimating: 337it [03:32,  1.67it/s]Extractor Estimating: 338it [03:33,  1.70it/s]Extractor Estimating: 339it [03:33,  1.69it/s]Extractor Estimating: 340it [03:34,  1.65it/s]Extractor Estimating: 341it [03:34,  1.63it/s]Extractor Estimating: 342it [03:35,  1.62it/s]Extractor Estimating: 343it [03:36,  1.64it/s]Extractor Estimating: 344it [03:36,  1.68it/s]Extractor Estimating: 345it [03:37,  1.49it/s]Extractor Estimating: 346it [03:38,  1.50it/s]Extractor Estimating: 347it [03:38,  1.50it/s]Extractor Estimating: 348it [03:39,  1.53it/s]Extractor Estimating: 349it [03:40,  1.52it/s]Extractor Estimating: 350it [03:40,  1.53it/s]Extractor Estimating: 351it [03:41,  1.52it/s]Extractor Estimating: 352it [03:42,  1.54it/s]Extractor Estimating: 353it [03:42,  1.52it/s]Extractor Estimating: 354it [03:43,  1.52it/s]Extractor Estimating: 355it [03:44,  1.58it/s]Extractor Estimating: 356it [03:44,  1.59it/s]Extractor Estimating: 357it [03:45,  1.61it/s]Extractor Estimating: 358it [03:45,  1.62it/s]Extractor Estimating: 359it [03:46,  1.66it/s]Extractor Estimating: 360it [03:47,  1.66it/s]Extractor Estimating: 361it [03:47,  1.66it/s]Extractor Estimating: 362it [03:48,  1.65it/s]Extractor Estimating: 363it [03:48,  1.58it/s]Extractor Estimating: 364it [03:49,  1.52it/s]Extractor Estimating: 365it [03:50,  1.54it/s]Extractor Estimating: 366it [03:51,  1.53it/s]Extractor Estimating: 367it [03:51,  1.54it/s]Extractor Estimating: 368it [03:52,  1.54it/s]Extractor Estimating: 369it [03:52,  1.56it/s]Extractor Estimating: 370it [03:53,  1.60it/s]Extractor Estimating: 371it [03:54,  1.52it/s]Extractor Estimating: 372it [03:54,  1.54it/s]Extractor Estimating: 373it [03:55,  1.56it/s]Extractor Estimating: 374it [03:56,  1.56it/s]Extractor Estimating: 375it [03:56,  1.55it/s]Extractor Estimating: 376it [03:57,  1.56it/s]Extractor Estimating: 377it [03:58,  1.56it/s]Extractor Estimating: 378it [03:58,  1.53it/s]Extractor Estimating: 379it [03:59,  1.57it/s]Extractor Estimating: 380it [03:59,  1.57it/s]Extractor Estimating: 381it [04:00,  1.56it/s]Extractor Estimating: 382it [04:01,  1.57it/s]Extractor Estimating: 383it [04:01,  1.60it/s]Extractor Estimating: 384it [04:02,  1.61it/s]Extractor Estimating: 385it [04:03,  1.59it/s]Extractor Estimating: 386it [04:03,  1.59it/s]Extractor Estimating: 387it [04:04,  1.60it/s]Extractor Estimating: 388it [04:04,  1.65it/s]Extractor Estimating: 389it [04:05,  1.62it/s]Extractor Estimating: 390it [04:06,  1.67it/s]Extractor Estimating: 391it [04:06,  1.58it/s]Extractor Estimating: 392it [04:07,  1.63it/s]Extractor Estimating: 393it [04:08,  1.62it/s]Extractor Estimating: 394it [04:08,  1.57it/s]Extractor Estimating: 395it [04:09,  1.51it/s]Extractor Estimating: 396it [04:10,  1.56it/s]Extractor Estimating: 397it [04:10,  1.53it/s]Extractor Estimating: 398it [04:11,  1.61it/s]Extractor Estimating: 399it [04:11,  1.64it/s]Extractor Estimating: 400it [04:12,  1.62it/s]Extractor Estimating: 401it [04:13,  1.60it/s]Extractor Estimating: 402it [04:13,  1.59it/s]Extractor Estimating: 403it [04:14,  1.56it/s]Extractor Estimating: 404it [04:14,  1.64it/s]Extractor Estimating: 405it [04:15,  1.60it/s]Extractor Estimating: 406it [04:16,  1.61it/s]Extractor Estimating: 407it [04:16,  1.60it/s]Extractor Estimating: 408it [04:17,  1.60it/s]Extractor Estimating: 409it [04:18,  1.64it/s]Extractor Estimating: 410it [04:18,  1.62it/s]Extractor Estimating: 411it [04:19,  1.64it/s]Extractor Estimating: 412it [04:19,  1.61it/s]Extractor Estimating: 413it [04:20,  1.59it/s]Extractor Estimating: 414it [04:21,  1.60it/s]Extractor Estimating: 415it [04:21,  1.64it/s]Extractor Estimating: 416it [04:22,  1.65it/s]Extractor Estimating: 417it [04:22,  1.66it/s]Extractor Estimating: 418it [04:23,  1.64it/s]Extractor Estimating: 419it [04:24,  1.65it/s]Extractor Estimating: 420it [04:24,  1.64it/s]Extractor Estimating: 421it [04:25,  1.66it/s]Extractor Estimating: 422it [04:26,  1.48it/s]Extractor Estimating: 423it [04:26,  1.52it/s]Extractor Estimating: 424it [04:27,  1.51it/s]Extractor Estimating: 425it [04:28,  1.55it/s]Extractor Estimating: 426it [04:28,  1.50it/s]Extractor Estimating: 427it [04:29,  1.53it/s]Extractor Estimating: 428it [04:30,  1.58it/s]Extractor Estimating: 429it [04:30,  1.59it/s]Extractor Estimating: 430it [04:31,  1.59it/s]Extractor Estimating: 431it [04:31,  1.57it/s]Extractor Estimating: 432it [04:32,  1.56it/s]Extractor Estimating: 433it [04:33,  1.56it/s]Extractor Estimating: 434it [04:33,  1.59it/s]Extractor Estimating: 435it [04:34,  1.58it/s]Extractor Estimating: 436it [04:35,  1.54it/s]Extractor Estimating: 437it [04:35,  1.58it/s]Extractor Estimating: 438it [04:36,  1.54it/s]Extractor Estimating: 439it [04:37,  1.56it/s]Extractor Estimating: 440it [04:37,  1.57it/s]Extractor Estimating: 441it [04:38,  1.63it/s]Extractor Estimating: 442it [04:38,  1.61it/s]Extractor Estimating: 443it [04:39,  1.62it/s]Extractor Estimating: 444it [04:40,  1.61it/s]Extractor Estimating: 445it [04:40,  1.55it/s]Extractor Estimating: 446it [04:41,  1.60it/s]Extractor Estimating: 447it [04:42,  1.58it/s]Extractor Estimating: 448it [04:42,  1.55it/s]Extractor Estimating: 449it [04:43,  1.54it/s]Extractor Estimating: 450it [04:44,  1.55it/s]Extractor Estimating: 451it [04:44,  1.61it/s]Extractor Estimating: 452it [04:45,  1.61it/s]Extractor Estimating: 453it [04:45,  1.62it/s]Extractor Estimating: 454it [04:46,  1.56it/s]Extractor Estimating: 455it [04:47,  1.58it/s]Extractor Estimating: 456it [04:47,  1.56it/s]Extractor Estimating: 457it [04:48,  1.52it/s]Extractor Estimating: 458it [04:49,  1.14it/s]Extractor Estimating: 459it [04:50,  1.22it/s]Extractor Estimating: 460it [04:51,  1.30it/s]Extractor Estimating: 461it [04:51,  1.35it/s]Extractor Estimating: 462it [04:52,  1.35it/s]Extractor Estimating: 463it [04:53,  1.38it/s]Extractor Estimating: 464it [04:54,  1.41it/s]Extractor Estimating: 465it [04:54,  1.44it/s]Extractor Estimating: 466it [04:55,  1.48it/s]Extractor Estimating: 467it [04:55,  1.51it/s]Extractor Estimating: 468it [04:56,  1.54it/s]Extractor Estimating: 469it [04:57,  1.55it/s]Extractor Estimating: 470it [04:57,  1.53it/s]Extractor Estimating: 471it [04:58,  1.47it/s]Extractor Estimating: 472it [04:59,  1.45it/s]Extractor Estimating: 473it [04:59,  1.47it/s]Extractor Estimating: 474it [05:00,  1.51it/s]Extractor Estimating: 475it [05:01,  1.53it/s]Extractor Estimating: 476it [05:01,  1.58it/s]Extractor Estimating: 477it [05:02,  1.60it/s]Extractor Estimating: 478it [05:03,  1.61it/s]Extractor Estimating: 479it [05:03,  1.60it/s]Extractor Estimating: 480it [05:04,  1.59it/s]Extractor Estimating: 481it [05:04,  1.55it/s]Extractor Estimating: 482it [05:05,  1.54it/s]Extractor Estimating: 483it [05:06,  1.52it/s]Extractor Estimating: 484it [05:06,  1.52it/s]Extractor Estimating: 485it [05:07,  1.55it/s]Extractor Estimating: 486it [05:08,  1.56it/s]Extractor Estimating: 487it [05:08,  1.52it/s]Extractor Estimating: 488it [05:09,  1.53it/s]Extractor Estimating: 489it [05:10,  1.56it/s]Extractor Estimating: 490it [05:10,  1.52it/s]Extractor Estimating: 491it [05:11,  1.60it/s]Extractor Estimating: 492it [05:12,  1.58it/s]Extractor Estimating: 493it [05:12,  1.60it/s]Extractor Estimating: 494it [05:13,  1.59it/s]Extractor Estimating: 495it [05:14,  1.54it/s]Extractor Estimating: 496it [05:14,  1.46it/s]Extractor Estimating: 497it [05:15,  1.49it/s]Extractor Estimating: 498it [05:16,  1.50it/s]Extractor Estimating: 499it [05:16,  1.47it/s]Extractor Estimating: 500it [05:17,  1.71it/s]Extractor Estimating: 500it [05:17,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:24:02,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:24:02,753 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:24:02,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:24:02,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:24:02,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:24:03,365 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:24:03,366 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:24:03,910 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:24:04,977 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:24:04,977 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:24:07,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:24:07,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:24:07,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:24:07,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:24:07,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:24:08,414 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:24:08,415 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:24:08,991 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:24:09,146 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:24:09,146 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 08:41:42,493 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 08:41:42,498 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9950 mean pseudo reward: 0.94409147683898
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 26824
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26924, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26924, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.095, loss:662.4156
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.096, loss:634.9058
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.084, loss:659.2407
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.092, loss:628.9953
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 85, avg_time 1.102, loss:599.2694
>> valid entity prec:0.4986, rec:0.4000, f1:0.4439
>> valid relation prec:0.0987, rec:0.0192, f1:0.0321
>> valid relation with NER prec:0.0987, rec:0.0192, f1:0.0321
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 185, avg_time 2.816, loss:630.5048
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 285, avg_time 1.102, loss:625.0426
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 385, avg_time 1.084, loss:651.8018
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 70, avg_time 1.086, loss:614.8674
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 170, avg_time 1.094, loss:618.0677
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4458, rec:0.3911, f1:0.4167
>> valid relation prec:0.0611, rec:0.0136, f1:0.0222
>> valid relation with NER prec:0.0611, rec:0.0136, f1:0.0222
g_step 1100, step 270, avg_time 2.826, loss:645.9570
g_step 1200, step 370, avg_time 1.095, loss:662.1938
g_step 1300, step 55, avg_time 1.091, loss:629.7655
g_step 1400, step 155, avg_time 1.091, loss:592.8539
g_step 1500, step 255, avg_time 1.082, loss:628.4137
>> valid entity prec:0.4743, rec:0.4492, f1:0.4614
>> valid relation prec:0.0635, rec:0.0109, f1:0.0186
>> valid relation with NER prec:0.0635, rec:0.0109, f1:0.0186
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 355, avg_time 2.836, loss:642.9745
g_step 1700, step 40, avg_time 1.091, loss:603.2717
g_step 1800, step 140, avg_time 1.087, loss:586.4771
g_step 1900, step 240, avg_time 1.090, loss:587.2935
g_step 2000, step 340, avg_time 1.097, loss:583.5909
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4562, rec:0.4845, f1:0.4699
>> valid relation prec:0.0965, rec:0.0194, f1:0.0323
>> valid relation with NER prec:0.0965, rec:0.0194, f1:0.0323
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 25, avg_time 2.841, loss:614.5720
g_step 2200, step 125, avg_time 1.098, loss:558.8945
g_step 2300, step 225, avg_time 1.087, loss:563.3450
g_step 2400, step 325, avg_time 1.099, loss:571.9162
g_step 2500, step 10, avg_time 1.078, loss:591.3316
>> valid entity prec:0.4520, rec:0.3977, f1:0.4231
>> valid relation prec:0.0818, rec:0.0134, f1:0.0230
>> valid relation with NER prec:0.0818, rec:0.0134, f1:0.0230
g_step 2600, step 110, avg_time 2.817, loss:521.4731
g_step 2700, step 210, avg_time 1.096, loss:538.9646
g_step 2800, step 310, avg_time 1.087, loss:544.1426
g_step 2900, step 410, avg_time 1.098, loss:590.2364
g_step 3000, step 95, avg_time 1.075, loss:499.7864
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4515, rec:0.3687, f1:0.4059
>> valid relation prec:0.0875, rec:0.0161, f1:0.0271
>> valid relation with NER prec:0.0875, rec:0.0161, f1:0.0271
g_step 3100, step 195, avg_time 2.838, loss:513.3324
g_step 3200, step 295, avg_time 1.100, loss:546.5531
g_step 3300, step 395, avg_time 1.091, loss:537.5695
g_step 3400, step 80, avg_time 1.069, loss:507.8155
g_step 3500, step 180, avg_time 1.089, loss:515.9860
>> valid entity prec:0.4347, rec:0.4400, f1:0.4373
>> valid relation prec:0.0749, rec:0.0194, f1:0.0308
>> valid relation with NER prec:0.0749, rec:0.0194, f1:0.0308
g_step 3600, step 280, avg_time 2.826, loss:500.3212
g_step 3700, step 380, avg_time 1.098, loss:508.2453
g_step 3800, step 65, avg_time 1.098, loss:488.8487
g_step 3900, step 165, avg_time 1.090, loss:470.8338
g_step 4000, step 265, avg_time 1.091, loss:513.3245
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5026, rec:0.4073, f1:0.4500
>> valid relation prec:0.1090, rec:0.0237, f1:0.0389
>> valid relation with NER prec:0.1090, rec:0.0237, f1:0.0389
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 365, avg_time 2.827, loss:507.5073
g_step 4200, step 50, avg_time 1.080, loss:455.1321
g_step 4300, step 150, avg_time 1.101, loss:467.0284
g_step 4400, step 250, avg_time 1.095, loss:464.8518
g_step 4500, step 350, avg_time 1.090, loss:475.9760
>> valid entity prec:0.4699, rec:0.3877, f1:0.4249
>> valid relation prec:0.0764, rec:0.0144, f1:0.0243
>> valid relation with NER prec:0.0764, rec:0.0144, f1:0.0243
g_step 4600, step 35, avg_time 2.835, loss:472.5098
g_step 4700, step 135, avg_time 1.096, loss:449.0543
g_step 4800, step 235, avg_time 1.100, loss:457.2086
g_step 4900, step 335, avg_time 1.085, loss:482.2094
g_step 5000, step 20, avg_time 1.094, loss:481.7948
learning rate was adjusted to 0.0008
>> valid entity prec:0.4916, rec:0.3713, f1:0.4231
>> valid relation prec:0.0999, rec:0.0175, f1:0.0298
>> valid relation with NER prec:0.0999, rec:0.0175, f1:0.0298
g_step 5100, step 120, avg_time 2.819, loss:438.0828
g_step 5200, step 220, avg_time 1.071, loss:427.8214
g_step 5300, step 320, avg_time 1.101, loss:463.3152
g_step 5400, step 5, avg_time 1.096, loss:451.3485
g_step 5500, step 105, avg_time 1.093, loss:420.5645
>> valid entity prec:0.4602, rec:0.4194, f1:0.4389
>> valid relation prec:0.0787, rec:0.0157, f1:0.0261
>> valid relation with NER prec:0.0787, rec:0.0157, f1:0.0261
g_step 5600, step 205, avg_time 2.830, loss:428.3837
g_step 5700, step 305, avg_time 1.089, loss:430.3455
g_step 5800, step 405, avg_time 1.080, loss:452.7920
g_step 5900, step 90, avg_time 1.096, loss:403.0761
g_step 6000, step 190, avg_time 1.103, loss:415.1097
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4599, rec:0.4215, f1:0.4399
>> valid relation prec:0.0451, rec:0.0111, f1:0.0178
>> valid relation with NER prec:0.0451, rec:0.0111, f1:0.0178
g_step 6100, step 290, avg_time 2.817, loss:431.0722
g_step 6200, step 390, avg_time 1.083, loss:427.6687
g_step 6300, step 75, avg_time 1.093, loss:375.5336
g_step 6400, step 175, avg_time 1.079, loss:408.0020
g_step 6500, step 275, avg_time 1.114, loss:402.5865
>> valid entity prec:0.4390, rec:0.4371, f1:0.4381
>> valid relation prec:0.0570, rec:0.0130, f1:0.0211
>> valid relation with NER prec:0.0570, rec:0.0130, f1:0.0211
g_step 6600, step 375, avg_time 2.856, loss:413.4737
g_step 6700, step 60, avg_time 1.097, loss:389.0913
g_step 6800, step 160, avg_time 1.102, loss:392.0560
g_step 6900, step 260, avg_time 1.097, loss:399.7775
g_step 7000, step 360, avg_time 1.090, loss:385.7865
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4424, rec:0.4409, f1:0.4417
>> valid relation prec:0.1082, rec:0.0330, f1:0.0505
>> valid relation with NER prec:0.1082, rec:0.0330, f1:0.0505
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 7100, step 45, avg_time 2.843, loss:390.8299
g_step 7200, step 145, avg_time 1.099, loss:368.3086
g_step 7300, step 245, avg_time 1.107, loss:391.8686
g_step 7400, step 345, avg_time 1.108, loss:384.1786
g_step 7500, step 30, avg_time 1.086, loss:389.9452
>> valid entity prec:0.4897, rec:0.4364, f1:0.4615
>> valid relation prec:0.0761, rec:0.0216, f1:0.0337
>> valid relation with NER prec:0.0761, rec:0.0216, f1:0.0337
g_step 7600, step 130, avg_time 2.846, loss:355.1224
g_step 7700, step 230, avg_time 1.078, loss:351.7467
g_step 7800, step 330, avg_time 1.100, loss:397.8126
g_step 7900, step 15, avg_time 1.093, loss:378.9029
g_step 8000, step 115, avg_time 1.098, loss:342.1606
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4470, rec:0.4308, f1:0.4388
>> valid relation prec:0.0777, rec:0.0214, f1:0.0336
>> valid relation with NER prec:0.0777, rec:0.0214, f1:0.0336
g_step 8100, step 215, avg_time 2.837, loss:366.2433
g_step 8200, step 315, avg_time 1.098, loss:359.6187
g_step 8300, step 415, avg_time 1.089, loss:363.9117
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:41:42 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:41:42 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-41-42_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:41:43 - WARNING - datasets.builder -   Using custom data configuration default-588b472f28a4ae3b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-588b472f28a4ae3b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:41:43,775 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:41:43,776 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:41:43,777 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:41:43,778 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:41:43,786 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:43,792 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:43,792 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:43,792 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:43,792 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:43,792 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:43,792 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:41:43,921 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:41:47,024 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:41:47,027 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-588b472f28a4ae3b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.17ba/s] 20%|██        | 2/10 [00:00<00:02,  2.96ba/s] 30%|███       | 3/10 [00:00<00:01,  3.56ba/s] 40%|████      | 4/10 [00:01<00:01,  3.92ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.16ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.30ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.42ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.50ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.53ba/s]100%|██████████| 10/10 [00:02<00:00,  4.56ba/s]100%|██████████| 10/10 [00:02<00:00,  4.19ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.12ba/s] 40%|████      | 2/5 [00:00<00:00,  4.31ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.37ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.40ba/s]100%|██████████| 5/5 [00:01<00:00,  4.63ba/s]100%|██████████| 5/5 [00:01<00:00,  4.49ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  8.29ba/s] 30%|███       | 3/10 [00:00<00:00,  9.57ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.79ba/s] 70%|███████   | 7/10 [00:00<00:00,  9.87ba/s] 90%|█████████ | 9/10 [00:00<00:00,  9.97ba/s]100%|██████████| 10/10 [00:01<00:00,  9.85ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.26ba/s] 40%|████      | 2/5 [00:00<00:00,  9.13ba/s] 60%|██████    | 3/5 [00:00<00:00,  5.84ba/s] 80%|████████  | 4/5 [00:00<00:00,  6.98ba/s]100%|██████████| 5/5 [00:00<00:00,  7.65ba/s]
[INFO|trainer.py:414] 2023-08-29 08:41:52,620 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:41:52,631 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:41:52,631 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 08:41:52,632 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:41:52,632 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:41:52,632 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:41:52,632 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:41:52,632 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:57,  3.29it/s]  0%|          | 2/780 [00:00<03:49,  3.39it/s]  0%|          | 3/780 [00:00<03:47,  3.41it/s]  1%|          | 4/780 [00:01<03:45,  3.44it/s]  1%|          | 5/780 [00:01<03:44,  3.45it/s]  1%|          | 6/780 [00:01<03:43,  3.46it/s]  1%|          | 7/780 [00:02<03:42,  3.47it/s]  1%|          | 8/780 [00:02<03:42,  3.47it/s]  1%|          | 9/780 [00:02<03:41,  3.47it/s]  1%|▏         | 10/780 [00:02<03:41,  3.47it/s]  1%|▏         | 11/780 [00:03<03:41,  3.47it/s]  2%|▏         | 12/780 [00:03<03:41,  3.47it/s]  2%|▏         | 13/780 [00:03<03:40,  3.47it/s]  2%|▏         | 14/780 [00:04<03:40,  3.47it/s]  2%|▏         | 15/780 [00:04<03:40,  3.47it/s]  2%|▏         | 16/780 [00:04<03:40,  3.47it/s]  2%|▏         | 17/780 [00:04<03:40,  3.47it/s]  2%|▏         | 18/780 [00:05<03:39,  3.46it/s]  2%|▏         | 19/780 [00:05<03:39,  3.47it/s]  3%|▎         | 20/780 [00:05<03:39,  3.47it/s]  3%|▎         | 21/780 [00:06<03:38,  3.47it/s]  3%|▎         | 22/780 [00:06<03:38,  3.47it/s]  3%|▎         | 23/780 [00:06<03:38,  3.47it/s]  3%|▎         | 24/780 [00:06<03:37,  3.47it/s]  3%|▎         | 25/780 [00:07<03:38,  3.46it/s]  3%|▎         | 26/780 [00:07<03:37,  3.46it/s]  3%|▎         | 27/780 [00:07<03:37,  3.47it/s]  4%|▎         | 28/780 [00:08<03:36,  3.47it/s]  4%|▎         | 29/780 [00:08<03:36,  3.47it/s]  4%|▍         | 30/780 [00:08<03:36,  3.47it/s]  4%|▍         | 31/780 [00:08<03:35,  3.47it/s]  4%|▍         | 32/780 [00:09<03:35,  3.47it/s]  4%|▍         | 33/780 [00:09<03:35,  3.47it/s]  4%|▍         | 34/780 [00:09<03:35,  3.47it/s]  4%|▍         | 35/780 [00:10<03:34,  3.47it/s]  5%|▍         | 36/780 [00:10<03:34,  3.46it/s]  5%|▍         | 37/780 [00:10<03:34,  3.46it/s]  5%|▍         | 38/780 [00:10<03:34,  3.46it/s]  5%|▌         | 39/780 [00:11<03:34,  3.46it/s]  5%|▌         | 40/780 [00:11<03:33,  3.46it/s]  5%|▌         | 41/780 [00:11<03:33,  3.46it/s]  5%|▌         | 42/780 [00:12<03:32,  3.47it/s]  6%|▌         | 43/780 [00:12<03:32,  3.47it/s]  6%|▌         | 44/780 [00:12<03:32,  3.47it/s]  6%|▌         | 45/780 [00:12<03:32,  3.47it/s]  6%|▌         | 46/780 [00:13<03:31,  3.47it/s]  6%|▌         | 47/780 [00:13<03:32,  3.46it/s]  6%|▌         | 48/780 [00:13<03:31,  3.46it/s]  6%|▋         | 49/780 [00:14<03:31,  3.46it/s]  6%|▋         | 50/780 [00:14<03:30,  3.46it/s]  7%|▋         | 51/780 [00:14<03:30,  3.46it/s]  7%|▋         | 52/780 [00:15<03:30,  3.46it/s]  7%|▋         | 53/780 [00:15<03:30,  3.46it/s]  7%|▋         | 54/780 [00:15<03:29,  3.46it/s]  7%|▋         | 55/780 [00:15<03:29,  3.46it/s]  7%|▋         | 56/780 [00:16<03:29,  3.46it/s]  7%|▋         | 57/780 [00:16<03:28,  3.46it/s]  7%|▋         | 58/780 [00:16<03:28,  3.45it/s]  8%|▊         | 59/780 [00:17<03:28,  3.46it/s]  8%|▊         | 60/780 [00:17<03:28,  3.46it/s]  8%|▊         | 61/780 [00:17<03:27,  3.46it/s]  8%|▊         | 62/780 [00:17<03:27,  3.46it/s]  8%|▊         | 63/780 [00:18<03:27,  3.46it/s]  8%|▊         | 64/780 [00:18<03:26,  3.46it/s]  8%|▊         | 65/780 [00:18<03:26,  3.46it/s]  8%|▊         | 66/780 [00:19<03:26,  3.46it/s]  9%|▊         | 67/780 [00:19<03:26,  3.46it/s]  9%|▊         | 68/780 [00:19<03:25,  3.46it/s]  9%|▉         | 69/780 [00:19<03:26,  3.44it/s]  9%|▉         | 70/780 [00:20<03:25,  3.45it/s]  9%|▉         | 71/780 [00:20<03:25,  3.45it/s]  9%|▉         | 72/780 [00:20<03:25,  3.45it/s]  9%|▉         | 73/780 [00:21<03:24,  3.45it/s]  9%|▉         | 74/780 [00:21<03:24,  3.45it/s] 10%|▉         | 75/780 [00:21<03:24,  3.45it/s] 10%|▉         | 76/780 [00:21<03:23,  3.45it/s] 10%|▉         | 77/780 [00:22<03:23,  3.46it/s] 10%|█         | 78/780 [00:22<03:23,  3.46it/s] 10%|█         | 79/780 [00:22<03:22,  3.46it/s] 10%|█         | 80/780 [00:23<03:22,  3.45it/s] 10%|█         | 81/780 [00:23<03:22,  3.45it/s] 11%|█         | 82/780 [00:23<03:21,  3.46it/s] 11%|█         | 83/780 [00:23<03:22,  3.45it/s] 11%|█         | 84/780 [00:24<03:21,  3.45it/s] 11%|█         | 85/780 [00:24<03:21,  3.45it/s] 11%|█         | 86/780 [00:24<03:20,  3.45it/s] 11%|█         | 87/780 [00:25<03:20,  3.45it/s] 11%|█▏        | 88/780 [00:25<03:20,  3.45it/s] 11%|█▏        | 89/780 [00:25<03:20,  3.45it/s] 12%|█▏        | 90/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 91/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 92/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 93/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 94/780 [00:27<03:19,  3.45it/s] 12%|█▏        | 95/780 [00:27<03:18,  3.45it/s] 12%|█▏        | 96/780 [00:27<03:18,  3.45it/s] 12%|█▏        | 97/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 98/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 99/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 100/780 [00:28<03:16,  3.45it/s] 13%|█▎        | 101/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 102/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 103/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 104/780 [00:30<03:15,  3.45it/s] 13%|█▎        | 105/780 [00:30<03:16,  3.44it/s] 14%|█▎        | 106/780 [00:30<03:15,  3.44it/s] 14%|█▎        | 107/780 [00:30<03:15,  3.45it/s] 14%|█▍        | 108/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 109/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 110/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 111/780 [00:32<03:13,  3.45it/s] 14%|█▍        | 112/780 [00:32<03:13,  3.45it/s] 14%|█▍        | 113/780 [00:32<03:13,  3.45it/s] 15%|█▍        | 114/780 [00:32<03:12,  3.45it/s] 15%|█▍        | 115/780 [00:33<03:12,  3.45it/s] 15%|█▍        | 116/780 [00:33<03:13,  3.44it/s] 15%|█▌        | 117/780 [00:33<03:12,  3.44it/s] 15%|█▌        | 118/780 [00:34<03:12,  3.44it/s] 15%|█▌        | 119/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 120/780 [00:34<03:11,  3.45it/s] 16%|█▌        | 121/780 [00:34<03:11,  3.45it/s] 16%|█▌        | 122/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 123/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 124/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 125/780 [00:36<03:09,  3.45it/s] 16%|█▌        | 126/780 [00:36<03:09,  3.45it/s] 16%|█▋        | 127/780 [00:36<03:09,  3.44it/s] 16%|█▋        | 128/780 [00:37<03:09,  3.44it/s] 17%|█▋        | 129/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 130/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 131/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 132/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 133/780 [00:38<03:07,  3.44it/s] 17%|█▋        | 134/780 [00:38<03:07,  3.44it/s] 17%|█▋        | 135/780 [00:39<03:07,  3.45it/s] 17%|█▋        | 136/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 137/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 138/780 [00:39<03:06,  3.44it/s] 18%|█▊        | 139/780 [00:40<03:06,  3.44it/s] 18%|█▊        | 140/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 141/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 142/780 [00:41<03:05,  3.45it/s] 18%|█▊        | 143/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 144/780 [00:41<03:04,  3.45it/s] 19%|█▊        | 145/780 [00:41<03:04,  3.45it/s] 19%|█▊        | 146/780 [00:42<03:03,  3.45it/s] 19%|█▉        | 147/780 [00:42<03:03,  3.45it/s] 19%|█▉        | 148/780 [00:42<03:03,  3.45it/s] 19%|█▉        | 149/780 [00:43<03:03,  3.44it/s] 19%|█▉        | 150/780 [00:43<03:03,  3.44it/s] 19%|█▉        | 151/780 [00:43<03:02,  3.44it/s] 19%|█▉        | 152/780 [00:43<03:02,  3.45it/s] 20%|█▉        | 153/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 154/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 155/780 [00:44<03:01,  3.45it/s] 20%|██        | 156/780 [00:45<03:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 08:42:37,832 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:42:37,832 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 08:42:37,832 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.98it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.39it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.72it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.06it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.57it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.07it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.89it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.54it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.59it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.62it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.53it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.56it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.64it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.53it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.56it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.34it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.13it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.18it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.34it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.50it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.56it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.52it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.59it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.51it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.46it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.46it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.33it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.38it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.39it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.41it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.51it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.59it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.61it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.60it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.47it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.39it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.40it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.40it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.48it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.51it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.58it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.54it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.57it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.49it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.55it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.43it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.47it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.45it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.41it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.41it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.54it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.55it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.49it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.60it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.38it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.55it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.52it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.42it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.47it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.38it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.39it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.54it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.49it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.50it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.52it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 44.94it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.48it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.81it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.98it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.19it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.27it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.34it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.50it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.34it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.27it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.35it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.33it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.49it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.59it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.50it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.48it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.56it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.36it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.36it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.37it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.26it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.20it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.37it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.48it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.52it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.58it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.34it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.41it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.37it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.43it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.43it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.46it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.35it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.47it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.48it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.47it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.54it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.48it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.36it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.37it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.36it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.42it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.47it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.47it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.51it/s][A
 92%|█████████▏| 558/608 [00:11<00:01, 46.43it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.50it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.39it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.38it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.45it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.34it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.49it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.47it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.48it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.48it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.42it/s][A                                                 
                                                 [A 20%|██        | 156/780 [00:58<03:00,  3.45it/s]
100%|██████████| 608/608 [00:13<00:00, 46.42it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:42:50,977 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 08:42:50,992 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:42:53,139 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:42:53,157 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:42:53,166 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:05<1:04:45,  6.24s/it] 20%|██        | 158/780 [01:05<46:10,  4.45s/it]   20%|██        | 159/780 [01:05<33:09,  3.20s/it] 21%|██        | 160/780 [01:06<24:04,  2.33s/it] 21%|██        | 161/780 [01:06<17:43,  1.72s/it] 21%|██        | 162/780 [01:06<13:16,  1.29s/it] 21%|██        | 163/780 [01:07<10:10,  1.01it/s] 21%|██        | 164/780 [01:07<08:00,  1.28it/s] 21%|██        | 165/780 [01:07<06:29,  1.58it/s] 21%|██▏       | 166/780 [01:07<05:25,  1.89it/s] 21%|██▏       | 167/780 [01:08<04:40,  2.18it/s] 22%|██▏       | 168/780 [01:08<04:09,  2.46it/s] 22%|██▏       | 169/780 [01:08<03:47,  2.68it/s] 22%|██▏       | 170/780 [01:09<03:32,  2.87it/s] 22%|██▏       | 171/780 [01:09<03:21,  3.02it/s] 22%|██▏       | 172/780 [01:09<03:13,  3.14it/s] 22%|██▏       | 173/780 [01:09<03:08,  3.23it/s] 22%|██▏       | 174/780 [01:10<03:04,  3.29it/s] 22%|██▏       | 175/780 [01:10<03:01,  3.34it/s] 23%|██▎       | 176/780 [01:10<02:59,  3.37it/s] 23%|██▎       | 177/780 [01:11<02:57,  3.39it/s] 23%|██▎       | 178/780 [01:11<02:56,  3.41it/s] 23%|██▎       | 179/780 [01:11<02:55,  3.42it/s] 23%|██▎       | 180/780 [01:11<02:55,  3.41it/s] 23%|██▎       | 181/780 [01:12<02:55,  3.42it/s] 23%|██▎       | 182/780 [01:12<02:54,  3.43it/s] 23%|██▎       | 183/780 [01:12<02:53,  3.43it/s] 24%|██▎       | 184/780 [01:13<02:53,  3.44it/s] 24%|██▎       | 185/780 [01:13<02:52,  3.44it/s] 24%|██▍       | 186/780 [01:13<02:52,  3.44it/s] 24%|██▍       | 187/780 [01:13<02:52,  3.45it/s] 24%|██▍       | 188/780 [01:14<02:51,  3.44it/s] 24%|██▍       | 189/780 [01:14<02:51,  3.45it/s] 24%|██▍       | 190/780 [01:14<02:51,  3.45it/s] 24%|██▍       | 191/780 [01:15<02:53,  3.40it/s] 25%|██▍       | 192/780 [01:15<02:52,  3.42it/s] 25%|██▍       | 193/780 [01:15<02:51,  3.43it/s] 25%|██▍       | 194/780 [01:16<02:50,  3.43it/s] 25%|██▌       | 195/780 [01:16<02:50,  3.44it/s] 25%|██▌       | 196/780 [01:16<02:49,  3.44it/s] 25%|██▌       | 197/780 [01:16<02:49,  3.44it/s] 25%|██▌       | 198/780 [01:17<02:49,  3.44it/s] 26%|██▌       | 199/780 [01:17<02:48,  3.44it/s] 26%|██▌       | 200/780 [01:17<02:48,  3.44it/s] 26%|██▌       | 201/780 [01:18<02:48,  3.45it/s] 26%|██▌       | 202/780 [01:18<02:49,  3.40it/s] 26%|██▌       | 203/780 [01:18<02:49,  3.41it/s] 26%|██▌       | 204/780 [01:18<02:48,  3.43it/s] 26%|██▋       | 205/780 [01:19<02:47,  3.43it/s] 26%|██▋       | 206/780 [01:19<02:47,  3.44it/s] 27%|██▋       | 207/780 [01:19<02:46,  3.44it/s] 27%|██▋       | 208/780 [01:20<02:46,  3.44it/s] 27%|██▋       | 209/780 [01:20<02:45,  3.44it/s] 27%|██▋       | 210/780 [01:20<02:45,  3.45it/s] 27%|██▋       | 211/780 [01:20<02:45,  3.45it/s] 27%|██▋       | 212/780 [01:21<02:44,  3.45it/s] 27%|██▋       | 213/780 [01:21<02:44,  3.44it/s] 27%|██▋       | 214/780 [01:21<02:44,  3.44it/s] 28%|██▊       | 215/780 [01:22<02:44,  3.44it/s] 28%|██▊       | 216/780 [01:22<02:43,  3.44it/s] 28%|██▊       | 217/780 [01:22<02:43,  3.45it/s] 28%|██▊       | 218/780 [01:22<02:43,  3.45it/s] 28%|██▊       | 219/780 [01:23<02:42,  3.45it/s] 28%|██▊       | 220/780 [01:23<02:42,  3.45it/s] 28%|██▊       | 221/780 [01:23<02:42,  3.45it/s] 28%|██▊       | 222/780 [01:24<02:41,  3.45it/s] 29%|██▊       | 223/780 [01:24<02:41,  3.45it/s] 29%|██▊       | 224/780 [01:24<02:41,  3.45it/s] 29%|██▉       | 225/780 [01:25<02:40,  3.45it/s] 29%|██▉       | 226/780 [01:25<02:40,  3.45it/s] 29%|██▉       | 227/780 [01:25<02:40,  3.44it/s] 29%|██▉       | 228/780 [01:25<02:40,  3.44it/s] 29%|██▉       | 229/780 [01:26<02:39,  3.45it/s] 29%|██▉       | 230/780 [01:26<02:39,  3.44it/s] 30%|██▉       | 231/780 [01:26<02:39,  3.45it/s] 30%|██▉       | 232/780 [01:27<02:39,  3.44it/s] 30%|██▉       | 233/780 [01:27<02:38,  3.45it/s] 30%|███       | 234/780 [01:27<02:38,  3.45it/s] 30%|███       | 235/780 [01:27<02:38,  3.45it/s] 30%|███       | 236/780 [01:28<02:37,  3.45it/s] 30%|███       | 237/780 [01:28<02:37,  3.45it/s] 31%|███       | 238/780 [01:28<02:37,  3.44it/s] 31%|███       | 239/780 [01:29<02:37,  3.44it/s] 31%|███       | 240/780 [01:29<02:36,  3.44it/s] 31%|███       | 241/780 [01:29<02:36,  3.44it/s] 31%|███       | 242/780 [01:29<02:36,  3.45it/s] 31%|███       | 243/780 [01:30<02:35,  3.45it/s] 31%|███▏      | 244/780 [01:30<02:35,  3.45it/s] 31%|███▏      | 245/780 [01:30<02:35,  3.45it/s] 32%|███▏      | 246/780 [01:31<02:34,  3.45it/s] 32%|███▏      | 247/780 [01:31<02:34,  3.45it/s] 32%|███▏      | 248/780 [01:31<02:34,  3.45it/s] 32%|███▏      | 249/780 [01:31<02:34,  3.43it/s] 32%|███▏      | 250/780 [01:32<02:34,  3.44it/s] 32%|███▏      | 251/780 [01:32<02:33,  3.44it/s] 32%|███▏      | 252/780 [01:32<02:33,  3.44it/s] 32%|███▏      | 253/780 [01:33<02:32,  3.45it/s] 33%|███▎      | 254/780 [01:33<02:32,  3.45it/s] 33%|███▎      | 255/780 [01:33<02:32,  3.44it/s] 33%|███▎      | 256/780 [01:34<02:32,  3.44it/s] 33%|███▎      | 257/780 [01:34<02:31,  3.45it/s] 33%|███▎      | 258/780 [01:34<02:31,  3.45it/s] 33%|███▎      | 259/780 [01:34<02:31,  3.45it/s] 33%|███▎      | 260/780 [01:35<02:31,  3.43it/s] 33%|███▎      | 261/780 [01:35<02:31,  3.43it/s] 34%|███▎      | 262/780 [01:35<02:30,  3.43it/s] 34%|███▎      | 263/780 [01:36<02:30,  3.44it/s] 34%|███▍      | 264/780 [01:36<02:29,  3.44it/s] 34%|███▍      | 265/780 [01:36<02:29,  3.44it/s] 34%|███▍      | 266/780 [01:36<02:29,  3.44it/s] 34%|███▍      | 267/780 [01:37<02:28,  3.44it/s] 34%|███▍      | 268/780 [01:37<02:28,  3.45it/s] 34%|███▍      | 269/780 [01:37<02:28,  3.45it/s] 35%|███▍      | 270/780 [01:38<02:27,  3.45it/s] 35%|███▍      | 271/780 [01:38<02:27,  3.45it/s] 35%|███▍      | 272/780 [01:38<02:27,  3.45it/s] 35%|███▌      | 273/780 [01:38<02:27,  3.44it/s] 35%|███▌      | 274/780 [01:39<02:27,  3.44it/s] 35%|███▌      | 275/780 [01:39<02:26,  3.44it/s] 35%|███▌      | 276/780 [01:39<02:26,  3.44it/s] 36%|███▌      | 277/780 [01:40<02:26,  3.45it/s] 36%|███▌      | 278/780 [01:40<02:25,  3.45it/s] 36%|███▌      | 279/780 [01:40<02:25,  3.45it/s] 36%|███▌      | 280/780 [01:40<02:25,  3.43it/s] 36%|███▌      | 281/780 [01:41<02:25,  3.44it/s] 36%|███▌      | 282/780 [01:41<02:24,  3.44it/s] 36%|███▋      | 283/780 [01:41<02:24,  3.44it/s] 36%|███▋      | 284/780 [01:42<02:24,  3.44it/s] 37%|███▋      | 285/780 [01:42<02:23,  3.44it/s] 37%|███▋      | 286/780 [01:42<02:23,  3.44it/s] 37%|███▋      | 287/780 [01:43<02:23,  3.44it/s] 37%|███▋      | 288/780 [01:43<02:22,  3.45it/s] 37%|███▋      | 289/780 [01:43<02:22,  3.44it/s] 37%|███▋      | 290/780 [01:43<02:22,  3.44it/s] 37%|███▋      | 291/780 [01:44<02:21,  3.44it/s] 37%|███▋      | 292/780 [01:44<02:21,  3.45it/s] 38%|███▊      | 293/780 [01:44<02:21,  3.45it/s] 38%|███▊      | 294/780 [01:45<02:21,  3.45it/s] 38%|███▊      | 295/780 [01:45<02:20,  3.44it/s] 38%|███▊      | 296/780 [01:45<02:20,  3.44it/s] 38%|███▊      | 297/780 [01:45<02:20,  3.43it/s] 38%|███▊      | 298/780 [01:46<02:20,  3.43it/s] 38%|███▊      | 299/780 [01:46<02:20,  3.43it/s] 38%|███▊      | 300/780 [01:46<02:19,  3.44it/s] 39%|███▊      | 301/780 [01:47<02:19,  3.44it/s] 39%|███▊      | 302/780 [01:47<02:18,  3.44it/s] 39%|███▉      | 303/780 [01:47<02:18,  3.44it/s] 39%|███▉      | 304/780 [01:47<02:18,  3.44it/s] 39%|███▉      | 305/780 [01:48<02:17,  3.45it/s] 39%|███▉      | 306/780 [01:48<02:17,  3.44it/s] 39%|███▉      | 307/780 [01:48<02:17,  3.44it/s] 39%|███▉      | 308/780 [01:49<02:17,  3.44it/s] 40%|███▉      | 309/780 [01:49<02:17,  3.44it/s] 40%|███▉      | 310/780 [01:49<02:16,  3.44it/s] 40%|███▉      | 311/780 [01:50<02:16,  3.44it/s] 40%|████      | 312/780 [01:50<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 08:43:42,969 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:43:42,969 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 08:43:42,969 >>   Batch size = 8
{'eval_loss': 0.9444142580032349, 'eval_runtime': 13.1242, 'eval_samples_per_second': 370.612, 'eval_steps_per_second': 46.326, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.96it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.31it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.50it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.72it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.35it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.05it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.73it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.37it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.41it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.44it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.44it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.49it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.47it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.56it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.51it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.36it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.15it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 44.11it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 44.98it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.52it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.83it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.04it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.17it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.24it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.18it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.99it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 45.88it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.03it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.27it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.31it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.35it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.51it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.45it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.45it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.38it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.14it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.24it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.36it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.26it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.46it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.53it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.46it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.53it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.45it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.32it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.26it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.35it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.27it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.38it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.53it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.45it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.46it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.44it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.31it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.28it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.31it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.30it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.35it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.47it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.53it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.47it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.36it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.32it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.28it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.41it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.40it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.29it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.48it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.48it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.49it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.53it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.40it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.30it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.34it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.26it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.31it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.22it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.35it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.43it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.30it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.19it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.17it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.29it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.27it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.30it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.22it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.22it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.39it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.34it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.30it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.27it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.20it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.24it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.25it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.35it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.32it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.43it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.32it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.28it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.23it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.21it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.22it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.27it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.11it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.36it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.37it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.36it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.38it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.33it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.16it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.22it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.14it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.13it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.24it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.32it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.34it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.24it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.24it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.25it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.23it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.29it/s][A                                                 
                                                 [A 40%|████      | 312/780 [02:03<02:16,  3.44it/s]
100%|██████████| 608/608 [00:13<00:00, 46.29it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:43:56,152 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 08:43:56,171 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:43:58,318 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:43:58,335 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:43:58,347 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:10<49:06,  6.31s/it] 40%|████      | 314/780 [02:10<34:59,  4.50s/it] 40%|████      | 315/780 [02:11<25:06,  3.24s/it] 41%|████      | 316/780 [02:11<18:12,  2.36s/it] 41%|████      | 317/780 [02:11<13:23,  1.74s/it] 41%|████      | 318/780 [02:12<10:01,  1.30s/it] 41%|████      | 319/780 [02:12<07:40,  1.00it/s] 41%|████      | 320/780 [02:12<06:01,  1.27it/s] 41%|████      | 321/780 [02:12<04:52,  1.57it/s] 41%|████▏     | 322/780 [02:13<04:04,  1.88it/s] 41%|████▏     | 323/780 [02:13<03:30,  2.17it/s] 42%|████▏     | 324/780 [02:13<03:06,  2.44it/s] 42%|████▏     | 325/780 [02:14<02:51,  2.66it/s] 42%|████▏     | 326/780 [02:14<02:39,  2.85it/s] 42%|████▏     | 327/780 [02:14<02:30,  3.01it/s] 42%|████▏     | 328/780 [02:15<02:24,  3.13it/s] 42%|████▏     | 329/780 [02:15<02:20,  3.22it/s] 42%|████▏     | 330/780 [02:15<02:16,  3.29it/s] 42%|████▏     | 331/780 [02:15<02:14,  3.33it/s] 43%|████▎     | 332/780 [02:16<02:13,  3.37it/s] 43%|████▎     | 333/780 [02:16<02:11,  3.39it/s] 43%|████▎     | 334/780 [02:16<02:10,  3.41it/s] 43%|████▎     | 335/780 [02:17<02:10,  3.42it/s] 43%|████▎     | 336/780 [02:17<02:10,  3.41it/s] 43%|████▎     | 337/780 [02:17<02:09,  3.42it/s] 43%|████▎     | 338/780 [02:17<02:08,  3.43it/s] 43%|████▎     | 339/780 [02:18<02:08,  3.44it/s] 44%|████▎     | 340/780 [02:18<02:07,  3.44it/s] 44%|████▎     | 341/780 [02:18<02:07,  3.44it/s] 44%|████▍     | 342/780 [02:19<02:07,  3.44it/s] 44%|████▍     | 343/780 [02:19<02:06,  3.44it/s] 44%|████▍     | 344/780 [02:19<02:06,  3.44it/s] 44%|████▍     | 345/780 [02:19<02:06,  3.45it/s] 44%|████▍     | 346/780 [02:20<02:05,  3.45it/s] 44%|████▍     | 347/780 [02:20<02:05,  3.44it/s] 45%|████▍     | 348/780 [02:20<02:05,  3.44it/s] 45%|████▍     | 349/780 [02:21<02:05,  3.45it/s] 45%|████▍     | 350/780 [02:21<02:04,  3.45it/s] 45%|████▌     | 351/780 [02:21<02:04,  3.45it/s] 45%|████▌     | 352/780 [02:21<02:04,  3.44it/s] 45%|████▌     | 353/780 [02:22<02:03,  3.45it/s] 45%|████▌     | 354/780 [02:22<02:03,  3.44it/s] 46%|████▌     | 355/780 [02:22<02:03,  3.45it/s] 46%|████▌     | 356/780 [02:23<02:03,  3.45it/s] 46%|████▌     | 357/780 [02:23<02:02,  3.45it/s] 46%|████▌     | 358/780 [02:23<02:02,  3.44it/s] 46%|████▌     | 359/780 [02:24<02:02,  3.44it/s] 46%|████▌     | 360/780 [02:24<02:02,  3.44it/s] 46%|████▋     | 361/780 [02:24<02:01,  3.44it/s] 46%|████▋     | 362/780 [02:24<02:01,  3.44it/s] 47%|████▋     | 363/780 [02:25<02:01,  3.44it/s] 47%|████▋     | 364/780 [02:25<02:00,  3.45it/s] 47%|████▋     | 365/780 [02:25<02:00,  3.45it/s] 47%|████▋     | 366/780 [02:26<02:00,  3.45it/s] 47%|████▋     | 367/780 [02:26<01:59,  3.45it/s] 47%|████▋     | 368/780 [02:26<01:59,  3.45it/s] 47%|████▋     | 369/780 [02:26<01:59,  3.44it/s] 47%|████▋     | 370/780 [02:27<01:59,  3.44it/s] 48%|████▊     | 371/780 [02:27<01:58,  3.45it/s] 48%|████▊     | 372/780 [02:27<01:58,  3.44it/s] 48%|████▊     | 373/780 [02:28<01:58,  3.45it/s] 48%|████▊     | 374/780 [02:28<01:57,  3.45it/s] 48%|████▊     | 375/780 [02:28<01:57,  3.45it/s] 48%|████▊     | 376/780 [02:28<01:57,  3.45it/s] 48%|████▊     | 377/780 [02:29<01:56,  3.45it/s] 48%|████▊     | 378/780 [02:29<01:56,  3.45it/s] 49%|████▊     | 379/780 [02:29<01:56,  3.45it/s] 49%|████▊     | 380/780 [02:30<01:56,  3.44it/s] 49%|████▉     | 381/780 [02:30<01:55,  3.44it/s] 49%|████▉     | 382/780 [02:30<01:55,  3.44it/s] 49%|████▉     | 383/780 [02:30<01:55,  3.44it/s] 49%|████▉     | 384/780 [02:31<01:54,  3.45it/s] 49%|████▉     | 385/780 [02:31<01:54,  3.45it/s] 49%|████▉     | 386/780 [02:31<01:54,  3.45it/s] 50%|████▉     | 387/780 [02:32<01:54,  3.45it/s] 50%|████▉     | 388/780 [02:32<01:53,  3.44it/s] 50%|████▉     | 389/780 [02:32<01:53,  3.45it/s] 50%|█████     | 390/780 [02:33<01:53,  3.45it/s] 50%|█████     | 391/780 [02:33<01:53,  3.44it/s] 50%|█████     | 392/780 [02:33<01:52,  3.44it/s] 50%|█████     | 393/780 [02:33<01:52,  3.44it/s] 51%|█████     | 394/780 [02:34<01:52,  3.44it/s] 51%|█████     | 395/780 [02:34<01:51,  3.44it/s] 51%|█████     | 396/780 [02:34<01:51,  3.45it/s] 51%|█████     | 397/780 [02:35<01:51,  3.45it/s] 51%|█████     | 398/780 [02:35<01:50,  3.44it/s] 51%|█████     | 399/780 [02:35<01:50,  3.45it/s] 51%|█████▏    | 400/780 [02:35<01:50,  3.45it/s] 51%|█████▏    | 401/780 [02:36<01:49,  3.45it/s] 52%|█████▏    | 402/780 [02:36<01:49,  3.45it/s] 52%|█████▏    | 403/780 [02:36<01:49,  3.45it/s] 52%|█████▏    | 404/780 [02:37<01:49,  3.44it/s] 52%|█████▏    | 405/780 [02:37<01:49,  3.44it/s] 52%|█████▏    | 406/780 [02:37<01:48,  3.44it/s] 52%|█████▏    | 407/780 [02:37<01:48,  3.45it/s] 52%|█████▏    | 408/780 [02:38<01:47,  3.45it/s] 52%|█████▏    | 409/780 [02:38<01:47,  3.45it/s] 53%|█████▎    | 410/780 [02:38<01:47,  3.45it/s] 53%|█████▎    | 411/780 [02:39<01:47,  3.45it/s] 53%|█████▎    | 412/780 [02:39<01:46,  3.45it/s] 53%|█████▎    | 413/780 [02:39<01:46,  3.45it/s] 53%|█████▎    | 414/780 [02:39<01:46,  3.45it/s] 53%|█████▎    | 415/780 [02:40<01:45,  3.45it/s] 53%|█████▎    | 416/780 [02:40<01:45,  3.44it/s] 53%|█████▎    | 417/780 [02:40<01:45,  3.44it/s] 54%|█████▎    | 418/780 [02:41<01:45,  3.44it/s] 54%|█████▎    | 419/780 [02:41<01:44,  3.45it/s] 54%|█████▍    | 420/780 [02:41<01:44,  3.44it/s] 54%|█████▍    | 421/780 [02:42<01:44,  3.44it/s] 54%|█████▍    | 422/780 [02:42<01:43,  3.44it/s] 54%|█████▍    | 423/780 [02:42<01:43,  3.44it/s] 54%|█████▍    | 424/780 [02:42<01:43,  3.44it/s] 54%|█████▍    | 425/780 [02:43<01:43,  3.44it/s] 55%|█████▍    | 426/780 [02:43<01:42,  3.44it/s] 55%|█████▍    | 427/780 [02:43<01:43,  3.43it/s] 55%|█████▍    | 428/780 [02:44<01:42,  3.43it/s] 55%|█████▌    | 429/780 [02:44<01:42,  3.44it/s] 55%|█████▌    | 430/780 [02:44<01:41,  3.44it/s] 55%|█████▌    | 431/780 [02:44<01:41,  3.44it/s] 55%|█████▌    | 432/780 [02:45<01:41,  3.44it/s] 56%|█████▌    | 433/780 [02:45<01:40,  3.44it/s] 56%|█████▌    | 434/780 [02:45<01:40,  3.44it/s] 56%|█████▌    | 435/780 [02:46<01:40,  3.45it/s] 56%|█████▌    | 436/780 [02:46<01:39,  3.45it/s] 56%|█████▌    | 437/780 [02:46<01:39,  3.45it/s] 56%|█████▌    | 438/780 [02:46<01:39,  3.43it/s] 56%|█████▋    | 439/780 [02:47<01:39,  3.43it/s] 56%|█████▋    | 440/780 [02:47<01:38,  3.44it/s] 57%|█████▋    | 441/780 [02:47<01:38,  3.44it/s] 57%|█████▋    | 442/780 [02:48<01:38,  3.44it/s] 57%|█████▋    | 443/780 [02:48<01:37,  3.44it/s] 57%|█████▋    | 444/780 [02:48<01:37,  3.45it/s] 57%|█████▋    | 445/780 [02:48<01:37,  3.44it/s] 57%|█████▋    | 446/780 [02:49<01:36,  3.45it/s] 57%|█████▋    | 447/780 [02:49<01:36,  3.45it/s] 57%|█████▋    | 448/780 [02:49<01:36,  3.44it/s] 58%|█████▊    | 449/780 [02:50<01:36,  3.43it/s] 58%|█████▊    | 450/780 [02:50<01:36,  3.44it/s] 58%|█████▊    | 451/780 [02:50<01:35,  3.44it/s] 58%|█████▊    | 452/780 [02:51<01:35,  3.42it/s] 58%|█████▊    | 453/780 [02:51<01:35,  3.43it/s] 58%|█████▊    | 454/780 [02:51<01:34,  3.43it/s] 58%|█████▊    | 455/780 [02:51<01:34,  3.44it/s] 58%|█████▊    | 456/780 [02:52<01:34,  3.44it/s] 59%|█████▊    | 457/780 [02:52<01:34,  3.43it/s] 59%|█████▊    | 458/780 [02:52<01:33,  3.43it/s] 59%|█████▉    | 459/780 [02:53<01:33,  3.44it/s] 59%|█████▉    | 460/780 [02:53<01:33,  3.43it/s] 59%|█████▉    | 461/780 [02:53<01:32,  3.44it/s] 59%|█████▉    | 462/780 [02:53<01:32,  3.44it/s] 59%|█████▉    | 463/780 [02:54<01:32,  3.44it/s] 59%|█████▉    | 464/780 [02:54<01:31,  3.44it/s] 60%|█████▉    | 465/780 [02:54<01:31,  3.45it/s] 60%|█████▉    | 466/780 [02:55<01:31,  3.45it/s] 60%|█████▉    | 467/780 [02:55<01:30,  3.45it/s] 60%|██████    | 468/780 [02:55<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 08:44:48,349 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:44:48,350 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 08:44:48,350 >>   Batch size = 8
{'eval_loss': 0.9556635022163391, 'eval_runtime': 13.1661, 'eval_samples_per_second': 369.435, 'eval_steps_per_second': 46.179, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.09it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.50it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.57it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.97it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.24it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.90it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.48it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.09it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.25it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.36it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.27it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.35it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.42it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.49it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.54it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.33it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.06it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.06it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.21it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.25it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.46it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.43it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.53it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.51it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.45it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.33it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.23it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.24it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.30it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.24it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.41it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.53it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.58it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.51it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.90it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.02it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.10it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.26it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.28it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.30it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.34it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.37it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.33it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.37it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.30it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.32it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.37it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.33it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.37it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.45it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.42it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.41it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.40it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.37it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.32it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.31it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.28it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.35it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.37it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.42it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.35it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.39it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.29it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.30it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.30it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.35it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.29it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.34it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.30it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.49it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.46it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.46it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.31it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.32it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.28it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.28it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.31it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.42it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.38it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.43it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.38it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.27it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.33it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.38it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.39it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.21it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.34it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.31it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.37it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.44it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.43it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.33it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.33it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.27it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.35it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.36it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.45it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.39it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.29it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.44it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.38it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.44it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.35it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.38it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.25it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.29it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.39it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.43it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.42it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.35it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.32it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.29it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.29it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.28it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.30it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.33it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.30it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.33it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.39it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.39it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.40it/s][A                                                 
                                                 [A 60%|██████    | 468/780 [03:08<01:30,  3.44it/s]
100%|██████████| 608/608 [00:13<00:00, 46.40it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:45:01,528 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 08:45:01,549 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:45:03,708 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:45:03,724 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:45:03,735 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:15<32:14,  6.22s/it] 60%|██████    | 470/780 [03:16<22:57,  4.44s/it] 60%|██████    | 471/780 [03:16<16:27,  3.20s/it] 61%|██████    | 472/780 [03:16<11:55,  2.32s/it] 61%|██████    | 473/780 [03:16<08:46,  1.71s/it] 61%|██████    | 474/780 [03:17<06:33,  1.29s/it] 61%|██████    | 475/780 [03:17<05:01,  1.01it/s] 61%|██████    | 476/780 [03:17<03:56,  1.28it/s] 61%|██████    | 477/780 [03:18<03:11,  1.58it/s] 61%|██████▏   | 478/780 [03:18<02:39,  1.89it/s] 61%|██████▏   | 479/780 [03:18<02:17,  2.19it/s] 62%|██████▏   | 480/780 [03:18<02:02,  2.46it/s] 62%|██████▏   | 481/780 [03:19<01:51,  2.68it/s] 62%|██████▏   | 482/780 [03:19<01:43,  2.88it/s] 62%|██████▏   | 483/780 [03:19<01:38,  3.03it/s] 62%|██████▏   | 484/780 [03:20<01:34,  3.14it/s] 62%|██████▏   | 485/780 [03:20<01:31,  3.23it/s] 62%|██████▏   | 486/780 [03:20<01:29,  3.29it/s] 62%|██████▏   | 487/780 [03:20<01:27,  3.33it/s] 63%|██████▎   | 488/780 [03:21<01:26,  3.37it/s] 63%|██████▎   | 489/780 [03:21<01:25,  3.39it/s] 63%|██████▎   | 490/780 [03:21<01:25,  3.41it/s] 63%|██████▎   | 491/780 [03:22<01:24,  3.42it/s] 63%|██████▎   | 492/780 [03:22<01:24,  3.42it/s] 63%|██████▎   | 493/780 [03:22<01:23,  3.43it/s] 63%|██████▎   | 494/780 [03:22<01:23,  3.43it/s] 63%|██████▎   | 495/780 [03:23<01:23,  3.43it/s] 64%|██████▎   | 496/780 [03:23<01:22,  3.44it/s] 64%|██████▎   | 497/780 [03:23<01:22,  3.44it/s] 64%|██████▍   | 498/780 [03:24<01:21,  3.44it/s] 64%|██████▍   | 499/780 [03:24<01:21,  3.44it/s] 64%|██████▍   | 500/780 [03:24<01:21,  3.45it/s]                                                  64%|██████▍   | 500/780 [03:24<01:21,  3.45it/s] 64%|██████▍   | 501/780 [03:25<01:21,  3.44it/s] 64%|██████▍   | 502/780 [03:25<01:20,  3.45it/s] 64%|██████▍   | 503/780 [03:25<01:20,  3.44it/s] 65%|██████▍   | 504/780 [03:25<01:20,  3.44it/s] 65%|██████▍   | 505/780 [03:26<01:19,  3.44it/s] 65%|██████▍   | 506/780 [03:26<01:19,  3.44it/s] 65%|██████▌   | 507/780 [03:26<01:19,  3.45it/s] 65%|██████▌   | 508/780 [03:27<01:18,  3.45it/s] 65%|██████▌   | 509/780 [03:27<01:18,  3.45it/s] 65%|██████▌   | 510/780 [03:27<01:18,  3.45it/s] 66%|██████▌   | 511/780 [03:27<01:17,  3.45it/s] 66%|██████▌   | 512/780 [03:28<01:17,  3.45it/s] 66%|██████▌   | 513/780 [03:28<01:17,  3.45it/s] 66%|██████▌   | 514/780 [03:28<01:17,  3.43it/s] 66%|██████▌   | 515/780 [03:29<01:17,  3.44it/s] 66%|██████▌   | 516/780 [03:29<01:16,  3.44it/s] 66%|██████▋   | 517/780 [03:29<01:16,  3.44it/s] 66%|██████▋   | 518/780 [03:29<01:16,  3.44it/s] 67%|██████▋   | 519/780 [03:30<01:15,  3.44it/s] 67%|██████▋   | 520/780 [03:30<01:15,  3.44it/s] 67%|██████▋   | 521/780 [03:30<01:15,  3.45it/s] 67%|██████▋   | 522/780 [03:31<01:14,  3.44it/s] 67%|██████▋   | 523/780 [03:31<01:14,  3.45it/s] 67%|██████▋   | 524/780 [03:31<01:14,  3.45it/s] 67%|██████▋   | 525/780 [03:31<01:14,  3.43it/s] 67%|██████▋   | 526/780 [03:32<01:13,  3.44it/s] 68%|██████▊   | 527/780 [03:32<01:13,  3.44it/s] 68%|██████▊   | 528/780 [03:32<01:13,  3.44it/s] 68%|██████▊   | 529/780 [03:33<01:12,  3.44it/s] 68%|██████▊   | 530/780 [03:33<01:12,  3.44it/s] 68%|██████▊   | 531/780 [03:33<01:12,  3.44it/s] 68%|██████▊   | 532/780 [03:34<01:12,  3.44it/s] 68%|██████▊   | 533/780 [03:34<01:11,  3.44it/s] 68%|██████▊   | 534/780 [03:34<01:11,  3.45it/s] 69%|██████▊   | 535/780 [03:34<01:11,  3.44it/s] 69%|██████▊   | 536/780 [03:35<01:11,  3.43it/s] 69%|██████▉   | 537/780 [03:35<01:10,  3.44it/s] 69%|██████▉   | 538/780 [03:35<01:10,  3.44it/s] 69%|██████▉   | 539/780 [03:36<01:10,  3.44it/s] 69%|██████▉   | 540/780 [03:36<01:09,  3.44it/s] 69%|██████▉   | 541/780 [03:36<01:09,  3.44it/s] 69%|██████▉   | 542/780 [03:36<01:09,  3.44it/s] 70%|██████▉   | 543/780 [03:37<01:08,  3.44it/s] 70%|██████▉   | 544/780 [03:37<01:08,  3.44it/s] 70%|██████▉   | 545/780 [03:37<01:08,  3.44it/s] 70%|███████   | 546/780 [03:38<01:08,  3.44it/s] 70%|███████   | 547/780 [03:38<01:07,  3.44it/s] 70%|███████   | 548/780 [03:38<01:07,  3.43it/s] 70%|███████   | 549/780 [03:38<01:07,  3.44it/s] 71%|███████   | 550/780 [03:39<01:06,  3.44it/s] 71%|███████   | 551/780 [03:39<01:06,  3.44it/s] 71%|███████   | 552/780 [03:39<01:06,  3.44it/s] 71%|███████   | 553/780 [03:40<01:05,  3.44it/s] 71%|███████   | 554/780 [03:40<01:05,  3.44it/s] 71%|███████   | 555/780 [03:40<01:05,  3.44it/s] 71%|███████▏  | 556/780 [03:40<01:05,  3.44it/s] 71%|███████▏  | 557/780 [03:41<01:04,  3.44it/s] 72%|███████▏  | 558/780 [03:41<01:04,  3.44it/s] 72%|███████▏  | 559/780 [03:41<01:04,  3.44it/s] 72%|███████▏  | 560/780 [03:42<01:03,  3.44it/s] 72%|███████▏  | 561/780 [03:42<01:03,  3.44it/s] 72%|███████▏  | 562/780 [03:42<01:03,  3.44it/s] 72%|███████▏  | 563/780 [03:43<01:03,  3.44it/s] 72%|███████▏  | 564/780 [03:43<01:02,  3.44it/s] 72%|███████▏  | 565/780 [03:43<01:02,  3.44it/s] 73%|███████▎  | 566/780 [03:43<01:02,  3.44it/s] 73%|███████▎  | 567/780 [03:44<01:01,  3.44it/s] 73%|███████▎  | 568/780 [03:44<01:01,  3.44it/s] 73%|███████▎  | 569/780 [03:44<01:01,  3.44it/s] 73%|███████▎  | 570/780 [03:45<01:01,  3.43it/s] 73%|███████▎  | 571/780 [03:45<01:00,  3.43it/s] 73%|███████▎  | 572/780 [03:45<01:00,  3.43it/s] 73%|███████▎  | 573/780 [03:45<01:00,  3.44it/s] 74%|███████▎  | 574/780 [03:46<00:59,  3.44it/s] 74%|███████▎  | 575/780 [03:46<00:59,  3.44it/s] 74%|███████▍  | 576/780 [03:46<00:59,  3.44it/s] 74%|███████▍  | 577/780 [03:47<00:58,  3.44it/s] 74%|███████▍  | 578/780 [03:47<00:58,  3.44it/s] 74%|███████▍  | 579/780 [03:47<00:58,  3.45it/s] 74%|███████▍  | 580/780 [03:47<00:58,  3.45it/s] 74%|███████▍  | 581/780 [03:48<00:57,  3.43it/s] 75%|███████▍  | 582/780 [03:48<00:57,  3.43it/s] 75%|███████▍  | 583/780 [03:48<00:57,  3.44it/s] 75%|███████▍  | 584/780 [03:49<00:56,  3.44it/s] 75%|███████▌  | 585/780 [03:49<00:56,  3.44it/s] 75%|███████▌  | 586/780 [03:49<00:56,  3.44it/s] 75%|███████▌  | 587/780 [03:50<00:56,  3.44it/s] 75%|███████▌  | 588/780 [03:50<00:55,  3.44it/s] 76%|███████▌  | 589/780 [03:50<00:55,  3.44it/s] 76%|███████▌  | 590/780 [03:50<00:55,  3.44it/s] 76%|███████▌  | 591/780 [03:51<00:55,  3.43it/s] 76%|███████▌  | 592/780 [03:51<00:54,  3.42it/s] 76%|███████▌  | 593/780 [03:51<00:54,  3.43it/s] 76%|███████▌  | 594/780 [03:52<00:54,  3.43it/s] 76%|███████▋  | 595/780 [03:52<00:53,  3.44it/s] 76%|███████▋  | 596/780 [03:52<00:54,  3.37it/s] 77%|███████▋  | 597/780 [03:52<00:53,  3.39it/s] 77%|███████▋  | 598/780 [03:53<00:53,  3.41it/s] 77%|███████▋  | 599/780 [03:53<00:52,  3.42it/s] 77%|███████▋  | 600/780 [03:53<00:52,  3.43it/s] 77%|███████▋  | 601/780 [03:54<00:52,  3.43it/s] 77%|███████▋  | 602/780 [03:54<00:51,  3.43it/s] 77%|███████▋  | 603/780 [03:54<00:51,  3.43it/s] 77%|███████▋  | 604/780 [03:54<00:51,  3.43it/s] 78%|███████▊  | 605/780 [03:55<00:50,  3.44it/s] 78%|███████▊  | 606/780 [03:55<00:50,  3.44it/s] 78%|███████▊  | 607/780 [03:55<00:50,  3.44it/s] 78%|███████▊  | 608/780 [03:56<00:49,  3.44it/s] 78%|███████▊  | 609/780 [03:56<00:49,  3.44it/s] 78%|███████▊  | 610/780 [03:56<00:49,  3.44it/s] 78%|███████▊  | 611/780 [03:57<00:49,  3.44it/s] 78%|███████▊  | 612/780 [03:57<00:48,  3.44it/s] 79%|███████▊  | 613/780 [03:57<00:48,  3.44it/s] 79%|███████▊  | 614/780 [03:57<00:48,  3.44it/s] 79%|███████▉  | 615/780 [03:58<00:47,  3.44it/s] 79%|███████▉  | 616/780 [03:58<00:47,  3.44it/s] 79%|███████▉  | 617/780 [03:58<00:47,  3.44it/s] 79%|███████▉  | 618/780 [03:59<00:47,  3.43it/s] 79%|███████▉  | 619/780 [03:59<00:46,  3.44it/s] 79%|███████▉  | 620/780 [03:59<00:46,  3.44it/s] 80%|███████▉  | 621/780 [03:59<00:46,  3.44it/s] 80%|███████▉  | 622/780 [04:00<00:45,  3.44it/s] 80%|███████▉  | 623/780 [04:00<00:45,  3.44it/s] 80%|████████  | 624/780 [04:00<00:45,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 08:45:53,462 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:45:53,463 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 08:45:53,463 >>   Batch size = 8
{'eval_loss': 0.9736651182174683, 'eval_runtime': 13.1462, 'eval_samples_per_second': 369.992, 'eval_steps_per_second': 46.249, 'epoch': 3.0}
{'loss': 0.6023, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.12it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.18it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.48it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.69it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.19it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.01it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.75it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.31it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.36it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.40it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.47it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.56it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.55it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.57it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.48it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.32it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.18it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.24it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.30it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.39it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.43it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.45it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.49it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.33it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.36it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.24it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.28it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.25it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.30it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.27it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.45it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.44it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.47it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.31it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.24it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.14it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.33it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.33it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.38it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.38it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.30it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.34it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.34it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.29it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.33it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.30it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.32it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.29it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.30it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.44it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.47it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.26it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.23it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.15it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.33it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.43it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.44it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.32it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.40it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.33it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.31it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.34it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 45.67it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 45.93it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.03it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.17it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.23it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.21it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.34it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.27it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.16it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.30it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.21it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.34it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.33it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.26it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.27it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.31it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.33it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.33it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.23it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.23it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.31it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.38it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.47it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.36it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.19it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.19it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.14it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.25it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.25it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.36it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.33it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.41it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.35it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.31it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.33it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.29it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.32it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.34it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.22it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.33it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.36it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.42it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.34it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.30it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.28it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.34it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.32it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.28it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.26it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.37it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.30it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.29it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.36it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.21it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.28it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.37it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.30it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.33it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.34it/s][A                                                 
                                                 [A 80%|████████  | 624/780 [04:13<00:45,  3.44it/s]
100%|██████████| 608/608 [00:13<00:00, 46.34it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:46:06,641 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 08:46:06,658 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:46:08,913 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:46:08,932 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:46:08,957 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:21<16:28,  6.38s/it] 80%|████████  | 626/780 [04:21<11:40,  4.55s/it] 80%|████████  | 627/780 [04:21<08:20,  3.27s/it] 81%|████████  | 628/780 [04:22<06:01,  2.38s/it] 81%|████████  | 629/780 [04:22<04:24,  1.75s/it] 81%|████████  | 630/780 [04:22<03:16,  1.31s/it] 81%|████████  | 631/780 [04:23<02:29,  1.01s/it] 81%|████████  | 632/780 [04:23<01:57,  1.26it/s] 81%|████████  | 633/780 [04:23<01:34,  1.56it/s] 81%|████████▏ | 634/780 [04:23<01:18,  1.87it/s] 81%|████████▏ | 635/780 [04:24<01:06,  2.16it/s] 82%|████████▏ | 636/780 [04:24<00:59,  2.44it/s] 82%|████████▏ | 637/780 [04:24<00:53,  2.67it/s] 82%|████████▏ | 638/780 [04:25<00:49,  2.87it/s] 82%|████████▏ | 639/780 [04:25<00:46,  3.02it/s] 82%|████████▏ | 640/780 [04:25<00:44,  3.14it/s] 82%|████████▏ | 641/780 [04:25<00:43,  3.22it/s] 82%|████████▏ | 642/780 [04:26<00:41,  3.29it/s] 82%|████████▏ | 643/780 [04:26<00:41,  3.33it/s] 83%|████████▎ | 644/780 [04:26<00:40,  3.37it/s] 83%|████████▎ | 645/780 [04:27<00:39,  3.39it/s] 83%|████████▎ | 646/780 [04:27<00:39,  3.40it/s] 83%|████████▎ | 647/780 [04:27<00:38,  3.42it/s] 83%|████████▎ | 648/780 [04:28<00:38,  3.41it/s] 83%|████████▎ | 649/780 [04:28<00:38,  3.42it/s] 83%|████████▎ | 650/780 [04:28<00:37,  3.43it/s] 83%|████████▎ | 651/780 [04:28<00:37,  3.44it/s] 84%|████████▎ | 652/780 [04:29<00:37,  3.44it/s] 84%|████████▎ | 653/780 [04:29<00:36,  3.44it/s] 84%|████████▍ | 654/780 [04:29<00:36,  3.44it/s] 84%|████████▍ | 655/780 [04:30<00:36,  3.44it/s] 84%|████████▍ | 656/780 [04:30<00:36,  3.44it/s] 84%|████████▍ | 657/780 [04:30<00:35,  3.45it/s] 84%|████████▍ | 658/780 [04:30<00:35,  3.44it/s] 84%|████████▍ | 659/780 [04:31<00:35,  3.43it/s] 85%|████████▍ | 660/780 [04:31<00:34,  3.44it/s] 85%|████████▍ | 661/780 [04:31<00:34,  3.44it/s] 85%|████████▍ | 662/780 [04:32<00:34,  3.44it/s] 85%|████████▌ | 663/780 [04:32<00:33,  3.44it/s] 85%|████████▌ | 664/780 [04:32<00:33,  3.44it/s] 85%|████████▌ | 665/780 [04:32<00:33,  3.44it/s] 85%|████████▌ | 666/780 [04:33<00:33,  3.45it/s] 86%|████████▌ | 667/780 [04:33<00:32,  3.45it/s] 86%|████████▌ | 668/780 [04:33<00:32,  3.45it/s] 86%|████████▌ | 669/780 [04:34<00:32,  3.45it/s] 86%|████████▌ | 670/780 [04:34<00:31,  3.44it/s] 86%|████████▌ | 671/780 [04:34<00:31,  3.44it/s] 86%|████████▌ | 672/780 [04:35<00:31,  3.44it/s] 86%|████████▋ | 673/780 [04:35<00:31,  3.45it/s] 86%|████████▋ | 674/780 [04:35<00:30,  3.45it/s] 87%|████████▋ | 675/780 [04:35<00:30,  3.45it/s] 87%|████████▋ | 676/780 [04:36<00:30,  3.45it/s] 87%|████████▋ | 677/780 [04:36<00:29,  3.45it/s] 87%|████████▋ | 678/780 [04:36<00:29,  3.44it/s] 87%|████████▋ | 679/780 [04:37<00:29,  3.44it/s] 87%|████████▋ | 680/780 [04:37<00:29,  3.44it/s] 87%|████████▋ | 681/780 [04:37<00:28,  3.43it/s] 87%|████████▋ | 682/780 [04:37<00:28,  3.44it/s] 88%|████████▊ | 683/780 [04:38<00:28,  3.44it/s] 88%|████████▊ | 684/780 [04:38<00:27,  3.44it/s] 88%|████████▊ | 685/780 [04:38<00:27,  3.45it/s] 88%|████████▊ | 686/780 [04:39<00:27,  3.44it/s] 88%|████████▊ | 687/780 [04:39<00:26,  3.45it/s] 88%|████████▊ | 688/780 [04:39<00:26,  3.45it/s] 88%|████████▊ | 689/780 [04:39<00:26,  3.45it/s] 88%|████████▊ | 690/780 [04:40<00:26,  3.44it/s] 89%|████████▊ | 691/780 [04:40<00:25,  3.45it/s] 89%|████████▊ | 692/780 [04:40<00:25,  3.43it/s] 89%|████████▉ | 693/780 [04:41<00:25,  3.44it/s] 89%|████████▉ | 694/780 [04:41<00:24,  3.44it/s] 89%|████████▉ | 695/780 [04:41<00:24,  3.44it/s] 89%|████████▉ | 696/780 [04:41<00:24,  3.44it/s] 89%|████████▉ | 697/780 [04:42<00:24,  3.44it/s] 89%|████████▉ | 698/780 [04:42<00:23,  3.44it/s] 90%|████████▉ | 699/780 [04:42<00:23,  3.44it/s] 90%|████████▉ | 700/780 [04:43<00:23,  3.44it/s] 90%|████████▉ | 701/780 [04:43<00:22,  3.45it/s] 90%|█████████ | 702/780 [04:43<00:22,  3.44it/s] 90%|█████████ | 703/780 [04:44<00:22,  3.44it/s] 90%|█████████ | 704/780 [04:44<00:22,  3.44it/s] 90%|█████████ | 705/780 [04:44<00:21,  3.44it/s] 91%|█████████ | 706/780 [04:44<00:21,  3.45it/s] 91%|█████████ | 707/780 [04:45<00:21,  3.43it/s] 91%|█████████ | 708/780 [04:45<00:20,  3.44it/s] 91%|█████████ | 709/780 [04:45<00:20,  3.43it/s] 91%|█████████ | 710/780 [04:46<00:20,  3.44it/s] 91%|█████████ | 711/780 [04:46<00:20,  3.44it/s] 91%|█████████▏| 712/780 [04:46<00:19,  3.44it/s] 91%|█████████▏| 713/780 [04:46<00:19,  3.44it/s] 92%|█████████▏| 714/780 [04:47<00:19,  3.44it/s] 92%|█████████▏| 715/780 [04:47<00:18,  3.44it/s] 92%|█████████▏| 716/780 [04:47<00:18,  3.44it/s] 92%|█████████▏| 717/780 [04:48<00:18,  3.44it/s] 92%|█████████▏| 718/780 [04:48<00:18,  3.43it/s] 92%|█████████▏| 719/780 [04:48<00:17,  3.43it/s] 92%|█████████▏| 720/780 [04:48<00:17,  3.44it/s] 92%|█████████▏| 721/780 [04:49<00:17,  3.44it/s] 93%|█████████▎| 722/780 [04:49<00:16,  3.44it/s] 93%|█████████▎| 723/780 [04:49<00:16,  3.44it/s] 93%|█████████▎| 724/780 [04:50<00:16,  3.44it/s] 93%|█████████▎| 725/780 [04:50<00:15,  3.44it/s] 93%|█████████▎| 726/780 [04:50<00:15,  3.44it/s] 93%|█████████▎| 727/780 [04:50<00:15,  3.45it/s] 93%|█████████▎| 728/780 [04:51<00:15,  3.43it/s] 93%|█████████▎| 729/780 [04:51<00:14,  3.42it/s] 94%|█████████▎| 730/780 [04:51<00:14,  3.43it/s] 94%|█████████▎| 731/780 [04:52<00:14,  3.43it/s] 94%|█████████▍| 732/780 [04:52<00:13,  3.44it/s] 94%|█████████▍| 733/780 [04:52<00:14,  3.35it/s] 94%|█████████▍| 734/780 [04:53<00:13,  3.37it/s] 94%|█████████▍| 735/780 [04:53<00:13,  3.39it/s] 94%|█████████▍| 736/780 [04:53<00:12,  3.41it/s] 94%|█████████▍| 737/780 [04:53<00:12,  3.42it/s] 95%|█████████▍| 738/780 [04:54<00:12,  3.43it/s] 95%|█████████▍| 739/780 [04:54<00:11,  3.43it/s] 95%|█████████▍| 740/780 [04:54<00:11,  3.43it/s] 95%|█████████▌| 741/780 [04:55<00:11,  3.43it/s] 95%|█████████▌| 742/780 [04:55<00:11,  3.44it/s] 95%|█████████▌| 743/780 [04:55<00:10,  3.44it/s] 95%|█████████▌| 744/780 [04:55<00:10,  3.44it/s] 96%|█████████▌| 745/780 [04:56<00:10,  3.44it/s] 96%|█████████▌| 746/780 [04:56<00:09,  3.44it/s] 96%|█████████▌| 747/780 [04:56<00:09,  3.44it/s] 96%|█████████▌| 748/780 [04:57<00:09,  3.44it/s] 96%|█████████▌| 749/780 [04:57<00:09,  3.44it/s] 96%|█████████▌| 750/780 [04:57<00:08,  3.44it/s] 96%|█████████▋| 751/780 [04:57<00:08,  3.43it/s] 96%|█████████▋| 752/780 [04:58<00:08,  3.44it/s] 97%|█████████▋| 753/780 [04:58<00:07,  3.44it/s] 97%|█████████▋| 754/780 [04:58<00:07,  3.44it/s] 97%|█████████▋| 755/780 [04:59<00:07,  3.44it/s] 97%|█████████▋| 756/780 [04:59<00:06,  3.44it/s] 97%|█████████▋| 757/780 [04:59<00:06,  3.45it/s] 97%|█████████▋| 758/780 [05:00<00:06,  3.45it/s] 97%|█████████▋| 759/780 [05:00<00:06,  3.45it/s] 97%|█████████▋| 760/780 [05:00<00:05,  3.44it/s] 98%|█████████▊| 761/780 [05:00<00:05,  3.45it/s] 98%|█████████▊| 762/780 [05:01<00:05,  3.44it/s] 98%|█████████▊| 763/780 [05:01<00:04,  3.44it/s] 98%|█████████▊| 764/780 [05:01<00:04,  3.44it/s] 98%|█████████▊| 765/780 [05:02<00:04,  3.44it/s] 98%|█████████▊| 766/780 [05:02<00:04,  3.44it/s] 98%|█████████▊| 767/780 [05:02<00:03,  3.44it/s] 98%|█████████▊| 768/780 [05:02<00:03,  3.44it/s] 99%|█████████▊| 769/780 [05:03<00:03,  3.44it/s] 99%|█████████▊| 770/780 [05:03<00:02,  3.44it/s] 99%|█████████▉| 771/780 [05:03<00:02,  3.44it/s] 99%|█████████▉| 772/780 [05:04<00:02,  3.44it/s] 99%|█████████▉| 773/780 [05:04<00:02,  3.43it/s] 99%|█████████▉| 774/780 [05:04<00:01,  3.44it/s] 99%|█████████▉| 775/780 [05:04<00:01,  3.44it/s] 99%|█████████▉| 776/780 [05:05<00:01,  3.44it/s]100%|█████████▉| 777/780 [05:05<00:00,  3.44it/s]100%|█████████▉| 778/780 [05:05<00:00,  3.44it/s]100%|█████████▉| 779/780 [05:06<00:00,  3.44it/s]100%|██████████| 780/780 [05:06<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 08:46:59,053 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:46:59,054 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 08:46:59,054 >>   Batch size = 8
{'eval_loss': 0.9747095704078674, 'eval_runtime': 13.1561, 'eval_samples_per_second': 369.715, 'eval_steps_per_second': 46.214, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.21it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.13it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.42it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.80it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.40it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.08it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.71it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.31it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.32it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.40it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.49it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.60it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.61it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.58it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.44it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.34it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.25it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.24it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.19it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.30it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.33it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.45it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.48it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.57it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.41it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.29it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.23it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.23it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.28it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.39it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.35it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.43it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.47it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.42it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.33it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.24it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.18it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.26it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.36it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.29it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.36it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.42it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.36it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.33it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 45.63it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.79it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 46.10it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.14it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.09it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.22it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.41it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.43it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.29it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.25it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.09it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.30it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.38it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.32it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.34it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.35it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.33it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.37it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.31it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.34it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.22it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.29it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.21it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.26it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.37it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.45it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.42it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.33it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.26it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.34it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.26it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.37it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.19it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.19it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.29it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.39it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.38it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.49it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.41it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.31it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.29it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.27it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.34it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.30it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.34it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.31it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.37it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.21it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.21it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.32it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.34it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.14it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.14it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.25it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.40it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.45it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.48it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.25it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.24it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.30it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.28it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.24it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.23it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.17it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.27it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.29it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.41it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.46it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.44it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.30it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.84it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.34it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.35it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.34it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.39it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.23it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.22it/s][A                                                 
                                                 [A100%|██████████| 780/780 [05:19<00:00,  3.44it/s]
100%|██████████| 608/608 [00:13<00:00, 46.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:47:12,207 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 08:47:12,222 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:47:14,701 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:47:14,727 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:47:14,738 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 08:47:20,015 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 08:47:20,019 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156 (score: 0.9444142580032349).
                                                 100%|██████████| 780/780 [05:29<00:00,  3.44it/s]100%|██████████| 780/780 [05:29<00:00,  2.37it/s]
[INFO|trainer.py:1894] 2023-08-29 08:47:21,756 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 08:47:21,772 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:47:24,059 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:47:24,078 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:47:24,091 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:47:24,299 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:24,299 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:24,299 >>   train_loss               =     0.5918
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:24,299 >>   train_runtime            = 0:05:29.12
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:24,299 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:24,299 >>   train_samples_per_second =    151.919
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:24,300 >>   train_steps_per_second   =       2.37
{'eval_loss': 0.9828705787658691, 'eval_runtime': 13.1314, 'eval_samples_per_second': 370.411, 'eval_steps_per_second': 46.301, 'epoch': 5.0}
{'train_runtime': 329.122, 'train_samples_per_second': 151.919, 'train_steps_per_second': 2.37, 'train_loss': 0.5918397169846755, 'epoch': 5.0}
08/29/2023 08:47:24 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 08:47:24,338 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:47:24,338 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 08:47:24,338 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 56.99it/s]  2%|▏         | 12/608 [00:00<00:11, 50.58it/s]  3%|▎         | 18/608 [00:00<00:12, 48.78it/s]  4%|▍         | 23/608 [00:00<00:12, 48.09it/s]  5%|▍         | 28/608 [00:00<00:12, 47.61it/s]  5%|▌         | 33/608 [00:00<00:12, 47.14it/s]  6%|▋         | 38/608 [00:00<00:12, 47.03it/s]  7%|▋         | 43/608 [00:00<00:12, 46.76it/s]  8%|▊         | 48/608 [00:01<00:12, 46.56it/s]  9%|▊         | 53/608 [00:01<00:11, 46.50it/s] 10%|▉         | 58/608 [00:01<00:11, 46.38it/s] 10%|█         | 63/608 [00:01<00:11, 46.42it/s] 11%|█         | 68/608 [00:01<00:11, 46.57it/s] 12%|█▏        | 73/608 [00:01<00:11, 46.62it/s] 13%|█▎        | 78/608 [00:01<00:11, 46.67it/s] 14%|█▎        | 83/608 [00:01<00:11, 46.60it/s] 14%|█▍        | 88/608 [00:01<00:11, 46.62it/s] 15%|█▌        | 93/608 [00:01<00:11, 46.50it/s] 16%|█▌        | 98/608 [00:02<00:11, 46.36it/s] 17%|█▋        | 103/608 [00:02<00:10, 46.42it/s] 18%|█▊        | 108/608 [00:02<00:10, 46.43it/s] 19%|█▊        | 113/608 [00:02<00:10, 46.52it/s] 19%|█▉        | 118/608 [00:02<00:10, 46.52it/s] 20%|██        | 123/608 [00:02<00:10, 46.39it/s] 21%|██        | 128/608 [00:02<00:10, 46.42it/s] 22%|██▏       | 133/608 [00:02<00:10, 46.50it/s] 23%|██▎       | 138/608 [00:02<00:10, 46.51it/s] 24%|██▎       | 143/608 [00:03<00:10, 46.49it/s] 24%|██▍       | 148/608 [00:03<00:09, 46.40it/s] 25%|██▌       | 153/608 [00:03<00:09, 46.41it/s] 26%|██▌       | 158/608 [00:03<00:09, 46.48it/s] 27%|██▋       | 163/608 [00:03<00:09, 46.35it/s] 28%|██▊       | 168/608 [00:03<00:09, 46.49it/s] 28%|██▊       | 173/608 [00:03<00:09, 46.52it/s] 29%|██▉       | 178/608 [00:03<00:09, 46.41it/s] 30%|███       | 183/608 [00:03<00:09, 46.40it/s] 31%|███       | 188/608 [00:04<00:09, 46.36it/s] 32%|███▏      | 193/608 [00:04<00:08, 46.33it/s] 33%|███▎      | 198/608 [00:04<00:08, 46.46it/s] 33%|███▎      | 203/608 [00:04<00:08, 46.51it/s] 34%|███▍      | 208/608 [00:04<00:08, 46.43it/s] 35%|███▌      | 213/608 [00:04<00:08, 46.32it/s] 36%|███▌      | 218/608 [00:04<00:08, 46.43it/s] 37%|███▋      | 223/608 [00:04<00:08, 46.50it/s] 38%|███▊      | 228/608 [00:04<00:08, 46.51it/s] 38%|███▊      | 233/608 [00:04<00:08, 46.41it/s] 39%|███▉      | 238/608 [00:05<00:07, 46.33it/s] 40%|███▉      | 243/608 [00:05<00:07, 46.39it/s] 41%|████      | 248/608 [00:05<00:07, 46.35it/s] 42%|████▏     | 253/608 [00:05<00:07, 46.38it/s] 42%|████▏     | 258/608 [00:05<00:07, 46.47it/s] 43%|████▎     | 263/608 [00:05<00:07, 46.41it/s] 44%|████▍     | 268/608 [00:05<00:07, 46.48it/s] 45%|████▍     | 273/608 [00:05<00:07, 46.52it/s] 46%|████▌     | 278/608 [00:05<00:07, 46.32it/s] 47%|████▋     | 283/608 [00:06<00:07, 46.27it/s] 47%|████▋     | 288/608 [00:06<00:06, 46.34it/s] 48%|████▊     | 293/608 [00:06<00:06, 46.36it/s] 49%|████▉     | 298/608 [00:06<00:06, 46.47it/s] 50%|████▉     | 303/608 [00:06<00:06, 46.42it/s] 51%|█████     | 308/608 [00:06<00:06, 46.36it/s] 51%|█████▏    | 313/608 [00:06<00:06, 46.44it/s] 52%|█████▏    | 318/608 [00:06<00:06, 46.38it/s] 53%|█████▎    | 323/608 [00:06<00:06, 46.35it/s] 54%|█████▍    | 328/608 [00:07<00:06, 46.29it/s] 55%|█████▍    | 333/608 [00:07<00:05, 46.19it/s] 56%|█████▌    | 338/608 [00:07<00:05, 46.32it/s] 56%|█████▋    | 343/608 [00:07<00:05, 46.39it/s] 57%|█████▋    | 348/608 [00:07<00:05, 46.36it/s] 58%|█████▊    | 353/608 [00:07<00:05, 46.37it/s] 59%|█████▉    | 358/608 [00:07<00:05, 46.42it/s] 60%|█████▉    | 363/608 [00:07<00:05, 46.27it/s] 61%|██████    | 368/608 [00:07<00:05, 46.28it/s] 61%|██████▏   | 373/608 [00:08<00:05, 46.37it/s] 62%|██████▏   | 378/608 [00:08<00:04, 46.30it/s] 63%|██████▎   | 383/608 [00:08<00:04, 46.36it/s] 64%|██████▍   | 388/608 [00:08<00:04, 46.40it/s] 65%|██████▍   | 393/608 [00:08<00:04, 46.33it/s] 65%|██████▌   | 398/608 [00:08<00:04, 46.46it/s] 66%|██████▋   | 403/608 [00:08<00:04, 46.48it/s] 67%|██████▋   | 408/608 [00:08<00:04, 46.45it/s] 68%|██████▊   | 413/608 [00:08<00:04, 46.49it/s] 69%|██████▉   | 418/608 [00:08<00:04, 46.54it/s] 70%|██████▉   | 423/608 [00:09<00:03, 46.49it/s] 70%|███████   | 428/608 [00:09<00:03, 46.46it/s] 71%|███████   | 433/608 [00:09<00:03, 46.56it/s] 72%|███████▏  | 438/608 [00:09<00:03, 46.60it/s] 73%|███████▎  | 443/608 [00:09<00:03, 46.51it/s] 74%|███████▎  | 448/608 [00:09<00:03, 46.52it/s] 75%|███████▍  | 453/608 [00:09<00:03, 46.53it/s] 75%|███████▌  | 458/608 [00:09<00:03, 46.53it/s] 76%|███████▌  | 463/608 [00:09<00:03, 46.52it/s] 77%|███████▋  | 468/608 [00:10<00:03, 46.56it/s] 78%|███████▊  | 473/608 [00:10<00:02, 46.43it/s] 79%|███████▊  | 478/608 [00:10<00:02, 46.53it/s] 79%|███████▉  | 483/608 [00:10<00:02, 46.56it/s] 80%|████████  | 488/608 [00:10<00:02, 46.49it/s] 81%|████████  | 493/608 [00:10<00:02, 46.47it/s] 82%|████████▏ | 498/608 [00:10<00:02, 46.54it/s] 83%|████████▎ | 503/608 [00:10<00:02, 46.37it/s] 84%|████████▎ | 508/608 [00:10<00:02, 46.47it/s] 84%|████████▍ | 513/608 [00:11<00:02, 46.42it/s] 85%|████████▌ | 518/608 [00:11<00:01, 46.50it/s] 86%|████████▌ | 523/608 [00:11<00:01, 46.54it/s] 87%|████████▋ | 528/608 [00:11<00:01, 46.60it/s] 88%|████████▊ | 533/608 [00:11<00:01, 46.56it/s] 88%|████████▊ | 538/608 [00:11<00:01, 46.61it/s] 89%|████████▉ | 543/608 [00:11<00:01, 46.53it/s] 90%|█████████ | 548/608 [00:11<00:01, 46.54it/s] 91%|█████████ | 553/608 [00:11<00:01, 46.51it/s] 92%|█████████▏| 558/608 [00:11<00:01, 46.52it/s] 93%|█████████▎| 563/608 [00:12<00:00, 46.39it/s] 93%|█████████▎| 568/608 [00:12<00:00, 46.52it/s] 94%|█████████▍| 573/608 [00:12<00:00, 46.53it/s] 95%|█████████▌| 578/608 [00:12<00:00, 46.51it/s] 96%|█████████▌| 583/608 [00:12<00:00, 46.57it/s] 97%|█████████▋| 588/608 [00:12<00:00, 46.60it/s] 98%|█████████▊| 593/608 [00:12<00:00, 46.50it/s] 98%|█████████▊| 598/608 [00:12<00:00, 46.52it/s] 99%|█████████▉| 603/608 [00:12<00:00, 46.56it/s]100%|██████████| 608/608 [00:13<00:00, 46.51it/s]100%|██████████| 608/608 [00:13<00:00, 46.51it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:47:37,432 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:37,432 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:37,432 >>   eval_loss               =     0.9444
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:37,432 >>   eval_runtime            = 0:00:13.09
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:37,432 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:37,432 >>   eval_samples_per_second =    371.477
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:37,432 >>   eval_steps_per_second   =     46.435
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:37,432 >>   perplexity              =     2.5713
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:44,240 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:44,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:44,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:44,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:44,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:47:44,830 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:47:44,831 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:47:45,404 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:47:46,423 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:47:46,423 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:49,474 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:49,478 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:49,478 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:49,478 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:49,478 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:47:50,103 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:47:50,104 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:47:50,678 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:47:50,829 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:47:50,829 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:08,  1.66it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.67it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.47it/s]Extractor Predicting: 25it [00:15,  1.44it/s]Extractor Predicting: 26it [00:16,  1.44it/s]Extractor Predicting: 27it [00:17,  1.45it/s]Extractor Predicting: 28it [00:17,  1.46it/s]Extractor Predicting: 29it [00:18,  1.49it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:23,  1.52it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:25,  1.52it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.52it/s]Extractor Predicting: 46it [00:29,  1.53it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:31,  1.52it/s]Extractor Predicting: 50it [00:32,  1.52it/s]Extractor Predicting: 51it [00:32,  1.51it/s]Extractor Predicting: 52it [00:33,  1.51it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:36,  1.56it/s]Extractor Predicting: 57it [00:36,  1.54it/s]Extractor Predicting: 58it [00:37,  1.53it/s]Extractor Predicting: 59it [00:38,  1.54it/s]Extractor Predicting: 60it [00:38,  1.51it/s]Extractor Predicting: 61it [00:39,  1.47it/s]Extractor Predicting: 62it [00:40,  1.47it/s]Extractor Predicting: 63it [00:40,  1.49it/s]Extractor Predicting: 64it [00:41,  1.51it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:42,  1.52it/s]Extractor Predicting: 67it [00:43,  1.54it/s]Extractor Predicting: 68it [00:44,  1.50it/s]Extractor Predicting: 69it [00:44,  1.50it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:46,  1.57it/s]Extractor Predicting: 72it [00:46,  1.56it/s]Extractor Predicting: 73it [00:47,  1.57it/s]Extractor Predicting: 74it [00:47,  1.55it/s]Extractor Predicting: 75it [00:48,  1.56it/s]Extractor Predicting: 76it [00:49,  1.55it/s]Extractor Predicting: 77it [00:49,  1.54it/s]Extractor Predicting: 78it [00:50,  1.57it/s]Extractor Predicting: 79it [00:51,  1.56it/s]Extractor Predicting: 80it [00:51,  1.58it/s]Extractor Predicting: 81it [00:52,  1.55it/s]Extractor Predicting: 82it [00:53,  1.53it/s]Extractor Predicting: 83it [00:53,  1.52it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:56,  1.54it/s]Extractor Predicting: 88it [00:57,  1.53it/s]Extractor Predicting: 89it [00:57,  1.46it/s]Extractor Predicting: 90it [00:58,  1.51it/s]Extractor Predicting: 91it [00:59,  1.53it/s]Extractor Predicting: 92it [00:59,  1.40it/s]Extractor Predicting: 93it [01:00,  1.44it/s]Extractor Predicting: 94it [01:01,  1.46it/s]Extractor Predicting: 95it [01:01,  1.43it/s]Extractor Predicting: 96it [01:02,  1.45it/s]Extractor Predicting: 97it [01:03,  1.47it/s]Extractor Predicting: 98it [01:03,  1.49it/s]Extractor Predicting: 99it [01:04,  1.52it/s]Extractor Predicting: 100it [01:05,  1.51it/s]Extractor Predicting: 101it [01:05,  1.52it/s]Extractor Predicting: 102it [01:06,  1.54it/s]Extractor Predicting: 103it [01:07,  1.55it/s]Extractor Predicting: 104it [01:07,  1.54it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:09,  1.52it/s]Extractor Predicting: 107it [01:09,  1.52it/s]Extractor Predicting: 108it [01:10,  1.55it/s]Extractor Predicting: 109it [01:11,  1.51it/s]Extractor Predicting: 110it [01:11,  1.48it/s]Extractor Predicting: 111it [01:12,  1.51it/s]Extractor Predicting: 112it [01:13,  1.51it/s]Extractor Predicting: 113it [01:13,  1.53it/s]Extractor Predicting: 114it [01:14,  1.56it/s]Extractor Predicting: 115it [01:14,  1.61it/s]Extractor Predicting: 116it [01:15,  1.61it/s]Extractor Predicting: 117it [01:16,  1.60it/s]Extractor Predicting: 118it [01:16,  1.62it/s]Extractor Predicting: 119it [01:17,  1.59it/s]Extractor Predicting: 120it [01:18,  1.60it/s]Extractor Predicting: 121it [01:18,  1.58it/s]Extractor Predicting: 122it [01:19,  1.52it/s]Extractor Predicting: 123it [01:20,  1.49it/s]Extractor Predicting: 124it [01:20,  1.46it/s]Extractor Predicting: 125it [01:21,  1.48it/s]Extractor Predicting: 126it [01:22,  1.50it/s]Extractor Predicting: 127it [01:22,  1.51it/s]Extractor Predicting: 128it [01:23,  1.47it/s]Extractor Predicting: 129it [01:24,  1.49it/s]Extractor Predicting: 130it [01:24,  1.52it/s]Extractor Predicting: 131it [01:25,  1.56it/s]Extractor Predicting: 132it [01:26,  1.51it/s]Extractor Predicting: 133it [01:26,  1.50it/s]Extractor Predicting: 134it [01:27,  1.51it/s]Extractor Predicting: 135it [01:28,  1.54it/s]Extractor Predicting: 136it [01:28,  1.54it/s]Extractor Predicting: 137it [01:29,  1.55it/s]Extractor Predicting: 138it [01:30,  1.53it/s]Extractor Predicting: 139it [01:30,  1.52it/s]Extractor Predicting: 140it [01:31,  1.51it/s]Extractor Predicting: 141it [01:31,  1.55it/s]Extractor Predicting: 142it [01:32,  1.52it/s]Extractor Predicting: 143it [01:33,  1.55it/s]Extractor Predicting: 144it [01:33,  1.55it/s]Extractor Predicting: 145it [01:34,  1.56it/s]Extractor Predicting: 146it [01:35,  1.51it/s]Extractor Predicting: 147it [01:35,  1.49it/s]Extractor Predicting: 148it [01:36,  1.50it/s]Extractor Predicting: 149it [01:37,  1.49it/s]Extractor Predicting: 150it [01:37,  1.51it/s]Extractor Predicting: 151it [01:38,  1.50it/s]Extractor Predicting: 152it [01:39,  1.54it/s]Extractor Predicting: 153it [01:39,  1.52it/s]Extractor Predicting: 154it [01:40,  1.52it/s]Extractor Predicting: 155it [01:41,  1.53it/s]Extractor Predicting: 156it [01:41,  1.56it/s]Extractor Predicting: 157it [01:42,  1.56it/s]Extractor Predicting: 158it [01:43,  1.57it/s]Extractor Predicting: 159it [01:43,  1.55it/s]Extractor Predicting: 160it [01:44,  1.56it/s]Extractor Predicting: 161it [01:45,  1.54it/s]Extractor Predicting: 162it [01:45,  1.51it/s]Extractor Predicting: 163it [01:46,  1.53it/s]Extractor Predicting: 164it [01:47,  1.52it/s]Extractor Predicting: 165it [01:47,  1.54it/s]Extractor Predicting: 166it [01:48,  1.51it/s]Extractor Predicting: 167it [01:49,  1.50it/s]Extractor Predicting: 168it [01:49,  1.47it/s]Extractor Predicting: 169it [01:50,  1.45it/s]Extractor Predicting: 170it [01:51,  1.46it/s]Extractor Predicting: 171it [01:51,  1.45it/s]Extractor Predicting: 172it [01:52,  1.47it/s]Extractor Predicting: 173it [01:53,  1.45it/s]Extractor Predicting: 174it [01:53,  1.42it/s]Extractor Predicting: 175it [01:54,  1.47it/s]Extractor Predicting: 176it [01:55,  1.45it/s]Extractor Predicting: 177it [01:55,  1.43it/s]Extractor Predicting: 178it [01:56,  1.41it/s]Extractor Predicting: 179it [01:57,  1.39it/s]Extractor Predicting: 180it [01:58,  1.40it/s]Extractor Predicting: 181it [01:58,  1.42it/s]Extractor Predicting: 182it [01:59,  1.42it/s]Extractor Predicting: 183it [02:00,  1.41it/s]Extractor Predicting: 184it [02:00,  1.41it/s]Extractor Predicting: 185it [02:01,  1.45it/s]Extractor Predicting: 186it [02:02,  1.31it/s]Extractor Predicting: 187it [02:03,  1.43it/s]Extractor Predicting: 187it [02:03,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:03,600 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:03,605 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:03,605 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:03,605 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:03,605 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:50:04,229 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:50:04,230 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:50:04,803 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:50:05,828 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:50:05,828 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:08,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:08,731 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:08,732 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:08,732 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:08,732 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:50:09,355 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:50:09,356 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:50:09,924 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:50:10,078 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:50:10,078 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5025125628140703,
  "recall": 0.02055921052631579,
  "score": 0.03950227138060438,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.50it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.48it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.51it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.48it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:23,  1.38it/s]Extractor Predicting: 36it [00:23,  1.44it/s]Extractor Predicting: 37it [00:24,  1.43it/s]Extractor Predicting: 38it [00:25,  1.43it/s]Extractor Predicting: 39it [00:26,  1.47it/s]Extractor Predicting: 40it [00:26,  1.47it/s]Extractor Predicting: 41it [00:27,  1.50it/s]Extractor Predicting: 42it [00:28,  1.49it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:29,  1.50it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:31,  1.47it/s]Extractor Predicting: 48it [00:32,  1.47it/s]Extractor Predicting: 49it [00:32,  1.45it/s]Extractor Predicting: 50it [00:33,  1.47it/s]Extractor Predicting: 51it [00:34,  1.47it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:35,  1.52it/s]Extractor Predicting: 54it [00:36,  1.52it/s]Extractor Predicting: 55it [00:36,  1.53it/s]Extractor Predicting: 56it [00:37,  1.53it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:39,  1.53it/s]Extractor Predicting: 60it [00:39,  1.55it/s]Extractor Predicting: 61it [00:40,  1.51it/s]Extractor Predicting: 62it [00:41,  1.52it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.51it/s]Extractor Predicting: 66it [00:43,  1.53it/s]Extractor Predicting: 67it [00:44,  1.54it/s]Extractor Predicting: 68it [00:45,  1.45it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:46,  1.45it/s]Extractor Predicting: 71it [00:47,  1.48it/s]Extractor Predicting: 72it [00:47,  1.49it/s]Extractor Predicting: 73it [00:48,  1.54it/s]Extractor Predicting: 74it [00:49,  1.55it/s]Extractor Predicting: 75it [00:49,  1.56it/s]Extractor Predicting: 76it [00:50,  1.52it/s]Extractor Predicting: 77it [00:51,  1.54it/s]Extractor Predicting: 78it [00:51,  1.57it/s]Extractor Predicting: 79it [00:52,  1.56it/s]Extractor Predicting: 80it [00:53,  1.57it/s]Extractor Predicting: 81it [00:53,  1.57it/s]Extractor Predicting: 82it [00:54,  1.56it/s]Extractor Predicting: 83it [00:55,  1.56it/s]Extractor Predicting: 84it [00:55,  1.51it/s]Extractor Predicting: 85it [00:56,  1.55it/s]Extractor Predicting: 86it [00:56,  1.57it/s]Extractor Predicting: 87it [00:57,  1.60it/s]Extractor Predicting: 88it [00:58,  1.62it/s]Extractor Predicting: 89it [00:58,  1.60it/s]Extractor Predicting: 90it [00:59,  1.56it/s]Extractor Predicting: 91it [01:00,  1.61it/s]Extractor Predicting: 92it [01:00,  1.62it/s]Extractor Predicting: 93it [01:01,  1.60it/s]Extractor Predicting: 94it [01:01,  1.61it/s]Extractor Predicting: 95it [01:02,  1.61it/s]Extractor Predicting: 96it [01:03,  1.62it/s]Extractor Predicting: 97it [01:03,  1.60it/s]Extractor Predicting: 98it [01:04,  1.58it/s]Extractor Predicting: 99it [01:05,  1.56it/s]Extractor Predicting: 100it [01:05,  1.59it/s]Extractor Predicting: 101it [01:06,  1.58it/s]Extractor Predicting: 102it [01:06,  1.58it/s]Extractor Predicting: 103it [01:07,  1.55it/s]Extractor Predicting: 104it [01:08,  1.56it/s]Extractor Predicting: 105it [01:08,  1.53it/s]Extractor Predicting: 106it [01:09,  1.52it/s]Extractor Predicting: 107it [01:10,  1.54it/s]Extractor Predicting: 108it [01:10,  1.54it/s]Extractor Predicting: 109it [01:11,  1.51it/s]Extractor Predicting: 110it [01:12,  1.51it/s]Extractor Predicting: 111it [01:12,  1.50it/s]Extractor Predicting: 112it [01:13,  1.53it/s]Extractor Predicting: 113it [01:14,  1.53it/s]Extractor Predicting: 114it [01:14,  1.55it/s]Extractor Predicting: 115it [01:15,  1.53it/s]Extractor Predicting: 116it [01:16,  1.53it/s]Extractor Predicting: 117it [01:16,  1.53it/s]Extractor Predicting: 118it [01:17,  1.52it/s]Extractor Predicting: 119it [01:18,  1.54it/s]Extractor Predicting: 120it [01:18,  1.51it/s]Extractor Predicting: 121it [01:19,  1.53it/s]Extractor Predicting: 122it [01:20,  1.54it/s]Extractor Predicting: 123it [01:20,  1.55it/s]Extractor Predicting: 124it [01:21,  1.55it/s]Extractor Predicting: 125it [01:21,  1.54it/s]Extractor Predicting: 126it [01:22,  1.53it/s]Extractor Predicting: 127it [01:23,  1.54it/s]Extractor Predicting: 128it [01:23,  1.51it/s]Extractor Predicting: 129it [01:24,  1.49it/s]Extractor Predicting: 130it [01:25,  1.53it/s]Extractor Predicting: 131it [01:25,  1.53it/s]Extractor Predicting: 132it [01:26,  1.51it/s]Extractor Predicting: 133it [01:27,  1.54it/s]Extractor Predicting: 134it [01:27,  1.52it/s]Extractor Predicting: 135it [01:28,  1.52it/s]Extractor Predicting: 136it [01:29,  1.53it/s]Extractor Predicting: 137it [01:29,  1.51it/s]Extractor Predicting: 138it [01:30,  1.50it/s]Extractor Predicting: 139it [01:31,  1.52it/s]Extractor Predicting: 140it [01:31,  1.51it/s]Extractor Predicting: 141it [01:32,  1.51it/s]Extractor Predicting: 142it [01:33,  1.50it/s]Extractor Predicting: 143it [01:33,  1.51it/s]Extractor Predicting: 144it [01:34,  1.51it/s]Extractor Predicting: 145it [01:35,  1.50it/s]Extractor Predicting: 146it [01:35,  1.50it/s]Extractor Predicting: 147it [01:36,  1.49it/s]Extractor Predicting: 148it [01:37,  1.32it/s]Extractor Predicting: 149it [01:38,  1.36it/s]Extractor Predicting: 150it [01:38,  1.36it/s]Extractor Predicting: 151it [01:39,  1.38it/s]Extractor Predicting: 152it [01:40,  1.42it/s]Extractor Predicting: 153it [01:40,  1.48it/s]Extractor Predicting: 154it [01:41,  1.50it/s]Extractor Predicting: 155it [01:42,  1.52it/s]Extractor Predicting: 156it [01:42,  1.50it/s]Extractor Predicting: 157it [01:43,  1.51it/s]Extractor Predicting: 158it [01:44,  1.51it/s]Extractor Predicting: 159it [01:44,  1.50it/s]Extractor Predicting: 160it [01:45,  1.50it/s]Extractor Predicting: 161it [01:46,  1.51it/s]Extractor Predicting: 162it [01:46,  1.50it/s]Extractor Predicting: 163it [01:47,  1.52it/s]Extractor Predicting: 164it [01:48,  1.51it/s]Extractor Predicting: 165it [01:48,  1.52it/s]Extractor Predicting: 166it [01:49,  1.53it/s]Extractor Predicting: 167it [01:50,  1.54it/s]Extractor Predicting: 168it [01:50,  1.51it/s]Extractor Predicting: 169it [01:51,  1.51it/s]Extractor Predicting: 170it [01:52,  1.53it/s]Extractor Predicting: 171it [01:52,  1.58it/s]Extractor Predicting: 172it [01:53,  1.58it/s]Extractor Predicting: 173it [01:54,  1.54it/s]Extractor Predicting: 174it [01:54,  1.52it/s]Extractor Predicting: 175it [01:55,  1.47it/s]Extractor Predicting: 176it [01:56,  1.46it/s]Extractor Predicting: 177it [01:56,  1.49it/s]Extractor Predicting: 178it [01:57,  1.48it/s]Extractor Predicting: 179it [01:58,  1.48it/s]Extractor Predicting: 180it [01:58,  1.46it/s]Extractor Predicting: 181it [01:59,  1.46it/s]Extractor Predicting: 182it [02:00,  1.46it/s]Extractor Predicting: 183it [02:00,  1.45it/s]Extractor Predicting: 184it [02:01,  1.45it/s]Extractor Predicting: 185it [02:02,  1.46it/s]Extractor Predicting: 186it [02:02,  1.47it/s]Extractor Predicting: 187it [02:03,  1.46it/s]Extractor Predicting: 188it [02:04,  1.48it/s]Extractor Predicting: 189it [02:04,  1.49it/s]Extractor Predicting: 190it [02:05,  1.47it/s]Extractor Predicting: 191it [02:06,  1.50it/s]Extractor Predicting: 192it [02:06,  1.49it/s]Extractor Predicting: 193it [02:07,  1.49it/s]Extractor Predicting: 194it [02:08,  1.50it/s]Extractor Predicting: 195it [02:08,  1.54it/s]Extractor Predicting: 196it [02:09,  1.56it/s]Extractor Predicting: 197it [02:10,  1.54it/s]Extractor Predicting: 198it [02:10,  1.54it/s]Extractor Predicting: 199it [02:11,  1.54it/s]Extractor Predicting: 200it [02:12,  1.55it/s]Extractor Predicting: 201it [02:12,  1.54it/s]Extractor Predicting: 202it [02:13,  1.53it/s]Extractor Predicting: 203it [02:14,  1.52it/s]Extractor Predicting: 204it [02:14,  1.52it/s]Extractor Predicting: 205it [02:15,  1.52it/s]Extractor Predicting: 206it [02:16,  1.54it/s]Extractor Predicting: 207it [02:16,  1.52it/s]Extractor Predicting: 208it [02:17,  1.56it/s]Extractor Predicting: 209it [02:17,  1.57it/s]Extractor Predicting: 210it [02:18,  1.54it/s]Extractor Predicting: 211it [02:19,  1.54it/s]Extractor Predicting: 212it [02:19,  1.55it/s]Extractor Predicting: 213it [02:20,  1.52it/s]Extractor Predicting: 214it [02:21,  1.49it/s]Extractor Predicting: 215it [02:21,  1.50it/s]Extractor Predicting: 216it [02:22,  1.50it/s]Extractor Predicting: 217it [02:23,  1.54it/s]Extractor Predicting: 218it [02:23,  1.51it/s]Extractor Predicting: 219it [02:24,  1.49it/s]Extractor Predicting: 220it [02:25,  1.53it/s]Extractor Predicting: 221it [02:25,  1.50it/s]Extractor Predicting: 222it [02:26,  1.49it/s]Extractor Predicting: 223it [02:27,  1.50it/s]Extractor Predicting: 224it [02:27,  1.52it/s]Extractor Predicting: 225it [02:28,  1.55it/s]Extractor Predicting: 226it [02:29,  1.55it/s]Extractor Predicting: 227it [02:29,  1.56it/s]Extractor Predicting: 228it [02:30,  1.56it/s]Extractor Predicting: 229it [02:31,  1.54it/s]Extractor Predicting: 230it [02:31,  1.57it/s]Extractor Predicting: 231it [02:32,  1.57it/s]Extractor Predicting: 232it [02:33,  1.57it/s]Extractor Predicting: 233it [02:33,  1.56it/s]Extractor Predicting: 234it [02:34,  1.53it/s]Extractor Predicting: 235it [02:35,  1.52it/s]Extractor Predicting: 236it [02:35,  1.53it/s]Extractor Predicting: 237it [02:36,  1.51it/s]Extractor Predicting: 238it [02:36,  1.53it/s]Extractor Predicting: 239it [02:37,  1.52it/s]Extractor Predicting: 240it [02:38,  1.54it/s]Extractor Predicting: 241it [02:38,  1.57it/s]Extractor Predicting: 242it [02:39,  1.54it/s]Extractor Predicting: 243it [02:40,  1.57it/s]Extractor Predicting: 244it [02:40,  1.58it/s]Extractor Predicting: 245it [02:41,  1.54it/s]Extractor Predicting: 246it [02:42,  1.56it/s]Extractor Predicting: 247it [02:42,  1.55it/s]Extractor Predicting: 248it [02:43,  1.57it/s]Extractor Predicting: 249it [02:44,  1.56it/s]Extractor Predicting: 250it [02:44,  1.61it/s]Extractor Predicting: 251it [02:45,  1.49it/s]Extractor Predicting: 252it [02:45,  1.53it/s]Extractor Predicting: 253it [02:46,  1.59it/s]Extractor Predicting: 254it [02:47,  1.56it/s]Extractor Predicting: 255it [02:47,  1.56it/s]Extractor Predicting: 256it [02:48,  1.57it/s]Extractor Predicting: 257it [02:49,  1.57it/s]Extractor Predicting: 258it [02:49,  1.58it/s]Extractor Predicting: 259it [02:50,  1.57it/s]Extractor Predicting: 260it [02:51,  1.56it/s]Extractor Predicting: 261it [02:51,  1.53it/s]Extractor Predicting: 262it [02:52,  1.55it/s]Extractor Predicting: 263it [02:53,  1.49it/s]Extractor Predicting: 264it [02:53,  1.53it/s]Extractor Predicting: 265it [02:54,  1.52it/s]Extractor Predicting: 266it [02:55,  1.50it/s]Extractor Predicting: 267it [02:55,  1.50it/s]Extractor Predicting: 268it [02:56,  1.49it/s]Extractor Predicting: 269it [02:57,  1.47it/s]Extractor Predicting: 270it [02:57,  1.49it/s]Extractor Predicting: 271it [02:58,  1.50it/s]Extractor Predicting: 272it [02:59,  1.54it/s]Extractor Predicting: 273it [02:59,  1.52it/s]Extractor Predicting: 274it [03:00,  1.50it/s]Extractor Predicting: 275it [03:01,  1.50it/s]Extractor Predicting: 276it [03:01,  1.53it/s]Extractor Predicting: 277it [03:02,  1.32it/s]Extractor Predicting: 278it [03:03,  1.37it/s]Extractor Predicting: 279it [03:04,  1.39it/s]Extractor Predicting: 280it [03:04,  1.42it/s]Extractor Predicting: 281it [03:05,  1.44it/s]Extractor Predicting: 282it [03:06,  1.45it/s]Extractor Predicting: 283it [03:06,  1.46it/s]Extractor Predicting: 284it [03:07,  1.49it/s]Extractor Predicting: 285it [03:08,  1.51it/s]Extractor Predicting: 286it [03:08,  1.51it/s]Extractor Predicting: 287it [03:09,  1.48it/s]Extractor Predicting: 288it [03:10,  1.48it/s]Extractor Predicting: 289it [03:10,  1.47it/s]Extractor Predicting: 290it [03:11,  1.45it/s]Extractor Predicting: 291it [03:12,  1.46it/s]Extractor Predicting: 292it [03:12,  1.48it/s]Extractor Predicting: 293it [03:13,  1.47it/s]Extractor Predicting: 294it [03:14,  1.42it/s]Extractor Predicting: 295it [03:14,  1.44it/s]Extractor Predicting: 296it [03:15,  1.45it/s]Extractor Predicting: 297it [03:16,  1.45it/s]Extractor Predicting: 298it [03:16,  1.45it/s]Extractor Predicting: 299it [03:17,  1.45it/s]Extractor Predicting: 300it [03:18,  1.41it/s]Extractor Predicting: 301it [03:19,  1.37it/s]Extractor Predicting: 302it [03:19,  1.36it/s]Extractor Predicting: 303it [03:20,  1.39it/s]Extractor Predicting: 304it [03:21,  1.35it/s]Extractor Predicting: 305it [03:22,  1.33it/s]Extractor Predicting: 306it [03:22,  1.39it/s]Extractor Predicting: 307it [03:23,  1.43it/s]Extractor Predicting: 308it [03:24,  1.45it/s]Extractor Predicting: 309it [03:24,  1.51it/s]Extractor Predicting: 310it [03:25,  1.50it/s]Extractor Predicting: 311it [03:26,  1.46it/s]Extractor Predicting: 312it [03:26,  1.47it/s]Extractor Predicting: 313it [03:27,  1.49it/s]Extractor Predicting: 314it [03:28,  1.48it/s]Extractor Predicting: 315it [03:28,  1.54it/s]Extractor Predicting: 316it [03:29,  1.55it/s]Extractor Predicting: 317it [03:30,  1.53it/s]Extractor Predicting: 318it [03:30,  1.53it/s]Extractor Predicting: 319it [03:31,  1.50it/s]Extractor Predicting: 320it [03:32,  1.52it/s]Extractor Predicting: 321it [03:32,  1.53it/s]Extractor Predicting: 322it [03:33,  1.57it/s]Extractor Predicting: 323it [03:33,  1.56it/s]Extractor Predicting: 324it [03:34,  1.57it/s]Extractor Predicting: 325it [03:35,  1.55it/s]Extractor Predicting: 326it [03:35,  1.57it/s]Extractor Predicting: 327it [03:36,  1.56it/s]Extractor Predicting: 328it [03:37,  1.55it/s]Extractor Predicting: 329it [03:37,  1.54it/s]Extractor Predicting: 330it [03:38,  1.52it/s]Extractor Predicting: 331it [03:39,  1.52it/s]Extractor Predicting: 332it [03:39,  1.54it/s]Extractor Predicting: 333it [03:40,  1.52it/s]Extractor Predicting: 334it [03:41,  1.52it/s]Extractor Predicting: 335it [03:41,  1.53it/s]Extractor Predicting: 336it [03:42,  1.55it/s]Extractor Predicting: 337it [03:43,  1.54it/s]Extractor Predicting: 338it [03:43,  1.55it/s]Extractor Predicting: 339it [03:44,  1.55it/s]Extractor Predicting: 340it [03:44,  1.55it/s]Extractor Predicting: 341it [03:45,  1.55it/s]Extractor Predicting: 342it [03:46,  1.53it/s]Extractor Predicting: 343it [03:46,  1.52it/s]Extractor Predicting: 344it [03:47,  1.56it/s]Extractor Predicting: 345it [03:48,  1.58it/s]Extractor Predicting: 346it [03:48,  1.61it/s]Extractor Predicting: 347it [03:49,  1.64it/s]Extractor Predicting: 348it [03:50,  1.61it/s]Extractor Predicting: 349it [03:50,  1.62it/s]Extractor Predicting: 350it [03:51,  1.61it/s]Extractor Predicting: 351it [03:51,  1.61it/s]Extractor Predicting: 352it [03:52,  1.60it/s]Extractor Predicting: 353it [03:53,  1.61it/s]Extractor Predicting: 354it [03:53,  1.61it/s]Extractor Predicting: 355it [03:54,  1.61it/s]Extractor Predicting: 356it [03:54,  1.64it/s]Extractor Predicting: 357it [03:55,  1.61it/s]Extractor Predicting: 358it [03:56,  1.60it/s]Extractor Predicting: 359it [03:56,  1.60it/s]Extractor Predicting: 360it [03:57,  1.58it/s]Extractor Predicting: 361it [03:58,  1.58it/s]Extractor Predicting: 362it [03:58,  1.59it/s]Extractor Predicting: 363it [03:59,  1.61it/s]Extractor Predicting: 364it [03:59,  1.64it/s]Extractor Predicting: 365it [04:00,  1.58it/s]Extractor Predicting: 366it [04:01,  1.54it/s]Extractor Predicting: 367it [04:02,  1.49it/s]Extractor Predicting: 368it [04:02,  1.48it/s]Extractor Predicting: 369it [04:03,  1.52it/s]Extractor Predicting: 370it [04:04,  1.49it/s]Extractor Predicting: 371it [04:04,  1.51it/s]Extractor Predicting: 372it [04:05,  1.53it/s]Extractor Predicting: 373it [04:05,  1.54it/s]Extractor Predicting: 374it [04:06,  1.54it/s]Extractor Predicting: 375it [04:07,  1.52it/s]Extractor Predicting: 376it [04:07,  1.52it/s]Extractor Predicting: 377it [04:08,  1.50it/s]Extractor Predicting: 378it [04:09,  1.49it/s]Extractor Predicting: 379it [04:09,  1.49it/s]Extractor Predicting: 380it [04:10,  1.49it/s]Extractor Predicting: 381it [04:11,  1.52it/s]Extractor Predicting: 382it [04:11,  1.54it/s]Extractor Predicting: 383it [04:12,  1.54it/s]Extractor Predicting: 384it [04:13,  1.56it/s]Extractor Predicting: 385it [04:13,  1.55it/s]Extractor Predicting: 386it [04:14,  1.56it/s]Extractor Predicting: 387it [04:15,  1.56it/s]Extractor Predicting: 388it [04:15,  1.60it/s]Extractor Predicting: 389it [04:16,  1.55it/s]Extractor Predicting: 390it [04:17,  1.52it/s]Extractor Predicting: 391it [04:17,  1.54it/s]Extractor Predicting: 392it [04:18,  1.59it/s]Extractor Predicting: 393it [04:18,  1.62it/s]Extractor Predicting: 394it [04:19,  1.61it/s]Extractor Predicting: 395it [04:20,  1.63it/s]Extractor Predicting: 396it [04:20,  1.65it/s]Extractor Predicting: 397it [04:21,  1.62it/s]Extractor Predicting: 398it [04:21,  1.60it/s]Extractor Predicting: 399it [04:22,  1.60it/s]Extractor Predicting: 400it [04:23,  1.58it/s]Extractor Predicting: 401it [04:23,  1.60it/s]Extractor Predicting: 402it [04:24,  1.62it/s]Extractor Predicting: 403it [04:25,  1.62it/s]Extractor Predicting: 404it [04:25,  1.62it/s]Extractor Predicting: 405it [04:26,  1.58it/s]Extractor Predicting: 406it [04:26,  1.59it/s]Extractor Predicting: 407it [04:27,  1.59it/s]Extractor Predicting: 408it [04:28,  1.60it/s]Extractor Predicting: 409it [04:29,  1.45it/s]Extractor Predicting: 410it [04:29,  1.48it/s]Extractor Predicting: 411it [04:30,  1.49it/s]Extractor Predicting: 412it [04:31,  1.50it/s]Extractor Predicting: 413it [04:32,  1.27it/s]Extractor Predicting: 414it [04:32,  1.29it/s]Extractor Predicting: 415it [04:33,  1.30it/s]Extractor Predicting: 416it [04:34,  1.32it/s]Extractor Predicting: 417it [04:34,  1.36it/s]Extractor Predicting: 418it [04:35,  1.40it/s]Extractor Predicting: 419it [04:36,  1.42it/s]Extractor Predicting: 420it [04:37,  1.40it/s]Extractor Predicting: 421it [04:37,  1.41it/s]Extractor Predicting: 422it [04:38,  1.42it/s]Extractor Predicting: 423it [04:39,  1.45it/s]Extractor Predicting: 424it [04:39,  1.44it/s]Extractor Predicting: 425it [04:40,  1.43it/s]Extractor Predicting: 426it [04:41,  1.43it/s]Extractor Predicting: 427it [04:41,  1.44it/s]Extractor Predicting: 428it [04:42,  1.45it/s]Extractor Predicting: 429it [04:43,  1.48it/s]Extractor Predicting: 430it [04:43,  1.48it/s]Extractor Predicting: 431it [04:44,  1.49it/s]Extractor Predicting: 432it [04:45,  1.48it/s]Extractor Predicting: 433it [04:45,  1.48it/s]Extractor Predicting: 434it [04:46,  1.47it/s]Extractor Predicting: 435it [04:47,  1.49it/s]Extractor Predicting: 436it [04:47,  1.52it/s]Extractor Predicting: 437it [04:48,  1.50it/s]Extractor Predicting: 438it [04:49,  1.52it/s]Extractor Predicting: 439it [04:49,  1.51it/s]Extractor Predicting: 440it [04:50,  1.50it/s]Extractor Predicting: 441it [04:51,  1.49it/s]Extractor Predicting: 442it [04:51,  1.49it/s]Extractor Predicting: 443it [04:52,  1.47it/s]Extractor Predicting: 444it [04:53,  1.47it/s]Extractor Predicting: 445it [04:54,  1.46it/s]Extractor Predicting: 446it [04:54,  1.46it/s]Extractor Predicting: 447it [04:55,  1.47it/s]Extractor Predicting: 448it [04:56,  1.46it/s]Extractor Predicting: 449it [04:56,  1.49it/s]Extractor Predicting: 450it [04:57,  1.52it/s]Extractor Predicting: 451it [04:58,  1.50it/s]Extractor Predicting: 452it [04:58,  1.49it/s]Extractor Predicting: 453it [04:59,  1.48it/s]Extractor Predicting: 454it [05:00,  1.49it/s]Extractor Predicting: 455it [05:00,  1.46it/s]Extractor Predicting: 456it [05:01,  1.48it/s]Extractor Predicting: 457it [05:02,  1.45it/s]Extractor Predicting: 458it [05:02,  1.47it/s]Extractor Predicting: 459it [05:03,  1.47it/s]Extractor Predicting: 460it [05:04,  1.46it/s]Extractor Predicting: 461it [05:04,  1.48it/s]Extractor Predicting: 462it [05:05,  1.48it/s]Extractor Predicting: 463it [05:06,  1.44it/s]Extractor Predicting: 464it [05:06,  1.42it/s]Extractor Predicting: 465it [05:07,  1.42it/s]Extractor Predicting: 466it [05:08,  1.44it/s]Extractor Predicting: 467it [05:08,  1.49it/s]Extractor Predicting: 468it [05:09,  1.50it/s]Extractor Predicting: 469it [05:10,  1.54it/s]Extractor Predicting: 470it [05:10,  1.52it/s]Extractor Predicting: 471it [05:11,  1.49it/s]Extractor Predicting: 472it [05:12,  1.50it/s]Extractor Predicting: 473it [05:12,  1.48it/s]Extractor Predicting: 474it [05:13,  1.46it/s]Extractor Predicting: 475it [05:14,  1.46it/s]Extractor Predicting: 476it [05:15,  1.48it/s]Extractor Predicting: 477it [05:15,  1.47it/s]Extractor Predicting: 478it [05:16,  1.47it/s]Extractor Predicting: 479it [05:17,  1.42it/s]Extractor Predicting: 480it [05:17,  1.39it/s]Extractor Predicting: 481it [05:18,  1.38it/s]Extractor Predicting: 482it [05:19,  1.38it/s]Extractor Predicting: 483it [05:20,  1.36it/s]Extractor Predicting: 484it [05:20,  1.35it/s]Extractor Predicting: 485it [05:21,  1.36it/s]Extractor Predicting: 486it [05:22,  1.39it/s]Extractor Predicting: 487it [05:22,  1.42it/s]Extractor Predicting: 488it [05:23,  1.40it/s]Extractor Predicting: 489it [05:24,  1.42it/s]Extractor Predicting: 490it [05:25,  1.39it/s]Extractor Predicting: 491it [05:25,  1.38it/s]Extractor Predicting: 492it [05:26,  1.38it/s]Extractor Predicting: 493it [05:27,  1.39it/s]Extractor Predicting: 494it [05:27,  1.39it/s]Extractor Predicting: 495it [05:28,  1.38it/s]Extractor Predicting: 496it [05:29,  1.39it/s]Extractor Predicting: 497it [05:30,  1.38it/s]Extractor Predicting: 498it [05:30,  1.40it/s]Extractor Predicting: 499it [05:31,  1.37it/s]Extractor Predicting: 500it [05:32,  1.39it/s]Extractor Predicting: 501it [05:33,  1.41it/s]Extractor Predicting: 502it [05:33,  1.38it/s]Extractor Predicting: 503it [05:34,  1.36it/s]Extractor Predicting: 504it [05:35,  1.34it/s]Extractor Predicting: 505it [05:36,  1.33it/s]Extractor Predicting: 506it [05:36,  1.32it/s]Extractor Predicting: 507it [05:37,  1.36it/s]Extractor Predicting: 508it [05:38,  1.51it/s]Extractor Predicting: 508it [05:38,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:55:58,591 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:55:58,596 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:55:58,596 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:55:58,596 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:55:58,596 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:55:59,239 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:55:59,240 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:55:59,798 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:56:00,868 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:56:00,868 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:56:03,717 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:56:03,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:56:03,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:56:03,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:56:03,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:56:04,347 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:56:04,348 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:56:04,924 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:56:05,080 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:56:05,080 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.24097610574478903,
  "recall": 0.03891306132501437,
  "score": 0.06700593723494488,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:06,  1.43it/s]Extractor Predicting: 10it [00:06,  1.42it/s]Extractor Predicting: 11it [00:07,  1.42it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.44it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:11,  1.44it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:13,  1.44it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.43it/s]Extractor Predicting: 23it [00:15,  1.42it/s]Extractor Predicting: 24it [00:16,  1.44it/s]Extractor Predicting: 25it [00:17,  1.48it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.45it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:20,  1.42it/s]Extractor Predicting: 30it [00:20,  1.46it/s]Extractor Predicting: 31it [00:21,  1.50it/s]Extractor Predicting: 32it [00:21,  1.53it/s]Extractor Predicting: 33it [00:22,  1.54it/s]Extractor Predicting: 34it [00:23,  1.55it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:24,  1.54it/s]Extractor Predicting: 37it [00:25,  1.52it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:27,  1.53it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:28,  1.56it/s]Extractor Predicting: 43it [00:29,  1.55it/s]Extractor Predicting: 44it [00:29,  1.58it/s]Extractor Predicting: 45it [00:30,  1.55it/s]Extractor Predicting: 46it [00:31,  1.55it/s]Extractor Predicting: 47it [00:31,  1.55it/s]Extractor Predicting: 48it [00:32,  1.53it/s]Extractor Predicting: 49it [00:33,  1.52it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.50it/s]Extractor Predicting: 54it [00:36,  1.45it/s]Extractor Predicting: 55it [00:37,  1.47it/s]Extractor Predicting: 56it [00:37,  1.50it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:39,  1.50it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.50it/s]Extractor Predicting: 61it [00:41,  1.38it/s]Extractor Predicting: 62it [00:41,  1.41it/s]Extractor Predicting: 63it [00:42,  1.43it/s]Extractor Predicting: 64it [00:43,  1.45it/s]Extractor Predicting: 65it [00:44,  1.43it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:45,  1.43it/s]Extractor Predicting: 68it [00:46,  1.42it/s]Extractor Predicting: 69it [00:46,  1.39it/s]Extractor Predicting: 70it [00:47,  1.40it/s]Extractor Predicting: 71it [00:48,  1.39it/s]Extractor Predicting: 72it [00:49,  1.40it/s]Extractor Predicting: 73it [00:49,  1.42it/s]Extractor Predicting: 74it [00:50,  1.38it/s]Extractor Predicting: 75it [00:50,  1.55it/s]Extractor Predicting: 75it [00:50,  1.47it/s]
[INFO|configuration_utils.py:515] 2023-08-29 08:56:57,210 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:56:57,214 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:56:57,222 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:56:57,223 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 08:56:57,225 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:57:00,288 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 08:57:00,289 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 08:57:00,297 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:57:00,297 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:57:00,302 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:57:00,306 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:57:00,306 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:57:00,307 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:57:00,307 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:57:00,307 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:57:00,307 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5843520782396088,
  "recall": 0.06020151133501259,
  "score": 0.1091573418588719,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 08:57:00,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:01,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:01,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:02,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:03,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:04,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:04,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:05,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:06,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:07,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:07,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:08,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:09,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:09,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:10,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:11,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:12,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:12,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:13,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:14,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:32, 14.33s/it][WARNING|generation_utils.py:914] 2023-08-29 08:57:14,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:15,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:16,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:16,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:17,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:18,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:18,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:19,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:20,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:20,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:21,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:22,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:23,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:23,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:24,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:25,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:25,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:26,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:27,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:27,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:28,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:28<04:18, 14.38s/it][WARNING|generation_utils.py:914] 2023-08-29 08:57:29,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:30,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:31,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:31,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:32,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:33,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:33,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:34,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:35,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:35,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:37,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:37,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:38,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:39,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:40,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:41,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:41,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:42,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:43,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:44,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:44,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:45,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:45<04:25, 15.61s/it][WARNING|generation_utils.py:914] 2023-08-29 08:57:46,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:47,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:47,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:48,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:49,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:49,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:50,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:51,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:52,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:52,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:53,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:54,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:54,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:55,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:56,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:57,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:57,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:58,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:59,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:59,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:00,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:00<04:05, 15.34s/it][WARNING|generation_utils.py:914] 2023-08-29 08:58:01,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:02,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:02,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:03,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:04,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:04,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:05,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:06,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:06,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:07,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:08,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:08,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:09,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:10,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:10,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:11,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:12,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:12,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:13,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:14,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:14,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:15,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:15<03:47, 15.18s/it][WARNING|generation_utils.py:914] 2023-08-29 08:58:16,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:16,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:17,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:18,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:19,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:19,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:20,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:20,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:21,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:22,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:23,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:23,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:24,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:25,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:25,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:26,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:27,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:27,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:28,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:29,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:29,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:30,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:30<03:32, 15.18s/it][WARNING|generation_utils.py:914] 2023-08-29 08:58:31,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:32,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:32,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:33,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:34,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:34,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:35,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:36,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:36,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:37,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:38,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:38,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:39,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:40,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:40,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:41,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:42,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:42,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:43,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:44,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:44,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:45,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:45<03:14, 15.00s/it][WARNING|generation_utils.py:914] 2023-08-29 08:58:45,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:46,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:47,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:48,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:48,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:49,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:50,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:51,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:52,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:52,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:53,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:54,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:55,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:55,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:56,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:57,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:58,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:59,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:58:59,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:00,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:00<03:01, 15.16s/it][WARNING|generation_utils.py:914] 2023-08-29 08:59:01,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:02,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:02,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:03,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:04,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:04,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:05,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:06,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:07,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:07,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:08,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:09,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:10,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:10,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:11,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:12,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:13,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:13,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:14,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:15,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:16,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:16<02:47, 15.21s/it][WARNING|generation_utils.py:914] 2023-08-29 08:59:16,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:17,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:18,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:18,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:19,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:20,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:21,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:21,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:22,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:23,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:23,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:24,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:25,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:26,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:26,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:27,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:28,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:28,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:29,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:30,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:30,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:30<02:30, 15.00s/it][WARNING|generation_utils.py:914] 2023-08-29 08:59:31,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:32,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:32,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:33,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:34,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:35,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:35,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:36,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:37,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:38,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:39,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:39,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:40,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:41,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:42,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:43,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:43,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:44,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:45,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:45,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:46,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:47,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:47,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:48<02:21, 15.74s/it][WARNING|generation_utils.py:914] 2023-08-29 08:59:48,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:49,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:50,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:50,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:51,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:52,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:52,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:53,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:54,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:54,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:55,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:56,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:56,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:57,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:58,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:59,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:59:59,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:00,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:01,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:02,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:02,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:03<02:03, 15.48s/it][WARNING|generation_utils.py:914] 2023-08-29 09:00:03,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:04,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:05,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:05,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:06,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:07,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:07,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:08,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:08,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:09,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:10,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:10,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:11,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:12,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:12,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:13,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:14,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:15,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:15,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:16,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:17,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:17<01:45, 15.07s/it][WARNING|generation_utils.py:914] 2023-08-29 09:00:17,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:18,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:19,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:19,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:20,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:21,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:21,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:22,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:23,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:23,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:24,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:25,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:25,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:26,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:27,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:27,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:28,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:29,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:29,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:30,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:31,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:31<01:28, 14.79s/it][WARNING|generation_utils.py:914] 2023-08-29 09:00:31,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:32,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:33,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:34,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:34,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:35,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:36,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:36,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:37,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:38,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:39,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:39,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:40,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:40,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:41,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:42,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:43,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:43,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:44,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:45,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:45<01:12, 14.51s/it][WARNING|generation_utils.py:914] 2023-08-29 09:00:45,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:46,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:47,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:48,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:48,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:49,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:50,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:50,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:51,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:52,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:53,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:54,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:54,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:55,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:56,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:57,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:57,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:58,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:00:59,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:00,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:00,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:01,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:02,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:02<01:01, 15.42s/it][WARNING|generation_utils.py:914] 2023-08-29 09:01:03,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:03,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:04,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:05,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:06,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:06,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:07,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:08,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:08,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:09,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:10,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:10,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:11,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:12,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:12,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:13,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:14,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:14,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:15,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:16,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:16<00:44, 14.89s/it][WARNING|generation_utils.py:914] 2023-08-29 09:01:16,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:17,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:18,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:19,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:19,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:20,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:21,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:21,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:22,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:23,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:24,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:24,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:25,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:26,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:26,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:27,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:28,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:28,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:29,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:30,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:30,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:30<00:29, 14.78s/it][WARNING|generation_utils.py:914] 2023-08-29 09:01:31,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:32,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:32,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:33,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:34,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:35,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:35,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:36,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:37,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:38,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:38,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:39,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:40,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:40,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:41,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:42,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:43,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:43,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:44,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:45,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:45<00:14, 14.68s/it][WARNING|generation_utils.py:914] 2023-08-29 09:01:45,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:46,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:47,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:47,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:48,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:49,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:50,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:50,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:51,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:52,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:53,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:53,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:54,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:55,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:56,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:56,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:57,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:58,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:59,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:01:59,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:02:00,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:02:01,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:01<00:00, 15.03s/it]Generating: 100%|██████████| 20/20 [05:01<00:00, 15.06s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:08,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:08,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:08,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:08,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:08,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:02:08,340 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:02:08,341 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:02:08,605 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:02:09,767 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:02:09,769 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:11,127 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:11,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:11,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:11,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:11,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:02:11,873 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:02:11,875 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:02:12,143 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:02:12,304 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:02:12,304 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9765625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.8958333333333334, 'errors': {'', "('Sig Sauer M249', 'product or material produced', '', 'It is a compact airsoft gun designed by Raytheon to provide a more compact and survivable defensive firearm than the Sig Sauer M249 .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : student .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : conflict .', 'success_rate': 0.875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : continent .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9122023809523809, 'errors': {'', "('Charles Koch', 'field of work', '', 'In May , 2008 , he appeared in a campaign video for the Democratic candidate of the same name , running alongside fellow Democratic campaign managers Charles Koch , Dan OBrien , Susan E. Rice , and others .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : founded by .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8315217391304348, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : movement .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.8355978260869565, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : producer .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9241071428571429, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 12074
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12174, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.45it/s]Extractor Estimating: 2it [00:01,  1.33it/s]Extractor Estimating: 3it [00:02,  1.37it/s]Extractor Estimating: 4it [00:02,  1.35it/s]Extractor Estimating: 5it [00:03,  1.38it/s]Extractor Estimating: 6it [00:04,  1.42it/s]Extractor Estimating: 7it [00:04,  1.43it/s]Extractor Estimating: 8it [00:05,  1.46it/s]Extractor Estimating: 9it [00:06,  1.46it/s]Extractor Estimating: 10it [00:07,  1.45it/s]Extractor Estimating: 11it [00:07,  1.45it/s]Extractor Estimating: 12it [00:08,  1.48it/s]Extractor Estimating: 13it [00:09,  1.50it/s]Extractor Estimating: 14it [00:09,  1.48it/s]Extractor Estimating: 15it [00:10,  1.51it/s]Extractor Estimating: 16it [00:11,  1.49it/s]Extractor Estimating: 17it [00:11,  1.52it/s]Extractor Estimating: 18it [00:12,  1.49it/s]Extractor Estimating: 19it [00:13,  1.49it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:14,  1.50it/s]Extractor Estimating: 22it [00:15,  1.50it/s]Extractor Estimating: 23it [00:15,  1.50it/s]Extractor Estimating: 24it [00:16,  1.47it/s]Extractor Estimating: 25it [00:17,  1.48it/s]Extractor Estimating: 26it [00:17,  1.56it/s]Extractor Estimating: 27it [00:18,  1.63it/s]Extractor Estimating: 28it [00:18,  1.66it/s]Extractor Estimating: 29it [00:19,  1.68it/s]Extractor Estimating: 30it [00:19,  1.64it/s]Extractor Estimating: 31it [00:20,  1.61it/s]Extractor Estimating: 32it [00:21,  1.64it/s]Extractor Estimating: 33it [00:21,  1.66it/s]Extractor Estimating: 34it [00:22,  1.67it/s]Extractor Estimating: 35it [00:22,  1.69it/s]Extractor Estimating: 36it [00:23,  1.71it/s]Extractor Estimating: 37it [00:24,  1.78it/s]Extractor Estimating: 38it [00:24,  1.74it/s]Extractor Estimating: 39it [00:25,  1.76it/s]Extractor Estimating: 40it [00:25,  1.81it/s]Extractor Estimating: 41it [00:26,  1.84it/s]Extractor Estimating: 42it [00:26,  1.78it/s]Extractor Estimating: 43it [00:27,  1.78it/s]Extractor Estimating: 44it [00:27,  1.78it/s]Extractor Estimating: 45it [00:28,  1.76it/s]Extractor Estimating: 46it [00:29,  1.76it/s]Extractor Estimating: 47it [00:29,  1.63it/s]Extractor Estimating: 48it [00:30,  1.58it/s]Extractor Estimating: 49it [00:31,  1.63it/s]Extractor Estimating: 50it [00:31,  1.64it/s]Extractor Estimating: 51it [00:32,  1.61it/s]Extractor Estimating: 52it [00:32,  1.59it/s]Extractor Estimating: 53it [00:33,  1.57it/s]Extractor Estimating: 54it [00:34,  1.56it/s]Extractor Estimating: 55it [00:34,  1.63it/s]Extractor Estimating: 56it [00:35,  1.66it/s]Extractor Estimating: 57it [00:35,  1.69it/s]Extractor Estimating: 58it [00:36,  1.68it/s]Extractor Estimating: 59it [00:37,  1.66it/s]Extractor Estimating: 60it [00:37,  1.66it/s]Extractor Estimating: 61it [00:39,  1.07it/s]Extractor Estimating: 62it [00:40,  1.20it/s]Extractor Estimating: 63it [00:40,  1.33it/s]Extractor Estimating: 64it [00:41,  1.43it/s]Extractor Estimating: 65it [00:41,  1.46it/s]Extractor Estimating: 66it [00:42,  1.45it/s]Extractor Estimating: 67it [00:43,  1.45it/s]Extractor Estimating: 68it [00:43,  1.51it/s]Extractor Estimating: 69it [00:44,  1.52it/s]Extractor Estimating: 70it [00:45,  1.56it/s]Extractor Estimating: 71it [00:45,  1.59it/s]Extractor Estimating: 72it [00:46,  1.61it/s]Extractor Estimating: 73it [00:46,  1.61it/s]Extractor Estimating: 74it [00:47,  1.65it/s]Extractor Estimating: 75it [00:48,  1.60it/s]Extractor Estimating: 76it [00:48,  1.62it/s]Extractor Estimating: 77it [00:49,  1.64it/s]Extractor Estimating: 78it [00:49,  1.65it/s]Extractor Estimating: 79it [00:50,  1.69it/s]Extractor Estimating: 80it [00:51,  1.68it/s]Extractor Estimating: 81it [00:51,  1.68it/s]Extractor Estimating: 82it [00:52,  1.68it/s]Extractor Estimating: 83it [00:52,  1.67it/s]Extractor Estimating: 84it [00:53,  1.70it/s]Extractor Estimating: 85it [00:54,  1.71it/s]Extractor Estimating: 86it [00:54,  1.69it/s]Extractor Estimating: 87it [00:55,  1.71it/s]Extractor Estimating: 88it [00:55,  1.72it/s]Extractor Estimating: 89it [00:56,  1.67it/s]Extractor Estimating: 90it [00:57,  1.71it/s]Extractor Estimating: 91it [00:57,  1.65it/s]Extractor Estimating: 92it [00:58,  1.66it/s]Extractor Estimating: 93it [00:58,  1.67it/s]Extractor Estimating: 94it [00:59,  1.69it/s]Extractor Estimating: 95it [01:00,  1.53it/s]Extractor Estimating: 96it [01:00,  1.55it/s]Extractor Estimating: 97it [01:01,  1.63it/s]Extractor Estimating: 98it [01:01,  1.63it/s]Extractor Estimating: 99it [01:02,  1.67it/s]Extractor Estimating: 100it [01:03,  1.66it/s]Extractor Estimating: 101it [01:03,  1.64it/s]Extractor Estimating: 102it [01:04,  1.64it/s]Extractor Estimating: 103it [01:05,  1.65it/s]Extractor Estimating: 104it [01:05,  1.65it/s]Extractor Estimating: 105it [01:06,  1.66it/s]Extractor Estimating: 106it [01:06,  1.67it/s]Extractor Estimating: 107it [01:07,  1.71it/s]Extractor Estimating: 108it [01:07,  1.71it/s]Extractor Estimating: 109it [01:08,  1.71it/s]Extractor Estimating: 110it [01:09,  1.67it/s]Extractor Estimating: 111it [01:09,  1.71it/s]Extractor Estimating: 112it [01:10,  1.73it/s]Extractor Estimating: 113it [01:10,  1.72it/s]Extractor Estimating: 114it [01:11,  1.72it/s]Extractor Estimating: 115it [01:12,  1.70it/s]Extractor Estimating: 116it [01:12,  1.68it/s]Extractor Estimating: 117it [01:13,  1.68it/s]Extractor Estimating: 118it [01:13,  1.71it/s]Extractor Estimating: 119it [01:14,  1.71it/s]Extractor Estimating: 120it [01:15,  1.67it/s]Extractor Estimating: 121it [01:15,  1.69it/s]Extractor Estimating: 122it [01:16,  1.71it/s]Extractor Estimating: 123it [01:16,  1.79it/s]Extractor Estimating: 124it [01:17,  1.74it/s]Extractor Estimating: 125it [01:17,  1.76it/s]Extractor Estimating: 126it [01:18,  1.77it/s]Extractor Estimating: 127it [01:18,  1.75it/s]Extractor Estimating: 128it [01:19,  1.75it/s]Extractor Estimating: 129it [01:20,  1.69it/s]Extractor Estimating: 130it [01:20,  1.68it/s]Extractor Estimating: 131it [01:21,  1.73it/s]Extractor Estimating: 132it [01:21,  1.74it/s]Extractor Estimating: 133it [01:22,  1.74it/s]Extractor Estimating: 134it [01:23,  1.75it/s]Extractor Estimating: 135it [01:23,  1.80it/s]Extractor Estimating: 136it [01:24,  1.77it/s]Extractor Estimating: 137it [01:24,  1.73it/s]Extractor Estimating: 138it [01:25,  1.74it/s]Extractor Estimating: 139it [01:25,  1.72it/s]Extractor Estimating: 140it [01:26,  1.71it/s]Extractor Estimating: 141it [01:27,  1.71it/s]Extractor Estimating: 142it [01:27,  1.72it/s]Extractor Estimating: 143it [01:28,  1.68it/s]Extractor Estimating: 144it [01:28,  1.68it/s]Extractor Estimating: 145it [01:29,  1.69it/s]Extractor Estimating: 146it [01:30,  1.73it/s]Extractor Estimating: 147it [01:30,  1.75it/s]Extractor Estimating: 148it [01:31,  1.75it/s]Extractor Estimating: 149it [01:31,  1.70it/s]Extractor Estimating: 150it [01:32,  1.73it/s]Extractor Estimating: 151it [01:32,  1.74it/s]Extractor Estimating: 152it [01:33,  1.78it/s]Extractor Estimating: 153it [01:33,  1.82it/s]Extractor Estimating: 154it [01:34,  1.88it/s]Extractor Estimating: 155it [01:35,  1.85it/s]Extractor Estimating: 156it [01:35,  1.85it/s]Extractor Estimating: 157it [01:36,  1.88it/s]Extractor Estimating: 158it [01:36,  1.89it/s]Extractor Estimating: 159it [01:37,  1.93it/s]Extractor Estimating: 160it [01:37,  1.88it/s]Extractor Estimating: 161it [01:38,  1.86it/s]Extractor Estimating: 162it [01:38,  1.86it/s]Extractor Estimating: 163it [01:39,  1.87it/s]Extractor Estimating: 164it [01:39,  1.90it/s]Extractor Estimating: 165it [01:40,  1.94it/s]Extractor Estimating: 166it [01:40,  2.01it/s]Extractor Estimating: 167it [01:41,  1.93it/s]Extractor Estimating: 168it [01:41,  1.91it/s]Extractor Estimating: 169it [01:42,  1.87it/s]Extractor Estimating: 170it [01:42,  1.91it/s]Extractor Estimating: 171it [01:43,  1.90it/s]Extractor Estimating: 172it [01:43,  1.93it/s]Extractor Estimating: 173it [01:44,  1.92it/s]Extractor Estimating: 174it [01:44,  1.96it/s]Extractor Estimating: 175it [01:45,  1.90it/s]Extractor Estimating: 176it [01:46,  1.79it/s]Extractor Estimating: 177it [01:46,  1.74it/s]Extractor Estimating: 178it [01:47,  1.69it/s]Extractor Estimating: 179it [01:48,  1.61it/s]Extractor Estimating: 180it [01:48,  1.56it/s]Extractor Estimating: 181it [01:49,  1.51it/s]Extractor Estimating: 182it [01:50,  1.48it/s]Extractor Estimating: 183it [01:50,  1.49it/s]Extractor Estimating: 184it [01:51,  1.48it/s]Extractor Estimating: 185it [01:52,  1.43it/s]Extractor Estimating: 186it [01:52,  1.43it/s]Extractor Estimating: 187it [01:53,  1.44it/s]Extractor Estimating: 188it [01:54,  1.47it/s]Extractor Estimating: 189it [01:54,  1.46it/s]Extractor Estimating: 190it [01:55,  1.30it/s]Extractor Estimating: 191it [01:56,  1.33it/s]Extractor Estimating: 192it [01:57,  1.38it/s]Extractor Estimating: 193it [01:58,  1.39it/s]Extractor Estimating: 194it [01:58,  1.38it/s]Extractor Estimating: 195it [01:59,  1.37it/s]Extractor Estimating: 196it [02:00,  1.42it/s]Extractor Estimating: 197it [02:00,  1.42it/s]Extractor Estimating: 198it [02:01,  1.42it/s]Extractor Estimating: 199it [02:02,  1.43it/s]Extractor Estimating: 200it [02:02,  1.44it/s]Extractor Estimating: 201it [02:03,  1.49it/s]Extractor Estimating: 202it [02:04,  1.56it/s]Extractor Estimating: 203it [02:04,  1.61it/s]Extractor Estimating: 204it [02:05,  1.62it/s]Extractor Estimating: 205it [02:05,  1.67it/s]Extractor Estimating: 206it [02:06,  1.69it/s]Extractor Estimating: 207it [02:07,  1.69it/s]Extractor Estimating: 208it [02:07,  1.70it/s]Extractor Estimating: 209it [02:08,  1.70it/s]Extractor Estimating: 210it [02:08,  1.67it/s]Extractor Estimating: 211it [02:09,  1.68it/s]Extractor Estimating: 212it [02:09,  1.70it/s]Extractor Estimating: 213it [02:10,  1.68it/s]Extractor Estimating: 214it [02:11,  1.65it/s]Extractor Estimating: 215it [02:11,  1.70it/s]Extractor Estimating: 216it [02:12,  1.74it/s]Extractor Estimating: 217it [02:12,  1.75it/s]Extractor Estimating: 218it [02:13,  1.79it/s]Extractor Estimating: 219it [02:13,  1.81it/s]Extractor Estimating: 220it [02:14,  1.80it/s]Extractor Estimating: 221it [02:15,  1.80it/s]Extractor Estimating: 222it [02:15,  1.76it/s]Extractor Estimating: 223it [02:16,  1.72it/s]Extractor Estimating: 224it [02:16,  1.72it/s]Extractor Estimating: 225it [02:17,  1.72it/s]Extractor Estimating: 226it [02:18,  1.71it/s]Extractor Estimating: 227it [02:18,  1.69it/s]Extractor Estimating: 228it [02:19,  1.70it/s]Extractor Estimating: 229it [02:19,  1.70it/s]Extractor Estimating: 230it [02:20,  1.69it/s]Extractor Estimating: 231it [02:20,  1.72it/s]Extractor Estimating: 232it [02:21,  1.74it/s]Extractor Estimating: 233it [02:22,  1.75it/s]Extractor Estimating: 234it [02:22,  1.71it/s]Extractor Estimating: 235it [02:23,  1.69it/s]Extractor Estimating: 236it [02:23,  1.73it/s]Extractor Estimating: 237it [02:24,  1.75it/s]Extractor Estimating: 238it [02:25,  1.65it/s]Extractor Estimating: 239it [02:25,  1.63it/s]Extractor Estimating: 240it [02:26,  1.67it/s]Extractor Estimating: 241it [02:26,  1.67it/s]Extractor Estimating: 242it [02:27,  1.69it/s]Extractor Estimating: 243it [02:28,  1.68it/s]Extractor Estimating: 244it [02:28,  1.65it/s]Extractor Estimating: 245it [02:29,  1.70it/s]Extractor Estimating: 246it [02:29,  1.68it/s]Extractor Estimating: 247it [02:30,  1.71it/s]Extractor Estimating: 248it [02:30,  1.78it/s]Extractor Estimating: 249it [02:31,  1.75it/s]Extractor Estimating: 250it [02:32,  1.74it/s]Extractor Estimating: 251it [02:32,  1.73it/s]Extractor Estimating: 252it [02:33,  1.69it/s]Extractor Estimating: 253it [02:33,  1.65it/s]Extractor Estimating: 254it [02:34,  1.63it/s]Extractor Estimating: 255it [02:35,  1.61it/s]Extractor Estimating: 256it [02:35,  1.56it/s]Extractor Estimating: 257it [02:36,  1.55it/s]Extractor Estimating: 258it [02:37,  1.56it/s]Extractor Estimating: 259it [02:37,  1.59it/s]Extractor Estimating: 260it [02:38,  1.58it/s]Extractor Estimating: 261it [02:39,  1.57it/s]Extractor Estimating: 262it [02:39,  1.55it/s]Extractor Estimating: 263it [02:40,  1.58it/s]Extractor Estimating: 264it [02:41,  1.39it/s]Extractor Estimating: 265it [02:41,  1.46it/s]Extractor Estimating: 266it [02:42,  1.53it/s]Extractor Estimating: 267it [02:43,  1.54it/s]Extractor Estimating: 268it [02:43,  1.54it/s]Extractor Estimating: 269it [02:44,  1.60it/s]Extractor Estimating: 270it [02:44,  1.60it/s]Extractor Estimating: 271it [02:45,  1.62it/s]Extractor Estimating: 272it [02:46,  1.61it/s]Extractor Estimating: 273it [02:46,  1.62it/s]Extractor Estimating: 274it [02:47,  1.61it/s]Extractor Estimating: 275it [02:48,  1.57it/s]Extractor Estimating: 276it [02:48,  1.59it/s]Extractor Estimating: 277it [02:49,  1.58it/s]Extractor Estimating: 278it [02:49,  1.59it/s]Extractor Estimating: 279it [02:50,  1.60it/s]Extractor Estimating: 280it [02:51,  1.60it/s]Extractor Estimating: 281it [02:51,  1.56it/s]Extractor Estimating: 282it [02:52,  1.59it/s]Extractor Estimating: 283it [02:53,  1.60it/s]Extractor Estimating: 284it [02:53,  1.59it/s]Extractor Estimating: 285it [02:54,  1.60it/s]Extractor Estimating: 286it [02:54,  1.59it/s]Extractor Estimating: 287it [02:55,  1.57it/s]Extractor Estimating: 288it [02:56,  1.57it/s]Extractor Estimating: 289it [02:56,  1.53it/s]Extractor Estimating: 290it [02:57,  1.56it/s]Extractor Estimating: 291it [02:58,  1.55it/s]Extractor Estimating: 292it [02:58,  1.51it/s]Extractor Estimating: 293it [02:59,  1.49it/s]Extractor Estimating: 294it [03:00,  1.50it/s]Extractor Estimating: 295it [03:00,  1.51it/s]Extractor Estimating: 296it [03:01,  1.51it/s]Extractor Estimating: 297it [03:02,  1.53it/s]Extractor Estimating: 298it [03:02,  1.55it/s]Extractor Estimating: 299it [03:03,  1.57it/s]Extractor Estimating: 300it [03:04,  1.62it/s]Extractor Estimating: 301it [03:04,  1.67it/s]Extractor Estimating: 302it [03:05,  1.69it/s]Extractor Estimating: 303it [03:05,  1.67it/s]Extractor Estimating: 304it [03:06,  1.72it/s]Extractor Estimating: 305it [03:06,  1.68it/s]Extractor Estimating: 306it [03:07,  1.73it/s]Extractor Estimating: 307it [03:08,  1.74it/s]Extractor Estimating: 308it [03:08,  1.76it/s]Extractor Estimating: 309it [03:09,  1.76it/s]Extractor Estimating: 310it [03:09,  1.79it/s]Extractor Estimating: 311it [03:10,  1.76it/s]Extractor Estimating: 312it [03:10,  1.69it/s]Extractor Estimating: 313it [03:11,  1.65it/s]Extractor Estimating: 314it [03:12,  1.66it/s]Extractor Estimating: 315it [03:12,  1.66it/s]Extractor Estimating: 316it [03:13,  1.75it/s]Extractor Estimating: 317it [03:13,  1.75it/s]Extractor Estimating: 318it [03:14,  1.71it/s]Extractor Estimating: 319it [03:14,  1.78it/s]Extractor Estimating: 320it [03:15,  1.74it/s]Extractor Estimating: 321it [03:16,  1.73it/s]Extractor Estimating: 322it [03:16,  1.71it/s]Extractor Estimating: 323it [03:17,  1.70it/s]Extractor Estimating: 324it [03:17,  1.67it/s]Extractor Estimating: 325it [03:18,  1.67it/s]Extractor Estimating: 326it [03:19,  1.71it/s]Extractor Estimating: 327it [03:19,  1.74it/s]Extractor Estimating: 328it [03:20,  1.71it/s]Extractor Estimating: 329it [03:20,  1.75it/s]Extractor Estimating: 330it [03:21,  1.69it/s]Extractor Estimating: 331it [03:22,  1.74it/s]Extractor Estimating: 332it [03:22,  1.74it/s]Extractor Estimating: 333it [03:23,  1.73it/s]Extractor Estimating: 334it [03:23,  1.76it/s]Extractor Estimating: 335it [03:24,  1.73it/s]Extractor Estimating: 336it [03:24,  1.74it/s]Extractor Estimating: 337it [03:25,  1.69it/s]Extractor Estimating: 338it [03:26,  1.71it/s]Extractor Estimating: 339it [03:26,  1.71it/s]Extractor Estimating: 340it [03:27,  1.68it/s]Extractor Estimating: 341it [03:27,  1.69it/s]Extractor Estimating: 342it [03:28,  1.70it/s]Extractor Estimating: 343it [03:29,  1.71it/s]Extractor Estimating: 344it [03:29,  1.71it/s]Extractor Estimating: 345it [03:30,  1.71it/s]Extractor Estimating: 346it [03:30,  1.77it/s]Extractor Estimating: 347it [03:31,  1.77it/s]Extractor Estimating: 348it [03:32,  1.56it/s]Extractor Estimating: 349it [03:32,  1.56it/s]Extractor Estimating: 350it [03:33,  1.54it/s]Extractor Estimating: 351it [03:34,  1.58it/s]Extractor Estimating: 352it [03:34,  1.55it/s]Extractor Estimating: 353it [03:35,  1.58it/s]Extractor Estimating: 354it [03:35,  1.58it/s]Extractor Estimating: 355it [03:36,  1.61it/s]Extractor Estimating: 356it [03:37,  1.59it/s]Extractor Estimating: 357it [03:37,  1.59it/s]Extractor Estimating: 358it [03:38,  1.59it/s]Extractor Estimating: 359it [03:39,  1.61it/s]Extractor Estimating: 360it [03:39,  1.56it/s]Extractor Estimating: 361it [03:40,  1.59it/s]Extractor Estimating: 362it [03:40,  1.61it/s]Extractor Estimating: 363it [03:41,  1.63it/s]Extractor Estimating: 364it [03:42,  1.64it/s]Extractor Estimating: 365it [03:42,  1.64it/s]Extractor Estimating: 366it [03:43,  1.66it/s]Extractor Estimating: 367it [03:43,  1.68it/s]Extractor Estimating: 368it [03:44,  1.66it/s]Extractor Estimating: 369it [03:45,  1.63it/s]Extractor Estimating: 370it [03:45,  1.61it/s]Extractor Estimating: 371it [03:46,  1.67it/s]Extractor Estimating: 372it [03:46,  1.71it/s]Extractor Estimating: 373it [03:47,  1.73it/s]Extractor Estimating: 374it [03:48,  1.70it/s]Extractor Estimating: 375it [03:48,  1.61it/s]Extractor Estimating: 376it [03:49,  1.64it/s]Extractor Estimating: 377it [03:49,  1.62it/s]Extractor Estimating: 378it [03:50,  1.65it/s]Extractor Estimating: 379it [03:51,  1.64it/s]Extractor Estimating: 380it [03:51,  1.64it/s]Extractor Estimating: 381it [03:52,  1.61it/s]Extractor Estimating: 382it [03:53,  1.64it/s]Extractor Estimating: 383it [03:53,  1.66it/s]Extractor Estimating: 384it [03:54,  1.66it/s]Extractor Estimating: 385it [03:54,  1.67it/s]Extractor Estimating: 386it [03:55,  1.63it/s]Extractor Estimating: 387it [03:56,  1.62it/s]Extractor Estimating: 388it [03:56,  1.58it/s]Extractor Estimating: 389it [03:57,  1.56it/s]Extractor Estimating: 390it [03:57,  1.61it/s]Extractor Estimating: 391it [03:58,  1.65it/s]Extractor Estimating: 392it [03:59,  1.63it/s]Extractor Estimating: 393it [03:59,  1.62it/s]Extractor Estimating: 394it [04:00,  1.63it/s]Extractor Estimating: 395it [04:00,  1.64it/s]Extractor Estimating: 396it [04:01,  1.66it/s]Extractor Estimating: 397it [04:02,  1.70it/s]Extractor Estimating: 398it [04:02,  1.66it/s]Extractor Estimating: 399it [04:03,  1.64it/s]Extractor Estimating: 400it [04:03,  1.65it/s]Extractor Estimating: 401it [04:04,  1.69it/s]Extractor Estimating: 402it [04:05,  1.63it/s]Extractor Estimating: 403it [04:05,  1.63it/s]Extractor Estimating: 404it [04:06,  1.69it/s]Extractor Estimating: 405it [04:07,  1.66it/s]Extractor Estimating: 406it [04:07,  1.68it/s]Extractor Estimating: 407it [04:08,  1.66it/s]Extractor Estimating: 408it [04:08,  1.71it/s]Extractor Estimating: 409it [04:09,  1.72it/s]Extractor Estimating: 410it [04:09,  1.74it/s]Extractor Estimating: 411it [04:10,  1.68it/s]Extractor Estimating: 412it [04:11,  1.68it/s]Extractor Estimating: 413it [04:11,  1.65it/s]Extractor Estimating: 414it [04:12,  1.66it/s]Extractor Estimating: 415it [04:13,  1.61it/s]Extractor Estimating: 416it [04:13,  1.62it/s]Extractor Estimating: 417it [04:14,  1.63it/s]Extractor Estimating: 418it [04:14,  1.65it/s]Extractor Estimating: 419it [04:15,  1.67it/s]Extractor Estimating: 420it [04:16,  1.65it/s]Extractor Estimating: 421it [04:16,  1.67it/s]Extractor Estimating: 422it [04:17,  1.69it/s]Extractor Estimating: 423it [04:17,  1.66it/s]Extractor Estimating: 424it [04:18,  1.67it/s]Extractor Estimating: 425it [04:18,  1.66it/s]Extractor Estimating: 426it [04:19,  1.64it/s]Extractor Estimating: 427it [04:20,  1.63it/s]Extractor Estimating: 428it [04:20,  1.65it/s]Extractor Estimating: 429it [04:21,  1.63it/s]Extractor Estimating: 430it [04:22,  1.56it/s]Extractor Estimating: 431it [04:22,  1.56it/s]Extractor Estimating: 432it [04:23,  1.63it/s]Extractor Estimating: 433it [04:24,  1.59it/s]Extractor Estimating: 434it [04:24,  1.59it/s]Extractor Estimating: 435it [04:25,  1.62it/s]Extractor Estimating: 436it [04:25,  1.58it/s]Extractor Estimating: 437it [04:26,  1.56it/s]Extractor Estimating: 438it [04:27,  1.62it/s]Extractor Estimating: 439it [04:27,  1.45it/s]Extractor Estimating: 440it [04:28,  1.51it/s]Extractor Estimating: 441it [04:29,  1.54it/s]Extractor Estimating: 442it [04:29,  1.58it/s]Extractor Estimating: 443it [04:30,  1.60it/s]Extractor Estimating: 444it [04:31,  1.62it/s]Extractor Estimating: 445it [04:31,  1.66it/s]Extractor Estimating: 446it [04:32,  1.65it/s]Extractor Estimating: 447it [04:32,  1.61it/s]Extractor Estimating: 448it [04:33,  1.58it/s]Extractor Estimating: 449it [04:34,  1.56it/s]Extractor Estimating: 450it [04:34,  1.61it/s]Extractor Estimating: 451it [04:35,  1.58it/s]Extractor Estimating: 452it [04:36,  1.50it/s]Extractor Estimating: 453it [04:36,  1.50it/s]Extractor Estimating: 454it [04:37,  1.50it/s]Extractor Estimating: 455it [04:38,  1.50it/s]Extractor Estimating: 456it [04:38,  1.52it/s]Extractor Estimating: 457it [04:39,  1.50it/s]Extractor Estimating: 458it [04:40,  1.52it/s]Extractor Estimating: 459it [04:40,  1.57it/s]Extractor Estimating: 460it [04:41,  1.57it/s]Extractor Estimating: 461it [04:41,  1.56it/s]Extractor Estimating: 462it [04:42,  1.54it/s]Extractor Estimating: 463it [04:43,  1.53it/s]Extractor Estimating: 464it [04:43,  1.55it/s]Extractor Estimating: 465it [04:44,  1.55it/s]Extractor Estimating: 466it [04:45,  1.51it/s]Extractor Estimating: 467it [04:45,  1.49it/s]Extractor Estimating: 468it [04:46,  1.51it/s]Extractor Estimating: 469it [04:47,  1.54it/s]Extractor Estimating: 470it [04:47,  1.54it/s]Extractor Estimating: 471it [04:48,  1.56it/s]Extractor Estimating: 472it [04:49,  1.56it/s]Extractor Estimating: 473it [04:49,  1.56it/s]Extractor Estimating: 474it [04:50,  1.56it/s]Extractor Estimating: 475it [04:51,  1.61it/s]Extractor Estimating: 476it [04:51,  1.64it/s]Extractor Estimating: 477it [04:52,  1.66it/s]Extractor Estimating: 478it [04:52,  1.66it/s]Extractor Estimating: 479it [04:53,  1.68it/s]Extractor Estimating: 480it [04:53,  1.69it/s]Extractor Estimating: 481it [04:54,  1.72it/s]Extractor Estimating: 482it [04:55,  1.72it/s]Extractor Estimating: 483it [04:55,  1.66it/s]Extractor Estimating: 484it [04:56,  1.68it/s]Extractor Estimating: 485it [04:56,  1.68it/s]Extractor Estimating: 486it [04:57,  1.70it/s]Extractor Estimating: 487it [04:58,  1.70it/s]Extractor Estimating: 488it [04:58,  1.70it/s]Extractor Estimating: 489it [04:59,  1.73it/s]Extractor Estimating: 490it [04:59,  1.71it/s]Extractor Estimating: 491it [05:00,  1.71it/s]Extractor Estimating: 492it [05:01,  1.66it/s]Extractor Estimating: 493it [05:01,  1.69it/s]Extractor Estimating: 494it [05:02,  1.66it/s]Extractor Estimating: 495it [05:02,  1.65it/s]Extractor Estimating: 496it [05:03,  1.64it/s]Extractor Estimating: 497it [05:04,  1.65it/s]Extractor Estimating: 498it [05:04,  1.68it/s]Extractor Estimating: 499it [05:05,  1.63it/s]Extractor Estimating: 500it [05:05,  1.86it/s]Extractor Estimating: 500it [05:05,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:35,068 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:35,073 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:35,073 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:35,073 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:35,073 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:07:35,382 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:07:35,384 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:07:35,649 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:07:36,731 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:07:36,731 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:38,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:38,032 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:38,032 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:38,032 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:38,032 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:07:38,369 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:07:38,374 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:07:38,636 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:07:38,796 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:07:38,796 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 12:23:00,909 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 12:23:01,198 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9983 mean pseudo reward: 0.9503629323992846
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 23493
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23593, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23593, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.086, loss:634.2653
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.093, loss:598.9675
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.061, loss:614.2291
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.079, loss:570.3796
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.078, loss:579.9431
>> valid entity prec:0.4842, rec:0.3984, f1:0.4372
>> valid relation prec:0.0679, rec:0.0105, f1:0.0182
>> valid relation with NER prec:0.0679, rec:0.0105, f1:0.0182
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.804, loss:581.8443
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.069, loss:572.1817
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.081, loss:573.4150
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.065, loss:590.9898
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.075, loss:570.6574
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4535, rec:0.4154, f1:0.4336
>> valid relation prec:0.0939, rec:0.0148, f1:0.0256
>> valid relation with NER prec:0.0939, rec:0.0148, f1:0.0256
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 268, avg_time 2.826, loss:598.3715
g_step 1200, step 368, avg_time 1.071, loss:586.6868
g_step 1300, step 52, avg_time 1.068, loss:568.8676
g_step 1400, step 152, avg_time 1.069, loss:549.7249
g_step 1500, step 252, avg_time 1.071, loss:559.0367
>> valid entity prec:0.4621, rec:0.3870, f1:0.4212
>> valid relation prec:0.1369, rec:0.0225, f1:0.0386
>> valid relation with NER prec:0.1369, rec:0.0225, f1:0.0386
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 352, avg_time 2.818, loss:575.2712
g_step 1700, step 36, avg_time 1.082, loss:569.5220
g_step 1800, step 136, avg_time 1.069, loss:532.1189
g_step 1900, step 236, avg_time 1.076, loss:522.1021
g_step 2000, step 336, avg_time 1.069, loss:558.3690
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4599, rec:0.4043, f1:0.4303
>> valid relation prec:0.1436, rec:0.0179, f1:0.0319
>> valid relation with NER prec:0.1436, rec:0.0179, f1:0.0319
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 2.831, loss:531.1385
g_step 2200, step 120, avg_time 1.064, loss:489.8546
g_step 2300, step 220, avg_time 1.071, loss:508.9877
g_step 2400, step 320, avg_time 1.063, loss:500.3011
g_step 2500, step 4, avg_time 1.084, loss:523.1609
>> valid entity prec:0.4474, rec:0.3680, f1:0.4038
>> valid relation prec:0.1133, rec:0.0157, f1:0.0275
>> valid relation with NER prec:0.1133, rec:0.0157, f1:0.0275
g_step 2600, step 104, avg_time 2.802, loss:481.2349
g_step 2700, step 204, avg_time 1.075, loss:488.8161
g_step 2800, step 304, avg_time 1.078, loss:517.9161
g_step 2900, step 404, avg_time 1.067, loss:499.2782
g_step 3000, step 88, avg_time 1.071, loss:458.1497
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4963, rec:0.3253, f1:0.3930
>> valid relation prec:0.1454, rec:0.0183, f1:0.0326
>> valid relation with NER prec:0.1454, rec:0.0183, f1:0.0326
g_step 3100, step 188, avg_time 2.813, loss:478.2146
g_step 3200, step 288, avg_time 1.073, loss:479.3283
g_step 3300, step 388, avg_time 1.077, loss:487.7516
g_step 3400, step 72, avg_time 1.079, loss:438.2837
g_step 3500, step 172, avg_time 1.084, loss:455.3057
>> valid entity prec:0.4493, rec:0.4754, f1:0.4620
>> valid relation prec:0.0591, rec:0.0150, f1:0.0240
>> valid relation with NER prec:0.0591, rec:0.0150, f1:0.0240
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 272, avg_time 2.845, loss:469.3014
g_step 3700, step 372, avg_time 1.062, loss:465.3103
g_step 3800, step 56, avg_time 1.069, loss:434.1715
g_step 3900, step 156, avg_time 1.077, loss:414.8470
g_step 4000, step 256, avg_time 1.068, loss:427.1888
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4850, rec:0.3557, f1:0.4104
>> valid relation prec:0.0838, rec:0.0119, f1:0.0209
>> valid relation with NER prec:0.0838, rec:0.0119, f1:0.0209
g_step 4100, step 356, avg_time 2.824, loss:471.5322
g_step 4200, step 40, avg_time 1.067, loss:442.7033
g_step 4300, step 140, avg_time 1.079, loss:428.9880
g_step 4400, step 240, avg_time 1.071, loss:423.2316
g_step 4500, step 340, avg_time 1.102, loss:424.6264
>> valid entity prec:0.4465, rec:0.3829, f1:0.4123
>> valid relation prec:0.0745, rec:0.0130, f1:0.0221
>> valid relation with NER prec:0.0745, rec:0.0130, f1:0.0221
g_step 4600, step 24, avg_time 2.814, loss:415.9914
g_step 4700, step 124, avg_time 1.071, loss:402.2537
g_step 4800, step 224, avg_time 1.071, loss:415.2999
g_step 4900, step 324, avg_time 1.076, loss:406.0665
g_step 5000, step 8, avg_time 1.070, loss:427.3634
learning rate was adjusted to 0.0008
>> valid entity prec:0.4473, rec:0.4074, f1:0.4265
>> valid relation prec:0.0595, rec:0.0126, f1:0.0207
>> valid relation with NER prec:0.0595, rec:0.0126, f1:0.0207
g_step 5100, step 108, avg_time 2.874, loss:393.2337
g_step 5200, step 208, avg_time 1.061, loss:404.2354
g_step 5300, step 308, avg_time 1.071, loss:401.9823
g_step 5400, step 408, avg_time 1.070, loss:397.2060
g_step 5500, step 92, avg_time 1.081, loss:365.7260
>> valid entity prec:0.4877, rec:0.3115, f1:0.3802
>> valid relation prec:0.0643, rec:0.0084, f1:0.0149
>> valid relation with NER prec:0.0643, rec:0.0084, f1:0.0149
g_step 5600, step 192, avg_time 2.803, loss:379.9288
g_step 5700, step 292, avg_time 1.057, loss:398.5942
g_step 5800, step 392, avg_time 1.064, loss:405.1872
g_step 5900, step 76, avg_time 1.063, loss:374.6209
g_step 6000, step 176, avg_time 1.082, loss:371.5296
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4364, rec:0.3526, f1:0.3900
>> valid relation prec:0.0806, rec:0.0138, f1:0.0236
>> valid relation with NER prec:0.0806, rec:0.0138, f1:0.0236
g_step 6100, step 276, avg_time 2.811, loss:364.0272
g_step 6200, step 376, avg_time 1.067, loss:389.1423
g_step 6300, step 60, avg_time 1.060, loss:364.5016
g_step 6400, step 160, avg_time 1.079, loss:361.6976
g_step 6500, step 260, avg_time 1.080, loss:360.5158
>> valid entity prec:0.4342, rec:0.3570, f1:0.3918
>> valid relation prec:0.0559, rec:0.0089, f1:0.0153
>> valid relation with NER prec:0.0559, rec:0.0089, f1:0.0153
g_step 6600, step 360, avg_time 2.806, loss:363.9112
g_step 6700, step 44, avg_time 1.079, loss:355.3717
g_step 6800, step 144, avg_time 1.075, loss:337.4468
g_step 6900, step 244, avg_time 1.069, loss:351.1599
g_step 7000, step 344, avg_time 1.068, loss:357.7620
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4374, rec:0.3855, f1:0.4098
>> valid relation prec:0.0896, rec:0.0185, f1:0.0307
>> valid relation with NER prec:0.0896, rec:0.0185, f1:0.0307
g_step 7100, step 28, avg_time 2.808, loss:353.6040
g_step 7200, step 128, avg_time 1.081, loss:330.8394
g_step 7300, step 228, avg_time 1.072, loss:337.6259
g_step 7400, step 328, avg_time 1.053, loss:352.7119
g_step 7500, step 12, avg_time 1.071, loss:338.5297
>> valid entity prec:0.4722, rec:0.4130, f1:0.4406
>> valid relation prec:0.0662, rec:0.0142, f1:0.0234
>> valid relation with NER prec:0.0662, rec:0.0142, f1:0.0234
g_step 7600, step 112, avg_time 2.810, loss:306.2652
g_step 7700, step 212, avg_time 1.068, loss:320.4461
g_step 7800, step 312, avg_time 1.070, loss:346.4521
g_step 7900, step 412, avg_time 1.068, loss:347.4830
g_step 8000, step 96, avg_time 1.069, loss:304.1496
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4583, rec:0.3797, f1:0.4153
>> valid relation prec:0.0562, rec:0.0119, f1:0.0197
>> valid relation with NER prec:0.0562, rec:0.0119, f1:0.0197
g_step 8100, step 196, avg_time 2.814, loss:312.6721
g_step 8200, step 296, avg_time 1.068, loss:313.2757
g_step 8300, step 396, avg_time 1.076, loss:334.8275
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 12:23:01 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 12:23:01 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_12-23-00_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 12:23:02 - WARNING - datasets.builder -   Using custom data configuration default-539f322c548b1401
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-539f322c548b1401/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 12:23:04,520 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:23:04,554 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 12:23:04,555 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:23:04,556 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 12:23:04,683 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:23:04,761 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:23:04,761 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:23:04,761 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:23:04,761 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:23:04,761 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:23:04,761 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 12:23:05,198 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 12:23:24,845 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 12:23:24,871 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-539f322c548b1401/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:03,  2.49ba/s] 20%|██        | 2/10 [00:00<00:02,  3.48ba/s] 30%|███       | 3/10 [00:00<00:01,  4.00ba/s] 40%|████      | 4/10 [00:01<00:01,  4.32ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.48ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.57ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.64ba/s] 80%|████████  | 8/10 [00:02<00:00,  3.84ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.09ba/s]100%|██████████| 10/10 [00:02<00:00,  4.29ba/s]100%|██████████| 10/10 [00:02<00:00,  4.13ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.99ba/s] 40%|████      | 2/5 [00:00<00:00,  3.53ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.91ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.11ba/s]100%|██████████| 5/5 [00:01<00:00,  4.42ba/s]100%|██████████| 5/5 [00:01<00:00,  4.08ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  4.38ba/s] 20%|██        | 2/10 [00:00<00:01,  6.53ba/s] 40%|████      | 4/10 [00:00<00:00,  8.37ba/s] 60%|██████    | 6/10 [00:00<00:00,  9.13ba/s] 80%|████████  | 8/10 [00:00<00:00,  9.55ba/s]100%|██████████| 10/10 [00:01<00:00,  9.70ba/s]100%|██████████| 10/10 [00:01<00:00,  8.93ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.51ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.69ba/s] 80%|████████  | 4/5 [00:00<00:00,  7.49ba/s]100%|██████████| 5/5 [00:00<00:00,  7.70ba/s]
[INFO|trainer.py:414] 2023-08-29 12:23:31,606 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 12:23:31,716 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 12:23:31,716 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-29 12:23:31,716 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 12:23:31,716 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 12:23:31,716 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 12:23:31,717 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 12:23:31,717 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:56,  3.29it/s]  0%|          | 2/780 [00:00<03:48,  3.41it/s]  0%|          | 3/780 [00:00<03:50,  3.37it/s]  1%|          | 4/780 [00:01<03:47,  3.41it/s]  1%|          | 5/780 [00:01<03:45,  3.44it/s]  1%|          | 6/780 [00:01<03:43,  3.46it/s]  1%|          | 7/780 [00:02<03:43,  3.47it/s]  1%|          | 8/780 [00:02<03:42,  3.48it/s]  1%|          | 9/780 [00:02<03:41,  3.48it/s]  1%|▏         | 10/780 [00:02<03:41,  3.48it/s]  1%|▏         | 11/780 [00:03<03:40,  3.48it/s]  2%|▏         | 12/780 [00:03<03:40,  3.48it/s]  2%|▏         | 13/780 [00:03<03:40,  3.48it/s]  2%|▏         | 14/780 [00:04<03:45,  3.39it/s]  2%|▏         | 15/780 [00:04<03:43,  3.42it/s]  2%|▏         | 16/780 [00:04<03:42,  3.44it/s]  2%|▏         | 17/780 [00:04<03:41,  3.45it/s]  2%|▏         | 18/780 [00:05<03:40,  3.46it/s]  2%|▏         | 19/780 [00:05<03:39,  3.46it/s]  3%|▎         | 20/780 [00:05<03:39,  3.47it/s]  3%|▎         | 21/780 [00:06<03:38,  3.47it/s]  3%|▎         | 22/780 [00:06<03:38,  3.48it/s]  3%|▎         | 23/780 [00:06<03:37,  3.48it/s]  3%|▎         | 24/780 [00:06<03:37,  3.48it/s]  3%|▎         | 25/780 [00:07<03:52,  3.24it/s]  3%|▎         | 26/780 [00:07<03:47,  3.31it/s]  3%|▎         | 27/780 [00:07<03:44,  3.36it/s]  4%|▎         | 28/780 [00:08<03:41,  3.39it/s]  4%|▎         | 29/780 [00:08<03:39,  3.42it/s]  4%|▍         | 30/780 [00:08<03:38,  3.44it/s]  4%|▍         | 31/780 [00:09<03:37,  3.45it/s]  4%|▍         | 32/780 [00:09<03:36,  3.46it/s]  4%|▍         | 33/780 [00:09<03:35,  3.46it/s]  4%|▍         | 34/780 [00:09<03:35,  3.46it/s]  4%|▍         | 35/780 [00:10<03:34,  3.47it/s]  5%|▍         | 36/780 [00:10<03:47,  3.27it/s]  5%|▍         | 37/780 [00:10<03:43,  3.33it/s]  5%|▍         | 38/780 [00:11<03:40,  3.37it/s]  5%|▌         | 39/780 [00:11<03:37,  3.40it/s]  5%|▌         | 40/780 [00:11<03:36,  3.43it/s]  5%|▌         | 41/780 [00:11<03:34,  3.44it/s]  5%|▌         | 42/780 [00:12<03:33,  3.45it/s]  6%|▌         | 43/780 [00:12<03:33,  3.46it/s]  6%|▌         | 44/780 [00:12<03:32,  3.46it/s]  6%|▌         | 45/780 [00:13<03:32,  3.47it/s]  6%|▌         | 46/780 [00:13<03:31,  3.47it/s]  6%|▌         | 47/780 [00:13<03:48,  3.20it/s]  6%|▌         | 48/780 [00:14<03:43,  3.28it/s]  6%|▋         | 49/780 [00:14<03:39,  3.34it/s]  6%|▋         | 50/780 [00:14<03:36,  3.38it/s]  7%|▋         | 51/780 [00:14<03:33,  3.41it/s]  7%|▋         | 52/780 [00:15<03:32,  3.43it/s]  7%|▋         | 53/780 [00:15<03:31,  3.44it/s]  7%|▋         | 54/780 [00:15<03:30,  3.45it/s]  7%|▋         | 55/780 [00:16<03:29,  3.46it/s]  7%|▋         | 56/780 [00:16<03:29,  3.46it/s]  7%|▋         | 57/780 [00:16<03:28,  3.47it/s]  7%|▋         | 58/780 [00:16<03:36,  3.33it/s]  8%|▊         | 59/780 [00:17<03:33,  3.37it/s]  8%|▊         | 60/780 [00:17<03:31,  3.40it/s]  8%|▊         | 61/780 [00:17<03:30,  3.42it/s]  8%|▊         | 62/780 [00:18<03:28,  3.44it/s]  8%|▊         | 63/780 [00:18<03:28,  3.45it/s]  8%|▊         | 64/780 [00:18<03:27,  3.45it/s]  8%|▊         | 65/780 [00:18<03:26,  3.46it/s]  8%|▊         | 66/780 [00:19<03:26,  3.46it/s]  9%|▊         | 67/780 [00:19<03:25,  3.46it/s]  9%|▊         | 68/780 [00:19<03:25,  3.46it/s]  9%|▉         | 69/780 [00:20<03:30,  3.38it/s]  9%|▉         | 70/780 [00:20<03:28,  3.41it/s]  9%|▉         | 71/780 [00:20<03:27,  3.42it/s]  9%|▉         | 72/780 [00:21<03:26,  3.44it/s]  9%|▉         | 73/780 [00:21<03:25,  3.44it/s]  9%|▉         | 74/780 [00:21<03:24,  3.45it/s] 10%|▉         | 75/780 [00:21<03:24,  3.45it/s] 10%|▉         | 76/780 [00:22<03:23,  3.46it/s] 10%|▉         | 77/780 [00:22<03:23,  3.46it/s] 10%|█         | 78/780 [00:22<03:22,  3.46it/s] 10%|█         | 79/780 [00:23<03:22,  3.46it/s] 10%|█         | 80/780 [00:23<03:29,  3.35it/s] 10%|█         | 81/780 [00:23<03:26,  3.38it/s] 11%|█         | 82/780 [00:23<03:24,  3.41it/s] 11%|█         | 83/780 [00:24<03:23,  3.43it/s] 11%|█         | 84/780 [00:24<03:22,  3.44it/s] 11%|█         | 85/780 [00:24<03:21,  3.44it/s] 11%|█         | 86/780 [00:25<03:21,  3.45it/s] 11%|█         | 87/780 [00:25<03:20,  3.45it/s] 11%|█▏        | 88/780 [00:25<03:20,  3.46it/s] 11%|█▏        | 89/780 [00:25<03:19,  3.46it/s] 12%|█▏        | 90/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 91/780 [00:26<03:23,  3.39it/s] 12%|█▏        | 92/780 [00:26<03:28,  3.30it/s] 12%|█▏        | 93/780 [00:27<03:25,  3.34it/s] 12%|█▏        | 94/780 [00:27<03:23,  3.37it/s] 12%|█▏        | 95/780 [00:27<03:21,  3.40it/s] 12%|█▏        | 96/780 [00:28<03:20,  3.42it/s] 12%|█▏        | 97/780 [00:28<03:18,  3.43it/s] 13%|█▎        | 98/780 [00:28<03:18,  3.44it/s] 13%|█▎        | 99/780 [00:28<03:17,  3.44it/s] 13%|█▎        | 100/780 [00:29<03:17,  3.45it/s] 13%|█▎        | 101/780 [00:29<04:26,  2.55it/s] 13%|█▎        | 102/780 [00:30<04:12,  2.68it/s] 13%|█▎        | 103/780 [00:30<03:55,  2.88it/s] 13%|█▎        | 104/780 [00:30<03:42,  3.03it/s] 13%|█▎        | 105/780 [00:31<03:34,  3.15it/s] 14%|█▎        | 106/780 [00:31<03:28,  3.24it/s] 14%|█▎        | 107/780 [00:31<03:23,  3.30it/s] 14%|█▍        | 108/780 [00:31<03:20,  3.35it/s] 14%|█▍        | 109/780 [00:32<03:18,  3.38it/s] 14%|█▍        | 110/780 [00:32<03:16,  3.41it/s] 14%|█▍        | 111/780 [00:32<03:15,  3.42it/s] 14%|█▍        | 112/780 [00:33<03:14,  3.43it/s] 14%|█▍        | 113/780 [00:33<03:13,  3.44it/s] 15%|█▍        | 114/780 [00:33<03:13,  3.45it/s] 15%|█▍        | 115/780 [00:33<03:12,  3.45it/s] 15%|█▍        | 116/780 [00:34<03:12,  3.46it/s] 15%|█▌        | 117/780 [00:34<03:11,  3.46it/s] 15%|█▌        | 118/780 [00:34<03:14,  3.39it/s] 15%|█▌        | 119/780 [00:35<03:13,  3.41it/s] 15%|█▌        | 120/780 [00:35<03:12,  3.43it/s] 16%|█▌        | 121/780 [00:35<03:11,  3.44it/s] 16%|█▌        | 122/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 123/780 [00:36<03:10,  3.45it/s] 16%|█▌        | 124/780 [00:36<03:09,  3.46it/s] 16%|█▌        | 125/780 [00:36<03:09,  3.46it/s] 16%|█▌        | 126/780 [00:37<03:09,  3.46it/s] 16%|█▋        | 127/780 [00:37<03:08,  3.46it/s] 16%|█▋        | 128/780 [00:37<03:08,  3.46it/s] 17%|█▋        | 129/780 [00:38<03:15,  3.33it/s] 17%|█▋        | 130/780 [00:38<03:13,  3.37it/s] 17%|█▋        | 131/780 [00:38<03:11,  3.40it/s] 17%|█▋        | 132/780 [00:38<03:09,  3.42it/s] 17%|█▋        | 133/780 [00:39<03:08,  3.43it/s] 17%|█▋        | 134/780 [00:39<03:07,  3.44it/s] 17%|█▋        | 135/780 [00:39<03:07,  3.45it/s] 17%|█▋        | 136/780 [00:40<03:06,  3.45it/s] 18%|█▊        | 137/780 [00:40<03:06,  3.45it/s] 18%|█▊        | 138/780 [00:40<03:05,  3.46it/s] 18%|█▊        | 139/780 [00:40<03:05,  3.46it/s] 18%|█▊        | 140/780 [00:41<03:09,  3.38it/s] 18%|█▊        | 141/780 [00:41<03:07,  3.40it/s] 18%|█▊        | 142/780 [00:41<03:06,  3.42it/s] 18%|█▊        | 143/780 [00:42<03:05,  3.43it/s] 18%|█▊        | 144/780 [00:42<03:04,  3.44it/s] 19%|█▊        | 145/780 [00:42<03:04,  3.45it/s] 19%|█▊        | 146/780 [00:42<03:03,  3.45it/s] 19%|█▉        | 147/780 [00:43<03:03,  3.45it/s] 19%|█▉        | 148/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 149/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 150/780 [00:44<03:02,  3.46it/s] 19%|█▉        | 151/780 [00:44<03:08,  3.33it/s] 19%|█▉        | 152/780 [00:44<03:06,  3.37it/s] 20%|█▉        | 153/780 [00:45<03:04,  3.40it/s] 20%|█▉        | 154/780 [00:45<03:03,  3.41it/s] 20%|█▉        | 155/780 [00:45<03:02,  3.43it/s] 20%|██        | 156/780 [00:45<03:01,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 12:24:17,650 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:24:17,650 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 12:24:17,650 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.88it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.71it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.92it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.15it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.65it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.38it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.11it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.63it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.68it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.52it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.73it/s][A
 10%|█         | 63/608 [00:01<00:12, 44.31it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.08it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.62it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.97it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.03it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.21it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.39it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.47it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.28it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.35it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.39it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.59it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.72it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.62it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.58it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.65it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.56it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.49it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.45it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.53it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.61it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.75it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.81it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.66it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.72it/s][A
 31%|███       | 188/608 [00:04<00:08, 46.71it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.61it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.56it/s][A
 33%|███▎      | 203/608 [00:04<00:09, 43.14it/s][A
 34%|███▍      | 208/608 [00:04<00:09, 44.11it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 44.83it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.47it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.91it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.18it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.34it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.43it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.18it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.33it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.45it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.51it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.63it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.62it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.70it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.63it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.71it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.51it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.54it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.52it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.61it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.55it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.60it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.59it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.61it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.57it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.43it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.44it/s][A
 56%|█████▋    | 343/608 [00:07<00:06, 44.12it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 44.91it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.42it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.89it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 45.94it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.25it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.34it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.39it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.29it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.21it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.23it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.40it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.40it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.56it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.60it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.54it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.64it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.47it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.45it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.50it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.54it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.56it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.60it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.50it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.53it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.53it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.56it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.55it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 44.82it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.29it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.66it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 45.89it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.16it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.31it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.42it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.34it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.37it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.37it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.43it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.48it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.53it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.44it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.55it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.62it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.58it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.60it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.50it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.46it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.49it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.50it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.54it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.61it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.66it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.46it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.46it/s][A 20%|██        | 156/780 [00:59<03:01,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:24:31,145 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 12:24:31,365 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:24:35,210 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:24:35,345 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:24:35,435 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:14<1:30:44,  8.74s/it] 20%|██        | 158/780 [01:14<1:04:26,  6.22s/it] 20%|██        | 159/780 [01:14<45:56,  4.44s/it]   21%|██        | 160/780 [01:15<32:59,  3.19s/it] 21%|██        | 161/780 [01:15<23:57,  2.32s/it] 21%|██        | 162/780 [01:15<17:38,  1.71s/it] 21%|██        | 163/780 [01:16<13:12,  1.29s/it] 21%|██        | 164/780 [01:16<10:07,  1.01it/s] 21%|██        | 165/780 [01:16<07:57,  1.29it/s] 21%|██▏       | 166/780 [01:16<06:27,  1.59it/s] 21%|██▏       | 167/780 [01:17<05:23,  1.89it/s] 22%|██▏       | 168/780 [01:17<04:39,  2.19it/s] 22%|██▏       | 169/780 [01:17<04:20,  2.34it/s] 22%|██▏       | 170/780 [01:18<03:55,  2.59it/s] 22%|██▏       | 171/780 [01:18<03:37,  2.80it/s] 22%|██▏       | 172/780 [01:18<03:24,  2.97it/s] 22%|██▏       | 173/780 [01:19<03:15,  3.11it/s] 22%|██▏       | 174/780 [01:19<03:09,  3.20it/s] 22%|██▏       | 175/780 [01:19<03:04,  3.27it/s] 23%|██▎       | 176/780 [01:19<03:01,  3.33it/s] 23%|██▎       | 177/780 [01:20<02:59,  3.37it/s] 23%|██▎       | 178/780 [01:20<02:57,  3.39it/s] 23%|██▎       | 179/780 [01:20<02:55,  3.42it/s] 23%|██▎       | 180/780 [01:21<03:00,  3.32it/s] 23%|██▎       | 181/780 [01:21<02:58,  3.36it/s] 23%|██▎       | 182/780 [01:21<02:56,  3.39it/s] 23%|██▎       | 183/780 [01:21<02:54,  3.41it/s] 24%|██▎       | 184/780 [01:22<02:53,  3.43it/s] 24%|██▎       | 185/780 [01:22<02:53,  3.44it/s] 24%|██▍       | 186/780 [01:22<02:52,  3.44it/s] 24%|██▍       | 187/780 [01:23<02:51,  3.45it/s] 24%|██▍       | 188/780 [01:23<02:51,  3.45it/s] 24%|██▍       | 189/780 [01:23<02:51,  3.46it/s] 24%|██▍       | 190/780 [01:24<02:50,  3.46it/s] 24%|██▍       | 191/780 [01:24<02:56,  3.34it/s] 25%|██▍       | 192/780 [01:24<02:53,  3.38it/s] 25%|██▍       | 193/780 [01:24<02:52,  3.41it/s] 25%|██▍       | 194/780 [01:25<02:51,  3.42it/s] 25%|██▌       | 195/780 [01:25<02:50,  3.43it/s] 25%|██▌       | 196/780 [01:25<02:49,  3.44it/s] 25%|██▌       | 197/780 [01:26<02:49,  3.45it/s] 25%|██▌       | 198/780 [01:26<02:48,  3.45it/s] 26%|██▌       | 199/780 [01:26<02:48,  3.46it/s] 26%|██▌       | 200/780 [01:26<02:47,  3.46it/s] 26%|██▌       | 201/780 [01:27<02:47,  3.46it/s] 26%|██▌       | 202/780 [01:27<02:51,  3.38it/s] 26%|██▌       | 203/780 [01:27<02:49,  3.40it/s] 26%|██▌       | 204/780 [01:28<02:48,  3.42it/s] 26%|██▋       | 205/780 [01:28<02:47,  3.43it/s] 26%|██▋       | 206/780 [01:28<02:46,  3.44it/s] 27%|██▋       | 207/780 [01:28<02:46,  3.45it/s] 27%|██▋       | 208/780 [01:29<02:45,  3.45it/s] 27%|██▋       | 209/780 [01:29<02:45,  3.45it/s] 27%|██▋       | 210/780 [01:29<02:51,  3.32it/s] 27%|██▋       | 211/780 [01:30<02:49,  3.35it/s] 27%|██▋       | 212/780 [01:30<02:47,  3.38it/s] 27%|██▋       | 213/780 [01:30<02:50,  3.32it/s] 27%|██▋       | 214/780 [01:31<02:48,  3.36it/s] 28%|██▊       | 215/780 [01:31<02:46,  3.39it/s] 28%|██▊       | 216/780 [01:31<02:45,  3.41it/s] 28%|██▊       | 217/780 [01:31<02:44,  3.42it/s] 28%|██▊       | 218/780 [01:32<02:43,  3.43it/s] 28%|██▊       | 219/780 [01:32<02:43,  3.44it/s] 28%|██▊       | 220/780 [01:32<02:42,  3.45it/s] 28%|██▊       | 221/780 [01:33<02:42,  3.45it/s] 28%|██▊       | 222/780 [01:33<02:42,  3.44it/s] 29%|██▊       | 223/780 [01:33<02:41,  3.45it/s] 29%|██▊       | 224/780 [01:33<02:40,  3.46it/s] 29%|██▉       | 225/780 [01:34<02:40,  3.45it/s] 29%|██▉       | 226/780 [01:34<02:40,  3.46it/s] 29%|██▉       | 227/780 [01:34<02:40,  3.46it/s] 29%|██▉       | 228/780 [01:35<02:39,  3.46it/s] 29%|██▉       | 229/780 [01:35<02:39,  3.46it/s] 29%|██▉       | 230/780 [01:35<02:39,  3.46it/s] 30%|██▉       | 231/780 [01:35<02:38,  3.46it/s] 30%|██▉       | 232/780 [01:36<02:45,  3.31it/s] 30%|██▉       | 233/780 [01:36<02:43,  3.35it/s] 30%|███       | 234/780 [01:36<02:41,  3.39it/s] 30%|███       | 235/780 [01:37<02:40,  3.41it/s] 30%|███       | 236/780 [01:37<02:39,  3.42it/s] 30%|███       | 237/780 [01:37<02:38,  3.43it/s] 31%|███       | 238/780 [01:38<02:38,  3.43it/s] 31%|███       | 239/780 [01:38<02:37,  3.44it/s] 31%|███       | 240/780 [01:38<02:36,  3.45it/s] 31%|███       | 241/780 [01:38<02:36,  3.45it/s] 31%|███       | 242/780 [01:39<02:35,  3.45it/s] 31%|███       | 243/780 [01:39<02:42,  3.31it/s] 31%|███▏      | 244/780 [01:39<02:39,  3.36it/s] 31%|███▏      | 245/780 [01:40<02:37,  3.39it/s] 32%|███▏      | 246/780 [01:40<02:36,  3.41it/s] 32%|███▏      | 247/780 [01:40<02:35,  3.42it/s] 32%|███▏      | 248/780 [01:40<02:35,  3.43it/s] 32%|███▏      | 249/780 [01:41<02:34,  3.44it/s] 32%|███▏      | 250/780 [01:41<02:33,  3.44it/s] 32%|███▏      | 251/780 [01:41<02:33,  3.45it/s] 32%|███▏      | 252/780 [01:42<02:32,  3.45it/s] 32%|███▏      | 253/780 [01:42<02:32,  3.45it/s] 33%|███▎      | 254/780 [01:42<02:40,  3.29it/s] 33%|███▎      | 255/780 [01:43<02:37,  3.33it/s] 33%|███▎      | 256/780 [01:43<02:35,  3.37it/s] 33%|███▎      | 257/780 [01:43<02:33,  3.40it/s] 33%|███▎      | 258/780 [01:43<02:32,  3.41it/s] 33%|███▎      | 259/780 [01:44<02:31,  3.43it/s] 33%|███▎      | 260/780 [01:44<02:31,  3.43it/s] 33%|███▎      | 261/780 [01:44<02:30,  3.44it/s] 34%|███▎      | 262/780 [01:45<02:30,  3.44it/s] 34%|███▎      | 263/780 [01:45<02:30,  3.45it/s] 34%|███▍      | 264/780 [01:45<02:29,  3.44it/s] 34%|███▍      | 265/780 [01:45<02:33,  3.35it/s] 34%|███▍      | 266/780 [01:46<02:32,  3.38it/s] 34%|███▍      | 267/780 [01:46<02:30,  3.40it/s] 34%|███▍      | 268/780 [01:46<02:29,  3.41it/s] 34%|███▍      | 269/780 [01:47<02:29,  3.42it/s] 35%|███▍      | 270/780 [01:47<02:28,  3.43it/s] 35%|███▍      | 271/780 [01:47<02:28,  3.44it/s] 35%|███▍      | 272/780 [01:48<02:27,  3.44it/s] 35%|███▌      | 273/780 [01:48<02:27,  3.44it/s] 35%|███▌      | 274/780 [01:48<02:26,  3.45it/s] 35%|███▌      | 275/780 [01:48<02:26,  3.45it/s] 35%|███▌      | 276/780 [01:49<02:31,  3.32it/s] 36%|███▌      | 277/780 [01:49<02:29,  3.35it/s] 36%|███▌      | 278/780 [01:49<02:28,  3.38it/s] 36%|███▌      | 279/780 [01:50<02:27,  3.40it/s] 36%|███▌      | 280/780 [01:50<02:26,  3.42it/s] 36%|███▌      | 281/780 [01:50<02:25,  3.43it/s] 36%|███▌      | 282/780 [01:50<02:25,  3.43it/s] 36%|███▋      | 283/780 [01:51<02:24,  3.44it/s] 36%|███▋      | 284/780 [01:51<02:24,  3.44it/s] 37%|███▋      | 285/780 [01:51<02:23,  3.44it/s] 37%|███▋      | 286/780 [01:52<02:23,  3.45it/s] 37%|███▋      | 287/780 [01:52<02:28,  3.31it/s] 37%|███▋      | 288/780 [01:52<02:26,  3.35it/s] 37%|███▋      | 289/780 [01:53<02:25,  3.38it/s] 37%|███▋      | 290/780 [01:53<02:24,  3.40it/s] 37%|███▋      | 291/780 [01:53<02:23,  3.41it/s] 37%|███▋      | 292/780 [01:53<02:22,  3.43it/s] 38%|███▊      | 293/780 [01:54<02:21,  3.43it/s] 38%|███▊      | 294/780 [01:54<02:21,  3.44it/s] 38%|███▊      | 295/780 [01:54<02:20,  3.44it/s] 38%|███▊      | 296/780 [01:55<02:20,  3.44it/s] 38%|███▊      | 297/780 [01:55<02:20,  3.45it/s] 38%|███▊      | 298/780 [01:55<02:24,  3.33it/s] 38%|███▊      | 299/780 [01:55<02:22,  3.37it/s] 38%|███▊      | 300/780 [01:56<02:21,  3.39it/s] 39%|███▊      | 301/780 [01:56<02:20,  3.41it/s] 39%|███▊      | 302/780 [01:56<02:19,  3.42it/s] 39%|███▉      | 303/780 [01:57<02:18,  3.43it/s] 39%|███▉      | 304/780 [01:57<02:18,  3.43it/s] 39%|███▉      | 305/780 [01:57<02:18,  3.44it/s] 39%|███▉      | 306/780 [01:57<02:17,  3.44it/s] 39%|███▉      | 307/780 [01:58<02:17,  3.44it/s] 39%|███▉      | 308/780 [01:58<02:17,  3.44it/s] 40%|███▉      | 309/780 [01:58<02:20,  3.34it/s] 40%|███▉      | 310/780 [01:59<02:19,  3.37it/s] 40%|███▉      | 311/780 [01:59<02:18,  3.39it/s] 40%|████      | 312/780 [01:59<02:17,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 12:25:31,524 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:25:31,525 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 12:25:31,525 >>   Batch size = 8
{'eval_loss': 0.9717044830322266, 'eval_runtime': 13.1561, 'eval_samples_per_second': 369.714, 'eval_steps_per_second': 46.214, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.62it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.28it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.57it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.90it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.44it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.15it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.86it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.53it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.54it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.57it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.61it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.53it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.62it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.49it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.47it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.53it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.31it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 45.01it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.48it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.68it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.96it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.11it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.29it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.34it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.31it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.24it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.45it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.50it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.42it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.38it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.40it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.39it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.39it/s][A
 28%|██▊       | 173/608 [00:03<00:10, 41.24it/s][A
 29%|██▉       | 178/608 [00:03<00:10, 42.73it/s][A
 30%|███       | 183/608 [00:03<00:09, 43.79it/s][A
 31%|███       | 188/608 [00:04<00:09, 44.64it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.16it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 45.57it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.95it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.19it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.80it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.81it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.05it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.20it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.30it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.47it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.40it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.47it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.48it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.20it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.17it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.17it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.33it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 46.37it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.44it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.44it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.49it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.49it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.41it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.23it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 43.36it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 44.18it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 44.87it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.34it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.64it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.94it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.17it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.27it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.06it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.06it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.24it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.24it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.33it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.37it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.46it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.47it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.51it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.37it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.43it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.43it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.44it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.53it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.44it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.54it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.41it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.40it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.50it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.41it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 43.55it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 44.32it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 44.98it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.50it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.70it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.05it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.08it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.14it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.14it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.19it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.32it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 46.34it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.46it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.53it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.42it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.48it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.49it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.40it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.40it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.36it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.34it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.43it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.43it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.46it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.40it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 40.90it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 42.44it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 43.65it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 39.62it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 41.52it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 42.96it/s][A
100%|██████████| 608/608 [00:13<00:00, 43.88it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 43.88it/s][A 40%|████      | 312/780 [02:13<02:17,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:25:45,281 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 12:25:45,642 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:25:50,306 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:25:50,542 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:25:50,641 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:27<1:07:24,  8.66s/it] 40%|████      | 314/780 [02:28<47:50,  6.16s/it]   40%|████      | 315/780 [02:28<34:05,  4.40s/it] 41%|████      | 316/780 [02:28<24:28,  3.17s/it] 41%|████      | 317/780 [02:29<17:46,  2.30s/it] 41%|████      | 318/780 [02:29<13:04,  1.70s/it] 41%|████      | 319/780 [02:29<09:48,  1.28s/it] 41%|████      | 320/780 [02:30<07:37,  1.01it/s] 41%|████      | 321/780 [02:30<05:59,  1.28it/s] 41%|████▏     | 322/780 [02:30<04:50,  1.57it/s] 41%|████▏     | 323/780 [02:30<04:02,  1.88it/s] 42%|████▏     | 324/780 [02:31<03:29,  2.18it/s] 42%|████▏     | 325/780 [02:31<03:10,  2.39it/s] 42%|████▏     | 326/780 [02:31<02:52,  2.63it/s] 42%|████▏     | 327/780 [02:32<02:39,  2.84it/s] 42%|████▏     | 328/780 [02:32<02:30,  3.00it/s] 42%|████▏     | 329/780 [02:32<02:24,  3.12it/s] 42%|████▏     | 330/780 [02:32<02:19,  3.22it/s] 42%|████▏     | 331/780 [02:33<02:16,  3.29it/s] 43%|████▎     | 332/780 [02:33<02:14,  3.34it/s] 43%|████▎     | 333/780 [02:33<02:12,  3.38it/s] 43%|████▎     | 334/780 [02:34<02:11,  3.40it/s] 43%|████▎     | 335/780 [02:34<02:10,  3.42it/s] 43%|████▎     | 336/780 [02:34<02:09,  3.43it/s] 43%|████▎     | 337/780 [02:34<02:08,  3.44it/s] 43%|████▎     | 338/780 [02:35<02:08,  3.45it/s] 43%|████▎     | 339/780 [02:35<02:07,  3.45it/s] 44%|████▎     | 340/780 [02:35<02:07,  3.45it/s] 44%|████▎     | 341/780 [02:36<02:07,  3.45it/s] 44%|████▍     | 342/780 [02:36<02:06,  3.45it/s] 44%|████▍     | 343/780 [02:36<02:06,  3.46it/s] 44%|████▍     | 344/780 [02:37<02:06,  3.46it/s] 44%|████▍     | 345/780 [02:37<02:05,  3.46it/s] 44%|████▍     | 346/780 [02:37<02:10,  3.33it/s] 44%|████▍     | 347/780 [02:37<02:08,  3.37it/s] 45%|████▍     | 348/780 [02:38<02:07,  3.39it/s] 45%|████▍     | 349/780 [02:38<02:06,  3.41it/s] 45%|████▍     | 350/780 [02:38<02:05,  3.43it/s] 45%|████▌     | 351/780 [02:39<02:04,  3.44it/s] 45%|████▌     | 352/780 [02:39<02:04,  3.44it/s] 45%|████▌     | 353/780 [02:39<02:03,  3.45it/s] 45%|████▌     | 354/780 [02:39<02:03,  3.45it/s] 46%|████▌     | 355/780 [02:40<02:03,  3.45it/s] 46%|████▌     | 356/780 [02:40<02:02,  3.45it/s] 46%|████▌     | 357/780 [02:40<02:06,  3.34it/s] 46%|████▌     | 358/780 [02:41<02:05,  3.37it/s] 46%|████▌     | 359/780 [02:41<02:03,  3.40it/s] 46%|████▌     | 360/780 [02:41<02:02,  3.42it/s] 46%|████▋     | 361/780 [02:42<02:02,  3.43it/s] 46%|████▋     | 362/780 [02:42<02:01,  3.44it/s] 47%|████▋     | 363/780 [02:42<02:01,  3.45it/s] 47%|████▋     | 364/780 [02:42<02:00,  3.45it/s] 47%|████▋     | 365/780 [02:43<02:00,  3.45it/s] 47%|████▋     | 366/780 [02:43<02:00,  3.45it/s] 47%|████▋     | 367/780 [02:43<01:59,  3.45it/s] 47%|████▋     | 368/780 [02:44<02:05,  3.28it/s] 47%|████▋     | 369/780 [02:44<02:03,  3.33it/s] 47%|████▋     | 370/780 [02:44<02:01,  3.37it/s] 48%|████▊     | 371/780 [02:44<02:00,  3.40it/s] 48%|████▊     | 372/780 [02:45<01:59,  3.42it/s] 48%|████▊     | 373/780 [02:45<01:58,  3.43it/s] 48%|████▊     | 374/780 [02:45<01:57,  3.44it/s] 48%|████▊     | 375/780 [02:46<01:57,  3.44it/s] 48%|████▊     | 376/780 [02:46<01:57,  3.45it/s] 48%|████▊     | 377/780 [02:46<01:56,  3.45it/s] 48%|████▊     | 378/780 [02:46<01:56,  3.45it/s] 49%|████▊     | 379/780 [02:47<02:01,  3.30it/s] 49%|████▊     | 380/780 [02:47<01:59,  3.35it/s] 49%|████▉     | 381/780 [02:47<01:58,  3.38it/s] 49%|████▉     | 382/780 [02:48<01:56,  3.40it/s] 49%|████▉     | 383/780 [02:48<01:56,  3.42it/s] 49%|████▉     | 384/780 [02:48<01:55,  3.43it/s] 49%|████▉     | 385/780 [02:49<01:54,  3.44it/s] 49%|████▉     | 386/780 [02:49<01:54,  3.44it/s] 50%|████▉     | 387/780 [02:49<01:53,  3.45it/s] 50%|████▉     | 388/780 [02:49<01:53,  3.45it/s] 50%|████▉     | 389/780 [02:50<01:53,  3.45it/s] 50%|█████     | 390/780 [02:50<01:58,  3.29it/s] 50%|█████     | 391/780 [02:50<01:56,  3.34it/s] 50%|█████     | 392/780 [02:51<01:54,  3.38it/s] 50%|█████     | 393/780 [02:51<01:53,  3.40it/s] 51%|█████     | 394/780 [02:51<01:52,  3.42it/s] 51%|█████     | 395/780 [02:51<01:52,  3.43it/s] 51%|█████     | 396/780 [02:52<01:51,  3.44it/s] 51%|█████     | 397/780 [02:52<01:51,  3.44it/s] 51%|█████     | 398/780 [02:52<01:50,  3.45it/s] 51%|█████     | 399/780 [02:53<01:50,  3.45it/s] 51%|█████▏    | 400/780 [02:53<01:50,  3.45it/s] 51%|█████▏    | 401/780 [02:53<01:53,  3.33it/s] 52%|█████▏    | 402/780 [02:54<01:52,  3.37it/s] 52%|█████▏    | 403/780 [02:54<01:51,  3.39it/s] 52%|█████▏    | 404/780 [02:54<01:50,  3.41it/s] 52%|█████▏    | 405/780 [02:54<01:49,  3.42it/s] 52%|█████▏    | 406/780 [02:55<01:49,  3.43it/s] 52%|█████▏    | 407/780 [02:55<01:48,  3.44it/s] 52%|█████▏    | 408/780 [02:55<01:48,  3.44it/s] 52%|█████▏    | 409/780 [02:56<01:47,  3.44it/s] 53%|█████▎    | 410/780 [02:56<01:47,  3.44it/s] 53%|█████▎    | 411/780 [02:56<01:47,  3.45it/s] 53%|█████▎    | 412/780 [02:57<01:54,  3.22it/s] 53%|█████▎    | 413/780 [02:57<01:51,  3.28it/s] 53%|█████▎    | 414/780 [02:57<01:49,  3.33it/s] 53%|█████▎    | 415/780 [02:57<01:48,  3.37it/s] 53%|█████▎    | 416/780 [02:58<01:47,  3.39it/s] 53%|█████▎    | 417/780 [02:58<01:46,  3.40it/s] 54%|█████▎    | 418/780 [02:58<01:45,  3.42it/s] 54%|█████▎    | 419/780 [02:59<01:45,  3.43it/s] 54%|█████▍    | 420/780 [02:59<01:44,  3.43it/s] 54%|█████▍    | 421/780 [02:59<01:44,  3.43it/s] 54%|█████▍    | 422/780 [02:59<01:44,  3.44it/s] 54%|█████▍    | 423/780 [03:00<01:48,  3.30it/s] 54%|█████▍    | 424/780 [03:00<01:46,  3.34it/s] 54%|█████▍    | 425/780 [03:00<01:45,  3.37it/s] 55%|█████▍    | 426/780 [03:01<01:44,  3.40it/s] 55%|█████▍    | 427/780 [03:01<01:43,  3.41it/s] 55%|█████▍    | 428/780 [03:01<01:42,  3.42it/s] 55%|█████▌    | 429/780 [03:01<01:42,  3.43it/s] 55%|█████▌    | 430/780 [03:02<01:41,  3.44it/s] 55%|█████▌    | 431/780 [03:02<01:41,  3.44it/s] 55%|█████▌    | 432/780 [03:02<01:41,  3.44it/s] 56%|█████▌    | 433/780 [03:03<01:40,  3.44it/s] 56%|█████▌    | 434/780 [03:03<01:42,  3.38it/s] 56%|█████▌    | 435/780 [03:03<01:41,  3.40it/s] 56%|█████▌    | 436/780 [03:04<01:40,  3.42it/s] 56%|█████▌    | 437/780 [03:04<01:40,  3.43it/s] 56%|█████▌    | 438/780 [03:04<01:39,  3.43it/s] 56%|█████▋    | 439/780 [03:04<01:39,  3.44it/s] 56%|█████▋    | 440/780 [03:05<01:39,  3.41it/s] 57%|█████▋    | 441/780 [03:05<01:39,  3.42it/s] 57%|█████▋    | 442/780 [03:05<01:38,  3.43it/s] 57%|█████▋    | 443/780 [03:06<01:38,  3.43it/s] 57%|█████▋    | 444/780 [03:06<01:37,  3.44it/s] 57%|█████▋    | 445/780 [03:06<01:37,  3.44it/s] 57%|█████▋    | 446/780 [03:06<01:37,  3.44it/s] 57%|█████▋    | 447/780 [03:07<01:36,  3.44it/s] 57%|█████▋    | 448/780 [03:07<01:36,  3.44it/s] 58%|█████▊    | 449/780 [03:07<01:36,  3.45it/s] 58%|█████▊    | 450/780 [03:08<01:35,  3.45it/s] 58%|█████▊    | 451/780 [03:08<01:39,  3.30it/s] 58%|█████▊    | 452/780 [03:08<01:38,  3.34it/s] 58%|█████▊    | 453/780 [03:09<01:37,  3.37it/s] 58%|█████▊    | 454/780 [03:09<01:36,  3.39it/s] 58%|█████▊    | 455/780 [03:09<01:35,  3.41it/s] 58%|█████▊    | 456/780 [03:09<01:34,  3.42it/s] 59%|█████▊    | 457/780 [03:10<01:34,  3.43it/s] 59%|█████▊    | 458/780 [03:10<01:33,  3.43it/s] 59%|█████▉    | 459/780 [03:10<01:33,  3.44it/s] 59%|█████▉    | 460/780 [03:11<01:33,  3.44it/s] 59%|█████▉    | 461/780 [03:11<01:32,  3.44it/s] 59%|█████▉    | 462/780 [03:11<01:37,  3.26it/s] 59%|█████▉    | 463/780 [03:11<01:35,  3.32it/s] 59%|█████▉    | 464/780 [03:12<01:34,  3.35it/s] 60%|█████▉    | 465/780 [03:12<01:33,  3.38it/s] 60%|█████▉    | 466/780 [03:12<01:32,  3.40it/s] 60%|█████▉    | 467/780 [03:13<01:31,  3.42it/s] 60%|██████    | 468/780 [03:13<01:31,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 12:26:45,191 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:26:45,191 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 12:26:45,191 >>   Batch size = 8
{'eval_loss': 0.9833757281303406, 'eval_runtime': 13.3211, 'eval_samples_per_second': 365.134, 'eval_steps_per_second': 45.642, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.28it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.18it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.46it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.73it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.43it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.14it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.86it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.37it/s][A
  8%|▊         | 48/608 [00:01<00:13, 40.18it/s][A
  9%|▊         | 53/608 [00:01<00:13, 42.01it/s][A
 10%|▉         | 58/608 [00:01<00:12, 43.27it/s][A
 10%|█         | 63/608 [00:01<00:12, 44.21it/s][A
 11%|█         | 68/608 [00:01<00:12, 44.97it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.37it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.76it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.99it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.79it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.83it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.09it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.19it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.22it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.38it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.51it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.42it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.48it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.33it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 46.13it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.14it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.30it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.35it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.44it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.51it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.46it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.52it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.54it/s][A
 30%|███       | 183/608 [00:04<00:09, 46.50it/s][A
 31%|███       | 188/608 [00:04<00:10, 41.17it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 42.55it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 43.64it/s][A
 33%|███▎      | 203/608 [00:04<00:09, 44.44it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.07it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.50it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.85it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.05it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.01it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.14it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.28it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.31it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.32it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.39it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.52it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.46it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.42it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.41it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 46.39it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.42it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.53it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.43it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.37it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.40it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.52it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.51it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.52it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 46.40it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 40.62it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 42.27it/s][A
 56%|█████▌    | 338/608 [00:07<00:06, 43.36it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 44.36it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 44.91it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.34it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.71it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 45.91it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.98it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.08it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.20it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.21it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.29it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.35it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.38it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.38it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.37it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 46.11it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.25it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.36it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.35it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.40it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.48it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.45it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.41it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.33it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.35it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.37it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 44.37it/s][A
 78%|███████▊  | 473/608 [00:10<00:03, 45.00it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.31it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.73it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.97it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.16it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.26it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.35it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 46.12it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.20it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.17it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.34it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.32it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.41it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.50it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.31it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.29it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 46.20it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 44.13it/s][A
 93%|█████████▎| 564/608 [00:12<00:00, 45.86it/s][A
 94%|█████████▎| 569/608 [00:12<00:00, 40.99it/s][A
 94%|█████████▍| 574/608 [00:12<00:00, 40.65it/s][A
 95%|█████████▌| 579/608 [00:12<00:00, 42.26it/s][A
 96%|█████████▌| 584/608 [00:12<00:00, 43.47it/s][A
 97%|█████████▋| 589/608 [00:12<00:00, 44.36it/s][A
 98%|█████████▊| 594/608 [00:13<00:00, 45.06it/s][A
 99%|█████████▊| 599/608 [00:13<00:00, 45.44it/s][A
 99%|█████████▉| 604/608 [00:13<00:00, 44.31it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.31it/s][A 60%|██████    | 468/780 [03:26<01:31,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:26:58,630 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 12:26:58,971 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:27:02,758 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:27:02,949 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:27:03,049 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:40<42:43,  8.24s/it] 60%|██████    | 470/780 [03:40<30:19,  5.87s/it] 60%|██████    | 471/780 [03:40<21:36,  4.19s/it] 61%|██████    | 472/780 [03:41<15:31,  3.02s/it] 61%|██████    | 473/780 [03:41<11:16,  2.20s/it] 61%|██████    | 474/780 [03:41<08:18,  1.63s/it] 61%|██████    | 475/780 [03:41<06:14,  1.23s/it] 61%|██████    | 476/780 [03:42<04:47,  1.06it/s] 61%|██████    | 477/780 [03:42<03:46,  1.34it/s] 61%|██████▏   | 478/780 [03:42<03:04,  1.64it/s] 61%|██████▏   | 479/780 [03:43<02:34,  1.95it/s] 62%|██████▏   | 480/780 [03:43<02:13,  2.24it/s] 62%|██████▏   | 481/780 [03:43<02:01,  2.45it/s] 62%|██████▏   | 482/780 [03:44<01:50,  2.69it/s] 62%|██████▏   | 483/780 [03:44<01:43,  2.88it/s] 62%|██████▏   | 484/780 [03:44<01:37,  3.04it/s] 62%|██████▏   | 485/780 [03:44<01:33,  3.15it/s] 62%|██████▏   | 486/780 [03:45<01:30,  3.24it/s] 62%|██████▏   | 487/780 [03:45<01:28,  3.30it/s] 63%|██████▎   | 488/780 [03:45<01:27,  3.35it/s] 63%|██████▎   | 489/780 [03:46<01:26,  3.38it/s] 63%|██████▎   | 490/780 [03:46<01:25,  3.40it/s] 63%|██████▎   | 491/780 [03:46<01:24,  3.42it/s] 63%|██████▎   | 492/780 [03:46<01:28,  3.26it/s] 63%|██████▎   | 493/780 [03:47<01:26,  3.32it/s] 63%|██████▎   | 494/780 [03:47<01:25,  3.36it/s] 63%|██████▎   | 495/780 [03:47<01:24,  3.39it/s] 64%|██████▎   | 496/780 [03:48<01:23,  3.41it/s] 64%|██████▎   | 497/780 [03:48<01:22,  3.42it/s] 64%|██████▍   | 498/780 [03:48<01:22,  3.44it/s] 64%|██████▍   | 499/780 [03:49<01:21,  3.44it/s] 64%|██████▍   | 500/780 [03:49<01:21,  3.45it/s]                                                  64%|██████▍   | 500/780 [03:49<01:21,  3.45it/s] 64%|██████▍   | 501/780 [03:49<01:20,  3.45it/s] 64%|██████▍   | 502/780 [03:49<01:20,  3.45it/s] 64%|██████▍   | 503/780 [03:50<01:22,  3.35it/s] 65%|██████▍   | 504/780 [03:50<01:21,  3.38it/s] 65%|██████▍   | 505/780 [03:50<01:20,  3.40it/s] 65%|██████▍   | 506/780 [03:51<01:20,  3.42it/s] 65%|██████▌   | 507/780 [03:51<01:19,  3.43it/s] 65%|██████▌   | 508/780 [03:51<01:19,  3.44it/s] 65%|██████▌   | 509/780 [03:51<01:18,  3.45it/s] 65%|██████▌   | 510/780 [03:52<01:18,  3.45it/s] 66%|██████▌   | 511/780 [03:52<01:17,  3.45it/s] 66%|██████▌   | 512/780 [03:52<01:17,  3.46it/s] 66%|██████▌   | 513/780 [03:53<01:17,  3.46it/s] 66%|██████▌   | 514/780 [03:53<01:19,  3.35it/s] 66%|██████▌   | 515/780 [03:53<01:18,  3.38it/s] 66%|██████▌   | 516/780 [03:53<01:17,  3.41it/s] 66%|██████▋   | 517/780 [03:54<01:16,  3.42it/s] 66%|██████▋   | 518/780 [03:54<01:16,  3.43it/s] 67%|██████▋   | 519/780 [03:54<01:15,  3.44it/s] 67%|██████▋   | 520/780 [03:55<01:15,  3.44it/s] 67%|██████▋   | 521/780 [03:55<01:15,  3.45it/s] 67%|██████▋   | 522/780 [03:55<01:14,  3.45it/s] 67%|██████▋   | 523/780 [03:56<01:14,  3.45it/s] 67%|██████▋   | 524/780 [03:56<01:14,  3.46it/s] 67%|██████▋   | 525/780 [03:56<01:17,  3.31it/s] 67%|██████▋   | 526/780 [03:56<01:15,  3.35it/s] 68%|██████▊   | 527/780 [03:57<01:14,  3.38it/s] 68%|██████▊   | 528/780 [03:57<01:14,  3.40it/s] 68%|██████▊   | 529/780 [03:57<01:13,  3.42it/s] 68%|██████▊   | 530/780 [03:58<01:12,  3.43it/s] 68%|██████▊   | 531/780 [03:58<01:12,  3.44it/s] 68%|██████▊   | 532/780 [03:58<01:12,  3.44it/s] 68%|██████▊   | 533/780 [03:58<01:11,  3.45it/s] 68%|██████▊   | 534/780 [03:59<01:11,  3.45it/s] 69%|██████▊   | 535/780 [03:59<01:10,  3.45it/s] 69%|██████▊   | 536/780 [03:59<01:12,  3.37it/s] 69%|██████▉   | 537/780 [04:00<01:11,  3.39it/s] 69%|██████▉   | 538/780 [04:00<01:10,  3.41it/s] 69%|██████▉   | 539/780 [04:00<01:10,  3.42it/s] 69%|██████▉   | 540/780 [04:00<01:09,  3.43it/s] 69%|██████▉   | 541/780 [04:01<01:09,  3.43it/s] 69%|██████▉   | 542/780 [04:01<01:09,  3.44it/s] 70%|██████▉   | 543/780 [04:01<01:08,  3.44it/s] 70%|██████▉   | 544/780 [04:02<01:08,  3.44it/s] 70%|██████▉   | 545/780 [04:02<01:08,  3.44it/s] 70%|███████   | 546/780 [04:02<01:07,  3.45it/s] 70%|███████   | 547/780 [04:03<01:08,  3.38it/s] 70%|███████   | 548/780 [04:03<01:08,  3.40it/s] 70%|███████   | 549/780 [04:03<01:07,  3.42it/s] 71%|███████   | 550/780 [04:03<01:07,  3.43it/s] 71%|███████   | 551/780 [04:04<01:06,  3.43it/s] 71%|███████   | 552/780 [04:04<01:06,  3.44it/s] 71%|███████   | 553/780 [04:04<01:05,  3.44it/s] 71%|███████   | 554/780 [04:05<01:05,  3.44it/s] 71%|███████   | 555/780 [04:05<01:05,  3.44it/s] 71%|███████▏  | 556/780 [04:05<01:04,  3.45it/s] 71%|███████▏  | 557/780 [04:05<01:04,  3.44it/s] 72%|███████▏  | 558/780 [04:06<01:06,  3.36it/s] 72%|███████▏  | 559/780 [04:06<01:05,  3.38it/s] 72%|███████▏  | 560/780 [04:06<01:04,  3.40it/s] 72%|███████▏  | 561/780 [04:07<01:04,  3.41it/s] 72%|███████▏  | 562/780 [04:07<01:03,  3.42it/s] 72%|███████▏  | 563/780 [04:07<01:03,  3.43it/s] 72%|███████▏  | 564/780 [04:08<01:02,  3.43it/s] 72%|███████▏  | 565/780 [04:08<01:02,  3.44it/s] 73%|███████▎  | 566/780 [04:08<01:02,  3.44it/s] 73%|███████▎  | 567/780 [04:08<01:01,  3.44it/s] 73%|███████▎  | 568/780 [04:09<01:01,  3.44it/s] 73%|███████▎  | 569/780 [04:09<01:01,  3.44it/s] 73%|███████▎  | 570/780 [04:09<01:02,  3.37it/s] 73%|███████▎  | 571/780 [04:10<01:01,  3.40it/s] 73%|███████▎  | 572/780 [04:10<01:01,  3.41it/s] 73%|███████▎  | 573/780 [04:10<01:00,  3.42it/s] 74%|███████▎  | 574/780 [04:10<01:00,  3.43it/s] 74%|███████▎  | 575/780 [04:11<00:59,  3.44it/s] 74%|███████▍  | 576/780 [04:11<00:59,  3.44it/s] 74%|███████▍  | 577/780 [04:11<00:58,  3.44it/s] 74%|███████▍  | 578/780 [04:12<00:58,  3.44it/s] 74%|███████▍  | 579/780 [04:12<00:58,  3.45it/s] 74%|███████▍  | 580/780 [04:12<00:57,  3.45it/s] 74%|███████▍  | 581/780 [04:13<01:01,  3.26it/s] 75%|███████▍  | 582/780 [04:13<00:59,  3.31it/s] 75%|███████▍  | 583/780 [04:13<00:58,  3.35it/s] 75%|███████▍  | 584/780 [04:13<00:58,  3.38it/s] 75%|███████▌  | 585/780 [04:14<00:57,  3.40it/s] 75%|███████▌  | 586/780 [04:14<00:56,  3.41it/s] 75%|███████▌  | 587/780 [04:14<00:56,  3.42it/s] 75%|███████▌  | 588/780 [04:15<00:55,  3.43it/s] 76%|███████▌  | 589/780 [04:15<00:55,  3.44it/s] 76%|███████▌  | 590/780 [04:15<00:55,  3.44it/s] 76%|███████▌  | 591/780 [04:15<00:54,  3.44it/s] 76%|███████▌  | 592/780 [04:16<00:58,  3.24it/s] 76%|███████▌  | 593/780 [04:16<00:56,  3.30it/s] 76%|███████▌  | 594/780 [04:16<00:55,  3.34it/s] 76%|███████▋  | 595/780 [04:17<00:54,  3.37it/s] 76%|███████▋  | 596/780 [04:17<00:54,  3.39it/s] 77%|███████▋  | 597/780 [04:17<00:53,  3.41it/s] 77%|███████▋  | 598/780 [04:18<00:53,  3.42it/s] 77%|███████▋  | 599/780 [04:18<00:52,  3.43it/s] 77%|███████▋  | 600/780 [04:18<00:52,  3.43it/s] 77%|███████▋  | 601/780 [04:18<00:52,  3.44it/s] 77%|███████▋  | 602/780 [04:19<00:51,  3.44it/s] 77%|███████▋  | 603/780 [04:19<00:53,  3.29it/s] 77%|███████▋  | 604/780 [04:19<00:52,  3.33it/s] 78%|███████▊  | 605/780 [04:20<00:51,  3.37it/s] 78%|███████▊  | 606/780 [04:20<00:51,  3.39it/s] 78%|███████▊  | 607/780 [04:20<00:50,  3.40it/s] 78%|███████▊  | 608/780 [04:20<00:50,  3.42it/s] 78%|███████▊  | 609/780 [04:21<00:49,  3.42it/s] 78%|███████▊  | 610/780 [04:21<00:49,  3.43it/s] 78%|███████▊  | 611/780 [04:21<00:49,  3.43it/s] 78%|███████▊  | 612/780 [04:22<00:48,  3.44it/s] 79%|███████▊  | 613/780 [04:22<00:48,  3.44it/s] 79%|███████▊  | 614/780 [04:22<00:49,  3.34it/s] 79%|███████▉  | 615/780 [04:23<00:48,  3.38it/s] 79%|███████▉  | 616/780 [04:23<00:48,  3.40it/s] 79%|███████▉  | 617/780 [04:23<00:47,  3.41it/s] 79%|███████▉  | 618/780 [04:23<00:47,  3.42it/s] 79%|███████▉  | 619/780 [04:24<00:46,  3.43it/s] 79%|███████▉  | 620/780 [04:24<00:46,  3.43it/s] 80%|███████▉  | 621/780 [04:24<00:46,  3.44it/s] 80%|███████▉  | 622/780 [04:25<00:45,  3.44it/s] 80%|███████▉  | 623/780 [04:25<00:45,  3.44it/s] 80%|████████  | 624/780 [04:25<00:45,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 12:27:57,427 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:27:57,427 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 12:27:57,427 >>   Batch size = 8
{'eval_loss': 0.9939451813697815, 'eval_runtime': 13.3744, 'eval_samples_per_second': 363.68, 'eval_steps_per_second': 45.46, 'epoch': 3.0}
{'loss': 0.509, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.09it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.53it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.59it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.63it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.14it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.03it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.80it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.33it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.36it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.41it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.32it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.40it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.41it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.51it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.56it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.36it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.25it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.35it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.34it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.48it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.47it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.43it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.41it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.41it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.51it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.39it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.43it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 44.54it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.12it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.49it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.84it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.02it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.14it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.37it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.28it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.19it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.21it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.24it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.40it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.47it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.40it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.45it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.36it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.45it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.35it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.32it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.28it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.31it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.37it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.39it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.48it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.53it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.55it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.29it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 46.19it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 42.20it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 43.42it/s][A
 48%|████▊     | 293/608 [00:06<00:07, 42.76it/s][A
 49%|████▉     | 298/608 [00:06<00:07, 43.71it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 44.54it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.12it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.52it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 45.77it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.98it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.11it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.17it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.24it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.33it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.33it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.20it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.31it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.28it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.43it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.43it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.51it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.45it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.38it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.37it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.35it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.31it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.38it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.36it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.39it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 43.84it/s][A
 70%|███████   | 428/608 [00:09<00:04, 44.64it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.20it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.59it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.83it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.09it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.09it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.35it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.06it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.17it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.20it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.22it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.30it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.38it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.43it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.53it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.48it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 46.21it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.28it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.28it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.35it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.30it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.29it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.34it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.37it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.44it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.25it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.19it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.24it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.38it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.30it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.30it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.27it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.38it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.40it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.43it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.31it/s][A
100%|██████████| 608/608 [00:13<00:00, 44.90it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.90it/s][A 80%|████████  | 624/780 [04:38<00:45,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:28:10,848 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 12:28:11,010 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:28:15,307 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:28:15,525 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:28:15,611 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:54<22:31,  8.72s/it] 80%|████████  | 626/780 [04:54<15:54,  6.20s/it] 80%|████████  | 627/780 [04:54<11:17,  4.43s/it] 81%|████████  | 628/780 [04:54<08:04,  3.18s/it] 81%|████████  | 629/780 [04:55<05:49,  2.32s/it] 81%|████████  | 630/780 [04:55<04:16,  1.71s/it] 81%|████████  | 631/780 [04:55<03:11,  1.28s/it] 81%|████████  | 632/780 [04:56<02:25,  1.02it/s] 81%|████████  | 633/780 [04:56<01:54,  1.29it/s] 81%|████████▏ | 634/780 [04:56<01:31,  1.59it/s] 81%|████████▏ | 635/780 [04:56<01:16,  1.89it/s] 82%|████████▏ | 636/780 [04:57<01:05,  2.19it/s] 82%|████████▏ | 637/780 [04:57<00:58,  2.44it/s] 82%|████████▏ | 638/780 [04:57<00:53,  2.67it/s] 82%|████████▏ | 639/780 [04:58<00:49,  2.87it/s] 82%|████████▏ | 640/780 [04:58<00:46,  3.02it/s] 82%|████████▏ | 641/780 [04:58<00:44,  3.14it/s] 82%|████████▏ | 642/780 [04:58<00:42,  3.23it/s] 82%|████████▏ | 643/780 [04:59<00:41,  3.30it/s] 83%|████████▎ | 644/780 [04:59<00:40,  3.34it/s] 83%|████████▎ | 645/780 [04:59<00:39,  3.38it/s] 83%|████████▎ | 646/780 [05:00<00:39,  3.40it/s] 83%|████████▎ | 647/780 [05:00<00:38,  3.42it/s] 83%|████████▎ | 648/780 [05:00<00:39,  3.32it/s] 83%|████████▎ | 649/780 [05:01<00:38,  3.36it/s] 83%|████████▎ | 650/780 [05:01<00:38,  3.39it/s] 83%|████████▎ | 651/780 [05:01<00:37,  3.41it/s] 84%|████████▎ | 652/780 [05:01<00:37,  3.43it/s] 84%|████████▎ | 653/780 [05:02<00:36,  3.43it/s] 84%|████████▍ | 654/780 [05:02<00:36,  3.44it/s] 84%|████████▍ | 655/780 [05:02<00:36,  3.45it/s] 84%|████████▍ | 656/780 [05:03<00:35,  3.45it/s] 84%|████████▍ | 657/780 [05:03<00:35,  3.45it/s] 84%|████████▍ | 658/780 [05:03<00:35,  3.46it/s] 84%|████████▍ | 659/780 [05:03<00:35,  3.40it/s] 85%|████████▍ | 660/780 [05:04<00:35,  3.42it/s] 85%|████████▍ | 661/780 [05:04<00:34,  3.43it/s] 85%|████████▍ | 662/780 [05:04<00:34,  3.44it/s] 85%|████████▌ | 663/780 [05:05<00:33,  3.45it/s] 85%|████████▌ | 664/780 [05:05<00:33,  3.45it/s] 85%|████████▌ | 665/780 [05:05<00:33,  3.45it/s] 85%|████████▌ | 666/780 [05:05<00:32,  3.45it/s] 86%|████████▌ | 667/780 [05:06<00:32,  3.46it/s] 86%|████████▌ | 668/780 [05:06<00:32,  3.46it/s] 86%|████████▌ | 669/780 [05:06<00:32,  3.46it/s] 86%|████████▌ | 670/780 [05:07<00:32,  3.39it/s] 86%|████████▌ | 671/780 [05:07<00:31,  3.41it/s] 86%|████████▌ | 672/780 [05:07<00:31,  3.43it/s] 86%|████████▋ | 673/780 [05:07<00:31,  3.44it/s] 86%|████████▋ | 674/780 [05:08<00:30,  3.44it/s] 87%|████████▋ | 675/780 [05:08<00:30,  3.45it/s] 87%|████████▋ | 676/780 [05:08<00:30,  3.45it/s] 87%|████████▋ | 677/780 [05:09<00:29,  3.45it/s] 87%|████████▋ | 678/780 [05:09<00:29,  3.46it/s] 87%|████████▋ | 679/780 [05:09<00:29,  3.45it/s] 87%|████████▋ | 680/780 [05:10<00:28,  3.46it/s] 87%|████████▋ | 681/780 [05:10<00:28,  3.46it/s] 87%|████████▋ | 682/780 [05:10<00:28,  3.46it/s] 88%|████████▊ | 683/780 [05:10<00:28,  3.46it/s] 88%|████████▊ | 684/780 [05:11<00:28,  3.39it/s] 88%|████████▊ | 685/780 [05:11<00:27,  3.41it/s] 88%|████████▊ | 686/780 [05:11<00:27,  3.43it/s] 88%|████████▊ | 687/780 [05:12<00:27,  3.44it/s] 88%|████████▊ | 688/780 [05:12<00:26,  3.44it/s] 88%|████████▊ | 689/780 [05:12<00:26,  3.45it/s] 88%|████████▊ | 690/780 [05:12<00:26,  3.44it/s] 89%|████████▊ | 691/780 [05:13<00:25,  3.45it/s] 89%|████████▊ | 692/780 [05:13<00:25,  3.45it/s] 89%|████████▉ | 693/780 [05:13<00:25,  3.45it/s] 89%|████████▉ | 694/780 [05:14<00:24,  3.45it/s] 89%|████████▉ | 695/780 [05:14<00:26,  3.22it/s] 89%|████████▉ | 696/780 [05:14<00:25,  3.29it/s] 89%|████████▉ | 697/780 [05:15<00:24,  3.33it/s] 89%|████████▉ | 698/780 [05:15<00:24,  3.37it/s] 90%|████████▉ | 699/780 [05:15<00:25,  3.21it/s] 90%|████████▉ | 700/780 [05:15<00:24,  3.27it/s] 90%|████████▉ | 701/780 [05:16<00:23,  3.33it/s] 90%|█████████ | 702/780 [05:16<00:23,  3.36it/s] 90%|█████████ | 703/780 [05:16<00:22,  3.39it/s] 90%|█████████ | 704/780 [05:17<00:22,  3.41it/s] 90%|█████████ | 705/780 [05:17<00:23,  3.25it/s] 91%|█████████ | 706/780 [05:17<00:22,  3.31it/s] 91%|█████████ | 707/780 [05:18<00:21,  3.35it/s] 91%|█████████ | 708/780 [05:18<00:21,  3.38it/s] 91%|█████████ | 709/780 [05:18<00:20,  3.40it/s] 91%|█████████ | 710/780 [05:18<00:20,  3.42it/s] 91%|█████████ | 711/780 [05:19<00:20,  3.43it/s] 91%|█████████▏| 712/780 [05:19<00:19,  3.44it/s] 91%|█████████▏| 713/780 [05:19<00:19,  3.44it/s] 92%|█████████▏| 714/780 [05:20<00:19,  3.44it/s] 92%|█████████▏| 715/780 [05:20<00:18,  3.44it/s] 92%|█████████▏| 716/780 [05:20<00:19,  3.23it/s] 92%|█████████▏| 717/780 [05:20<00:19,  3.29it/s] 92%|█████████▏| 718/780 [05:21<00:18,  3.34it/s] 92%|█████████▏| 719/780 [05:21<00:18,  3.37it/s] 92%|█████████▏| 720/780 [05:21<00:17,  3.40it/s] 92%|█████████▏| 721/780 [05:22<00:17,  3.41it/s] 93%|█████████▎| 722/780 [05:22<00:16,  3.42it/s] 93%|█████████▎| 723/780 [05:22<00:16,  3.43it/s] 93%|█████████▎| 724/780 [05:23<00:16,  3.43it/s] 93%|█████████▎| 725/780 [05:23<00:16,  3.44it/s] 93%|█████████▎| 726/780 [05:23<00:15,  3.44it/s] 93%|█████████▎| 727/780 [05:23<00:15,  3.33it/s] 93%|█████████▎| 728/780 [05:24<00:15,  3.37it/s] 93%|█████████▎| 729/780 [05:24<00:15,  3.39it/s] 94%|█████████▎| 730/780 [05:24<00:14,  3.41it/s] 94%|█████████▎| 731/780 [05:25<00:14,  3.42it/s] 94%|█████████▍| 732/780 [05:25<00:14,  3.43it/s] 94%|█████████▍| 733/780 [05:25<00:13,  3.43it/s] 94%|█████████▍| 734/780 [05:25<00:13,  3.44it/s] 94%|█████████▍| 735/780 [05:26<00:13,  3.44it/s] 94%|█████████▍| 736/780 [05:26<00:12,  3.44it/s] 94%|█████████▍| 737/780 [05:26<00:12,  3.45it/s] 95%|█████████▍| 738/780 [05:27<00:12,  3.32it/s] 95%|█████████▍| 739/780 [05:27<00:12,  3.36it/s] 95%|█████████▍| 740/780 [05:27<00:11,  3.38it/s] 95%|█████████▌| 741/780 [05:28<00:11,  3.40it/s] 95%|█████████▌| 742/780 [05:28<00:11,  3.42it/s] 95%|█████████▌| 743/780 [05:28<00:10,  3.43it/s] 95%|█████████▌| 744/780 [05:28<00:10,  3.43it/s] 96%|█████████▌| 745/780 [05:29<00:10,  3.44it/s] 96%|█████████▌| 746/780 [05:29<00:09,  3.44it/s] 96%|█████████▌| 747/780 [05:29<00:09,  3.44it/s] 96%|█████████▌| 748/780 [05:30<00:09,  3.44it/s] 96%|█████████▌| 749/780 [05:30<00:09,  3.32it/s] 96%|█████████▌| 750/780 [05:30<00:08,  3.35it/s] 96%|█████████▋| 751/780 [05:30<00:08,  3.38it/s] 96%|█████████▋| 752/780 [05:31<00:08,  3.40it/s] 97%|█████████▋| 753/780 [05:31<00:07,  3.41it/s] 97%|█████████▋| 754/780 [05:31<00:07,  3.42it/s] 97%|█████████▋| 755/780 [05:32<00:07,  3.32it/s] 97%|█████████▋| 756/780 [05:32<00:07,  3.35it/s] 97%|█████████▋| 757/780 [05:32<00:06,  3.38it/s] 97%|█████████▋| 758/780 [05:33<00:06,  3.40it/s] 97%|█████████▋| 759/780 [05:33<00:06,  3.41it/s] 97%|█████████▋| 760/780 [05:33<00:06,  3.28it/s] 98%|█████████▊| 761/780 [05:33<00:05,  3.32it/s] 98%|█████████▊| 762/780 [05:34<00:05,  3.36it/s] 98%|█████████▊| 763/780 [05:34<00:05,  3.39it/s] 98%|█████████▊| 764/780 [05:34<00:04,  3.40it/s] 98%|█████████▊| 765/780 [05:35<00:04,  3.42it/s] 98%|█████████▊| 766/780 [05:35<00:04,  3.43it/s] 98%|█████████▊| 767/780 [05:35<00:03,  3.43it/s] 98%|█████████▊| 768/780 [05:35<00:03,  3.44it/s] 99%|█████████▊| 769/780 [05:36<00:03,  3.44it/s] 99%|█████████▊| 770/780 [05:36<00:02,  3.44it/s] 99%|█████████▉| 771/780 [05:36<00:02,  3.34it/s] 99%|█████████▉| 772/780 [05:37<00:02,  3.37it/s] 99%|█████████▉| 773/780 [05:37<00:02,  3.40it/s] 99%|█████████▉| 774/780 [05:37<00:01,  3.41it/s] 99%|█████████▉| 775/780 [05:38<00:01,  3.42it/s] 99%|█████████▉| 776/780 [05:38<00:01,  3.43it/s]100%|█████████▉| 777/780 [05:38<00:00,  3.44it/s]100%|█████████▉| 778/780 [05:38<00:00,  3.44it/s]100%|█████████▉| 779/780 [05:39<00:00,  3.44it/s]100%|██████████| 780/780 [05:39<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 12:29:11,199 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:29:11,200 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 12:29:11,200 >>   Batch size = 8
{'eval_loss': 1.0016800165176392, 'eval_runtime': 13.2265, 'eval_samples_per_second': 367.747, 'eval_steps_per_second': 45.968, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.24it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.47it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.69it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.85it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.41it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.07it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.79it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.66it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.57it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.53it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.61it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.56it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.58it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.48it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.47it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.47it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.40it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 46.42it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 44.46it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 44.94it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.41it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.80it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.99it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.22it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.22it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.27it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.32it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.27it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.32it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.34it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.35it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.41it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.48it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.48it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.47it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.46it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.46it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.42it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.50it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.51it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.53it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.45it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.40it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.38it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.44it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.40it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 44.44it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.11it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.47it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.79it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.08it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.14it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.22it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.30it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.18it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.22it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.28it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.40it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.49it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.44it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.42it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.52it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.45it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.47it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.34it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.35it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.44it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.50it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.47it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.45it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.47it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.58it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.53it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.44it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 42.78it/s][A
 63%|██████▎   | 383/608 [00:08<00:05, 43.87it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 44.63it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.27it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.50it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.86it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.11it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.26it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.11it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 46.18it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.15it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.37it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.40it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.43it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.44it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.40it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.46it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.31it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.23it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.24it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.18it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.33it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.41it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.49it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.45it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.50it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.45it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.49it/s][A
 85%|████████▌ | 518/608 [00:11<00:02, 43.89it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 44.64it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.19it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.64it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.91it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.06it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.21it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.26it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.20it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.26it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.28it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.37it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.38it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.46it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.51it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 42.20it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 43.89it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 44.89it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.40it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.40it/s][A100%|██████████| 780/780 [05:52<00:00,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:29:24,743 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 12:29:24,978 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:29:28,949 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:29:29,178 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:29:29,283 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 12:29:38,009 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 12:29:38,034 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156 (score: 0.9717044830322266).
                                                 100%|██████████| 780/780 [06:16<00:00,  3.44it/s]100%|██████████| 780/780 [06:16<00:00,  2.07it/s]
[INFO|trainer.py:1894] 2023-08-29 12:29:48,217 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 12:29:48,509 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:29:53,181 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:29:53,858 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:29:54,054 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 12:29:54,791 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:29:54,791 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:29:54,791 >>   train_loss               =     0.5002
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:29:54,791 >>   train_runtime            = 0:06:16.28
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:29:54,792 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:29:54,792 >>   train_samples_per_second =    132.866
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:29:54,792 >>   train_steps_per_second   =      2.073
{'eval_loss': 1.0106821060180664, 'eval_runtime': 13.204, 'eval_samples_per_second': 368.372, 'eval_steps_per_second': 46.047, 'epoch': 5.0}
{'train_runtime': 376.2813, 'train_samples_per_second': 132.866, 'train_steps_per_second': 2.073, 'train_loss': 0.5001915173652844, 'epoch': 5.0}
08/29/2023 12:29:55 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 12:29:55,349 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:29:55,349 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 12:29:55,349 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 57.83it/s]  2%|▏         | 12/608 [00:00<00:11, 51.00it/s]  3%|▎         | 18/608 [00:00<00:11, 49.18it/s]  4%|▍         | 23/608 [00:00<00:12, 48.31it/s]  5%|▍         | 28/608 [00:00<00:12, 47.93it/s]  5%|▌         | 33/608 [00:00<00:12, 47.64it/s]  6%|▋         | 38/608 [00:00<00:12, 47.46it/s]  7%|▋         | 43/608 [00:00<00:11, 47.34it/s]  8%|▊         | 48/608 [00:01<00:14, 37.53it/s]  9%|▊         | 53/608 [00:01<00:13, 40.04it/s] 10%|▉         | 58/608 [00:01<00:13, 41.90it/s] 10%|█         | 63/608 [00:01<00:12, 43.32it/s] 11%|█         | 68/608 [00:01<00:12, 44.38it/s] 12%|█▏        | 73/608 [00:01<00:11, 45.06it/s] 13%|█▎        | 78/608 [00:01<00:11, 45.59it/s] 14%|█▎        | 83/608 [00:01<00:11, 46.04it/s] 14%|█▍        | 88/608 [00:01<00:11, 46.17it/s] 15%|█▌        | 93/608 [00:02<00:11, 46.25it/s] 16%|█▌        | 98/608 [00:02<00:10, 46.58it/s] 17%|█▋        | 103/608 [00:02<00:10, 46.74it/s] 18%|█▊        | 108/608 [00:02<00:10, 46.77it/s] 19%|█▊        | 113/608 [00:02<00:10, 46.79it/s] 19%|█▉        | 118/608 [00:02<00:10, 46.82it/s] 20%|██        | 123/608 [00:02<00:10, 46.85it/s] 21%|██        | 128/608 [00:02<00:10, 46.78it/s] 22%|██▏       | 133/608 [00:02<00:10, 46.67it/s] 23%|██▎       | 138/608 [00:03<00:10, 46.55it/s] 24%|██▎       | 143/608 [00:03<00:09, 46.67it/s] 24%|██▍       | 148/608 [00:03<00:09, 46.74it/s] 25%|██▌       | 153/608 [00:03<00:09, 46.73it/s] 26%|██▌       | 158/608 [00:03<00:09, 46.78it/s] 27%|██▋       | 163/608 [00:03<00:09, 46.82it/s] 28%|██▊       | 168/608 [00:03<00:09, 46.81it/s] 28%|██▊       | 173/608 [00:03<00:09, 46.85it/s] 29%|██▉       | 178/608 [00:03<00:09, 46.67it/s] 30%|███       | 183/608 [00:03<00:09, 46.55it/s] 31%|███       | 188/608 [00:04<00:09, 44.29it/s] 32%|███▏      | 193/608 [00:04<00:09, 45.10it/s] 33%|███▎      | 198/608 [00:04<00:08, 45.72it/s] 33%|███▎      | 203/608 [00:04<00:08, 46.06it/s] 34%|███▍      | 208/608 [00:04<00:08, 46.26it/s] 35%|███▌      | 213/608 [00:04<00:08, 46.49it/s] 36%|███▌      | 218/608 [00:04<00:08, 46.65it/s] 37%|███▋      | 223/608 [00:04<00:08, 46.63it/s] 38%|███▊      | 228/608 [00:04<00:08, 46.33it/s] 38%|███▊      | 233/608 [00:05<00:08, 46.41it/s] 39%|███▉      | 238/608 [00:05<00:07, 46.54it/s] 40%|███▉      | 243/608 [00:05<00:07, 46.65it/s] 41%|████      | 248/608 [00:05<00:07, 46.77it/s] 42%|████▏     | 253/608 [00:05<00:07, 46.83it/s] 42%|████▏     | 258/608 [00:05<00:07, 46.94it/s] 43%|████▎     | 263/608 [00:05<00:07, 46.95it/s] 44%|████▍     | 268/608 [00:05<00:07, 47.00it/s] 45%|████▍     | 273/608 [00:05<00:07, 46.82it/s] 46%|████▌     | 278/608 [00:06<00:07, 46.69it/s] 47%|████▋     | 283/608 [00:06<00:06, 46.69it/s] 47%|████▋     | 288/608 [00:06<00:06, 46.83it/s] 48%|████▊     | 293/608 [00:06<00:06, 46.83it/s] 49%|████▉     | 298/608 [00:06<00:06, 46.89it/s] 50%|████▉     | 303/608 [00:06<00:06, 46.89it/s] 51%|█████     | 308/608 [00:06<00:06, 46.94it/s] 51%|█████▏    | 313/608 [00:06<00:06, 46.93it/s] 52%|█████▏    | 318/608 [00:06<00:06, 46.89it/s] 53%|█████▎    | 323/608 [00:06<00:06, 46.65it/s] 54%|█████▍    | 328/608 [00:07<00:06, 45.92it/s] 55%|█████▍    | 333/608 [00:07<00:05, 46.30it/s] 56%|█████▌    | 338/608 [00:07<00:05, 46.44it/s] 56%|█████▋    | 343/608 [00:07<00:05, 46.64it/s] 57%|█████▋    | 348/608 [00:07<00:05, 46.67it/s] 58%|█████▊    | 353/608 [00:07<00:05, 46.85it/s] 59%|█████▉    | 358/608 [00:07<00:05, 46.99it/s] 60%|█████▉    | 363/608 [00:07<00:05, 46.98it/s] 61%|██████    | 368/608 [00:07<00:05, 46.84it/s] 61%|██████▏   | 373/608 [00:08<00:05, 46.79it/s] 62%|██████▏   | 378/608 [00:08<00:04, 46.89it/s] 63%|██████▎   | 383/608 [00:08<00:04, 46.95it/s] 64%|██████▍   | 388/608 [00:08<00:04, 47.03it/s] 65%|██████▍   | 393/608 [00:08<00:05, 42.75it/s] 65%|██████▌   | 398/608 [00:08<00:04, 44.66it/s] 66%|██████▋   | 403/608 [00:08<00:04, 45.33it/s] 67%|██████▋   | 408/608 [00:08<00:04, 45.90it/s] 68%|██████▊   | 413/608 [00:08<00:04, 46.28it/s] 69%|██████▉   | 418/608 [00:09<00:04, 46.45it/s] 70%|██████▉   | 423/608 [00:09<00:03, 46.64it/s] 70%|███████   | 428/608 [00:09<00:03, 46.87it/s] 71%|███████   | 433/608 [00:09<00:03, 46.91it/s] 72%|███████▏  | 438/608 [00:09<00:03, 46.71it/s] 73%|███████▎  | 443/608 [00:09<00:03, 46.60it/s] 74%|███████▎  | 448/608 [00:09<00:03, 46.78it/s] 75%|███████▍  | 453/608 [00:09<00:03, 46.91it/s] 75%|███████▌  | 458/608 [00:09<00:03, 46.95it/s] 76%|███████▌  | 463/608 [00:09<00:03, 46.99it/s] 77%|███████▋  | 468/608 [00:10<00:03, 43.52it/s] 78%|███████▊  | 473/608 [00:10<00:03, 44.51it/s] 79%|███████▊  | 478/608 [00:10<00:02, 45.30it/s] 79%|███████▉  | 483/608 [00:10<00:02, 45.85it/s] 80%|████████  | 488/608 [00:10<00:02, 46.14it/s] 81%|████████  | 493/608 [00:10<00:02, 46.49it/s] 82%|████████▏ | 498/608 [00:10<00:02, 46.61it/s] 83%|████████▎ | 503/608 [00:10<00:02, 46.75it/s] 84%|████████▎ | 508/608 [00:10<00:02, 46.65it/s] 84%|████████▍ | 513/608 [00:11<00:02, 46.67it/s] 85%|████████▌ | 518/608 [00:11<00:01, 46.66it/s] 86%|████████▌ | 523/608 [00:11<00:01, 46.74it/s] 87%|████████▋ | 528/608 [00:11<00:01, 46.83it/s] 88%|████████▊ | 533/608 [00:11<00:01, 46.87it/s] 88%|████████▊ | 538/608 [00:11<00:01, 46.90it/s] 89%|████████▉ | 543/608 [00:11<00:01, 46.89it/s] 90%|█████████ | 548/608 [00:11<00:01, 46.90it/s] 91%|█████████ | 553/608 [00:11<00:01, 46.72it/s] 92%|█████████▏| 558/608 [00:12<00:01, 46.71it/s] 93%|█████████▎| 563/608 [00:12<00:00, 46.74it/s] 93%|█████████▎| 568/608 [00:12<00:00, 46.77it/s] 94%|█████████▍| 573/608 [00:12<00:00, 46.82it/s] 95%|█████████▌| 578/608 [00:12<00:00, 46.90it/s] 96%|█████████▌| 583/608 [00:12<00:00, 46.98it/s] 97%|█████████▋| 588/608 [00:12<00:00, 46.91it/s] 98%|█████████▊| 593/608 [00:12<00:00, 46.96it/s] 98%|█████████▊| 598/608 [00:12<00:00, 46.89it/s] 99%|█████████▉| 603/608 [00:13<00:00, 46.81it/s]100%|██████████| 608/608 [00:13<00:00, 43.04it/s]100%|██████████| 608/608 [00:13<00:00, 46.21it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 12:30:08,526 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:30:08,526 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:30:08,526 >>   eval_loss               =     0.9717
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:30:08,527 >>   eval_runtime            = 0:00:13.17
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:30:08,527 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:30:08,527 >>   eval_samples_per_second =    369.115
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:30:08,527 >>   eval_steps_per_second   =     46.139
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:30:08,527 >>   perplexity              =     2.6424
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:30:18,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:30:18,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:30:18,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:30:18,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:30:18,350 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:30:18,890 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:30:18,891 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:30:19,224 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:30:20,373 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:30:20,373 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:30:22,278 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:30:22,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:30:22,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:30:22,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:30:22,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:30:22,857 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:30:22,858 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:30:23,191 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:30:23,421 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:30:23,421 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.57it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.63it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.65it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:14,  1.50it/s]Extractor Predicting: 25it [00:15,  1.46it/s]Extractor Predicting: 26it [00:16,  1.46it/s]Extractor Predicting: 27it [00:16,  1.48it/s]Extractor Predicting: 28it [00:17,  1.46it/s]Extractor Predicting: 29it [00:18,  1.49it/s]Extractor Predicting: 30it [00:18,  1.53it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:20,  1.54it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:22,  1.53it/s]Extractor Predicting: 37it [00:23,  1.52it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:24,  1.51it/s]Extractor Predicting: 40it [00:25,  1.51it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:29,  1.53it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:30,  1.55it/s]Extractor Predicting: 49it [00:31,  1.53it/s]Extractor Predicting: 50it [00:32,  1.53it/s]Extractor Predicting: 51it [00:32,  1.40it/s]Extractor Predicting: 52it [00:33,  1.43it/s]Extractor Predicting: 53it [00:34,  1.44it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.54it/s]Extractor Predicting: 56it [00:36,  1.52it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:37,  1.51it/s]Extractor Predicting: 59it [00:38,  1.52it/s]Extractor Predicting: 60it [00:38,  1.51it/s]Extractor Predicting: 61it [00:39,  1.45it/s]Extractor Predicting: 62it [00:40,  1.47it/s]Extractor Predicting: 63it [00:40,  1.48it/s]Extractor Predicting: 64it [00:41,  1.51it/s]Extractor Predicting: 65it [00:42,  1.50it/s]Extractor Predicting: 66it [00:42,  1.51it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.49it/s]Extractor Predicting: 69it [00:44,  1.49it/s]Extractor Predicting: 70it [00:45,  1.52it/s]Extractor Predicting: 71it [00:46,  1.55it/s]Extractor Predicting: 72it [00:46,  1.54it/s]Extractor Predicting: 73it [00:47,  1.56it/s]Extractor Predicting: 74it [00:48,  1.54it/s]Extractor Predicting: 75it [00:48,  1.55it/s]Extractor Predicting: 76it [00:49,  1.52it/s]Extractor Predicting: 77it [00:50,  1.52it/s]Extractor Predicting: 78it [00:50,  1.55it/s]Extractor Predicting: 79it [00:51,  1.55it/s]Extractor Predicting: 80it [00:51,  1.57it/s]Extractor Predicting: 81it [00:52,  1.53it/s]Extractor Predicting: 82it [00:53,  1.52it/s]Extractor Predicting: 83it [00:53,  1.51it/s]Extractor Predicting: 84it [00:54,  1.53it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:55,  1.51it/s]Extractor Predicting: 87it [00:56,  1.54it/s]Extractor Predicting: 88it [00:57,  1.53it/s]Extractor Predicting: 89it [00:57,  1.47it/s]Extractor Predicting: 90it [00:58,  1.51it/s]Extractor Predicting: 91it [00:59,  1.53it/s]Extractor Predicting: 92it [00:59,  1.54it/s]Extractor Predicting: 93it [01:00,  1.56it/s]Extractor Predicting: 94it [01:01,  1.54it/s]Extractor Predicting: 95it [01:01,  1.50it/s]Extractor Predicting: 96it [01:02,  1.52it/s]Extractor Predicting: 97it [01:03,  1.52it/s]Extractor Predicting: 98it [01:03,  1.53it/s]Extractor Predicting: 99it [01:04,  1.55it/s]Extractor Predicting: 100it [01:05,  1.54it/s]Extractor Predicting: 101it [01:05,  1.55it/s]Extractor Predicting: 102it [01:06,  1.57it/s]Extractor Predicting: 103it [01:06,  1.57it/s]Extractor Predicting: 104it [01:07,  1.56it/s]Extractor Predicting: 105it [01:08,  1.54it/s]Extractor Predicting: 106it [01:08,  1.54it/s]Extractor Predicting: 107it [01:09,  1.55it/s]Extractor Predicting: 108it [01:10,  1.56it/s]Extractor Predicting: 109it [01:10,  1.53it/s]Extractor Predicting: 110it [01:11,  1.50it/s]Extractor Predicting: 111it [01:12,  1.52it/s]Extractor Predicting: 112it [01:12,  1.52it/s]Extractor Predicting: 113it [01:13,  1.53it/s]Extractor Predicting: 114it [01:14,  1.56it/s]Extractor Predicting: 115it [01:14,  1.62it/s]Extractor Predicting: 116it [01:15,  1.62it/s]Extractor Predicting: 117it [01:15,  1.62it/s]Extractor Predicting: 118it [01:16,  1.62it/s]Extractor Predicting: 119it [01:17,  1.59it/s]Extractor Predicting: 120it [01:17,  1.62it/s]Extractor Predicting: 121it [01:18,  1.59it/s]Extractor Predicting: 122it [01:19,  1.54it/s]Extractor Predicting: 123it [01:19,  1.49it/s]Extractor Predicting: 124it [01:20,  1.46it/s]Extractor Predicting: 125it [01:21,  1.49it/s]Extractor Predicting: 126it [01:21,  1.51it/s]Extractor Predicting: 127it [01:22,  1.52it/s]Extractor Predicting: 128it [01:23,  1.45it/s]Extractor Predicting: 129it [01:23,  1.48it/s]Extractor Predicting: 130it [01:24,  1.50it/s]Extractor Predicting: 131it [01:25,  1.55it/s]Extractor Predicting: 132it [01:25,  1.51it/s]Extractor Predicting: 133it [01:26,  1.48it/s]Extractor Predicting: 134it [01:27,  1.49it/s]Extractor Predicting: 135it [01:27,  1.52it/s]Extractor Predicting: 136it [01:28,  1.53it/s]Extractor Predicting: 137it [01:29,  1.55it/s]Extractor Predicting: 138it [01:29,  1.52it/s]Extractor Predicting: 139it [01:30,  1.53it/s]Extractor Predicting: 140it [01:31,  1.52it/s]Extractor Predicting: 141it [01:31,  1.56it/s]Extractor Predicting: 142it [01:32,  1.53it/s]Extractor Predicting: 143it [01:32,  1.56it/s]Extractor Predicting: 144it [01:33,  1.56it/s]Extractor Predicting: 145it [01:34,  1.42it/s]Extractor Predicting: 146it [01:35,  1.41it/s]Extractor Predicting: 147it [01:35,  1.42it/s]Extractor Predicting: 148it [01:36,  1.44it/s]Extractor Predicting: 149it [01:37,  1.45it/s]Extractor Predicting: 150it [01:37,  1.46it/s]Extractor Predicting: 151it [01:38,  1.46it/s]Extractor Predicting: 152it [01:39,  1.51it/s]Extractor Predicting: 153it [01:39,  1.49it/s]Extractor Predicting: 154it [01:40,  1.50it/s]Extractor Predicting: 155it [01:41,  1.51it/s]Extractor Predicting: 156it [01:41,  1.54it/s]Extractor Predicting: 157it [01:42,  1.54it/s]Extractor Predicting: 158it [01:43,  1.56it/s]Extractor Predicting: 159it [01:43,  1.54it/s]Extractor Predicting: 160it [01:44,  1.53it/s]Extractor Predicting: 161it [01:45,  1.51it/s]Extractor Predicting: 162it [01:45,  1.49it/s]Extractor Predicting: 163it [01:46,  1.51it/s]Extractor Predicting: 164it [01:47,  1.50it/s]Extractor Predicting: 165it [01:47,  1.51it/s]Extractor Predicting: 166it [01:48,  1.49it/s]Extractor Predicting: 167it [01:49,  1.48it/s]Extractor Predicting: 168it [01:49,  1.46it/s]Extractor Predicting: 169it [01:50,  1.43it/s]Extractor Predicting: 170it [01:51,  1.44it/s]Extractor Predicting: 171it [01:51,  1.44it/s]Extractor Predicting: 172it [01:52,  1.46it/s]Extractor Predicting: 173it [01:53,  1.44it/s]Extractor Predicting: 174it [01:54,  1.42it/s]Extractor Predicting: 175it [01:54,  1.45it/s]Extractor Predicting: 176it [01:55,  1.44it/s]Extractor Predicting: 177it [01:56,  1.42it/s]Extractor Predicting: 178it [01:56,  1.41it/s]Extractor Predicting: 179it [01:57,  1.39it/s]Extractor Predicting: 180it [01:58,  1.39it/s]Extractor Predicting: 181it [01:59,  1.41it/s]Extractor Predicting: 182it [01:59,  1.41it/s]Extractor Predicting: 183it [02:00,  1.41it/s]Extractor Predicting: 184it [02:01,  1.42it/s]Extractor Predicting: 185it [02:01,  1.45it/s]Extractor Predicting: 186it [02:02,  1.45it/s]Extractor Predicting: 187it [02:03,  1.54it/s]Extractor Predicting: 187it [02:03,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:32:42,807 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:32:42,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:32:42,840 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:32:42,840 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:32:42,840 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:32:43,743 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:32:43,744 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:32:44,371 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:32:45,461 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:32:45,462 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:32:48,515 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:32:48,537 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:32:48,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:32:48,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:32:48,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:32:49,501 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:32:49,503 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:32:50,192 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:32:50,478 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:32:50,478 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.29296875,
  "recall": 0.015419407894736841,
  "score": 0.029296874999999997,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.49it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.48it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.50it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:20,  1.45it/s]Extractor Predicting: 32it [00:21,  1.45it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:23,  1.51it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.47it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.51it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:27,  1.50it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:29,  1.52it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:31,  1.48it/s]Extractor Predicting: 48it [00:31,  1.48it/s]Extractor Predicting: 49it [00:32,  1.45it/s]Extractor Predicting: 50it [00:33,  1.47it/s]Extractor Predicting: 51it [00:33,  1.46it/s]Extractor Predicting: 52it [00:34,  1.47it/s]Extractor Predicting: 53it [00:35,  1.53it/s]Extractor Predicting: 54it [00:35,  1.53it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:37,  1.53it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:39,  1.54it/s]Extractor Predicting: 60it [00:39,  1.56it/s]Extractor Predicting: 61it [00:40,  1.53it/s]Extractor Predicting: 62it [00:41,  1.50it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.52it/s]Extractor Predicting: 66it [00:43,  1.54it/s]Extractor Predicting: 67it [00:44,  1.54it/s]Extractor Predicting: 68it [00:45,  1.45it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:46,  1.46it/s]Extractor Predicting: 71it [00:47,  1.49it/s]Extractor Predicting: 72it [00:47,  1.48it/s]Extractor Predicting: 73it [00:48,  1.54it/s]Extractor Predicting: 74it [00:49,  1.55it/s]Extractor Predicting: 75it [00:49,  1.56it/s]Extractor Predicting: 76it [00:50,  1.52it/s]Extractor Predicting: 77it [00:51,  1.53it/s]Extractor Predicting: 78it [00:51,  1.57it/s]Extractor Predicting: 79it [00:52,  1.55it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:53,  1.57it/s]Extractor Predicting: 82it [00:54,  1.54it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:55,  1.51it/s]Extractor Predicting: 85it [00:56,  1.55it/s]Extractor Predicting: 86it [00:56,  1.57it/s]Extractor Predicting: 87it [00:57,  1.58it/s]Extractor Predicting: 88it [00:57,  1.61it/s]Extractor Predicting: 89it [00:58,  1.60it/s]Extractor Predicting: 90it [00:59,  1.55it/s]Extractor Predicting: 91it [00:59,  1.60it/s]Extractor Predicting: 92it [01:00,  1.61it/s]Extractor Predicting: 93it [01:01,  1.58it/s]Extractor Predicting: 94it [01:01,  1.61it/s]Extractor Predicting: 95it [01:02,  1.61it/s]Extractor Predicting: 96it [01:02,  1.63it/s]Extractor Predicting: 97it [01:03,  1.58it/s]Extractor Predicting: 98it [01:04,  1.58it/s]Extractor Predicting: 99it [01:04,  1.57it/s]Extractor Predicting: 100it [01:05,  1.59it/s]Extractor Predicting: 101it [01:06,  1.59it/s]Extractor Predicting: 102it [01:06,  1.58it/s]Extractor Predicting: 103it [01:07,  1.55it/s]Extractor Predicting: 104it [01:08,  1.56it/s]Extractor Predicting: 105it [01:08,  1.54it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:10,  1.55it/s]Extractor Predicting: 108it [01:10,  1.56it/s]Extractor Predicting: 109it [01:11,  1.53it/s]Extractor Predicting: 110it [01:12,  1.51it/s]Extractor Predicting: 111it [01:12,  1.50it/s]Extractor Predicting: 112it [01:13,  1.54it/s]Extractor Predicting: 113it [01:14,  1.54it/s]Extractor Predicting: 114it [01:14,  1.56it/s]Extractor Predicting: 115it [01:15,  1.52it/s]Extractor Predicting: 116it [01:15,  1.53it/s]Extractor Predicting: 117it [01:16,  1.53it/s]Extractor Predicting: 118it [01:17,  1.54it/s]Extractor Predicting: 119it [01:17,  1.55it/s]Extractor Predicting: 120it [01:18,  1.51it/s]Extractor Predicting: 121it [01:19,  1.54it/s]Extractor Predicting: 122it [01:19,  1.55it/s]Extractor Predicting: 123it [01:20,  1.55it/s]Extractor Predicting: 124it [01:21,  1.57it/s]Extractor Predicting: 125it [01:21,  1.54it/s]Extractor Predicting: 126it [01:22,  1.54it/s]Extractor Predicting: 127it [01:23,  1.55it/s]Extractor Predicting: 128it [01:23,  1.52it/s]Extractor Predicting: 129it [01:24,  1.50it/s]Extractor Predicting: 130it [01:25,  1.53it/s]Extractor Predicting: 131it [01:25,  1.53it/s]Extractor Predicting: 132it [01:26,  1.52it/s]Extractor Predicting: 133it [01:27,  1.54it/s]Extractor Predicting: 134it [01:27,  1.53it/s]Extractor Predicting: 135it [01:28,  1.53it/s]Extractor Predicting: 136it [01:28,  1.54it/s]Extractor Predicting: 137it [01:29,  1.52it/s]Extractor Predicting: 138it [01:30,  1.51it/s]Extractor Predicting: 139it [01:30,  1.53it/s]Extractor Predicting: 140it [01:31,  1.51it/s]Extractor Predicting: 141it [01:32,  1.33it/s]Extractor Predicting: 142it [01:33,  1.38it/s]Extractor Predicting: 143it [01:33,  1.42it/s]Extractor Predicting: 144it [01:34,  1.44it/s]Extractor Predicting: 145it [01:35,  1.43it/s]Extractor Predicting: 146it [01:35,  1.45it/s]Extractor Predicting: 147it [01:36,  1.46it/s]Extractor Predicting: 148it [01:37,  1.48it/s]Extractor Predicting: 149it [01:37,  1.48it/s]Extractor Predicting: 150it [01:38,  1.43it/s]Extractor Predicting: 151it [01:39,  1.43it/s]Extractor Predicting: 152it [01:40,  1.45it/s]Extractor Predicting: 153it [01:40,  1.50it/s]Extractor Predicting: 154it [01:41,  1.51it/s]Extractor Predicting: 155it [01:41,  1.54it/s]Extractor Predicting: 156it [01:42,  1.50it/s]Extractor Predicting: 157it [01:43,  1.52it/s]Extractor Predicting: 158it [01:44,  1.51it/s]Extractor Predicting: 159it [01:44,  1.50it/s]Extractor Predicting: 160it [01:45,  1.51it/s]Extractor Predicting: 161it [01:46,  1.49it/s]Extractor Predicting: 162it [01:46,  1.49it/s]Extractor Predicting: 163it [01:47,  1.51it/s]Extractor Predicting: 164it [01:48,  1.51it/s]Extractor Predicting: 165it [01:48,  1.52it/s]Extractor Predicting: 166it [01:49,  1.52it/s]Extractor Predicting: 167it [01:49,  1.52it/s]Extractor Predicting: 168it [01:50,  1.51it/s]Extractor Predicting: 169it [01:51,  1.50it/s]Extractor Predicting: 170it [01:51,  1.54it/s]Extractor Predicting: 171it [01:52,  1.58it/s]Extractor Predicting: 172it [01:53,  1.57it/s]Extractor Predicting: 173it [01:53,  1.55it/s]Extractor Predicting: 174it [01:54,  1.53it/s]Extractor Predicting: 175it [01:55,  1.48it/s]Extractor Predicting: 176it [01:55,  1.44it/s]Extractor Predicting: 177it [01:56,  1.47it/s]Extractor Predicting: 178it [01:57,  1.47it/s]Extractor Predicting: 179it [01:57,  1.48it/s]Extractor Predicting: 180it [01:58,  1.46it/s]Extractor Predicting: 181it [01:59,  1.43it/s]Extractor Predicting: 182it [02:00,  1.44it/s]Extractor Predicting: 183it [02:00,  1.44it/s]Extractor Predicting: 184it [02:01,  1.44it/s]Extractor Predicting: 185it [02:02,  1.46it/s]Extractor Predicting: 186it [02:02,  1.46it/s]Extractor Predicting: 187it [02:03,  1.45it/s]Extractor Predicting: 188it [02:04,  1.48it/s]Extractor Predicting: 189it [02:04,  1.49it/s]Extractor Predicting: 190it [02:05,  1.48it/s]Extractor Predicting: 191it [02:06,  1.49it/s]Extractor Predicting: 192it [02:06,  1.49it/s]Extractor Predicting: 193it [02:07,  1.49it/s]Extractor Predicting: 194it [02:08,  1.50it/s]Extractor Predicting: 195it [02:08,  1.54it/s]Extractor Predicting: 196it [02:09,  1.56it/s]Extractor Predicting: 197it [02:10,  1.55it/s]Extractor Predicting: 198it [02:10,  1.54it/s]Extractor Predicting: 199it [02:11,  1.55it/s]Extractor Predicting: 200it [02:11,  1.56it/s]Extractor Predicting: 201it [02:12,  1.54it/s]Extractor Predicting: 202it [02:13,  1.53it/s]Extractor Predicting: 203it [02:13,  1.53it/s]Extractor Predicting: 204it [02:14,  1.52it/s]Extractor Predicting: 205it [02:15,  1.52it/s]Extractor Predicting: 206it [02:15,  1.54it/s]Extractor Predicting: 207it [02:16,  1.52it/s]Extractor Predicting: 208it [02:17,  1.56it/s]Extractor Predicting: 209it [02:17,  1.57it/s]Extractor Predicting: 210it [02:18,  1.54it/s]Extractor Predicting: 211it [02:19,  1.56it/s]Extractor Predicting: 212it [02:19,  1.56it/s]Extractor Predicting: 213it [02:20,  1.53it/s]Extractor Predicting: 214it [02:21,  1.50it/s]Extractor Predicting: 215it [02:21,  1.51it/s]Extractor Predicting: 216it [02:22,  1.51it/s]Extractor Predicting: 217it [02:23,  1.54it/s]Extractor Predicting: 218it [02:23,  1.51it/s]Extractor Predicting: 219it [02:24,  1.50it/s]Extractor Predicting: 220it [02:25,  1.54it/s]Extractor Predicting: 221it [02:25,  1.50it/s]Extractor Predicting: 222it [02:26,  1.48it/s]Extractor Predicting: 223it [02:27,  1.49it/s]Extractor Predicting: 224it [02:27,  1.52it/s]Extractor Predicting: 225it [02:28,  1.55it/s]Extractor Predicting: 226it [02:28,  1.55it/s]Extractor Predicting: 227it [02:29,  1.55it/s]Extractor Predicting: 228it [02:30,  1.56it/s]Extractor Predicting: 229it [02:30,  1.55it/s]Extractor Predicting: 230it [02:31,  1.58it/s]Extractor Predicting: 231it [02:32,  1.57it/s]Extractor Predicting: 232it [02:32,  1.55it/s]Extractor Predicting: 233it [02:33,  1.56it/s]Extractor Predicting: 234it [02:34,  1.52it/s]Extractor Predicting: 235it [02:34,  1.51it/s]Extractor Predicting: 236it [02:35,  1.53it/s]Extractor Predicting: 237it [02:36,  1.49it/s]Extractor Predicting: 238it [02:36,  1.51it/s]Extractor Predicting: 239it [02:37,  1.51it/s]Extractor Predicting: 240it [02:38,  1.53it/s]Extractor Predicting: 241it [02:38,  1.56it/s]Extractor Predicting: 242it [02:39,  1.50it/s]Extractor Predicting: 243it [02:40,  1.55it/s]Extractor Predicting: 244it [02:40,  1.56it/s]Extractor Predicting: 245it [02:41,  1.53it/s]Extractor Predicting: 246it [02:41,  1.56it/s]Extractor Predicting: 247it [02:42,  1.55it/s]Extractor Predicting: 248it [02:43,  1.57it/s]Extractor Predicting: 249it [02:43,  1.54it/s]Extractor Predicting: 250it [02:44,  1.60it/s]Extractor Predicting: 251it [02:45,  1.57it/s]Extractor Predicting: 252it [02:45,  1.59it/s]Extractor Predicting: 253it [02:46,  1.42it/s]Extractor Predicting: 254it [02:47,  1.42it/s]Extractor Predicting: 255it [02:47,  1.46it/s]Extractor Predicting: 256it [02:48,  1.48it/s]Extractor Predicting: 257it [02:49,  1.50it/s]Extractor Predicting: 258it [02:49,  1.53it/s]Extractor Predicting: 259it [02:50,  1.52it/s]Extractor Predicting: 260it [02:51,  1.52it/s]Extractor Predicting: 261it [02:51,  1.50it/s]Extractor Predicting: 262it [02:52,  1.52it/s]Extractor Predicting: 263it [02:53,  1.47it/s]Extractor Predicting: 264it [02:53,  1.49it/s]Extractor Predicting: 265it [02:54,  1.49it/s]Extractor Predicting: 266it [02:55,  1.46it/s]Extractor Predicting: 267it [02:56,  1.47it/s]Extractor Predicting: 268it [02:56,  1.47it/s]Extractor Predicting: 269it [02:57,  1.40it/s]Extractor Predicting: 270it [02:58,  1.43it/s]Extractor Predicting: 271it [02:58,  1.45it/s]Extractor Predicting: 272it [02:59,  1.50it/s]Extractor Predicting: 273it [03:00,  1.50it/s]Extractor Predicting: 274it [03:00,  1.44it/s]Extractor Predicting: 275it [03:01,  1.45it/s]Extractor Predicting: 276it [03:02,  1.49it/s]Extractor Predicting: 277it [03:02,  1.48it/s]Extractor Predicting: 278it [03:03,  1.50it/s]Extractor Predicting: 279it [03:04,  1.45it/s]Extractor Predicting: 280it [03:04,  1.47it/s]Extractor Predicting: 281it [03:05,  1.48it/s]Extractor Predicting: 282it [03:06,  1.48it/s]Extractor Predicting: 283it [03:06,  1.49it/s]Extractor Predicting: 284it [03:07,  1.49it/s]Extractor Predicting: 285it [03:08,  1.51it/s]Extractor Predicting: 286it [03:08,  1.51it/s]Extractor Predicting: 287it [03:09,  1.49it/s]Extractor Predicting: 288it [03:10,  1.49it/s]Extractor Predicting: 289it [03:10,  1.47it/s]Extractor Predicting: 290it [03:11,  1.45it/s]Extractor Predicting: 291it [03:12,  1.46it/s]Extractor Predicting: 292it [03:12,  1.49it/s]Extractor Predicting: 293it [03:13,  1.48it/s]Extractor Predicting: 294it [03:14,  1.45it/s]Extractor Predicting: 295it [03:15,  1.45it/s]Extractor Predicting: 296it [03:15,  1.46it/s]Extractor Predicting: 297it [03:16,  1.46it/s]Extractor Predicting: 298it [03:17,  1.46it/s]Extractor Predicting: 299it [03:17,  1.46it/s]Extractor Predicting: 300it [03:18,  1.40it/s]Extractor Predicting: 301it [03:19,  1.37it/s]Extractor Predicting: 302it [03:20,  1.37it/s]Extractor Predicting: 303it [03:20,  1.40it/s]Extractor Predicting: 304it [03:21,  1.36it/s]Extractor Predicting: 305it [03:22,  1.32it/s]Extractor Predicting: 306it [03:22,  1.38it/s]Extractor Predicting: 307it [03:23,  1.43it/s]Extractor Predicting: 308it [03:24,  1.46it/s]Extractor Predicting: 309it [03:24,  1.52it/s]Extractor Predicting: 310it [03:25,  1.49it/s]Extractor Predicting: 311it [03:26,  1.46it/s]Extractor Predicting: 312it [03:26,  1.47it/s]Extractor Predicting: 313it [03:27,  1.48it/s]Extractor Predicting: 314it [03:28,  1.47it/s]Extractor Predicting: 315it [03:28,  1.52it/s]Extractor Predicting: 316it [03:29,  1.53it/s]Extractor Predicting: 317it [03:30,  1.52it/s]Extractor Predicting: 318it [03:30,  1.52it/s]Extractor Predicting: 319it [03:31,  1.50it/s]Extractor Predicting: 320it [03:32,  1.51it/s]Extractor Predicting: 321it [03:32,  1.52it/s]Extractor Predicting: 322it [03:33,  1.56it/s]Extractor Predicting: 323it [03:34,  1.55it/s]Extractor Predicting: 324it [03:34,  1.57it/s]Extractor Predicting: 325it [03:35,  1.53it/s]Extractor Predicting: 326it [03:36,  1.55it/s]Extractor Predicting: 327it [03:36,  1.56it/s]Extractor Predicting: 328it [03:37,  1.54it/s]Extractor Predicting: 329it [03:38,  1.54it/s]Extractor Predicting: 330it [03:38,  1.50it/s]Extractor Predicting: 331it [03:39,  1.50it/s]Extractor Predicting: 332it [03:40,  1.53it/s]Extractor Predicting: 333it [03:40,  1.51it/s]Extractor Predicting: 334it [03:41,  1.51it/s]Extractor Predicting: 335it [03:42,  1.50it/s]Extractor Predicting: 336it [03:42,  1.53it/s]Extractor Predicting: 337it [03:43,  1.53it/s]Extractor Predicting: 338it [03:43,  1.54it/s]Extractor Predicting: 339it [03:44,  1.54it/s]Extractor Predicting: 340it [03:45,  1.53it/s]Extractor Predicting: 341it [03:45,  1.53it/s]Extractor Predicting: 342it [03:46,  1.52it/s]Extractor Predicting: 343it [03:47,  1.51it/s]Extractor Predicting: 344it [03:47,  1.56it/s]Extractor Predicting: 345it [03:48,  1.57it/s]Extractor Predicting: 346it [03:49,  1.59it/s]Extractor Predicting: 347it [03:49,  1.63it/s]Extractor Predicting: 348it [03:50,  1.61it/s]Extractor Predicting: 349it [03:50,  1.62it/s]Extractor Predicting: 350it [03:51,  1.59it/s]Extractor Predicting: 351it [03:52,  1.60it/s]Extractor Predicting: 352it [03:52,  1.60it/s]Extractor Predicting: 353it [03:53,  1.61it/s]Extractor Predicting: 354it [03:54,  1.61it/s]Extractor Predicting: 355it [03:54,  1.60it/s]Extractor Predicting: 356it [03:55,  1.63it/s]Extractor Predicting: 357it [03:55,  1.61it/s]Extractor Predicting: 358it [03:56,  1.60it/s]Extractor Predicting: 359it [03:57,  1.60it/s]Extractor Predicting: 360it [03:57,  1.55it/s]Extractor Predicting: 361it [03:58,  1.56it/s]Extractor Predicting: 362it [03:59,  1.57it/s]Extractor Predicting: 363it [03:59,  1.60it/s]Extractor Predicting: 364it [04:00,  1.63it/s]Extractor Predicting: 365it [04:01,  1.55it/s]Extractor Predicting: 366it [04:01,  1.52it/s]Extractor Predicting: 367it [04:02,  1.48it/s]Extractor Predicting: 368it [04:03,  1.47it/s]Extractor Predicting: 369it [04:03,  1.52it/s]Extractor Predicting: 370it [04:04,  1.46it/s]Extractor Predicting: 371it [04:05,  1.50it/s]Extractor Predicting: 372it [04:05,  1.52it/s]Extractor Predicting: 373it [04:06,  1.54it/s]Extractor Predicting: 374it [04:07,  1.54it/s]Extractor Predicting: 375it [04:07,  1.51it/s]Extractor Predicting: 376it [04:08,  1.51it/s]Extractor Predicting: 377it [04:09,  1.49it/s]Extractor Predicting: 378it [04:09,  1.49it/s]Extractor Predicting: 379it [04:10,  1.50it/s]Extractor Predicting: 380it [04:11,  1.49it/s]Extractor Predicting: 381it [04:11,  1.53it/s]Extractor Predicting: 382it [04:12,  1.56it/s]Extractor Predicting: 383it [04:12,  1.56it/s]Extractor Predicting: 384it [04:13,  1.57it/s]Extractor Predicting: 385it [04:14,  1.56it/s]Extractor Predicting: 386it [04:14,  1.58it/s]Extractor Predicting: 387it [04:15,  1.58it/s]Extractor Predicting: 388it [04:16,  1.39it/s]Extractor Predicting: 389it [04:17,  1.40it/s]Extractor Predicting: 390it [04:17,  1.42it/s]Extractor Predicting: 391it [04:18,  1.46it/s]Extractor Predicting: 392it [04:18,  1.53it/s]Extractor Predicting: 393it [04:19,  1.56it/s]Extractor Predicting: 394it [04:20,  1.57it/s]Extractor Predicting: 395it [04:20,  1.60it/s]Extractor Predicting: 396it [04:21,  1.62it/s]Extractor Predicting: 397it [04:22,  1.59it/s]Extractor Predicting: 398it [04:22,  1.58it/s]Extractor Predicting: 399it [04:23,  1.58it/s]Extractor Predicting: 400it [04:24,  1.56it/s]Extractor Predicting: 401it [04:24,  1.58it/s]Extractor Predicting: 402it [04:25,  1.60it/s]Extractor Predicting: 403it [04:25,  1.59it/s]Extractor Predicting: 404it [04:26,  1.59it/s]Extractor Predicting: 405it [04:27,  1.55it/s]Extractor Predicting: 406it [04:27,  1.56it/s]Extractor Predicting: 407it [04:28,  1.56it/s]Extractor Predicting: 408it [04:29,  1.57it/s]Extractor Predicting: 409it [04:29,  1.42it/s]Extractor Predicting: 410it [04:30,  1.45it/s]Extractor Predicting: 411it [04:31,  1.47it/s]Extractor Predicting: 412it [04:31,  1.49it/s]Extractor Predicting: 413it [04:32,  1.43it/s]Extractor Predicting: 414it [04:33,  1.41it/s]Extractor Predicting: 415it [04:34,  1.38it/s]Extractor Predicting: 416it [04:34,  1.39it/s]Extractor Predicting: 417it [04:35,  1.41it/s]Extractor Predicting: 418it [04:36,  1.42it/s]Extractor Predicting: 419it [04:36,  1.43it/s]Extractor Predicting: 420it [04:37,  1.42it/s]Extractor Predicting: 421it [04:38,  1.42it/s]Extractor Predicting: 422it [04:39,  1.43it/s]Extractor Predicting: 423it [04:39,  1.45it/s]Extractor Predicting: 424it [04:40,  1.45it/s]Extractor Predicting: 425it [04:41,  1.44it/s]Extractor Predicting: 426it [04:41,  1.43it/s]Extractor Predicting: 427it [04:42,  1.45it/s]Extractor Predicting: 428it [04:43,  1.45it/s]Extractor Predicting: 429it [04:43,  1.48it/s]Extractor Predicting: 430it [04:44,  1.49it/s]Extractor Predicting: 431it [04:45,  1.49it/s]Extractor Predicting: 432it [04:45,  1.49it/s]Extractor Predicting: 433it [04:46,  1.49it/s]Extractor Predicting: 434it [04:47,  1.47it/s]Extractor Predicting: 435it [04:47,  1.50it/s]Extractor Predicting: 436it [04:48,  1.53it/s]Extractor Predicting: 437it [04:49,  1.51it/s]Extractor Predicting: 438it [04:49,  1.53it/s]Extractor Predicting: 439it [04:50,  1.51it/s]Extractor Predicting: 440it [04:51,  1.51it/s]Extractor Predicting: 441it [04:51,  1.49it/s]Extractor Predicting: 442it [04:52,  1.50it/s]Extractor Predicting: 443it [04:53,  1.48it/s]Extractor Predicting: 444it [04:53,  1.45it/s]Extractor Predicting: 445it [04:54,  1.45it/s]Extractor Predicting: 446it [04:55,  1.46it/s]Extractor Predicting: 447it [04:55,  1.47it/s]Extractor Predicting: 448it [04:56,  1.46it/s]Extractor Predicting: 449it [04:57,  1.48it/s]Extractor Predicting: 450it [04:57,  1.52it/s]Extractor Predicting: 451it [04:58,  1.50it/s]Extractor Predicting: 452it [04:59,  1.49it/s]Extractor Predicting: 453it [04:59,  1.49it/s]Extractor Predicting: 454it [05:00,  1.47it/s]Extractor Predicting: 455it [05:01,  1.45it/s]Extractor Predicting: 456it [05:01,  1.47it/s]Extractor Predicting: 457it [05:02,  1.45it/s]Extractor Predicting: 458it [05:03,  1.47it/s]Extractor Predicting: 459it [05:04,  1.44it/s]Extractor Predicting: 460it [05:04,  1.44it/s]Extractor Predicting: 461it [05:05,  1.47it/s]Extractor Predicting: 462it [05:06,  1.48it/s]Extractor Predicting: 463it [05:06,  1.45it/s]Extractor Predicting: 464it [05:07,  1.41it/s]Extractor Predicting: 465it [05:08,  1.42it/s]Extractor Predicting: 466it [05:08,  1.44it/s]Extractor Predicting: 467it [05:09,  1.49it/s]Extractor Predicting: 468it [05:10,  1.50it/s]Extractor Predicting: 469it [05:10,  1.54it/s]Extractor Predicting: 470it [05:11,  1.52it/s]Extractor Predicting: 471it [05:12,  1.49it/s]Extractor Predicting: 472it [05:12,  1.50it/s]Extractor Predicting: 473it [05:13,  1.48it/s]Extractor Predicting: 474it [05:14,  1.46it/s]Extractor Predicting: 475it [05:14,  1.46it/s]Extractor Predicting: 476it [05:15,  1.48it/s]Extractor Predicting: 477it [05:16,  1.48it/s]Extractor Predicting: 478it [05:16,  1.48it/s]Extractor Predicting: 479it [05:17,  1.42it/s]Extractor Predicting: 480it [05:18,  1.39it/s]Extractor Predicting: 481it [05:19,  1.38it/s]Extractor Predicting: 482it [05:19,  1.39it/s]Extractor Predicting: 483it [05:20,  1.37it/s]Extractor Predicting: 484it [05:21,  1.35it/s]Extractor Predicting: 485it [05:22,  1.36it/s]Extractor Predicting: 486it [05:22,  1.39it/s]Extractor Predicting: 487it [05:23,  1.42it/s]Extractor Predicting: 488it [05:24,  1.40it/s]Extractor Predicting: 489it [05:24,  1.41it/s]Extractor Predicting: 490it [05:25,  1.38it/s]Extractor Predicting: 491it [05:26,  1.37it/s]Extractor Predicting: 492it [05:27,  1.37it/s]Extractor Predicting: 493it [05:27,  1.39it/s]Extractor Predicting: 494it [05:28,  1.38it/s]Extractor Predicting: 495it [05:29,  1.38it/s]Extractor Predicting: 496it [05:30,  1.39it/s]Extractor Predicting: 497it [05:30,  1.39it/s]Extractor Predicting: 498it [05:31,  1.40it/s]Extractor Predicting: 499it [05:32,  1.36it/s]Extractor Predicting: 500it [05:33,  1.21it/s]Extractor Predicting: 501it [05:33,  1.27it/s]Extractor Predicting: 502it [05:34,  1.28it/s]Extractor Predicting: 503it [05:35,  1.28it/s]Extractor Predicting: 504it [05:36,  1.27it/s]Extractor Predicting: 505it [05:37,  1.28it/s]Extractor Predicting: 506it [05:37,  1.28it/s]Extractor Predicting: 507it [05:38,  1.30it/s]Extractor Predicting: 508it [05:39,  1.46it/s]Extractor Predicting: 508it [05:39,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:38:48,642 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:38:48,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:38:48,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:38:48,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:38:48,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:38:49,502 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:38:49,503 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:38:50,115 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:38:51,377 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:38:51,377 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:38:54,555 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:38:54,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:38:54,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:38:54,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:38:54,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:38:55,326 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:38:55,328 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:38:55,946 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:38:56,138 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:38:56,138 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.21700879765395895,
  "recall": 0.03645020934241852,
  "score": 0.0624165319462993,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.43it/s]Extractor Predicting: 9it [00:06,  1.43it/s]Extractor Predicting: 10it [00:06,  1.42it/s]Extractor Predicting: 11it [00:07,  1.40it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:09,  1.44it/s]Extractor Predicting: 14it [00:09,  1.44it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:11,  1.41it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:13,  1.43it/s]Extractor Predicting: 20it [00:13,  1.43it/s]Extractor Predicting: 21it [00:14,  1.43it/s]Extractor Predicting: 22it [00:15,  1.41it/s]Extractor Predicting: 23it [00:16,  1.41it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:17,  1.47it/s]Extractor Predicting: 26it [00:18,  1.45it/s]Extractor Predicting: 27it [00:18,  1.44it/s]Extractor Predicting: 28it [00:19,  1.43it/s]Extractor Predicting: 29it [00:20,  1.40it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:21,  1.48it/s]Extractor Predicting: 32it [00:22,  1.52it/s]Extractor Predicting: 33it [00:22,  1.53it/s]Extractor Predicting: 34it [00:23,  1.53it/s]Extractor Predicting: 35it [00:24,  1.52it/s]Extractor Predicting: 36it [00:24,  1.54it/s]Extractor Predicting: 37it [00:25,  1.51it/s]Extractor Predicting: 38it [00:26,  1.53it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:27,  1.54it/s]Extractor Predicting: 41it [00:28,  1.53it/s]Extractor Predicting: 42it [00:28,  1.56it/s]Extractor Predicting: 43it [00:29,  1.55it/s]Extractor Predicting: 44it [00:29,  1.58it/s]Extractor Predicting: 45it [00:30,  1.55it/s]Extractor Predicting: 46it [00:31,  1.55it/s]Extractor Predicting: 47it [00:31,  1.55it/s]Extractor Predicting: 48it [00:32,  1.54it/s]Extractor Predicting: 49it [00:33,  1.52it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:34,  1.48it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.50it/s]Extractor Predicting: 54it [00:36,  1.44it/s]Extractor Predicting: 55it [00:37,  1.46it/s]Extractor Predicting: 56it [00:38,  1.39it/s]Extractor Predicting: 57it [00:38,  1.42it/s]Extractor Predicting: 58it [00:39,  1.44it/s]Extractor Predicting: 59it [00:40,  1.47it/s]Extractor Predicting: 60it [00:40,  1.44it/s]Extractor Predicting: 61it [00:41,  1.47it/s]Extractor Predicting: 62it [00:42,  1.48it/s]Extractor Predicting: 63it [00:42,  1.49it/s]Extractor Predicting: 64it [00:43,  1.49it/s]Extractor Predicting: 65it [00:44,  1.45it/s]Extractor Predicting: 66it [00:44,  1.47it/s]Extractor Predicting: 67it [00:45,  1.44it/s]Extractor Predicting: 68it [00:46,  1.42it/s]Extractor Predicting: 69it [00:47,  1.40it/s]Extractor Predicting: 70it [00:47,  1.38it/s]Extractor Predicting: 71it [00:48,  1.39it/s]Extractor Predicting: 72it [00:49,  1.39it/s]Extractor Predicting: 73it [00:49,  1.42it/s]Extractor Predicting: 74it [00:50,  1.37it/s]Extractor Predicting: 75it [00:51,  1.53it/s]Extractor Predicting: 75it [00:51,  1.47it/s]
[INFO|configuration_utils.py:515] 2023-08-29 12:39:49,998 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:39:50,000 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 12:39:50,062 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:39:50,063 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 12:39:50,081 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 12:40:01,170 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 12:40:01,217 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 12:40:01,608 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:40:01,609 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 12:40:01,781 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:40:01,887 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:40:01,887 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:40:01,887 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:40:01,887 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:40:01,887 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:40:01,888 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5575916230366492,
  "recall": 0.05365239294710328,
  "score": 0.09788602941176472,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 12:40:02,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:03,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:04,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:04,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:05,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:06,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:07,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:07,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:08,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:09,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:10,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:10,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:11,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:12,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:13,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:13,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:14,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:15,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:16,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:17,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:50, 15.29s/it][WARNING|generation_utils.py:914] 2023-08-29 12:40:17,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:18,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:19,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:19,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:20,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:21,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:22,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:22,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:23,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:24,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:24,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:25,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:26,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:26,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:27,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:28,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:28,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:29,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:30,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:30,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:31,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:32,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:30<04:34, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-29 12:40:32,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:33,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:34,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:35,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:36,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:37,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:38,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:39,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:39,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:41,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:42,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:43,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:43,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:44,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:45,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:46,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:47,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:48,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:50,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:51,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:52,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:52,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:51<05:01, 17.71s/it][WARNING|generation_utils.py:914] 2023-08-29 12:40:53,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:54,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:55,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:55,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:56,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:57,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:57,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:58,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:40:59,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:00,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:00,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:01,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:02,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:03,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:03,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:04,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:05,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:05,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:06,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:07,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:05<04:23, 16.48s/it][WARNING|generation_utils.py:914] 2023-08-29 12:41:08,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:08,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:09,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:10,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:11,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:11,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:12,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:13,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:13,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:14,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:15,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:16,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:16,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:17,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:18,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:18,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:19,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:20,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:20,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:21,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:22,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:20<03:57, 15.82s/it][WARNING|generation_utils.py:914] 2023-08-29 12:41:22,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:23,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:24,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:24,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:25,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:26,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:27,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:27,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:28,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:29,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:29,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:30,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:31,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:32,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:32,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:33,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:34,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:34,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:35,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:36,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:36,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:34<03:35, 15.41s/it][WARNING|generation_utils.py:914] 2023-08-29 12:41:37,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:38,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:38,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:39,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:40,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:40,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:41,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:42,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:42,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:43,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:44,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:44,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:45,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:46,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:47,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:47,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:48,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:49,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:49,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:50,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:50,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:49<03:14, 14.98s/it][WARNING|generation_utils.py:914] 2023-08-29 12:41:51,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:52,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:53,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:53,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:54,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:55,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:56,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:56,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:57,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:58,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:41:59,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:00,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:00,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:01,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:02,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:03,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:03,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:04,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:05,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:06,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:04<03:02, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-29 12:42:07,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:08,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:08,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:09,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:10,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:10,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:11,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:12,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:13,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:14,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:14,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:15,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:16,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:17,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:17,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:18,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:19,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:20,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:21,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:21,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:19<02:47, 15.19s/it][WARNING|generation_utils.py:914] 2023-08-29 12:42:22,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:23,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:23,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:24,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:25,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:25,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:26,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:27,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:28,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:28,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:29,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:30,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:31,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:31,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:32,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:33,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:33,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:34,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:35,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:36,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:34<02:28, 14.89s/it][WARNING|generation_utils.py:914] 2023-08-29 12:42:36,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:37,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:38,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:39,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:39,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:40,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:41,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:42,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:42,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:43,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:44,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:45,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:46,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:46,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:47,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:48,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:49,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:49,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:50,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:51,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:52,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:52,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:51<02:19, 15.53s/it][WARNING|generation_utils.py:914] 2023-08-29 12:42:53,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:54,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:55,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:55,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:56,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:57,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:57,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:58,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:42:59,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:00,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:00,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:01,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:02,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:02,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:03,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:04,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:05,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:05,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:06,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:07,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:05<02:01, 15.22s/it][WARNING|generation_utils.py:914] 2023-08-29 12:43:08,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:08,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:09,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:10,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:10,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:11,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:12,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:12,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:13,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:14,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:14,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:15,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:16,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:16,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:17,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:18,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:19,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:19,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:20,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:21,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:22,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:20<01:45, 15.08s/it][WARNING|generation_utils.py:914] 2023-08-29 12:43:22,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:23,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:24,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:25,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:25,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:26,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:27,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:27,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:28,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:29,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:29,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:30,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:31,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:31,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:32,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:33,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:33,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:34,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:35,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:35,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:34<01:27, 14.63s/it][WARNING|generation_utils.py:914] 2023-08-29 12:43:36,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:37,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:37,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:38,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:39,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:40,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:40,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:41,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:42,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:42,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:43,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:44,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:44,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:45,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:46,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:47,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:47,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:48,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:49,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:49,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:48<01:12, 14.46s/it][WARNING|generation_utils.py:914] 2023-08-29 12:43:50,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:51,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:52,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:52,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:53,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:54,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:55,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:55,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:56,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:57,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:58,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:59,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:43:59,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:00,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:01,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:02,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:02,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:03,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:04,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:05,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:06,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:04<00:59, 14.97s/it][WARNING|generation_utils.py:914] 2023-08-29 12:44:06,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:07,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:08,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:08,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:09,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:10,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:10,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:11,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:12,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:12,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:13,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:14,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:14,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:15,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:16,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:16,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:17,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:18,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:18,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:19,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:17<00:43, 14.53s/it][WARNING|generation_utils.py:914] 2023-08-29 12:44:20,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:20,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:21,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:22,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:23,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:23,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:24,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:24,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:25,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:26,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:26,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:27,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:28,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:28,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:29,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:30,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:31,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:31,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:32,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:33,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:31<00:28, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-29 12:44:33,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:34,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:35,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:36,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:36,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:37,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:38,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:39,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:39,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:40,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:41,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:42,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:42,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:43,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:44,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:44,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:45,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:46,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:47,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:47,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:45<00:14, 14.37s/it][WARNING|generation_utils.py:914] 2023-08-29 12:44:48,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:49,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:49,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:50,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:51,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:51,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:52,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:53,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:54,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:54,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:55,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:56,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:56,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:57,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:58,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:58,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:44:59,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:45:00,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:45:01,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:45:01,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:45:02,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:00<00:00, 14.49s/it]Generating: 100%|██████████| 20/20 [05:00<00:00, 15.04s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:45:12,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:45:13,002 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:45:13,002 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:45:13,002 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:45:13,002 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:45:13,957 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:45:13,959 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:45:14,608 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:45:15,768 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:45:15,768 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:45:18,901 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:45:18,931 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:45:18,931 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:45:18,931 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:45:18,931 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:45:19,792 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:45:19,793 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:45:20,613 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:45:20,817 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:45:20,817 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9734375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.8806818181818182, 'errors': {'', "('Ford', 'product or material produced', '', 'The name reflects the American automotive industry during the 1960s and 1970s , such as Ford , Citroën , and General Motors .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 438, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 564, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : student .', 'success_rate': 0.9765625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : conflict .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : continent .', 'success_rate': 0.9226190476190477, 'errors': {'', "('Hainaut Island', 'continent', '', 'Hainaut Island lies at a distance ( between the two islands ) from the center of the island ; it has been named after him .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.959375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : founded by .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : given name .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : movement .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 562, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9734375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : producer .', 'success_rate': 0.9734375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9734375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : replaces .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 10849
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10949, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.17it/s]Extractor Estimating: 2it [00:01,  1.18it/s]Extractor Estimating: 3it [00:02,  1.28it/s]Extractor Estimating: 4it [00:03,  1.33it/s]Extractor Estimating: 5it [00:03,  1.34it/s]Extractor Estimating: 6it [00:04,  1.37it/s]Extractor Estimating: 7it [00:05,  1.40it/s]Extractor Estimating: 8it [00:05,  1.42it/s]Extractor Estimating: 9it [00:06,  1.42it/s]Extractor Estimating: 10it [00:07,  1.43it/s]Extractor Estimating: 11it [00:08,  1.37it/s]Extractor Estimating: 12it [00:08,  1.39it/s]Extractor Estimating: 13it [00:09,  1.42it/s]Extractor Estimating: 14it [00:10,  1.42it/s]Extractor Estimating: 15it [00:10,  1.44it/s]Extractor Estimating: 16it [00:11,  1.43it/s]Extractor Estimating: 17it [00:12,  1.39it/s]Extractor Estimating: 18it [00:13,  1.38it/s]Extractor Estimating: 19it [00:13,  1.39it/s]Extractor Estimating: 20it [00:14,  1.42it/s]Extractor Estimating: 21it [00:15,  1.44it/s]Extractor Estimating: 22it [00:15,  1.42it/s]Extractor Estimating: 23it [00:16,  1.44it/s]Extractor Estimating: 24it [00:17,  1.41it/s]Extractor Estimating: 25it [00:17,  1.41it/s]Extractor Estimating: 26it [00:18,  1.48it/s]Extractor Estimating: 27it [00:19,  1.51it/s]Extractor Estimating: 28it [00:19,  1.57it/s]Extractor Estimating: 29it [00:20,  1.60it/s]Extractor Estimating: 30it [00:20,  1.63it/s]Extractor Estimating: 31it [00:21,  1.62it/s]Extractor Estimating: 32it [00:22,  1.70it/s]Extractor Estimating: 33it [00:22,  1.67it/s]Extractor Estimating: 34it [00:23,  1.62it/s]Extractor Estimating: 35it [00:23,  1.68it/s]Extractor Estimating: 36it [00:24,  1.71it/s]Extractor Estimating: 37it [00:25,  1.72it/s]Extractor Estimating: 38it [00:25,  1.73it/s]Extractor Estimating: 39it [00:26,  1.69it/s]Extractor Estimating: 40it [00:26,  1.69it/s]Extractor Estimating: 41it [00:27,  1.73it/s]Extractor Estimating: 42it [00:28,  1.68it/s]Extractor Estimating: 43it [00:28,  1.72it/s]Extractor Estimating: 44it [00:29,  1.73it/s]Extractor Estimating: 45it [00:29,  1.67it/s]Extractor Estimating: 46it [00:30,  1.69it/s]Extractor Estimating: 47it [00:30,  1.74it/s]Extractor Estimating: 48it [00:31,  1.75it/s]Extractor Estimating: 49it [00:32,  1.72it/s]Extractor Estimating: 50it [00:32,  1.73it/s]Extractor Estimating: 51it [00:33,  1.67it/s]Extractor Estimating: 52it [00:33,  1.62it/s]Extractor Estimating: 53it [00:34,  1.63it/s]Extractor Estimating: 54it [00:35,  1.70it/s]Extractor Estimating: 55it [00:35,  1.67it/s]Extractor Estimating: 56it [00:36,  1.59it/s]Extractor Estimating: 57it [00:37,  1.61it/s]Extractor Estimating: 58it [00:37,  1.62it/s]Extractor Estimating: 59it [00:38,  1.59it/s]Extractor Estimating: 60it [00:38,  1.54it/s]Extractor Estimating: 61it [00:39,  1.47it/s]Extractor Estimating: 62it [00:40,  1.46it/s]Extractor Estimating: 63it [00:41,  1.39it/s]Extractor Estimating: 64it [00:41,  1.49it/s]Extractor Estimating: 65it [00:42,  1.51it/s]Extractor Estimating: 66it [00:43,  1.57it/s]Extractor Estimating: 67it [00:43,  1.52it/s]Extractor Estimating: 68it [00:44,  1.47it/s]Extractor Estimating: 69it [00:44,  1.56it/s]Extractor Estimating: 70it [00:45,  1.59it/s]Extractor Estimating: 71it [00:46,  1.62it/s]Extractor Estimating: 72it [00:46,  1.60it/s]Extractor Estimating: 73it [00:47,  1.64it/s]Extractor Estimating: 74it [00:48,  1.62it/s]Extractor Estimating: 75it [00:48,  1.65it/s]Extractor Estimating: 76it [00:49,  1.61it/s]Extractor Estimating: 77it [00:50,  1.49it/s]Extractor Estimating: 78it [00:50,  1.56it/s]Extractor Estimating: 79it [00:51,  1.64it/s]Extractor Estimating: 80it [00:51,  1.64it/s]Extractor Estimating: 81it [00:52,  1.68it/s]Extractor Estimating: 82it [00:52,  1.67it/s]Extractor Estimating: 83it [00:53,  1.75it/s]Extractor Estimating: 84it [00:54,  1.72it/s]Extractor Estimating: 85it [00:54,  1.68it/s]Extractor Estimating: 86it [00:55,  1.71it/s]Extractor Estimating: 87it [00:55,  1.68it/s]Extractor Estimating: 88it [00:56,  1.68it/s]Extractor Estimating: 89it [00:57,  1.66it/s]Extractor Estimating: 90it [00:57,  1.62it/s]Extractor Estimating: 91it [00:58,  1.65it/s]Extractor Estimating: 92it [00:58,  1.65it/s]Extractor Estimating: 93it [00:59,  1.61it/s]Extractor Estimating: 94it [01:00,  1.64it/s]Extractor Estimating: 95it [01:00,  1.68it/s]Extractor Estimating: 96it [01:01,  1.67it/s]Extractor Estimating: 97it [01:01,  1.70it/s]Extractor Estimating: 98it [01:02,  1.71it/s]Extractor Estimating: 99it [01:03,  1.66it/s]Extractor Estimating: 100it [01:03,  1.67it/s]Extractor Estimating: 101it [01:04,  1.65it/s]Extractor Estimating: 102it [01:04,  1.69it/s]Extractor Estimating: 103it [01:05,  1.70it/s]Extractor Estimating: 104it [01:06,  1.71it/s]Extractor Estimating: 105it [01:06,  1.69it/s]Extractor Estimating: 106it [01:07,  1.69it/s]Extractor Estimating: 107it [01:07,  1.68it/s]Extractor Estimating: 108it [01:08,  1.65it/s]Extractor Estimating: 109it [01:09,  1.63it/s]Extractor Estimating: 110it [01:09,  1.62it/s]Extractor Estimating: 111it [01:10,  1.64it/s]Extractor Estimating: 112it [01:10,  1.65it/s]Extractor Estimating: 113it [01:11,  1.63it/s]Extractor Estimating: 114it [01:12,  1.68it/s]Extractor Estimating: 115it [01:12,  1.62it/s]Extractor Estimating: 116it [01:13,  1.59it/s]Extractor Estimating: 117it [01:14,  1.60it/s]Extractor Estimating: 118it [01:14,  1.58it/s]Extractor Estimating: 119it [01:15,  1.59it/s]Extractor Estimating: 120it [01:15,  1.62it/s]Extractor Estimating: 121it [01:16,  1.62it/s]Extractor Estimating: 122it [01:17,  1.69it/s]Extractor Estimating: 123it [01:17,  1.69it/s]Extractor Estimating: 124it [01:18,  1.73it/s]Extractor Estimating: 125it [01:18,  1.72it/s]Extractor Estimating: 126it [01:19,  1.72it/s]Extractor Estimating: 127it [01:19,  1.71it/s]Extractor Estimating: 128it [01:20,  1.67it/s]Extractor Estimating: 129it [01:21,  1.70it/s]Extractor Estimating: 130it [01:21,  1.67it/s]Extractor Estimating: 131it [01:22,  1.69it/s]Extractor Estimating: 132it [01:22,  1.67it/s]Extractor Estimating: 133it [01:23,  1.67it/s]Extractor Estimating: 134it [01:24,  1.67it/s]Extractor Estimating: 135it [01:24,  1.65it/s]Extractor Estimating: 136it [01:25,  1.67it/s]Extractor Estimating: 137it [01:25,  1.69it/s]Extractor Estimating: 138it [01:26,  1.70it/s]Extractor Estimating: 139it [01:27,  1.67it/s]Extractor Estimating: 140it [01:27,  1.69it/s]Extractor Estimating: 141it [01:28,  1.70it/s]Extractor Estimating: 142it [01:28,  1.69it/s]Extractor Estimating: 143it [01:29,  1.69it/s]Extractor Estimating: 144it [01:30,  1.66it/s]Extractor Estimating: 145it [01:30,  1.66it/s]Extractor Estimating: 146it [01:31,  1.66it/s]Extractor Estimating: 147it [01:31,  1.71it/s]Extractor Estimating: 148it [01:32,  1.72it/s]Extractor Estimating: 149it [01:32,  1.77it/s]Extractor Estimating: 150it [01:33,  1.75it/s]Extractor Estimating: 151it [01:34,  1.76it/s]Extractor Estimating: 152it [01:34,  1.79it/s]Extractor Estimating: 153it [01:35,  1.78it/s]Extractor Estimating: 154it [01:35,  1.80it/s]Extractor Estimating: 155it [01:36,  1.83it/s]Extractor Estimating: 156it [01:36,  1.81it/s]Extractor Estimating: 157it [01:37,  1.84it/s]Extractor Estimating: 158it [01:37,  1.92it/s]Extractor Estimating: 159it [01:38,  1.87it/s]Extractor Estimating: 160it [01:38,  1.88it/s]Extractor Estimating: 161it [01:39,  1.93it/s]Extractor Estimating: 162it [01:40,  1.84it/s]Extractor Estimating: 163it [01:40,  1.88it/s]Extractor Estimating: 164it [01:41,  1.89it/s]Extractor Estimating: 165it [01:41,  1.81it/s]Extractor Estimating: 166it [01:42,  1.86it/s]Extractor Estimating: 167it [01:42,  1.93it/s]Extractor Estimating: 168it [01:43,  1.68it/s]Extractor Estimating: 169it [01:43,  1.74it/s]Extractor Estimating: 170it [01:44,  1.77it/s]Extractor Estimating: 171it [01:45,  1.80it/s]Extractor Estimating: 172it [01:45,  1.79it/s]Extractor Estimating: 173it [01:46,  1.81it/s]Extractor Estimating: 174it [01:46,  1.78it/s]Extractor Estimating: 175it [01:47,  1.80it/s]Extractor Estimating: 176it [01:47,  1.68it/s]Extractor Estimating: 177it [01:48,  1.58it/s]Extractor Estimating: 178it [01:49,  1.55it/s]Extractor Estimating: 179it [01:50,  1.49it/s]Extractor Estimating: 180it [01:50,  1.40it/s]Extractor Estimating: 181it [01:51,  1.41it/s]Extractor Estimating: 182it [01:52,  1.41it/s]Extractor Estimating: 183it [01:53,  1.37it/s]Extractor Estimating: 184it [01:53,  1.38it/s]Extractor Estimating: 185it [01:54,  1.39it/s]Extractor Estimating: 186it [01:55,  1.39it/s]Extractor Estimating: 187it [01:55,  1.36it/s]Extractor Estimating: 188it [01:56,  1.40it/s]Extractor Estimating: 189it [01:57,  1.38it/s]Extractor Estimating: 190it [01:58,  1.42it/s]Extractor Estimating: 191it [01:58,  1.45it/s]Extractor Estimating: 192it [01:59,  1.42it/s]Extractor Estimating: 193it [02:00,  1.43it/s]Extractor Estimating: 194it [02:00,  1.39it/s]Extractor Estimating: 195it [02:01,  1.35it/s]Extractor Estimating: 196it [02:02,  1.37it/s]Extractor Estimating: 197it [02:03,  1.34it/s]Extractor Estimating: 198it [02:03,  1.33it/s]Extractor Estimating: 199it [02:04,  1.32it/s]Extractor Estimating: 200it [02:05,  1.41it/s]Extractor Estimating: 201it [02:05,  1.47it/s]Extractor Estimating: 202it [02:06,  1.53it/s]Extractor Estimating: 203it [02:07,  1.59it/s]Extractor Estimating: 204it [02:07,  1.65it/s]Extractor Estimating: 205it [02:08,  1.70it/s]Extractor Estimating: 206it [02:08,  1.73it/s]Extractor Estimating: 207it [02:09,  1.74it/s]Extractor Estimating: 208it [02:09,  1.77it/s]Extractor Estimating: 209it [02:10,  1.84it/s]Extractor Estimating: 210it [02:10,  1.76it/s]Extractor Estimating: 211it [02:11,  1.76it/s]Extractor Estimating: 212it [02:12,  1.77it/s]Extractor Estimating: 213it [02:12,  1.76it/s]Extractor Estimating: 214it [02:13,  1.77it/s]Extractor Estimating: 215it [02:13,  1.76it/s]Extractor Estimating: 216it [02:14,  1.74it/s]Extractor Estimating: 217it [02:14,  1.74it/s]Extractor Estimating: 218it [02:15,  1.75it/s]Extractor Estimating: 219it [02:16,  1.69it/s]Extractor Estimating: 220it [02:16,  1.71it/s]Extractor Estimating: 221it [02:17,  1.68it/s]Extractor Estimating: 222it [02:17,  1.71it/s]Extractor Estimating: 223it [02:18,  1.70it/s]Extractor Estimating: 224it [02:19,  1.70it/s]Extractor Estimating: 225it [02:19,  1.67it/s]Extractor Estimating: 226it [02:20,  1.69it/s]Extractor Estimating: 227it [02:20,  1.69it/s]Extractor Estimating: 228it [02:21,  1.69it/s]Extractor Estimating: 229it [02:22,  1.70it/s]Extractor Estimating: 230it [02:22,  1.73it/s]Extractor Estimating: 231it [02:23,  1.70it/s]Extractor Estimating: 232it [02:23,  1.68it/s]Extractor Estimating: 233it [02:24,  1.72it/s]Extractor Estimating: 234it [02:25,  1.67it/s]Extractor Estimating: 235it [02:25,  1.63it/s]Extractor Estimating: 236it [02:26,  1.61it/s]Extractor Estimating: 237it [02:27,  1.53it/s]Extractor Estimating: 238it [02:27,  1.59it/s]Extractor Estimating: 239it [02:28,  1.63it/s]Extractor Estimating: 240it [02:28,  1.67it/s]Extractor Estimating: 241it [02:29,  1.68it/s]Extractor Estimating: 242it [02:29,  1.69it/s]Extractor Estimating: 243it [02:30,  1.69it/s]Extractor Estimating: 244it [02:31,  1.54it/s]Extractor Estimating: 245it [02:31,  1.55it/s]Extractor Estimating: 246it [02:32,  1.58it/s]Extractor Estimating: 247it [02:33,  1.61it/s]Extractor Estimating: 248it [02:33,  1.65it/s]Extractor Estimating: 249it [02:34,  1.66it/s]Extractor Estimating: 250it [02:34,  1.65it/s]Extractor Estimating: 251it [02:35,  1.66it/s]Extractor Estimating: 252it [02:36,  1.61it/s]Extractor Estimating: 253it [02:36,  1.56it/s]Extractor Estimating: 254it [02:37,  1.55it/s]Extractor Estimating: 255it [02:38,  1.55it/s]Extractor Estimating: 256it [02:38,  1.55it/s]Extractor Estimating: 257it [02:39,  1.56it/s]Extractor Estimating: 258it [02:40,  1.56it/s]Extractor Estimating: 259it [02:40,  1.53it/s]Extractor Estimating: 260it [02:41,  1.49it/s]Extractor Estimating: 261it [02:42,  1.50it/s]Extractor Estimating: 262it [02:42,  1.49it/s]Extractor Estimating: 263it [02:43,  1.50it/s]Extractor Estimating: 264it [02:44,  1.52it/s]Extractor Estimating: 265it [02:44,  1.51it/s]Extractor Estimating: 266it [02:45,  1.48it/s]Extractor Estimating: 267it [02:46,  1.53it/s]Extractor Estimating: 268it [02:46,  1.51it/s]Extractor Estimating: 269it [02:47,  1.50it/s]Extractor Estimating: 270it [02:48,  1.51it/s]Extractor Estimating: 271it [02:48,  1.48it/s]Extractor Estimating: 272it [02:49,  1.52it/s]Extractor Estimating: 273it [02:50,  1.48it/s]Extractor Estimating: 274it [02:50,  1.47it/s]Extractor Estimating: 275it [02:51,  1.51it/s]Extractor Estimating: 276it [02:52,  1.49it/s]Extractor Estimating: 277it [02:52,  1.53it/s]Extractor Estimating: 278it [02:53,  1.54it/s]Extractor Estimating: 279it [02:54,  1.56it/s]Extractor Estimating: 280it [02:54,  1.54it/s]Extractor Estimating: 281it [02:55,  1.56it/s]Extractor Estimating: 282it [02:55,  1.56it/s]Extractor Estimating: 283it [02:56,  1.59it/s]Extractor Estimating: 284it [02:57,  1.51it/s]Extractor Estimating: 285it [02:57,  1.55it/s]Extractor Estimating: 286it [02:58,  1.53it/s]Extractor Estimating: 287it [02:59,  1.55it/s]Extractor Estimating: 288it [02:59,  1.61it/s]Extractor Estimating: 289it [03:00,  1.57it/s]Extractor Estimating: 290it [03:01,  1.54it/s]Extractor Estimating: 291it [03:01,  1.59it/s]Extractor Estimating: 292it [03:02,  1.58it/s]Extractor Estimating: 293it [03:03,  1.56it/s]Extractor Estimating: 294it [03:03,  1.53it/s]Extractor Estimating: 295it [03:04,  1.53it/s]Extractor Estimating: 296it [03:05,  1.51it/s]Extractor Estimating: 297it [03:05,  1.50it/s]Extractor Estimating: 298it [03:06,  1.51it/s]Extractor Estimating: 299it [03:07,  1.50it/s]Extractor Estimating: 300it [03:07,  1.50it/s]Extractor Estimating: 301it [03:08,  1.55it/s]Extractor Estimating: 302it [03:08,  1.61it/s]Extractor Estimating: 303it [03:09,  1.64it/s]Extractor Estimating: 304it [03:10,  1.66it/s]Extractor Estimating: 305it [03:10,  1.67it/s]Extractor Estimating: 306it [03:11,  1.72it/s]Extractor Estimating: 307it [03:11,  1.73it/s]Extractor Estimating: 308it [03:12,  1.72it/s]Extractor Estimating: 309it [03:12,  1.73it/s]Extractor Estimating: 310it [03:13,  1.71it/s]Extractor Estimating: 311it [03:14,  1.72it/s]Extractor Estimating: 312it [03:14,  1.68it/s]Extractor Estimating: 313it [03:15,  1.66it/s]Extractor Estimating: 314it [03:15,  1.66it/s]Extractor Estimating: 315it [03:16,  1.65it/s]Extractor Estimating: 316it [03:17,  1.69it/s]Extractor Estimating: 317it [03:17,  1.63it/s]Extractor Estimating: 318it [03:18,  1.62it/s]Extractor Estimating: 319it [03:18,  1.65it/s]Extractor Estimating: 320it [03:19,  1.67it/s]Extractor Estimating: 321it [03:20,  1.67it/s]Extractor Estimating: 322it [03:20,  1.67it/s]Extractor Estimating: 323it [03:21,  1.62it/s]Extractor Estimating: 324it [03:22,  1.62it/s]Extractor Estimating: 325it [03:22,  1.60it/s]Extractor Estimating: 326it [03:23,  1.59it/s]Extractor Estimating: 327it [03:23,  1.61it/s]Extractor Estimating: 328it [03:24,  1.59it/s]Extractor Estimating: 329it [03:25,  1.59it/s]Extractor Estimating: 330it [03:26,  1.43it/s]Extractor Estimating: 331it [03:26,  1.46it/s]Extractor Estimating: 332it [03:27,  1.51it/s]Extractor Estimating: 333it [03:27,  1.53it/s]Extractor Estimating: 334it [03:28,  1.59it/s]Extractor Estimating: 335it [03:29,  1.62it/s]Extractor Estimating: 336it [03:29,  1.64it/s]Extractor Estimating: 337it [03:30,  1.66it/s]Extractor Estimating: 338it [03:30,  1.61it/s]Extractor Estimating: 339it [03:31,  1.65it/s]Extractor Estimating: 340it [03:32,  1.66it/s]Extractor Estimating: 341it [03:32,  1.64it/s]Extractor Estimating: 342it [03:33,  1.65it/s]Extractor Estimating: 343it [03:33,  1.65it/s]Extractor Estimating: 344it [03:34,  1.65it/s]Extractor Estimating: 345it [03:35,  1.67it/s]Extractor Estimating: 346it [03:35,  1.69it/s]Extractor Estimating: 347it [03:36,  1.69it/s]Extractor Estimating: 348it [03:36,  1.68it/s]Extractor Estimating: 349it [03:37,  1.62it/s]Extractor Estimating: 350it [03:38,  1.59it/s]Extractor Estimating: 351it [03:38,  1.60it/s]Extractor Estimating: 352it [03:39,  1.62it/s]Extractor Estimating: 353it [03:40,  1.65it/s]Extractor Estimating: 354it [03:40,  1.63it/s]Extractor Estimating: 355it [03:41,  1.62it/s]Extractor Estimating: 356it [03:41,  1.67it/s]Extractor Estimating: 357it [03:42,  1.69it/s]Extractor Estimating: 358it [03:43,  1.62it/s]Extractor Estimating: 359it [03:43,  1.64it/s]Extractor Estimating: 360it [03:44,  1.64it/s]Extractor Estimating: 361it [03:44,  1.63it/s]Extractor Estimating: 362it [03:45,  1.64it/s]Extractor Estimating: 363it [03:46,  1.66it/s]Extractor Estimating: 364it [03:46,  1.62it/s]Extractor Estimating: 365it [03:47,  1.65it/s]Extractor Estimating: 366it [03:48,  1.56it/s]Extractor Estimating: 367it [03:48,  1.61it/s]Extractor Estimating: 368it [03:49,  1.64it/s]Extractor Estimating: 369it [03:49,  1.63it/s]Extractor Estimating: 370it [03:50,  1.63it/s]Extractor Estimating: 371it [03:51,  1.67it/s]Extractor Estimating: 372it [03:51,  1.67it/s]Extractor Estimating: 373it [03:52,  1.66it/s]Extractor Estimating: 374it [03:52,  1.68it/s]Extractor Estimating: 375it [03:53,  1.64it/s]Extractor Estimating: 376it [03:54,  1.60it/s]Extractor Estimating: 377it [03:54,  1.65it/s]Extractor Estimating: 378it [03:55,  1.65it/s]Extractor Estimating: 379it [03:55,  1.68it/s]Extractor Estimating: 380it [03:56,  1.63it/s]Extractor Estimating: 381it [03:57,  1.64it/s]Extractor Estimating: 382it [03:57,  1.65it/s]Extractor Estimating: 383it [03:58,  1.62it/s]Extractor Estimating: 384it [03:59,  1.57it/s]Extractor Estimating: 385it [03:59,  1.61it/s]Extractor Estimating: 386it [04:00,  1.61it/s]Extractor Estimating: 387it [04:00,  1.66it/s]Extractor Estimating: 388it [04:01,  1.65it/s]Extractor Estimating: 389it [04:01,  1.67it/s]Extractor Estimating: 390it [04:02,  1.70it/s]Extractor Estimating: 391it [04:03,  1.73it/s]Extractor Estimating: 392it [04:03,  1.65it/s]Extractor Estimating: 393it [04:04,  1.60it/s]Extractor Estimating: 394it [04:05,  1.63it/s]Extractor Estimating: 395it [04:05,  1.62it/s]Extractor Estimating: 396it [04:06,  1.63it/s]Extractor Estimating: 397it [04:06,  1.61it/s]Extractor Estimating: 398it [04:07,  1.62it/s]Extractor Estimating: 399it [04:08,  1.61it/s]Extractor Estimating: 400it [04:08,  1.57it/s]Extractor Estimating: 401it [04:09,  1.63it/s]Extractor Estimating: 402it [04:09,  1.65it/s]Extractor Estimating: 403it [04:10,  1.65it/s]Extractor Estimating: 404it [04:11,  1.64it/s]Extractor Estimating: 405it [04:11,  1.64it/s]Extractor Estimating: 406it [04:12,  1.66it/s]Extractor Estimating: 407it [04:12,  1.65it/s]Extractor Estimating: 408it [04:13,  1.61it/s]Extractor Estimating: 409it [04:14,  1.64it/s]Extractor Estimating: 410it [04:14,  1.65it/s]Extractor Estimating: 411it [04:15,  1.66it/s]Extractor Estimating: 412it [04:16,  1.67it/s]Extractor Estimating: 413it [04:16,  1.69it/s]Extractor Estimating: 414it [04:17,  1.66it/s]Extractor Estimating: 415it [04:17,  1.65it/s]Extractor Estimating: 416it [04:18,  1.68it/s]Extractor Estimating: 417it [04:19,  1.63it/s]Extractor Estimating: 418it [04:19,  1.68it/s]Extractor Estimating: 419it [04:20,  1.66it/s]Extractor Estimating: 420it [04:20,  1.59it/s]Extractor Estimating: 421it [04:21,  1.63it/s]Extractor Estimating: 422it [04:22,  1.62it/s]Extractor Estimating: 423it [04:22,  1.59it/s]Extractor Estimating: 424it [04:23,  1.61it/s]Extractor Estimating: 425it [04:24,  1.55it/s]Extractor Estimating: 426it [04:24,  1.57it/s]Extractor Estimating: 427it [04:25,  1.59it/s]Extractor Estimating: 428it [04:26,  1.53it/s]Extractor Estimating: 429it [04:26,  1.53it/s]Extractor Estimating: 430it [04:27,  1.57it/s]Extractor Estimating: 431it [04:27,  1.62it/s]Extractor Estimating: 432it [04:28,  1.61it/s]Extractor Estimating: 433it [04:29,  1.57it/s]Extractor Estimating: 434it [04:29,  1.58it/s]Extractor Estimating: 435it [04:30,  1.58it/s]Extractor Estimating: 436it [04:31,  1.46it/s]Extractor Estimating: 437it [04:31,  1.47it/s]Extractor Estimating: 438it [04:32,  1.55it/s]Extractor Estimating: 439it [04:33,  1.54it/s]Extractor Estimating: 440it [04:33,  1.57it/s]Extractor Estimating: 441it [04:34,  1.50it/s]Extractor Estimating: 442it [04:35,  1.54it/s]Extractor Estimating: 443it [04:35,  1.57it/s]Extractor Estimating: 444it [04:36,  1.43it/s]Extractor Estimating: 445it [04:37,  1.42it/s]Extractor Estimating: 446it [04:37,  1.48it/s]Extractor Estimating: 447it [04:38,  1.53it/s]Extractor Estimating: 448it [04:39,  1.53it/s]Extractor Estimating: 449it [04:39,  1.52it/s]Extractor Estimating: 450it [04:40,  1.58it/s]Extractor Estimating: 451it [04:41,  1.52it/s]Extractor Estimating: 452it [04:41,  1.54it/s]Extractor Estimating: 453it [04:42,  1.50it/s]Extractor Estimating: 454it [04:43,  1.48it/s]Extractor Estimating: 455it [04:43,  1.52it/s]Extractor Estimating: 456it [04:44,  1.53it/s]Extractor Estimating: 457it [04:45,  1.48it/s]Extractor Estimating: 458it [04:45,  1.52it/s]Extractor Estimating: 459it [04:46,  1.52it/s]Extractor Estimating: 460it [04:46,  1.55it/s]Extractor Estimating: 461it [04:47,  1.46it/s]Extractor Estimating: 462it [04:48,  1.48it/s]Extractor Estimating: 463it [04:49,  1.46it/s]Extractor Estimating: 464it [04:49,  1.44it/s]Extractor Estimating: 465it [04:50,  1.46it/s]Extractor Estimating: 466it [04:51,  1.41it/s]Extractor Estimating: 467it [04:51,  1.47it/s]Extractor Estimating: 468it [04:52,  1.51it/s]Extractor Estimating: 469it [04:53,  1.47it/s]Extractor Estimating: 470it [04:53,  1.47it/s]Extractor Estimating: 471it [04:54,  1.40it/s]Extractor Estimating: 472it [04:55,  1.45it/s]Extractor Estimating: 473it [04:55,  1.49it/s]Extractor Estimating: 474it [04:56,  1.52it/s]Extractor Estimating: 475it [04:57,  1.52it/s]Extractor Estimating: 476it [04:57,  1.60it/s]Extractor Estimating: 477it [04:58,  1.61it/s]Extractor Estimating: 478it [04:59,  1.62it/s]Extractor Estimating: 479it [04:59,  1.55it/s]Extractor Estimating: 480it [05:00,  1.61it/s]Extractor Estimating: 481it [05:00,  1.67it/s]Extractor Estimating: 482it [05:01,  1.69it/s]Extractor Estimating: 483it [05:02,  1.63it/s]Extractor Estimating: 484it [05:02,  1.65it/s]Extractor Estimating: 485it [05:03,  1.67it/s]Extractor Estimating: 486it [05:03,  1.72it/s]Extractor Estimating: 487it [05:04,  1.75it/s]Extractor Estimating: 488it [05:04,  1.69it/s]Extractor Estimating: 489it [05:05,  1.69it/s]Extractor Estimating: 490it [05:06,  1.72it/s]Extractor Estimating: 491it [05:06,  1.72it/s]Extractor Estimating: 492it [05:07,  1.72it/s]Extractor Estimating: 493it [05:07,  1.74it/s]Extractor Estimating: 494it [05:08,  1.71it/s]Extractor Estimating: 495it [05:08,  1.74it/s]Extractor Estimating: 496it [05:09,  1.68it/s]Extractor Estimating: 497it [05:10,  1.55it/s]Extractor Estimating: 498it [05:11,  1.57it/s]Extractor Estimating: 499it [05:11,  1.64it/s]Extractor Estimating: 500it [05:12,  1.76it/s]Extractor Estimating: 500it [05:12,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:50:55,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:50:55,505 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:50:55,505 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:50:55,505 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:50:55,505 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:50:56,320 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:50:56,322 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:50:57,084 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:50:58,234 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:50:58,234 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:51:01,273 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:51:01,303 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:51:01,303 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:51:01,303 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:51:01,303 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:51:02,074 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:51:02,076 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:51:02,816 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:51:03,074 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:51:03,075 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 16:10:12,257 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 16:10:12,598 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9995 mean pseudo reward: 0.9416203008762203
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 19955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20055, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.091, loss:582.2830
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.093, loss:555.9917
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.104, loss:546.6393
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.099, loss:531.8946
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.110, loss:493.7792
>> valid entity prec:0.5193, rec:0.4206, f1:0.4648
>> valid relation prec:0.2719, rec:0.0249, f1:0.0457
>> valid relation with NER prec:0.2719, rec:0.0249, f1:0.0457
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.849, loss:517.9466
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.107, loss:510.6481
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.088, loss:517.0862
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.091, loss:493.1854
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.086, loss:473.4093
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5615, rec:0.5022, f1:0.5302
>> valid relation prec:0.4740, rec:0.1015, f1:0.1673
>> valid relation with NER prec:0.4740, rec:0.1015, f1:0.1673
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.858, loss:512.5385
g_step 1200, step 366, avg_time 1.099, loss:502.7435
g_step 1300, step 49, avg_time 1.111, loss:493.1312
g_step 1400, step 149, avg_time 1.096, loss:489.9165
g_step 1500, step 249, avg_time 1.085, loss:500.8195
>> valid entity prec:0.5353, rec:0.4597, f1:0.4946
>> valid relation prec:0.3063, rec:0.0608, f1:0.1014
>> valid relation with NER prec:0.3063, rec:0.0608, f1:0.1014
g_step 1600, step 349, avg_time 2.828, loss:491.4420
g_step 1700, step 32, avg_time 1.098, loss:458.6732
g_step 1800, step 132, avg_time 1.107, loss:441.6116
g_step 1900, step 232, avg_time 1.097, loss:464.2603
g_step 2000, step 332, avg_time 1.103, loss:482.7796
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5884, rec:0.4317, f1:0.4980
>> valid relation prec:0.3737, rec:0.0896, f1:0.1445
>> valid relation with NER prec:0.3737, rec:0.0896, f1:0.1445
g_step 2100, step 15, avg_time 2.827, loss:446.8016
g_step 2200, step 115, avg_time 1.104, loss:413.7945
g_step 2300, step 215, avg_time 1.096, loss:444.3632
g_step 2400, step 315, avg_time 1.091, loss:462.4817
g_step 2500, step 415, avg_time 1.101, loss:445.0283
>> valid entity prec:0.5303, rec:0.4893, f1:0.5090
>> valid relation prec:0.4402, rec:0.1380, f1:0.2101
>> valid relation with NER prec:0.4402, rec:0.1380, f1:0.2101
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 98, avg_time 2.858, loss:404.6983
g_step 2700, step 198, avg_time 1.093, loss:402.6571
g_step 2800, step 298, avg_time 1.107, loss:447.5779
g_step 2900, step 398, avg_time 1.090, loss:447.3504
g_step 3000, step 81, avg_time 1.106, loss:393.9816
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5591, rec:0.4677, f1:0.5093
>> valid relation prec:0.3973, rec:0.1195, f1:0.1837
>> valid relation with NER prec:0.3973, rec:0.1195, f1:0.1837
g_step 3100, step 181, avg_time 2.827, loss:407.8027
g_step 3200, step 281, avg_time 1.097, loss:406.9447
g_step 3300, step 381, avg_time 1.108, loss:425.4452
g_step 3400, step 64, avg_time 1.095, loss:395.9670
g_step 3500, step 164, avg_time 1.102, loss:394.1191
>> valid entity prec:0.5788, rec:0.4862, f1:0.5285
>> valid relation prec:0.4871, rec:0.1598, f1:0.2407
>> valid relation with NER prec:0.4871, rec:0.1598, f1:0.2407
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 264, avg_time 2.859, loss:398.1232
g_step 3700, step 364, avg_time 1.103, loss:414.6345
g_step 3800, step 47, avg_time 1.088, loss:390.8943
g_step 3900, step 147, avg_time 1.109, loss:376.2059
g_step 4000, step 247, avg_time 1.101, loss:396.4319
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5463, rec:0.4793, f1:0.5106
>> valid relation prec:0.3926, rec:0.1133, f1:0.1758
>> valid relation with NER prec:0.3926, rec:0.1133, f1:0.1758
g_step 4100, step 347, avg_time 2.861, loss:389.4193
g_step 4200, step 30, avg_time 1.090, loss:390.1235
g_step 4300, step 130, avg_time 1.094, loss:353.4098
g_step 4400, step 230, avg_time 1.111, loss:348.8614
g_step 4500, step 330, avg_time 1.075, loss:380.3436
>> valid entity prec:0.5418, rec:0.4845, f1:0.5115
>> valid relation prec:0.3504, rec:0.1055, f1:0.1621
>> valid relation with NER prec:0.3504, rec:0.1055, f1:0.1621
g_step 4600, step 13, avg_time 2.854, loss:375.9743
g_step 4700, step 113, avg_time 1.084, loss:336.3918
g_step 4800, step 213, avg_time 1.104, loss:376.1976
g_step 4900, step 313, avg_time 1.079, loss:375.5432
g_step 5000, step 413, avg_time 1.102, loss:376.1165
learning rate was adjusted to 0.0008
>> valid entity prec:0.5424, rec:0.5111, f1:0.5263
>> valid relation prec:0.3636, rec:0.1221, f1:0.1829
>> valid relation with NER prec:0.3636, rec:0.1221, f1:0.1829
g_step 5100, step 96, avg_time 2.810, loss:325.9822
g_step 5200, step 196, avg_time 1.098, loss:343.8111
g_step 5300, step 296, avg_time 1.099, loss:335.0563
g_step 5400, step 396, avg_time 1.102, loss:370.7021
g_step 5500, step 79, avg_time 1.096, loss:328.0756
>> valid entity prec:0.5527, rec:0.4819, f1:0.5149
>> valid relation prec:0.3338, rec:0.1061, f1:0.1610
>> valid relation with NER prec:0.3338, rec:0.1061, f1:0.1610
g_step 5600, step 179, avg_time 2.839, loss:336.3784
g_step 5700, step 279, avg_time 1.085, loss:328.7876
g_step 5800, step 379, avg_time 1.098, loss:347.6505
g_step 5900, step 62, avg_time 1.094, loss:318.6589
g_step 6000, step 162, avg_time 1.116, loss:317.0869
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5315, rec:0.4662, f1:0.4967
>> valid relation prec:0.2423, rec:0.0564, f1:0.0915
>> valid relation with NER prec:0.2423, rec:0.0564, f1:0.0915
g_step 6100, step 262, avg_time 2.822, loss:325.2108
g_step 6200, step 362, avg_time 1.077, loss:334.4158
g_step 6300, step 45, avg_time 1.091, loss:324.4406
g_step 6400, step 145, avg_time 1.100, loss:295.9319
g_step 6500, step 245, avg_time 1.084, loss:319.5621
>> valid entity prec:0.5357, rec:0.5157, f1:0.5255
>> valid relation prec:0.3593, rec:0.1228, f1:0.1830
>> valid relation with NER prec:0.3593, rec:0.1228, f1:0.1830
g_step 6600, step 345, avg_time 2.831, loss:339.0273
g_step 6700, step 28, avg_time 1.101, loss:317.0707
g_step 6800, step 128, avg_time 1.087, loss:297.9158
g_step 6900, step 228, avg_time 1.110, loss:308.5572
g_step 7000, step 328, avg_time 1.098, loss:333.1901
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5688, rec:0.4899, f1:0.5264
>> valid relation prec:0.3563, rec:0.0927, f1:0.1471
>> valid relation with NER prec:0.3563, rec:0.0927, f1:0.1471
g_step 7100, step 11, avg_time 2.823, loss:309.8617
g_step 7200, step 111, avg_time 1.100, loss:292.6418
g_step 7300, step 211, avg_time 1.108, loss:303.1033
g_step 7400, step 311, avg_time 1.097, loss:317.9518
g_step 7500, step 411, avg_time 1.094, loss:310.6100
>> valid entity prec:0.5693, rec:0.4727, f1:0.5165
>> valid relation prec:0.3533, rec:0.0945, f1:0.1492
>> valid relation with NER prec:0.3533, rec:0.0945, f1:0.1492
g_step 7600, step 94, avg_time 2.844, loss:283.5672
g_step 7700, step 194, avg_time 1.110, loss:287.3736
g_step 7800, step 294, avg_time 1.090, loss:290.6260
g_step 7900, step 394, avg_time 1.103, loss:309.0201
g_step 8000, step 77, avg_time 1.088, loss:278.0307
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5497, rec:0.4972, f1:0.5221
>> valid relation prec:0.2879, rec:0.0943, f1:0.1421
>> valid relation with NER prec:0.2879, rec:0.0943, f1:0.1421
g_step 8100, step 177, avg_time 2.842, loss:265.0243
g_step 8200, step 277, avg_time 1.085, loss:288.6647
g_step 8300, step 377, avg_time 1.113, loss:289.3382
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 16:10:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 16:10:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_16-10-12_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 16:10:13 - WARNING - datasets.builder -   Using custom data configuration default-dff7ba17116edfe0
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-dff7ba17116edfe0/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 16:10:18,353 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 16:10:18,354 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 16:10:18,355 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 16:10:18,356 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 16:10:18,602 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 16:10:18,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 16:10:18,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 16:10:18,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 16:10:18,723 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 16:10:18,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 16:10:18,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 16:10:19,456 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 16:10:43,418 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 16:10:43,461 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-dff7ba17116edfe0/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:03,  2.53ba/s] 20%|██        | 2/10 [00:00<00:02,  3.51ba/s] 30%|███       | 3/10 [00:00<00:01,  4.04ba/s] 40%|████      | 4/10 [00:01<00:01,  4.31ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.51ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.64ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.73ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.81ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.83ba/s]100%|██████████| 10/10 [00:02<00:00,  4.87ba/s]100%|██████████| 10/10 [00:02<00:00,  4.48ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.86ba/s] 40%|████      | 2/5 [00:00<00:01,  2.81ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.39ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.34ba/s]100%|██████████| 5/5 [00:01<00:00,  3.83ba/s]100%|██████████| 5/5 [00:01<00:00,  3.36ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  4.21ba/s] 30%|███       | 3/10 [00:00<00:00,  7.44ba/s] 40%|████      | 4/10 [00:00<00:00,  8.14ba/s] 60%|██████    | 6/10 [00:00<00:00,  9.04ba/s] 80%|████████  | 8/10 [00:00<00:00,  9.50ba/s]100%|██████████| 10/10 [00:01<00:00,  9.68ba/s]100%|██████████| 10/10 [00:01<00:00,  8.87ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.10ba/s] 60%|██████    | 3/5 [00:00<00:00,  6.35ba/s]100%|██████████| 5/5 [00:00<00:00,  8.03ba/s]100%|██████████| 5/5 [00:00<00:00,  7.05ba/s]
[INFO|trainer.py:414] 2023-08-29 16:10:53,131 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 16:10:53,341 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 16:10:53,341 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 16:10:53,341 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 16:10:53,341 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 16:10:53,341 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 16:10:53,341 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 16:10:53,342 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:02<29:57,  2.31s/it]  0%|          | 2/780 [00:03<24:46,  1.91s/it]  0%|          | 3/780 [00:04<19:29,  1.51s/it]  1%|          | 4/780 [00:05<15:11,  1.17s/it]  1%|          | 5/780 [00:06<13:30,  1.05s/it]  1%|          | 6/780 [00:06<10:31,  1.22it/s]  1%|          | 7/780 [00:07<09:49,  1.31it/s]  1%|          | 8/780 [00:07<08:20,  1.54it/s]  1%|          | 9/780 [00:08<06:52,  1.87it/s]  1%|▏         | 10/780 [00:08<05:52,  2.18it/s]  1%|▏         | 11/780 [00:08<05:11,  2.47it/s]  2%|▏         | 12/780 [00:09<05:09,  2.48it/s]  2%|▏         | 13/780 [00:09<04:41,  2.72it/s]  2%|▏         | 14/780 [00:09<04:22,  2.92it/s]  2%|▏         | 15/780 [00:09<04:08,  3.07it/s]  2%|▏         | 16/780 [00:10<03:59,  3.19it/s]  2%|▏         | 17/780 [00:10<04:17,  2.96it/s]  2%|▏         | 18/780 [00:11<04:26,  2.86it/s]  2%|▏         | 19/780 [00:11<04:11,  3.02it/s]  3%|▎         | 20/780 [00:11<04:01,  3.15it/s]  3%|▎         | 21/780 [00:11<03:53,  3.25it/s]  3%|▎         | 22/780 [00:12<04:12,  3.00it/s]  3%|▎         | 23/780 [00:12<04:01,  3.13it/s]  3%|▎         | 24/780 [00:12<03:53,  3.24it/s]  3%|▎         | 25/780 [00:13<03:48,  3.31it/s]  3%|▎         | 26/780 [00:13<03:44,  3.36it/s]  3%|▎         | 27/780 [00:13<03:41,  3.40it/s]  4%|▎         | 28/780 [00:14<04:01,  3.11it/s]  4%|▎         | 29/780 [00:14<03:54,  3.21it/s]  4%|▍         | 30/780 [00:14<03:47,  3.29it/s]  4%|▍         | 31/780 [00:14<03:43,  3.35it/s]  4%|▍         | 32/780 [00:15<04:01,  3.10it/s]  4%|▍         | 33/780 [00:15<03:52,  3.21it/s]  4%|▍         | 34/780 [00:15<03:46,  3.29it/s]  4%|▍         | 35/780 [00:16<03:42,  3.35it/s]  5%|▍         | 36/780 [00:16<03:39,  3.39it/s]  5%|▍         | 37/780 [00:16<03:37,  3.42it/s]  5%|▍         | 38/780 [00:17<03:51,  3.20it/s]  5%|▌         | 39/780 [00:17<03:46,  3.27it/s]  5%|▌         | 40/780 [00:17<03:42,  3.33it/s]  5%|▌         | 41/780 [00:18<03:39,  3.37it/s]  5%|▌         | 42/780 [00:18<03:45,  3.27it/s]  6%|▌         | 43/780 [00:18<03:41,  3.33it/s]  6%|▌         | 44/780 [00:18<03:38,  3.37it/s]  6%|▌         | 45/780 [00:19<04:12,  2.91it/s]  6%|▌         | 46/780 [00:19<04:00,  3.05it/s]  6%|▌         | 47/780 [00:19<03:51,  3.17it/s]  6%|▌         | 48/780 [00:23<14:19,  1.17s/it]  6%|▋         | 49/780 [00:23<11:03,  1.10it/s]  6%|▋         | 50/780 [00:23<08:46,  1.39it/s]  7%|▋         | 51/780 [00:23<07:10,  1.69it/s]  7%|▋         | 52/780 [00:24<06:03,  2.00it/s]  7%|▋         | 53/780 [00:24<05:16,  2.30it/s]  7%|▋         | 54/780 [00:24<05:04,  2.39it/s]  7%|▋         | 55/780 [00:25<04:51,  2.49it/s]  7%|▋         | 56/780 [00:25<04:25,  2.72it/s]  7%|▋         | 57/780 [00:25<04:07,  2.92it/s]  7%|▋         | 58/780 [00:26<03:55,  3.07it/s]  8%|▊         | 59/780 [00:26<03:46,  3.19it/s]  8%|▊         | 60/780 [00:26<03:39,  3.27it/s]  8%|▊         | 61/780 [00:27<03:35,  3.34it/s]  8%|▊         | 62/780 [00:27<03:32,  3.39it/s]  8%|▊         | 63/780 [00:27<03:29,  3.42it/s]  8%|▊         | 64/780 [00:27<03:28,  3.44it/s]  8%|▊         | 65/780 [00:28<03:27,  3.45it/s]  8%|▊         | 66/780 [00:28<03:38,  3.27it/s]  9%|▊         | 67/780 [00:28<03:33,  3.33it/s]  9%|▊         | 68/780 [00:29<03:30,  3.37it/s]  9%|▉         | 69/780 [00:29<03:28,  3.41it/s]  9%|▉         | 70/780 [00:29<03:27,  3.43it/s]  9%|▉         | 71/780 [00:29<03:25,  3.44it/s]  9%|▉         | 72/780 [00:30<03:25,  3.45it/s]  9%|▉         | 73/780 [00:30<03:24,  3.46it/s]  9%|▉         | 74/780 [00:30<03:23,  3.46it/s] 10%|▉         | 75/780 [00:31<03:23,  3.47it/s] 10%|▉         | 76/780 [00:31<03:22,  3.47it/s] 10%|▉         | 77/780 [00:31<03:37,  3.23it/s] 10%|█         | 78/780 [00:32<03:32,  3.30it/s] 10%|█         | 79/780 [00:32<03:29,  3.35it/s] 10%|█         | 80/780 [00:32<03:26,  3.38it/s] 10%|█         | 81/780 [00:32<03:24,  3.41it/s] 11%|█         | 82/780 [00:33<03:23,  3.43it/s] 11%|█         | 83/780 [00:33<03:22,  3.44it/s] 11%|█         | 84/780 [00:33<03:21,  3.45it/s] 11%|█         | 85/780 [00:34<03:21,  3.45it/s] 11%|█         | 86/780 [00:34<03:20,  3.46it/s] 11%|█         | 87/780 [00:34<03:20,  3.46it/s] 11%|█▏        | 88/780 [00:34<03:32,  3.25it/s] 11%|█▏        | 89/780 [00:35<03:28,  3.32it/s] 12%|█▏        | 90/780 [00:35<03:25,  3.36it/s] 12%|█▏        | 91/780 [00:35<03:23,  3.39it/s] 12%|█▏        | 92/780 [00:36<03:21,  3.42it/s] 12%|█▏        | 93/780 [00:36<03:20,  3.43it/s] 12%|█▏        | 94/780 [00:36<03:19,  3.44it/s] 12%|█▏        | 95/780 [00:36<03:18,  3.45it/s] 12%|█▏        | 96/780 [00:37<03:17,  3.46it/s] 12%|█▏        | 97/780 [00:37<03:17,  3.46it/s] 13%|█▎        | 98/780 [00:37<03:16,  3.47it/s] 13%|█▎        | 99/780 [00:38<03:37,  3.14it/s] 13%|█▎        | 100/780 [00:38<03:30,  3.23it/s] 13%|█▎        | 101/780 [00:38<03:25,  3.30it/s] 13%|█▎        | 102/780 [00:39<03:22,  3.35it/s] 13%|█▎        | 103/780 [00:39<03:20,  3.38it/s] 13%|█▎        | 104/780 [00:39<03:18,  3.41it/s] 13%|█▎        | 105/780 [00:39<03:17,  3.43it/s] 14%|█▎        | 106/780 [00:40<03:15,  3.44it/s] 14%|█▎        | 107/780 [00:40<03:15,  3.45it/s] 14%|█▍        | 108/780 [00:40<03:14,  3.45it/s] 14%|█▍        | 109/780 [00:41<03:13,  3.46it/s] 14%|█▍        | 110/780 [00:41<03:31,  3.16it/s] 14%|█▍        | 111/780 [00:41<03:25,  3.25it/s] 14%|█▍        | 112/780 [00:42<03:21,  3.31it/s] 14%|█▍        | 113/780 [00:42<03:18,  3.36it/s] 15%|█▍        | 114/780 [00:42<03:16,  3.39it/s] 15%|█▍        | 115/780 [00:42<03:14,  3.41it/s] 15%|█▍        | 116/780 [00:43<03:13,  3.43it/s] 15%|█▌        | 117/780 [00:43<03:12,  3.45it/s] 15%|█▌        | 118/780 [00:43<03:11,  3.45it/s] 15%|█▌        | 119/780 [00:44<03:11,  3.46it/s] 15%|█▌        | 120/780 [00:44<03:10,  3.46it/s] 16%|█▌        | 121/780 [00:44<03:22,  3.25it/s] 16%|█▌        | 122/780 [00:45<03:18,  3.31it/s] 16%|█▌        | 123/780 [00:45<03:15,  3.36it/s] 16%|█▌        | 124/780 [00:45<03:13,  3.39it/s] 16%|█▌        | 125/780 [00:45<03:11,  3.41it/s] 16%|█▌        | 126/780 [00:46<03:10,  3.43it/s] 16%|█▋        | 127/780 [00:46<03:09,  3.44it/s] 16%|█▋        | 128/780 [00:46<03:09,  3.44it/s] 17%|█▋        | 129/780 [00:47<03:08,  3.45it/s] 17%|█▋        | 130/780 [00:47<03:08,  3.45it/s] 17%|█▋        | 131/780 [00:47<03:07,  3.46it/s] 17%|█▋        | 132/780 [00:47<03:20,  3.24it/s] 17%|█▋        | 133/780 [00:48<03:15,  3.30it/s] 17%|█▋        | 134/780 [00:48<03:13,  3.35it/s] 17%|█▋        | 135/780 [00:48<03:10,  3.38it/s] 17%|█▋        | 136/780 [00:49<03:09,  3.40it/s] 18%|█▊        | 137/780 [00:49<03:07,  3.42it/s] 18%|█▊        | 138/780 [00:49<03:06,  3.43it/s] 18%|█▊        | 139/780 [00:49<03:06,  3.44it/s] 18%|█▊        | 140/780 [00:50<03:05,  3.45it/s] 18%|█▊        | 141/780 [00:50<03:05,  3.45it/s] 18%|█▊        | 142/780 [00:50<03:04,  3.45it/s] 18%|█▊        | 143/780 [00:51<03:14,  3.27it/s] 18%|█▊        | 144/780 [00:51<03:11,  3.33it/s] 19%|█▊        | 145/780 [00:51<03:08,  3.37it/s] 19%|█▊        | 146/780 [00:52<03:06,  3.39it/s] 19%|█▉        | 147/780 [00:52<03:05,  3.41it/s] 19%|█▉        | 148/780 [00:52<03:04,  3.43it/s] 19%|█▉        | 149/780 [00:53<03:20,  3.14it/s] 19%|█▉        | 150/780 [00:53<03:15,  3.23it/s] 19%|█▉        | 151/780 [00:53<03:10,  3.29it/s] 19%|█▉        | 152/780 [00:53<03:07,  3.34it/s] 20%|█▉        | 153/780 [00:54<03:05,  3.37it/s] 20%|█▉        | 154/780 [00:54<03:04,  3.40it/s] 20%|█▉        | 155/780 [00:54<03:02,  3.42it/s] 20%|██        | 156/780 [00:55<03:01,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 16:11:48,489 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 16:11:48,489 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 16:11:48,489 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 59.64it/s][A
  2%|▏         | 12/608 [00:00<00:11, 51.43it/s][A
  3%|▎         | 18/608 [00:00<00:11, 49.34it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.45it/s][A
  5%|▍         | 28/608 [00:00<00:13, 43.45it/s][A
  5%|▌         | 33/608 [00:00<00:12, 44.42it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.21it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.74it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.13it/s][A
  9%|▊         | 53/608 [00:01<00:13, 42.10it/s][A
 10%|▉         | 58/608 [00:01<00:12, 43.38it/s][A
 10%|█         | 63/608 [00:01<00:12, 44.37it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.06it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.58it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.84it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.15it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.34it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 46.46it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.60it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.64it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.68it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.65it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.67it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.80it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.69it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.75it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.77it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.65it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.70it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.77it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.79it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.80it/s][A
 28%|██▊       | 168/608 [00:03<00:10, 40.66it/s][A
 28%|██▊       | 173/608 [00:03<00:10, 42.33it/s][A
 29%|██▉       | 178/608 [00:03<00:10, 39.27it/s][A
 30%|███       | 183/608 [00:04<00:10, 41.30it/s][A
 31%|███       | 188/608 [00:04<00:09, 42.75it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 43.81it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 44.69it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.35it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.78it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.02it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.14it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.23it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 46.40it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.57it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.58it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.66it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.73it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.73it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.64it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.70it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.69it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.63it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 46.66it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.71it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.70it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.70it/s][A
 49%|████▉     | 298/608 [00:06<00:07, 39.49it/s][A
 50%|████▉     | 303/608 [00:06<00:08, 35.11it/s][A
 51%|█████     | 308/608 [00:06<00:07, 37.96it/s][A
 51%|█████▏    | 313/608 [00:06<00:07, 40.26it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 41.98it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 43.24it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 44.30it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.03it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.57it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.61it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.89it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.22it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.39it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 46.54it/s][A
 61%|██████    | 368/608 [00:08<00:05, 46.48it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.56it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.61it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.71it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.63it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.61it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.58it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.61it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 46.63it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 46.68it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.74it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.68it/s][A
 70%|███████   | 428/608 [00:09<00:04, 37.69it/s][A
 71%|███████   | 433/608 [00:09<00:04, 39.39it/s][A
 72%|███████▏  | 438/608 [00:09<00:05, 33.84it/s][A
 73%|███████▎  | 443/608 [00:09<00:04, 36.89it/s][A
 74%|███████▎  | 448/608 [00:10<00:04, 39.32it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 41.32it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 42.85it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 43.98it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 44.77it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.38it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.44it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.84it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.04it/s][A
 81%|████████  | 493/608 [00:11<00:02, 46.24it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 46.41it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 46.41it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 46.62it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.67it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.63it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.62it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.61it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.58it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.50it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 46.59it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 46.56it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 46.68it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.74it/s][A
 93%|█████████▎| 563/608 [00:12<00:01, 37.30it/s][A
 93%|█████████▎| 568/608 [00:12<00:01, 38.90it/s][A
 94%|█████████▍| 573/608 [00:12<00:01, 34.29it/s][A
 95%|█████████▌| 578/608 [00:13<00:00, 37.30it/s][A
 96%|█████████▌| 583/608 [00:13<00:00, 39.69it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 41.60it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 43.05it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 44.05it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 44.89it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.48it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.48it/s][A 20%|██        | 156/780 [01:08<03:01,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 16:12:03,373 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 16:12:04,095 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 16:12:10,997 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 16:12:11,320 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 16:12:11,442 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:36<2:11:51, 12.70s/it] 20%|██        | 158/780 [01:37<1:33:26,  9.01s/it] 20%|██        | 159/780 [01:37<1:06:11,  6.40s/it] 21%|██        | 160/780 [01:37<47:08,  4.56s/it]   21%|██        | 161/780 [01:37<33:50,  3.28s/it] 21%|██        | 162/780 [01:38<24:32,  2.38s/it] 21%|██        | 163/780 [01:38<18:01,  1.75s/it] 21%|██        | 164/780 [01:38<13:29,  1.31s/it] 21%|██        | 165/780 [01:39<10:18,  1.01s/it] 21%|██▏       | 166/780 [01:39<08:04,  1.27it/s] 21%|██▏       | 167/780 [01:39<06:31,  1.56it/s] 22%|██▏       | 168/780 [01:39<05:26,  1.87it/s] 22%|██▏       | 169/780 [01:40<04:57,  2.05it/s] 22%|██▏       | 170/780 [01:40<04:20,  2.34it/s] 22%|██▏       | 171/780 [01:40<03:54,  2.60it/s] 22%|██▏       | 172/780 [01:41<03:36,  2.81it/s] 22%|██▏       | 173/780 [01:41<03:23,  2.98it/s] 22%|██▏       | 174/780 [01:41<03:14,  3.11it/s] 22%|██▏       | 175/780 [01:42<03:08,  3.22it/s] 23%|██▎       | 176/780 [01:42<03:03,  3.29it/s] 23%|██▎       | 177/780 [01:42<03:00,  3.35it/s] 23%|██▎       | 178/780 [01:42<02:57,  3.39it/s] 23%|██▎       | 179/780 [01:43<02:55,  3.42it/s] 23%|██▎       | 180/780 [01:43<03:11,  3.13it/s] 23%|██▎       | 181/780 [01:43<03:05,  3.22it/s] 23%|██▎       | 182/780 [01:44<03:01,  3.30it/s] 23%|██▎       | 183/780 [01:44<02:58,  3.35it/s] 24%|██▎       | 184/780 [01:44<02:55,  3.39it/s] 24%|██▎       | 185/780 [01:45<02:54,  3.41it/s] 24%|██▍       | 186/780 [01:45<02:52,  3.44it/s] 24%|██▍       | 187/780 [01:45<02:52,  3.45it/s] 24%|██▍       | 188/780 [01:45<02:51,  3.46it/s] 24%|██▍       | 189/780 [01:46<02:50,  3.46it/s] 24%|██▍       | 190/780 [01:46<02:50,  3.47it/s] 24%|██▍       | 191/780 [01:46<02:56,  3.33it/s] 25%|██▍       | 192/780 [01:47<02:54,  3.37it/s] 25%|██▍       | 193/780 [01:47<02:52,  3.41it/s] 25%|██▍       | 194/780 [01:47<02:51,  3.43it/s] 25%|██▌       | 195/780 [01:47<02:50,  3.44it/s] 25%|██▌       | 196/780 [01:48<02:49,  3.45it/s] 25%|██▌       | 197/780 [01:48<02:48,  3.46it/s] 25%|██▌       | 198/780 [01:48<02:48,  3.46it/s] 26%|██▌       | 199/780 [01:49<02:47,  3.47it/s] 26%|██▌       | 200/780 [01:49<02:47,  3.47it/s] 26%|██▌       | 201/780 [01:49<02:46,  3.47it/s] 26%|██▌       | 202/780 [01:50<02:57,  3.26it/s] 26%|██▌       | 203/780 [01:50<02:53,  3.32it/s] 26%|██▌       | 204/780 [01:50<02:51,  3.36it/s] 26%|██▋       | 205/780 [01:50<02:49,  3.39it/s] 26%|██▋       | 206/780 [01:51<02:47,  3.42it/s] 27%|██▋       | 207/780 [01:51<02:47,  3.43it/s] 27%|██▋       | 208/780 [01:51<02:46,  3.44it/s] 27%|██▋       | 209/780 [01:52<02:45,  3.45it/s] 27%|██▋       | 210/780 [01:52<02:44,  3.46it/s] 27%|██▋       | 211/780 [01:52<02:44,  3.46it/s] 27%|██▋       | 212/780 [01:52<02:43,  3.47it/s] 27%|██▋       | 213/780 [01:53<02:56,  3.21it/s] 27%|██▋       | 214/780 [01:53<02:52,  3.29it/s] 28%|██▊       | 215/780 [01:53<02:49,  3.34it/s] 28%|██▊       | 216/780 [01:54<02:46,  3.38it/s] 28%|██▊       | 217/780 [01:54<02:55,  3.21it/s] 28%|██▊       | 218/780 [01:54<02:51,  3.28it/s] 28%|██▊       | 219/780 [01:55<02:48,  3.34it/s] 28%|██▊       | 220/780 [01:55<02:45,  3.37it/s] 28%|██▊       | 221/780 [01:55<02:44,  3.40it/s] 28%|██▊       | 222/780 [01:55<02:43,  3.42it/s] 29%|██▊       | 223/780 [01:56<02:42,  3.43it/s] 29%|██▊       | 224/780 [01:56<02:41,  3.45it/s] 29%|██▉       | 225/780 [01:56<02:40,  3.45it/s] 29%|██▉       | 226/780 [01:57<02:40,  3.46it/s] 29%|██▉       | 227/780 [01:57<02:49,  3.26it/s] 29%|██▉       | 228/780 [01:57<02:46,  3.32it/s] 29%|██▉       | 229/780 [01:58<02:43,  3.36it/s] 29%|██▉       | 230/780 [01:58<02:41,  3.40it/s] 30%|██▉       | 231/780 [01:58<02:40,  3.42it/s] 30%|██▉       | 232/780 [01:58<02:39,  3.43it/s] 30%|██▉       | 233/780 [01:59<02:38,  3.44it/s] 30%|███       | 234/780 [01:59<02:38,  3.45it/s] 30%|███       | 235/780 [01:59<02:37,  3.46it/s] 30%|███       | 236/780 [02:00<02:37,  3.46it/s] 30%|███       | 237/780 [02:00<02:36,  3.46it/s] 31%|███       | 238/780 [02:00<02:47,  3.24it/s] 31%|███       | 239/780 [02:00<02:43,  3.31it/s] 31%|███       | 240/780 [02:01<02:41,  3.35it/s] 31%|███       | 241/780 [02:01<02:39,  3.39it/s] 31%|███       | 242/780 [02:01<02:37,  3.41it/s] 31%|███       | 243/780 [02:02<02:36,  3.43it/s] 31%|███▏      | 244/780 [02:02<02:35,  3.44it/s] 31%|███▏      | 245/780 [02:02<02:35,  3.45it/s] 32%|███▏      | 246/780 [02:02<02:34,  3.45it/s] 32%|███▏      | 247/780 [02:03<02:34,  3.45it/s] 32%|███▏      | 248/780 [02:03<02:33,  3.46it/s] 32%|███▏      | 249/780 [02:03<02:41,  3.29it/s] 32%|███▏      | 250/780 [02:04<02:38,  3.34it/s] 32%|███▏      | 251/780 [02:04<02:36,  3.38it/s] 32%|███▏      | 252/780 [02:04<02:35,  3.40it/s] 32%|███▏      | 253/780 [02:05<02:34,  3.42it/s] 33%|███▎      | 254/780 [02:05<02:33,  3.43it/s] 33%|███▎      | 255/780 [02:05<02:32,  3.44it/s] 33%|███▎      | 256/780 [02:05<02:31,  3.45it/s] 33%|███▎      | 257/780 [02:06<02:31,  3.46it/s] 33%|███▎      | 258/780 [02:06<02:30,  3.46it/s] 33%|███▎      | 259/780 [02:06<02:30,  3.46it/s] 33%|███▎      | 260/780 [02:07<02:45,  3.14it/s] 33%|███▎      | 261/780 [02:07<02:40,  3.23it/s] 34%|███▎      | 262/780 [02:07<02:37,  3.30it/s] 34%|███▎      | 263/780 [02:08<02:34,  3.34it/s] 34%|███▍      | 264/780 [02:08<02:32,  3.38it/s] 34%|███▍      | 265/780 [02:08<02:31,  3.40it/s] 34%|███▍      | 266/780 [02:08<02:30,  3.42it/s] 34%|███▍      | 267/780 [02:09<02:29,  3.44it/s] 34%|███▍      | 268/780 [02:09<02:28,  3.44it/s] 34%|███▍      | 269/780 [02:09<02:28,  3.45it/s] 35%|███▍      | 270/780 [02:10<02:27,  3.45it/s] 35%|███▍      | 271/780 [02:10<02:42,  3.14it/s] 35%|███▍      | 272/780 [02:10<02:37,  3.23it/s] 35%|███▌      | 273/780 [02:11<02:33,  3.30it/s] 35%|███▌      | 274/780 [02:11<02:31,  3.35it/s] 35%|███▌      | 275/780 [02:11<02:29,  3.38it/s] 35%|███▌      | 276/780 [02:11<02:27,  3.41it/s] 36%|███▌      | 277/780 [02:12<02:26,  3.43it/s] 36%|███▌      | 278/780 [02:12<02:25,  3.44it/s] 36%|███▌      | 279/780 [02:12<02:25,  3.44it/s] 36%|███▌      | 280/780 [02:13<02:24,  3.45it/s] 36%|███▌      | 281/780 [02:13<02:24,  3.46it/s] 36%|███▌      | 282/780 [02:13<02:35,  3.20it/s] 36%|███▋      | 283/780 [02:13<02:31,  3.28it/s] 36%|███▋      | 284/780 [02:14<02:28,  3.33it/s] 37%|███▋      | 285/780 [02:14<02:26,  3.37it/s] 37%|███▋      | 286/780 [02:14<02:25,  3.40it/s] 37%|███▋      | 287/780 [02:15<02:24,  3.42it/s] 37%|███▋      | 288/780 [02:15<02:23,  3.43it/s] 37%|███▋      | 289/780 [02:15<02:22,  3.44it/s] 37%|███▋      | 290/780 [02:16<02:22,  3.45it/s] 37%|███▋      | 291/780 [02:16<02:21,  3.45it/s] 37%|███▋      | 292/780 [02:16<02:30,  3.24it/s] 38%|███▊      | 293/780 [02:17<03:32,  2.29it/s] 38%|███▊      | 294/780 [02:17<03:10,  2.55it/s] 38%|███▊      | 295/780 [02:18<03:05,  2.61it/s] 38%|███▊      | 296/780 [02:18<02:51,  2.82it/s] 38%|███▊      | 297/780 [02:18<02:41,  2.98it/s] 38%|███▊      | 298/780 [02:18<02:34,  3.11it/s] 38%|███▊      | 299/780 [02:19<02:29,  3.21it/s] 38%|███▊      | 300/780 [02:19<02:26,  3.28it/s] 39%|███▊      | 301/780 [02:19<02:23,  3.33it/s] 39%|███▊      | 302/780 [02:20<02:26,  3.26it/s] 39%|███▉      | 303/780 [02:20<02:23,  3.31it/s] 39%|███▉      | 304/780 [02:20<02:21,  3.35it/s] 39%|███▉      | 305/780 [02:20<02:20,  3.38it/s] 39%|███▉      | 306/780 [02:21<02:19,  3.41it/s] 39%|███▉      | 307/780 [02:21<02:18,  3.42it/s] 39%|███▉      | 308/780 [02:21<02:17,  3.43it/s] 40%|███▉      | 309/780 [02:22<02:17,  3.44it/s] 40%|███▉      | 310/780 [02:22<02:16,  3.44it/s] 40%|███▉      | 311/780 [02:22<02:16,  3.45it/s] 40%|████      | 312/780 [02:22<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 16:13:16,443 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 16:13:16,443 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 16:13:16,443 >>   Batch size = 8
{'eval_loss': 1.0138314962387085, 'eval_runtime': 13.743, 'eval_samples_per_second': 353.925, 'eval_steps_per_second': 44.241, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.77it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.37it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.46it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.89it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.55it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.38it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.18it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.89it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.82it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.75it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.79it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.70it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.67it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.71it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.69it/s][A
 14%|█▎        | 83/608 [00:01<00:13, 39.31it/s][A
 14%|█▍        | 88/608 [00:01<00:12, 41.26it/s][A
 15%|█▌        | 93/608 [00:02<00:12, 42.67it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 43.88it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 44.60it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.34it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.81it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.09it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.11it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.27it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.41it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 46.55it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.48it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.54it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.66it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.68it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.73it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.59it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.52it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.57it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.56it/s][A
 31%|███       | 188/608 [00:04<00:08, 46.71it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.63it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.61it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.74it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.71it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.70it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.66it/s][A
 37%|███▋      | 223/608 [00:04<00:09, 40.49it/s][A
 38%|███▊      | 228/608 [00:04<00:09, 42.16it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 43.54it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 44.50it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.09it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.60it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.99it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.27it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.00it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.19it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.29it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 46.47it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.59it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.56it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.58it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.60it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.70it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.54it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.44it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.54it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 46.65it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.60it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.71it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.63it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.70it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.74it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.59it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.50it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 42.11it/s][A
 61%|██████    | 368/608 [00:08<00:05, 43.45it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 44.45it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.13it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.58it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.02it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.26it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.46it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.07it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.20it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.34it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.50it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.60it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.58it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.77it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.77it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.74it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.64it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.50it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.48it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.47it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.53it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.66it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.68it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.74it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.75it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.64it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.50it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 44.42it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.16it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.54it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.01it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.14it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.38it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.56it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.56it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.31it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.22it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 46.35it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.53it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.68it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.72it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.61it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.74it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 29.97it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 33.84it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 36.96it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 39.39it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 41.39it/s][A
100%|██████████| 608/608 [00:13<00:00, 42.85it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 42.85it/s][A 40%|████      | 312/780 [02:36<02:15,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 16:13:30,248 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 16:13:30,586 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 16:13:35,107 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 16:13:35,318 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 16:13:35,422 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:53<1:12:46,  9.35s/it] 40%|████      | 314/780 [02:53<51:39,  6.65s/it]   40%|████      | 315/780 [02:54<36:44,  4.74s/it] 41%|████      | 316/780 [02:54<26:20,  3.41s/it] 41%|████      | 317/780 [02:54<19:03,  2.47s/it] 41%|████      | 318/780 [02:54<13:58,  1.82s/it] 41%|████      | 319/780 [02:55<10:25,  1.36s/it] 41%|████      | 320/780 [02:55<07:56,  1.04s/it] 41%|████      | 321/780 [02:55<06:17,  1.22it/s] 41%|████▏     | 322/780 [02:56<05:03,  1.51it/s] 41%|████▏     | 323/780 [02:56<04:11,  1.82it/s] 42%|████▏     | 324/780 [02:56<03:34,  2.12it/s] 42%|████▏     | 325/780 [02:57<03:09,  2.40it/s] 42%|████▏     | 326/780 [02:57<02:51,  2.65it/s] 42%|████▏     | 327/780 [02:57<02:38,  2.85it/s] 42%|████▏     | 328/780 [02:57<02:29,  3.01it/s] 42%|████▏     | 329/780 [02:58<02:23,  3.14it/s] 42%|████▏     | 330/780 [02:58<02:19,  3.23it/s] 42%|████▏     | 331/780 [02:58<02:16,  3.30it/s] 43%|████▎     | 332/780 [02:59<02:22,  3.15it/s] 43%|████▎     | 333/780 [02:59<02:18,  3.24it/s] 43%|████▎     | 334/780 [02:59<02:15,  3.30it/s] 43%|████▎     | 335/780 [02:59<02:12,  3.35it/s] 43%|████▎     | 336/780 [03:00<02:11,  3.38it/s] 43%|████▎     | 337/780 [03:00<02:09,  3.41it/s] 43%|████▎     | 338/780 [03:00<02:08,  3.43it/s] 43%|████▎     | 339/780 [03:01<02:08,  3.44it/s] 44%|████▎     | 340/780 [03:01<02:07,  3.45it/s] 44%|████▎     | 341/780 [03:01<02:06,  3.46it/s] 44%|████▍     | 342/780 [03:01<02:06,  3.46it/s] 44%|████▍     | 343/780 [03:02<02:13,  3.26it/s] 44%|████▍     | 344/780 [03:02<02:11,  3.32it/s] 44%|████▍     | 345/780 [03:02<02:09,  3.36it/s] 44%|████▍     | 346/780 [03:03<02:07,  3.39it/s] 44%|████▍     | 347/780 [03:03<02:06,  3.41it/s] 45%|████▍     | 348/780 [03:03<02:05,  3.43it/s] 45%|████▍     | 349/780 [03:04<02:05,  3.44it/s] 45%|████▍     | 350/780 [03:04<02:04,  3.45it/s] 45%|████▌     | 351/780 [03:04<02:04,  3.46it/s] 45%|████▌     | 352/780 [03:04<02:03,  3.46it/s] 45%|████▌     | 353/780 [03:05<02:03,  3.46it/s] 45%|████▌     | 354/780 [03:05<02:10,  3.27it/s] 46%|████▌     | 355/780 [03:05<02:07,  3.32it/s] 46%|████▌     | 356/780 [03:06<02:05,  3.37it/s] 46%|████▌     | 357/780 [03:06<02:04,  3.40it/s] 46%|████▌     | 358/780 [03:06<02:03,  3.42it/s] 46%|████▌     | 359/780 [03:07<02:02,  3.43it/s] 46%|████▌     | 360/780 [03:07<02:02,  3.44it/s] 46%|████▋     | 361/780 [03:07<02:01,  3.45it/s] 46%|████▋     | 362/780 [03:07<02:01,  3.45it/s] 47%|████▋     | 363/780 [03:08<02:00,  3.46it/s] 47%|████▋     | 364/780 [03:08<02:00,  3.46it/s] 47%|████▋     | 365/780 [03:08<02:09,  3.20it/s] 47%|████▋     | 366/780 [03:09<02:06,  3.28it/s] 47%|████▋     | 367/780 [03:09<02:03,  3.33it/s] 47%|████▋     | 368/780 [03:09<02:02,  3.37it/s] 47%|████▋     | 369/780 [03:09<02:00,  3.40it/s] 47%|████▋     | 370/780 [03:10<01:59,  3.42it/s] 48%|████▊     | 371/780 [03:10<01:59,  3.44it/s] 48%|████▊     | 372/780 [03:10<01:58,  3.44it/s] 48%|████▊     | 373/780 [03:11<01:57,  3.45it/s] 48%|████▊     | 374/780 [03:11<01:57,  3.45it/s] 48%|████▊     | 375/780 [03:11<01:57,  3.46it/s] 48%|████▊     | 376/780 [03:12<02:04,  3.23it/s] 48%|████▊     | 377/780 [03:12<02:02,  3.30it/s] 48%|████▊     | 378/780 [03:12<02:00,  3.35it/s] 49%|████▊     | 379/780 [03:12<01:58,  3.39it/s] 49%|████▊     | 380/780 [03:13<01:57,  3.41it/s] 49%|████▉     | 381/780 [03:13<01:56,  3.43it/s] 49%|████▉     | 382/780 [03:13<01:55,  3.44it/s] 49%|████▉     | 383/780 [03:14<01:55,  3.45it/s] 49%|████▉     | 384/780 [03:14<01:54,  3.45it/s] 49%|████▉     | 385/780 [03:14<01:54,  3.45it/s] 49%|████▉     | 386/780 [03:14<01:53,  3.46it/s] 50%|████▉     | 387/780 [03:15<01:59,  3.30it/s] 50%|████▉     | 388/780 [03:15<01:57,  3.35it/s] 50%|████▉     | 389/780 [03:15<01:55,  3.38it/s] 50%|█████     | 390/780 [03:16<01:54,  3.41it/s] 50%|█████     | 391/780 [03:16<01:53,  3.42it/s] 50%|█████     | 392/780 [03:16<01:55,  3.35it/s] 50%|█████     | 393/780 [03:17<01:59,  3.24it/s] 51%|█████     | 394/780 [03:17<02:18,  2.78it/s] 51%|█████     | 395/780 [03:17<02:10,  2.95it/s] 51%|█████     | 396/780 [03:18<02:06,  3.03it/s] 51%|█████     | 397/780 [03:18<02:04,  3.08it/s] 51%|█████     | 398/780 [03:18<02:00,  3.18it/s] 51%|█████     | 399/780 [03:19<01:56,  3.26it/s] 51%|█████▏    | 400/780 [03:19<01:54,  3.32it/s] 51%|█████▏    | 401/780 [03:19<01:52,  3.36it/s] 52%|█████▏    | 402/780 [03:19<01:51,  3.39it/s] 52%|█████▏    | 403/780 [03:20<01:50,  3.41it/s] 52%|█████▏    | 404/780 [03:20<01:49,  3.43it/s] 52%|█████▏    | 405/780 [03:20<01:49,  3.44it/s] 52%|█████▏    | 406/780 [03:21<01:48,  3.45it/s] 52%|█████▏    | 407/780 [03:21<01:48,  3.45it/s] 52%|█████▏    | 408/780 [03:21<01:51,  3.32it/s] 52%|█████▏    | 409/780 [03:21<01:50,  3.36it/s] 53%|█████▎    | 410/780 [03:22<01:49,  3.39it/s] 53%|█████▎    | 411/780 [03:22<01:48,  3.41it/s] 53%|█████▎    | 412/780 [03:22<01:47,  3.43it/s] 53%|█████▎    | 413/780 [03:23<01:46,  3.43it/s] 53%|█████▎    | 414/780 [03:23<01:46,  3.44it/s] 53%|█████▎    | 415/780 [03:23<01:45,  3.44it/s] 53%|█████▎    | 416/780 [03:24<01:45,  3.45it/s] 53%|█████▎    | 417/780 [03:24<01:45,  3.45it/s] 54%|█████▎    | 418/780 [03:24<01:44,  3.46it/s] 54%|█████▎    | 419/780 [03:24<01:49,  3.30it/s] 54%|█████▍    | 420/780 [03:25<01:47,  3.35it/s] 54%|█████▍    | 421/780 [03:25<01:46,  3.38it/s] 54%|█████▍    | 422/780 [03:25<01:45,  3.41it/s] 54%|█████▍    | 423/780 [03:26<01:44,  3.42it/s] 54%|█████▍    | 424/780 [03:26<01:43,  3.43it/s] 54%|█████▍    | 425/780 [03:26<01:43,  3.44it/s] 55%|█████▍    | 426/780 [03:26<01:42,  3.45it/s] 55%|█████▍    | 427/780 [03:27<01:42,  3.45it/s] 55%|█████▍    | 428/780 [03:27<01:41,  3.45it/s] 55%|█████▌    | 429/780 [03:27<01:41,  3.46it/s] 55%|█████▌    | 430/780 [03:28<01:41,  3.46it/s] 55%|█████▌    | 431/780 [03:28<01:40,  3.46it/s] 55%|█████▌    | 432/780 [03:28<01:40,  3.46it/s] 56%|█████▌    | 433/780 [03:28<01:40,  3.46it/s] 56%|█████▌    | 434/780 [03:29<01:40,  3.46it/s] 56%|█████▌    | 435/780 [03:29<01:44,  3.31it/s] 56%|█████▌    | 436/780 [03:29<01:42,  3.35it/s] 56%|█████▌    | 437/780 [03:30<01:41,  3.38it/s] 56%|█████▌    | 438/780 [03:30<01:40,  3.41it/s] 56%|█████▋    | 439/780 [03:30<01:39,  3.42it/s] 56%|█████▋    | 440/780 [03:31<01:39,  3.43it/s] 57%|█████▋    | 441/780 [03:31<01:38,  3.44it/s] 57%|█████▋    | 442/780 [03:31<01:38,  3.45it/s] 57%|█████▋    | 443/780 [03:31<01:37,  3.45it/s] 57%|█████▋    | 444/780 [03:32<01:37,  3.46it/s] 57%|█████▋    | 445/780 [03:32<01:36,  3.46it/s] 57%|█████▋    | 446/780 [03:32<01:41,  3.28it/s] 57%|█████▋    | 447/780 [03:33<01:39,  3.33it/s] 57%|█████▋    | 448/780 [03:33<01:38,  3.37it/s] 58%|█████▊    | 449/780 [03:33<01:37,  3.40it/s] 58%|█████▊    | 450/780 [03:33<01:36,  3.42it/s] 58%|█████▊    | 451/780 [03:34<01:35,  3.43it/s] 58%|█████▊    | 452/780 [03:34<01:35,  3.44it/s] 58%|█████▊    | 453/780 [03:34<01:34,  3.45it/s] 58%|█████▊    | 454/780 [03:35<01:34,  3.45it/s] 58%|█████▊    | 455/780 [03:35<01:34,  3.45it/s] 58%|█████▊    | 456/780 [03:35<01:33,  3.45it/s] 59%|█████▊    | 457/780 [03:36<01:55,  2.80it/s] 59%|█████▊    | 458/780 [03:36<01:48,  2.96it/s] 59%|█████▉    | 459/780 [03:36<01:43,  3.10it/s] 59%|█████▉    | 460/780 [03:37<01:40,  3.20it/s] 59%|█████▉    | 461/780 [03:37<01:37,  3.27it/s] 59%|█████▉    | 462/780 [03:37<01:35,  3.32it/s] 59%|█████▉    | 463/780 [03:37<01:34,  3.36it/s] 59%|█████▉    | 464/780 [03:38<01:33,  3.39it/s] 60%|█████▉    | 465/780 [03:38<01:32,  3.41it/s] 60%|█████▉    | 466/780 [03:38<01:31,  3.43it/s] 60%|█████▉    | 467/780 [03:39<01:33,  3.33it/s] 60%|██████    | 468/780 [03:39<01:32,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 16:14:32,822 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 16:14:32,822 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 16:14:32,822 >>   Batch size = 8
{'eval_loss': 1.035273790359497, 'eval_runtime': 13.4274, 'eval_samples_per_second': 362.245, 'eval_steps_per_second': 45.281, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.39it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.79it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.86it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.02it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.59it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.28it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.94it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.53it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.57it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.62it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.69it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.73it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.80it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.74it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.77it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.68it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.47it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.49it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.26it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.58it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.69it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.70it/s][A
 19%|█▉        | 118/608 [00:02<00:11, 43.12it/s][A
 20%|██        | 123/608 [00:02<00:10, 44.21it/s][A
 21%|██        | 128/608 [00:02<00:10, 44.96it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.52it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 45.89it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.07it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.24it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.28it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.05it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.04it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.26it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.38it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.44it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.57it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.66it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.62it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.61it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.36it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.25it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.22it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.38it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.51it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.54it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.62it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.63it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.65it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.63it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.55it/s][A
 42%|████▏     | 258/608 [00:05<00:08, 43.34it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 44.26it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 44.98it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 45.49it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 45.91it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.17it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.35it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.48it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.35it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.31it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.26it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.40it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.54it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.63it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.66it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.63it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.53it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.50it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.44it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.50it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.36it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.43it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.49it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.53it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.61it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.55it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.45it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.44it/s][A
 65%|██████▌   | 398/608 [00:08<00:05, 37.58it/s][A
 66%|██████▋   | 403/608 [00:08<00:05, 39.92it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 41.73it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 43.14it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 44.22it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 44.96it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.49it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.92it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.51it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.67it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.76it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.15it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.36it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.48it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.50it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.57it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.61it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.42it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.32it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.24it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.38it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.45it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 46.49it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.60it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.51it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.65it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.54it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.46it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 39.62it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 41.53it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 43.00it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 44.11it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 44.90it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.40it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.80it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.09it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.61it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.62it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 45.97it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.16it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 46.33it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.50it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.60it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.60it/s][A 60%|██████    | 468/780 [03:52<01:32,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 16:14:46,635 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 16:14:47,001 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 16:14:52,008 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 16:14:52,206 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 16:14:52,344 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [04:11<50:23,  9.72s/it] 60%|██████    | 470/780 [04:11<35:43,  6.91s/it] 60%|██████    | 471/780 [04:11<25:22,  4.93s/it] 61%|██████    | 472/780 [04:12<18:08,  3.54s/it] 61%|██████    | 473/780 [04:12<13:06,  2.56s/it] 61%|██████    | 474/780 [04:12<09:34,  1.88s/it] 61%|██████    | 475/780 [04:12<07:07,  1.40s/it] 61%|██████    | 476/780 [04:13<05:24,  1.07s/it] 61%|██████    | 477/780 [04:13<04:12,  1.20it/s] 61%|██████▏   | 478/780 [04:13<03:22,  1.49it/s] 61%|██████▏   | 479/780 [04:14<02:47,  1.80it/s] 62%|██████▏   | 480/780 [04:14<02:22,  2.10it/s] 62%|██████▏   | 481/780 [04:14<02:11,  2.27it/s] 62%|██████▏   | 482/780 [04:15<01:57,  2.53it/s] 62%|██████▏   | 483/780 [04:15<01:47,  2.76it/s] 62%|██████▏   | 484/780 [04:15<01:40,  2.94it/s] 62%|██████▏   | 485/780 [04:15<01:35,  3.08it/s] 62%|██████▏   | 486/780 [04:16<01:32,  3.19it/s] 62%|██████▏   | 487/780 [04:16<01:29,  3.26it/s] 63%|██████▎   | 488/780 [04:16<01:34,  3.09it/s] 63%|██████▎   | 489/780 [04:17<01:38,  2.97it/s] 63%|██████▎   | 490/780 [04:17<01:49,  2.65it/s] 63%|██████▎   | 491/780 [04:18<01:47,  2.70it/s] 63%|██████▎   | 492/780 [04:18<01:46,  2.72it/s] 63%|██████▎   | 493/780 [04:18<01:38,  2.90it/s] 63%|██████▎   | 494/780 [04:18<01:33,  3.05it/s] 63%|██████▎   | 495/780 [04:19<01:30,  3.16it/s] 64%|██████▎   | 496/780 [04:19<01:27,  3.25it/s] 64%|██████▎   | 497/780 [04:19<01:25,  3.31it/s] 64%|██████▍   | 498/780 [04:20<01:24,  3.36it/s] 64%|██████▍   | 499/780 [04:20<01:22,  3.39it/s] 64%|██████▍   | 500/780 [04:20<01:22,  3.41it/s]                                                  64%|██████▍   | 500/780 [04:20<01:22,  3.41it/s] 64%|██████▍   | 501/780 [04:21<01:26,  3.24it/s] 64%|██████▍   | 502/780 [04:21<01:24,  3.31it/s] 64%|██████▍   | 503/780 [04:21<01:22,  3.35it/s] 65%|██████▍   | 504/780 [04:21<01:21,  3.38it/s] 65%|██████▍   | 505/780 [04:22<01:20,  3.41it/s] 65%|██████▍   | 506/780 [04:22<01:20,  3.42it/s] 65%|██████▌   | 507/780 [04:22<01:19,  3.44it/s] 65%|██████▌   | 508/780 [04:23<01:18,  3.45it/s] 65%|██████▌   | 509/780 [04:23<01:18,  3.45it/s] 65%|██████▌   | 510/780 [04:23<01:18,  3.46it/s] 66%|██████▌   | 511/780 [04:23<01:17,  3.46it/s] 66%|██████▌   | 512/780 [04:24<01:23,  3.23it/s] 66%|██████▌   | 513/780 [04:24<01:21,  3.29it/s] 66%|██████▌   | 514/780 [04:24<01:19,  3.34it/s] 66%|██████▌   | 515/780 [04:25<01:18,  3.38it/s] 66%|██████▌   | 516/780 [04:25<01:17,  3.40it/s] 66%|██████▋   | 517/780 [04:25<01:16,  3.42it/s] 66%|██████▋   | 518/780 [04:26<01:16,  3.43it/s] 67%|██████▋   | 519/780 [04:26<01:15,  3.44it/s] 67%|██████▋   | 520/780 [04:26<01:15,  3.45it/s] 67%|██████▋   | 521/780 [04:26<01:15,  3.45it/s] 67%|██████▋   | 522/780 [04:27<01:14,  3.45it/s] 67%|██████▋   | 523/780 [04:27<01:17,  3.31it/s] 67%|██████▋   | 524/780 [04:27<01:16,  3.35it/s] 67%|██████▋   | 525/780 [04:28<01:15,  3.38it/s] 67%|██████▋   | 526/780 [04:28<01:14,  3.41it/s] 68%|██████▊   | 527/780 [04:28<01:13,  3.42it/s] 68%|██████▊   | 528/780 [04:28<01:13,  3.43it/s] 68%|██████▊   | 529/780 [04:29<01:12,  3.44it/s] 68%|██████▊   | 530/780 [04:29<01:12,  3.45it/s] 68%|██████▊   | 531/780 [04:29<01:12,  3.45it/s] 68%|██████▊   | 532/780 [04:30<01:11,  3.45it/s] 68%|██████▊   | 533/780 [04:30<01:11,  3.45it/s] 68%|██████▊   | 534/780 [04:30<01:11,  3.45it/s] 69%|██████▊   | 535/780 [04:31<01:13,  3.32it/s] 69%|██████▊   | 536/780 [04:31<01:12,  3.36it/s] 69%|██████▉   | 537/780 [04:31<01:11,  3.39it/s] 69%|██████▉   | 538/780 [04:31<01:11,  3.41it/s] 69%|██████▉   | 539/780 [04:32<01:10,  3.42it/s] 69%|██████▉   | 540/780 [04:32<01:09,  3.43it/s] 69%|██████▉   | 541/780 [04:32<01:09,  3.44it/s] 69%|██████▉   | 542/780 [04:33<01:08,  3.45it/s] 70%|██████▉   | 543/780 [04:33<01:08,  3.45it/s] 70%|██████▉   | 544/780 [04:33<01:08,  3.45it/s] 70%|██████▉   | 545/780 [04:33<01:07,  3.46it/s] 70%|███████   | 546/780 [04:34<01:10,  3.33it/s] 70%|███████   | 547/780 [04:34<01:09,  3.36it/s] 70%|███████   | 548/780 [04:34<01:08,  3.39it/s] 70%|███████   | 549/780 [04:35<01:07,  3.41it/s] 71%|███████   | 550/780 [04:35<01:07,  3.43it/s] 71%|███████   | 551/780 [04:35<01:06,  3.43it/s] 71%|███████   | 552/780 [04:35<01:06,  3.44it/s] 71%|███████   | 553/780 [04:36<01:15,  2.99it/s] 71%|███████   | 554/780 [04:36<01:12,  3.11it/s] 71%|███████   | 555/780 [04:36<01:10,  3.20it/s] 71%|███████▏  | 556/780 [04:37<01:12,  3.11it/s] 71%|███████▏  | 557/780 [04:37<01:09,  3.20it/s] 72%|███████▏  | 558/780 [04:37<01:07,  3.28it/s] 72%|███████▏  | 559/780 [04:38<01:06,  3.33it/s] 72%|███████▏  | 560/780 [04:38<01:05,  3.36it/s] 72%|███████▏  | 561/780 [04:38<01:04,  3.39it/s] 72%|███████▏  | 562/780 [04:39<01:03,  3.41it/s] 72%|███████▏  | 563/780 [04:39<01:03,  3.43it/s] 72%|███████▏  | 564/780 [04:39<01:02,  3.43it/s] 72%|███████▏  | 565/780 [04:39<01:02,  3.44it/s] 73%|███████▎  | 566/780 [04:40<01:02,  3.45it/s] 73%|███████▎  | 567/780 [04:40<01:05,  3.27it/s] 73%|███████▎  | 568/780 [04:40<01:03,  3.33it/s] 73%|███████▎  | 569/780 [04:41<01:02,  3.37it/s] 73%|███████▎  | 570/780 [04:41<01:01,  3.39it/s] 73%|███████▎  | 571/780 [04:41<01:01,  3.41it/s] 73%|███████▎  | 572/780 [04:42<01:00,  3.42it/s] 73%|███████▎  | 573/780 [04:42<01:00,  3.43it/s] 74%|███████▎  | 574/780 [04:42<00:59,  3.44it/s] 74%|███████▎  | 575/780 [04:42<00:59,  3.44it/s] 74%|███████▍  | 576/780 [04:43<00:59,  3.45it/s] 74%|███████▍  | 577/780 [04:43<00:58,  3.45it/s] 74%|███████▍  | 578/780 [04:43<01:09,  2.92it/s] 74%|███████▍  | 579/780 [04:44<01:05,  3.06it/s] 74%|███████▍  | 580/780 [04:44<01:03,  3.17it/s] 74%|███████▍  | 581/780 [04:44<01:01,  3.25it/s] 75%|███████▍  | 582/780 [04:45<00:59,  3.31it/s] 75%|███████▍  | 583/780 [04:45<00:58,  3.35it/s] 75%|███████▍  | 584/780 [04:45<00:57,  3.38it/s] 75%|███████▌  | 585/780 [04:45<00:57,  3.41it/s] 75%|███████▌  | 586/780 [04:46<00:56,  3.42it/s] 75%|███████▌  | 587/780 [04:46<00:56,  3.43it/s] 75%|███████▌  | 588/780 [04:46<00:59,  3.24it/s] 76%|███████▌  | 589/780 [04:47<00:57,  3.30it/s] 76%|███████▌  | 590/780 [04:47<00:56,  3.35it/s] 76%|███████▌  | 591/780 [04:47<00:55,  3.38it/s] 76%|███████▌  | 592/780 [04:48<00:55,  3.40it/s] 76%|███████▌  | 593/780 [04:48<00:54,  3.42it/s] 76%|███████▌  | 594/780 [04:48<00:54,  3.43it/s] 76%|███████▋  | 595/780 [04:48<00:53,  3.44it/s] 76%|███████▋  | 596/780 [04:49<00:53,  3.44it/s] 77%|███████▋  | 597/780 [04:49<00:53,  3.45it/s] 77%|███████▋  | 598/780 [04:49<00:52,  3.45it/s] 77%|███████▋  | 599/780 [04:50<00:55,  3.28it/s] 77%|███████▋  | 600/780 [04:50<00:54,  3.33it/s] 77%|███████▋  | 601/780 [04:50<00:53,  3.36it/s] 77%|███████▋  | 602/780 [04:50<00:52,  3.39it/s] 77%|███████▋  | 603/780 [04:51<00:51,  3.41it/s] 77%|███████▋  | 604/780 [04:51<00:51,  3.42it/s] 78%|███████▊  | 605/780 [04:51<00:51,  3.43it/s] 78%|███████▊  | 606/780 [04:52<00:50,  3.44it/s] 78%|███████▊  | 607/780 [04:52<00:50,  3.44it/s] 78%|███████▊  | 608/780 [04:52<00:49,  3.44it/s] 78%|███████▊  | 609/780 [04:53<00:49,  3.45it/s] 78%|███████▊  | 610/780 [04:53<00:51,  3.27it/s] 78%|███████▊  | 611/780 [04:53<00:50,  3.32it/s] 78%|███████▊  | 612/780 [04:53<00:49,  3.36it/s] 79%|███████▊  | 613/780 [04:54<00:49,  3.39it/s] 79%|███████▊  | 614/780 [04:54<00:48,  3.40it/s] 79%|███████▉  | 615/780 [04:54<00:48,  3.42it/s] 79%|███████▉  | 616/780 [04:55<00:47,  3.43it/s] 79%|███████▉  | 617/780 [04:55<00:47,  3.43it/s] 79%|███████▉  | 618/780 [04:55<00:47,  3.44it/s] 79%|███████▉  | 619/780 [04:55<00:46,  3.44it/s] 79%|███████▉  | 620/780 [04:56<00:46,  3.45it/s] 80%|███████▉  | 621/780 [04:56<00:48,  3.25it/s] 80%|███████▉  | 622/780 [04:56<00:47,  3.31it/s] 80%|███████▉  | 623/780 [04:57<00:46,  3.35it/s] 80%|████████  | 624/780 [04:57<00:46,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 16:15:50,865 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 16:15:50,865 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 16:15:50,865 >>   Batch size = 8
{'eval_loss': 1.0471818447113037, 'eval_runtime': 13.2917, 'eval_samples_per_second': 365.944, 'eval_steps_per_second': 45.743, 'epoch': 3.0}
{'loss': 0.4276, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.77it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.41it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.62it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.96it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.45it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.33it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.07it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.68it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.69it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.65it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.70it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.74it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.60it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.69it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.66it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.52it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.48it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.42it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.45it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.45it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.57it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.62it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.54it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.53it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.45it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.40it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.48it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.39it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.40it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.52it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.49it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.54it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.64it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.62it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.45it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.44it/s][A
 31%|███       | 188/608 [00:04<00:10, 41.82it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 43.23it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 44.20it/s][A
 33%|███▎      | 203/608 [00:04<00:09, 44.98it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.51it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.96it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.20it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.37it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 45.88it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.95it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 46.15it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.13it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.28it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.54it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.52it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.64it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.63it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.32it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.24it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.18it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.27it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.38it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.52it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.56it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.56it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.62it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.62it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 46.54it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 41.47it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 42.82it/s][A
 56%|█████▌    | 338/608 [00:07<00:06, 43.97it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 44.73it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.34it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.71it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.03it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.30it/s][A
 61%|██████    | 368/608 [00:07<00:05, 45.93it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.15it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.29it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.25it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.41it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.54it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.57it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.65it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.56it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.46it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.47it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.42it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.52it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.55it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.51it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.51it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.57it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.51it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.54it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.51it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 42.26it/s][A
 78%|███████▊  | 473/608 [00:10<00:03, 43.57it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 44.53it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.17it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.60it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.97it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.18it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.41it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.93it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.02it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.15it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.23it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.43it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.56it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.53it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.63it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.56it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.22it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.20it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.24it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.34it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.41it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.53it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.55it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.67it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.70it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.68it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.53it/s][A
100%|██████████| 608/608 [00:13<00:00, 39.02it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 39.02it/s][A 80%|████████  | 624/780 [05:10<00:46,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 16:16:04,862 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 16:16:05,446 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 16:16:12,631 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 16:16:13,193 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 16:16:13,426 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [05:35<30:00, 11.62s/it] 80%|████████  | 626/780 [05:35<21:09,  8.24s/it] 80%|████████  | 627/780 [05:36<14:55,  5.86s/it] 81%|████████  | 628/780 [05:36<10:48,  4.27s/it] 81%|████████  | 629/780 [05:37<07:44,  3.07s/it] 81%|████████  | 630/780 [05:37<05:35,  2.24s/it] 81%|████████  | 631/780 [05:37<04:06,  1.65s/it] 81%|████████  | 632/780 [05:37<03:03,  1.24s/it] 81%|████████  | 633/780 [05:38<02:20,  1.05it/s] 81%|████████▏ | 634/780 [05:38<01:50,  1.32it/s] 81%|████████▏ | 635/780 [05:38<01:29,  1.62it/s] 82%|████████▏ | 636/780 [05:39<01:15,  1.91it/s] 82%|████████▏ | 637/780 [05:39<01:04,  2.20it/s] 82%|████████▏ | 638/780 [05:39<00:57,  2.47it/s] 82%|████████▏ | 639/780 [05:39<00:52,  2.71it/s] 82%|████████▏ | 640/780 [05:40<00:48,  2.90it/s] 82%|████████▏ | 641/780 [05:40<00:45,  3.05it/s] 82%|████████▏ | 642/780 [05:40<00:43,  3.17it/s] 82%|████████▏ | 643/780 [05:41<00:42,  3.25it/s] 83%|████████▎ | 644/780 [05:41<00:40,  3.32it/s] 83%|████████▎ | 645/780 [05:41<00:40,  3.36it/s] 83%|████████▎ | 646/780 [05:41<00:39,  3.39it/s] 83%|████████▎ | 647/780 [05:42<00:41,  3.23it/s] 83%|████████▎ | 648/780 [05:42<00:40,  3.30it/s] 83%|████████▎ | 649/780 [05:42<00:39,  3.35it/s] 83%|████████▎ | 650/780 [05:43<00:38,  3.38it/s] 83%|████████▎ | 651/780 [05:43<00:37,  3.41it/s] 84%|████████▎ | 652/780 [05:43<00:37,  3.43it/s] 84%|████████▎ | 653/780 [05:44<00:36,  3.44it/s] 84%|████████▍ | 654/780 [05:44<00:36,  3.45it/s] 84%|████████▍ | 655/780 [05:44<00:36,  3.45it/s] 84%|████████▍ | 656/780 [05:44<00:35,  3.46it/s] 84%|████████▍ | 657/780 [05:45<00:35,  3.46it/s] 84%|████████▍ | 658/780 [05:45<00:37,  3.24it/s] 84%|████████▍ | 659/780 [05:45<00:36,  3.30it/s] 85%|████████▍ | 660/780 [05:46<00:35,  3.35it/s] 85%|████████▍ | 661/780 [05:46<00:35,  3.39it/s] 85%|████████▍ | 662/780 [05:46<00:34,  3.41it/s] 85%|████████▌ | 663/780 [05:46<00:34,  3.43it/s] 85%|████████▌ | 664/780 [05:47<00:33,  3.44it/s] 85%|████████▌ | 665/780 [05:47<00:33,  3.45it/s] 85%|████████▌ | 666/780 [05:47<00:32,  3.46it/s] 86%|████████▌ | 667/780 [05:48<00:32,  3.46it/s] 86%|████████▌ | 668/780 [05:48<00:32,  3.46it/s] 86%|████████▌ | 669/780 [05:48<00:33,  3.32it/s] 86%|████████▌ | 670/780 [05:49<00:32,  3.36it/s] 86%|████████▌ | 671/780 [05:49<00:32,  3.39it/s] 86%|████████▌ | 672/780 [05:49<00:31,  3.41it/s] 86%|████████▋ | 673/780 [05:49<00:31,  3.43it/s] 86%|████████▋ | 674/780 [05:50<00:30,  3.44it/s] 87%|████████▋ | 675/780 [05:50<00:30,  3.45it/s] 87%|████████▋ | 676/780 [05:50<00:30,  3.46it/s] 87%|████████▋ | 677/780 [05:51<00:29,  3.46it/s] 87%|████████▋ | 678/780 [05:51<00:29,  3.46it/s] 87%|████████▋ | 679/780 [05:51<00:29,  3.47it/s] 87%|████████▋ | 680/780 [05:51<00:30,  3.30it/s] 87%|████████▋ | 681/780 [05:52<00:29,  3.35it/s] 87%|████████▋ | 682/780 [05:52<00:28,  3.39it/s] 88%|████████▊ | 683/780 [05:52<00:28,  3.41it/s] 88%|████████▊ | 684/780 [05:53<00:28,  3.43it/s] 88%|████████▊ | 685/780 [05:53<00:27,  3.44it/s] 88%|████████▊ | 686/780 [05:53<00:27,  3.45it/s] 88%|████████▊ | 687/780 [05:53<00:26,  3.45it/s] 88%|████████▊ | 688/780 [05:54<00:26,  3.46it/s] 88%|████████▊ | 689/780 [05:54<00:26,  3.46it/s] 88%|████████▊ | 690/780 [05:54<00:26,  3.46it/s] 89%|████████▊ | 691/780 [05:55<00:26,  3.32it/s] 89%|████████▊ | 692/780 [05:55<00:26,  3.36it/s] 89%|████████▉ | 693/780 [05:55<00:25,  3.39it/s] 89%|████████▉ | 694/780 [05:56<00:25,  3.41it/s] 89%|████████▉ | 695/780 [05:56<00:24,  3.43it/s] 89%|████████▉ | 696/780 [05:56<00:24,  3.44it/s] 89%|████████▉ | 697/780 [05:56<00:24,  3.44it/s] 89%|████████▉ | 698/780 [05:57<00:23,  3.45it/s] 90%|████████▉ | 699/780 [05:57<00:23,  3.45it/s] 90%|████████▉ | 700/780 [05:57<00:23,  3.45it/s] 90%|████████▉ | 701/780 [05:58<00:22,  3.45it/s] 90%|█████████ | 702/780 [05:58<00:24,  3.14it/s] 90%|█████████ | 703/780 [05:58<00:23,  3.23it/s] 90%|█████████ | 704/780 [05:59<00:23,  3.30it/s] 90%|█████████ | 705/780 [05:59<00:22,  3.34it/s] 91%|█████████ | 706/780 [05:59<00:21,  3.38it/s] 91%|█████████ | 707/780 [05:59<00:21,  3.41it/s] 91%|█████████ | 708/780 [06:00<00:23,  3.13it/s] 91%|█████████ | 709/780 [06:00<00:22,  3.23it/s] 91%|█████████ | 710/780 [06:00<00:21,  3.29it/s] 91%|█████████ | 711/780 [06:01<00:20,  3.34it/s] 91%|█████████▏| 712/780 [06:01<00:20,  3.38it/s] 91%|█████████▏| 713/780 [06:01<00:19,  3.41it/s] 92%|█████████▏| 714/780 [06:01<00:19,  3.42it/s] 92%|█████████▏| 715/780 [06:02<00:18,  3.44it/s] 92%|█████████▏| 716/780 [06:02<00:18,  3.44it/s] 92%|█████████▏| 717/780 [06:02<00:18,  3.45it/s] 92%|█████████▏| 718/780 [06:03<00:19,  3.23it/s] 92%|█████████▏| 719/780 [06:03<00:18,  3.30it/s] 92%|█████████▏| 720/780 [06:03<00:17,  3.35it/s] 92%|█████████▏| 721/780 [06:04<00:17,  3.38it/s] 93%|█████████▎| 722/780 [06:04<00:17,  3.40it/s] 93%|█████████▎| 723/780 [06:04<00:16,  3.42it/s] 93%|█████████▎| 724/780 [06:04<00:16,  3.43it/s] 93%|█████████▎| 725/780 [06:05<00:16,  3.44it/s] 93%|█████████▎| 726/780 [06:05<00:15,  3.45it/s] 93%|█████████▎| 727/780 [06:05<00:15,  3.45it/s] 93%|█████████▎| 728/780 [06:06<00:15,  3.45it/s] 93%|█████████▎| 729/780 [06:06<00:16,  3.15it/s] 94%|█████████▎| 730/780 [06:06<00:15,  3.24it/s] 94%|█████████▎| 731/780 [06:07<00:14,  3.31it/s] 94%|█████████▍| 732/780 [06:07<00:14,  3.36it/s] 94%|█████████▍| 733/780 [06:07<00:13,  3.39it/s] 94%|█████████▍| 734/780 [06:07<00:13,  3.41it/s] 94%|█████████▍| 735/780 [06:08<00:13,  3.43it/s] 94%|█████████▍| 736/780 [06:08<00:12,  3.44it/s] 94%|█████████▍| 737/780 [06:08<00:12,  3.44it/s] 95%|█████████▍| 738/780 [06:09<00:12,  3.45it/s] 95%|█████████▍| 739/780 [06:09<00:11,  3.45it/s] 95%|█████████▍| 740/780 [06:09<00:12,  3.14it/s] 95%|█████████▌| 741/780 [06:10<00:12,  3.22it/s] 95%|█████████▌| 742/780 [06:10<00:11,  3.29it/s] 95%|█████████▌| 743/780 [06:10<00:11,  3.34it/s] 95%|█████████▌| 744/780 [06:10<00:10,  3.37it/s] 96%|█████████▌| 745/780 [06:11<00:10,  3.40it/s] 96%|█████████▌| 746/780 [06:11<00:09,  3.42it/s] 96%|█████████▌| 747/780 [06:11<00:09,  3.43it/s] 96%|█████████▌| 748/780 [06:12<00:09,  3.44it/s] 96%|█████████▌| 749/780 [06:12<00:08,  3.45it/s] 96%|█████████▌| 750/780 [06:12<00:08,  3.45it/s] 96%|█████████▋| 751/780 [06:13<00:09,  3.21it/s] 96%|█████████▋| 752/780 [06:13<00:08,  3.28it/s] 97%|█████████▋| 753/780 [06:13<00:08,  3.34it/s] 97%|█████████▋| 754/780 [06:13<00:07,  3.37it/s] 97%|█████████▋| 755/780 [06:14<00:07,  3.40it/s] 97%|█████████▋| 756/780 [06:14<00:07,  3.42it/s] 97%|█████████▋| 757/780 [06:14<00:06,  3.43it/s] 97%|█████████▋| 758/780 [06:15<00:06,  3.44it/s] 97%|█████████▋| 759/780 [06:15<00:06,  3.44it/s] 97%|█████████▋| 760/780 [06:15<00:05,  3.45it/s] 98%|█████████▊| 761/780 [06:15<00:05,  3.45it/s] 98%|█████████▊| 762/780 [06:16<00:05,  3.24it/s] 98%|█████████▊| 763/780 [06:16<00:05,  3.30it/s] 98%|█████████▊| 764/780 [06:16<00:04,  3.35it/s] 98%|█████████▊| 765/780 [06:17<00:04,  3.38it/s] 98%|█████████▊| 766/780 [06:17<00:04,  3.40it/s] 98%|█████████▊| 767/780 [06:17<00:03,  3.41it/s] 98%|█████████▊| 768/780 [06:17<00:03,  3.43it/s] 99%|█████████▊| 769/780 [06:18<00:03,  3.44it/s] 99%|█████████▊| 770/780 [06:18<00:02,  3.44it/s] 99%|█████████▉| 771/780 [06:18<00:02,  3.45it/s] 99%|█████████▉| 772/780 [06:19<00:02,  3.45it/s] 99%|█████████▉| 773/780 [06:19<00:02,  3.02it/s] 99%|█████████▉| 774/780 [06:19<00:01,  3.13it/s] 99%|█████████▉| 775/780 [06:20<00:01,  3.22it/s] 99%|█████████▉| 776/780 [06:20<00:01,  3.29it/s]100%|█████████▉| 777/780 [06:20<00:00,  3.34it/s]100%|█████████▉| 778/780 [06:21<00:00,  3.37it/s]100%|█████████▉| 779/780 [06:21<00:00,  3.40it/s]100%|██████████| 780/780 [06:21<00:00,  3.10it/s][INFO|trainer.py:2140] 2023-08-29 16:17:15,046 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 16:17:15,046 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 16:17:15,047 >>   Batch size = 8
{'eval_loss': 1.062766671180725, 'eval_runtime': 13.284, 'eval_samples_per_second': 366.156, 'eval_steps_per_second': 45.769, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.21it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.19it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.61it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.91it/s][A
  5%|▍         | 28/608 [00:00<00:15, 38.20it/s][A
  5%|▌         | 33/608 [00:00<00:14, 40.67it/s][A
  6%|▋         | 38/608 [00:00<00:13, 42.37it/s][A
  7%|▋         | 43/608 [00:00<00:12, 43.67it/s][A
  8%|▊         | 48/608 [00:01<00:12, 44.46it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.10it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.58it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.92it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.03it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.31it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.45it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.54it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.57it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 46.70it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.68it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.69it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.71it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.52it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.65it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.71it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.65it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.72it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 46.75it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.65it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.66it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.65it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.59it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.64it/s][A
 28%|██▊       | 168/608 [00:03<00:11, 38.25it/s][A
 28%|██▊       | 173/608 [00:03<00:10, 40.49it/s][A
 29%|██▉       | 178/608 [00:03<00:10, 42.23it/s][A
 30%|███       | 183/608 [00:04<00:09, 43.47it/s][A
 31%|███       | 188/608 [00:04<00:09, 44.42it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.10it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 45.58it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.00it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.95it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.03it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.30it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.48it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 46.49it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.63it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.72it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.66it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.66it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.58it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.55it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.28it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.56it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.66it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 46.72it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.67it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.78it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.77it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.66it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.55it/s][A
 51%|█████     | 308/608 [00:06<00:06, 42.97it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 44.12it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 44.90it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.34it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.77it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.11it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.25it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.36it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.19it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.21it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.32it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.52it/s][A
 61%|██████    | 368/608 [00:08<00:05, 46.60it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.66it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.60it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.69it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.70it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.58it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.41it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.48it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.52it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.92it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.25it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.40it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.49it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.54it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.56it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.56it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.49it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.48it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.46it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.54it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.57it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.62it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.71it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.75it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.68it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.59it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.55it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.48it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 46.58it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.55it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.59it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.58it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.63it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.69it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.70it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.55it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.53it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 42.68it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 43.80it/s][A
 93%|█████████▎| 563/608 [00:12<00:01, 44.67it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.32it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.70it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.99it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.24it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.40it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.15it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 46.20it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.28it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.48it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.48it/s][A100%|██████████| 780/780 [06:34<00:00,  3.10it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 16:17:28,575 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 16:17:28,754 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 16:17:35,159 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 16:17:35,406 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 16:17:35,488 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 16:17:48,861 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 16:17:48,894 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156 (score: 1.0138314962387085).
                                                 100%|██████████| 780/780 [07:10<00:00,  3.10it/s]100%|██████████| 780/780 [07:10<00:00,  1.81it/s]
[INFO|trainer.py:1894] 2023-08-29 16:18:03,481 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 16:18:03,740 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 16:18:11,974 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 16:18:12,635 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 16:18:12,973 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 16:18:16,026 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:16,130 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:16,130 >>   train_loss               =     0.4198
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:16,130 >>   train_runtime            = 0:07:10.04
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:16,130 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:16,130 >>   train_samples_per_second =    116.267
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:16,130 >>   train_steps_per_second   =      1.814
{'eval_loss': 1.0648685693740845, 'eval_runtime': 13.2627, 'eval_samples_per_second': 366.743, 'eval_steps_per_second': 45.843, 'epoch': 5.0}
{'train_runtime': 430.0439, 'train_samples_per_second': 116.267, 'train_steps_per_second': 1.814, 'train_loss': 0.41983578021709733, 'epoch': 5.0}
08/29/2023 16:18:17 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 16:18:17,785 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 16:18:17,785 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 16:18:17,785 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 58.59it/s]  2%|▏         | 12/608 [00:00<00:11, 51.56it/s]  3%|▎         | 18/608 [00:00<00:11, 49.56it/s]  4%|▍         | 23/608 [00:00<00:11, 48.90it/s]  5%|▍         | 28/608 [00:00<00:11, 48.46it/s]  5%|▌         | 33/608 [00:00<00:11, 48.06it/s]  6%|▋         | 38/608 [00:00<00:11, 47.92it/s]  7%|▋         | 43/608 [00:00<00:11, 47.79it/s]  8%|▊         | 48/608 [00:00<00:11, 47.45it/s]  9%|▊         | 53/608 [00:01<00:11, 47.40it/s] 10%|▉         | 58/608 [00:01<00:11, 47.42it/s] 10%|█         | 63/608 [00:01<00:11, 47.27it/s] 11%|█         | 68/608 [00:01<00:15, 34.62it/s] 12%|█▏        | 73/608 [00:01<00:14, 37.68it/s] 13%|█▎        | 78/608 [00:01<00:13, 40.16it/s] 14%|█▎        | 83/608 [00:01<00:12, 42.06it/s] 14%|█▍        | 88/608 [00:01<00:11, 43.59it/s] 15%|█▌        | 93/608 [00:02<00:11, 44.71it/s] 16%|█▌        | 98/608 [00:02<00:11, 45.48it/s] 17%|█▋        | 103/608 [00:02<00:10, 46.12it/s] 18%|█▊        | 108/608 [00:02<00:10, 45.91it/s] 19%|█▊        | 113/608 [00:02<00:10, 46.22it/s] 19%|█▉        | 118/608 [00:02<00:10, 46.39it/s] 20%|██        | 123/608 [00:02<00:10, 46.66it/s] 21%|██        | 128/608 [00:02<00:10, 46.86it/s] 22%|██▏       | 133/608 [00:02<00:10, 47.05it/s] 23%|██▎       | 138/608 [00:03<00:09, 47.11it/s] 24%|██▎       | 143/608 [00:03<00:09, 47.25it/s] 24%|██▍       | 148/608 [00:03<00:09, 47.34it/s] 25%|██▌       | 153/608 [00:03<00:09, 47.08it/s] 26%|██▌       | 158/608 [00:03<00:09, 46.82it/s] 27%|██▋       | 163/608 [00:03<00:09, 47.01it/s] 28%|██▊       | 168/608 [00:03<00:09, 47.04it/s] 28%|██▊       | 173/608 [00:03<00:09, 47.07it/s] 29%|██▉       | 178/608 [00:03<00:09, 47.23it/s] 30%|███       | 183/608 [00:03<00:08, 47.32it/s] 31%|███       | 188/608 [00:04<00:08, 47.35it/s] 32%|███▏      | 193/608 [00:04<00:08, 47.23it/s] 33%|███▎      | 198/608 [00:04<00:08, 47.30it/s] 33%|███▎      | 203/608 [00:04<00:08, 47.14it/s] 34%|███▍      | 208/608 [00:04<00:11, 36.02it/s] 35%|███▌      | 213/608 [00:04<00:10, 38.78it/s] 36%|███▌      | 218/608 [00:04<00:09, 40.97it/s] 37%|███▋      | 223/608 [00:04<00:09, 42.73it/s] 38%|███▊      | 228/608 [00:05<00:08, 44.05it/s] 38%|███▊      | 233/608 [00:05<00:08, 44.99it/s] 39%|███▉      | 238/608 [00:05<00:08, 45.68it/s] 40%|███▉      | 243/608 [00:05<00:07, 46.24it/s] 41%|████      | 248/608 [00:05<00:07, 46.13it/s] 42%|████▏     | 253/608 [00:05<00:07, 46.45it/s] 42%|████▏     | 258/608 [00:05<00:07, 46.75it/s] 43%|████▎     | 263/608 [00:05<00:07, 46.82it/s] 44%|████▍     | 268/608 [00:05<00:07, 46.93it/s] 45%|████▍     | 273/608 [00:05<00:07, 47.13it/s] 46%|████▌     | 278/608 [00:06<00:06, 47.23it/s] 47%|████▋     | 283/608 [00:06<00:06, 47.28it/s] 47%|████▋     | 288/608 [00:06<00:06, 47.26it/s] 48%|████▊     | 293/608 [00:06<00:06, 47.15it/s] 49%|████▉     | 298/608 [00:06<00:06, 46.98it/s] 50%|████▉     | 303/608 [00:06<00:06, 46.98it/s] 51%|█████     | 308/608 [00:06<00:06, 47.07it/s] 51%|█████▏    | 313/608 [00:06<00:06, 47.15it/s] 52%|█████▏    | 318/608 [00:06<00:06, 47.15it/s] 53%|█████▎    | 323/608 [00:07<00:06, 47.21it/s] 54%|█████▍    | 328/608 [00:07<00:05, 47.31it/s] 55%|█████▍    | 333/608 [00:07<00:05, 47.24it/s] 56%|█████▌    | 338/608 [00:07<00:05, 47.16it/s] 56%|█████▋    | 343/608 [00:07<00:05, 47.03it/s] 57%|█████▋    | 348/608 [00:07<00:09, 28.52it/s] 58%|█████▊    | 353/608 [00:07<00:07, 32.35it/s] 59%|█████▉    | 358/608 [00:08<00:07, 35.65it/s] 60%|█████▉    | 363/608 [00:08<00:06, 38.53it/s] 61%|██████    | 368/608 [00:08<00:05, 40.76it/s] 61%|██████▏   | 373/608 [00:08<00:05, 42.36it/s] 62%|██████▏   | 378/608 [00:08<00:05, 43.77it/s] 63%|██████▎   | 383/608 [00:08<00:05, 44.74it/s] 64%|██████▍   | 388/608 [00:08<00:04, 45.10it/s] 65%|██████▍   | 393/608 [00:08<00:04, 45.61it/s] 65%|██████▌   | 398/608 [00:08<00:04, 46.06it/s] 66%|██████▋   | 403/608 [00:08<00:04, 46.31it/s] 67%|██████▋   | 408/608 [00:09<00:04, 46.55it/s] 68%|██████▊   | 413/608 [00:09<00:04, 46.82it/s] 69%|██████▉   | 418/608 [00:09<00:04, 46.97it/s] 70%|██████▉   | 423/608 [00:09<00:03, 47.06it/s] 70%|███████   | 428/608 [00:09<00:03, 47.12it/s] 71%|███████   | 433/608 [00:09<00:03, 46.97it/s] 72%|███████▏  | 438/608 [00:09<00:03, 46.88it/s] 73%|███████▎  | 443/608 [00:09<00:03, 46.98it/s] 74%|███████▎  | 448/608 [00:09<00:03, 47.04it/s] 75%|███████▍  | 453/608 [00:10<00:03, 46.99it/s] 75%|███████▌  | 458/608 [00:10<00:03, 47.11it/s] 76%|███████▌  | 463/608 [00:10<00:03, 47.17it/s] 77%|███████▋  | 468/608 [00:10<00:02, 47.11it/s] 78%|███████▊  | 473/608 [00:10<00:02, 47.10it/s] 79%|███████▊  | 478/608 [00:10<00:02, 47.09it/s] 79%|███████▉  | 483/608 [00:10<00:03, 38.73it/s] 80%|████████  | 488/608 [00:10<00:02, 40.92it/s] 81%|████████  | 493/608 [00:10<00:02, 42.60it/s] 82%|████████▏ | 498/608 [00:11<00:02, 43.80it/s] 83%|████████▎ | 503/608 [00:11<00:02, 44.73it/s] 84%|████████▎ | 508/608 [00:11<00:02, 45.52it/s] 84%|████████▍ | 513/608 [00:11<00:02, 46.00it/s] 85%|████████▌ | 518/608 [00:11<00:01, 46.28it/s] 86%|████████▌ | 523/608 [00:11<00:01, 46.28it/s] 87%|████████▋ | 528/608 [00:11<00:01, 46.35it/s] 88%|████████▊ | 533/608 [00:11<00:01, 46.50it/s] 88%|████████▊ | 538/608 [00:11<00:01, 46.64it/s] 89%|████████▉ | 543/608 [00:12<00:01, 46.79it/s] 90%|█████████ | 548/608 [00:12<00:01, 46.72it/s] 91%|█████████ | 553/608 [00:12<00:01, 46.82it/s] 92%|█████████▏| 558/608 [00:12<00:01, 46.89it/s] 93%|█████████▎| 563/608 [00:12<00:00, 46.82it/s] 93%|█████████▎| 568/608 [00:12<00:00, 46.71it/s] 94%|█████████▍| 573/608 [00:12<00:00, 46.70it/s] 95%|█████████▌| 578/608 [00:12<00:00, 46.80it/s] 96%|█████████▌| 583/608 [00:12<00:00, 46.87it/s] 97%|█████████▋| 588/608 [00:12<00:00, 46.98it/s] 98%|█████████▊| 593/608 [00:13<00:00, 47.07it/s] 98%|█████████▊| 598/608 [00:13<00:00, 47.11it/s] 99%|█████████▉| 603/608 [00:13<00:00, 47.13it/s]100%|██████████| 608/608 [00:13<00:00, 47.03it/s]100%|██████████| 608/608 [00:13<00:00, 45.26it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 16:18:31,239 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:31,239 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:31,239 >>   eval_loss               =     1.0138
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:31,239 >>   eval_runtime            = 0:00:13.45
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:31,239 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:31,239 >>   eval_samples_per_second =    361.519
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:31,239 >>   eval_steps_per_second   =      45.19
[INFO|trainer_pt_utils.py:913] 2023-08-29 16:18:31,239 >>   perplexity              =     2.7561
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:18:49,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:18:49,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:18:49,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:18:49,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:18:49,617 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 16:18:50,181 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 16:18:50,182 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 16:18:50,964 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 16:18:52,096 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 16:18:52,096 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:18:54,032 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:18:54,076 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:18:54,076 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:18:54,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:18:54,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 16:18:54,696 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 16:18:54,697 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 16:18:55,095 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 16:18:55,350 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 16:18:55,350 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:08,  1.65it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.68it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.61it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:15,  1.47it/s]Extractor Predicting: 26it [00:16,  1.44it/s]Extractor Predicting: 27it [00:17,  1.47it/s]Extractor Predicting: 28it [00:17,  1.48it/s]Extractor Predicting: 29it [00:18,  1.51it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:19,  1.51it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.56it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.50it/s]Extractor Predicting: 37it [00:23,  1.50it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:25,  1.53it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:29,  1.54it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:31,  1.52it/s]Extractor Predicting: 50it [00:32,  1.53it/s]Extractor Predicting: 51it [00:32,  1.52it/s]Extractor Predicting: 52it [00:33,  1.53it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:35,  1.58it/s]Extractor Predicting: 56it [00:36,  1.56it/s]Extractor Predicting: 57it [00:36,  1.55it/s]Extractor Predicting: 58it [00:37,  1.52it/s]Extractor Predicting: 59it [00:38,  1.53it/s]Extractor Predicting: 60it [00:38,  1.53it/s]Extractor Predicting: 61it [00:39,  1.49it/s]Extractor Predicting: 62it [00:40,  1.50it/s]Extractor Predicting: 63it [00:40,  1.50it/s]Extractor Predicting: 64it [00:41,  1.52it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:42,  1.53it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:44,  1.48it/s]Extractor Predicting: 69it [00:44,  1.49it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:45,  1.58it/s]Extractor Predicting: 72it [00:46,  1.57it/s]Extractor Predicting: 73it [00:47,  1.56it/s]Extractor Predicting: 74it [00:47,  1.55it/s]Extractor Predicting: 75it [00:48,  1.57it/s]Extractor Predicting: 76it [00:49,  1.56it/s]Extractor Predicting: 77it [00:49,  1.55it/s]Extractor Predicting: 78it [00:50,  1.41it/s]Extractor Predicting: 79it [00:51,  1.45it/s]Extractor Predicting: 80it [00:51,  1.50it/s]Extractor Predicting: 81it [00:52,  1.49it/s]Extractor Predicting: 82it [00:53,  1.50it/s]Extractor Predicting: 83it [00:53,  1.48it/s]Extractor Predicting: 84it [00:54,  1.51it/s]Extractor Predicting: 85it [00:55,  1.51it/s]Extractor Predicting: 86it [00:55,  1.50it/s]Extractor Predicting: 87it [00:56,  1.53it/s]Extractor Predicting: 88it [00:57,  1.51it/s]Extractor Predicting: 89it [00:57,  1.45it/s]Extractor Predicting: 90it [00:58,  1.49it/s]Extractor Predicting: 91it [00:59,  1.52it/s]Extractor Predicting: 92it [00:59,  1.53it/s]Extractor Predicting: 93it [01:00,  1.55it/s]Extractor Predicting: 94it [01:01,  1.51it/s]Extractor Predicting: 95it [01:01,  1.47it/s]Extractor Predicting: 96it [01:02,  1.50it/s]Extractor Predicting: 97it [01:03,  1.51it/s]Extractor Predicting: 98it [01:03,  1.53it/s]Extractor Predicting: 99it [01:04,  1.53it/s]Extractor Predicting: 100it [01:05,  1.52it/s]Extractor Predicting: 101it [01:05,  1.53it/s]Extractor Predicting: 102it [01:06,  1.56it/s]Extractor Predicting: 103it [01:07,  1.58it/s]Extractor Predicting: 104it [01:07,  1.55it/s]Extractor Predicting: 105it [01:08,  1.53it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:10,  1.57it/s]Extractor Predicting: 109it [01:11,  1.49it/s]Extractor Predicting: 110it [01:11,  1.48it/s]Extractor Predicting: 111it [01:12,  1.51it/s]Extractor Predicting: 112it [01:13,  1.52it/s]Extractor Predicting: 113it [01:13,  1.54it/s]Extractor Predicting: 114it [01:14,  1.53it/s]Extractor Predicting: 115it [01:14,  1.60it/s]Extractor Predicting: 116it [01:15,  1.60it/s]Extractor Predicting: 117it [01:16,  1.59it/s]Extractor Predicting: 118it [01:16,  1.62it/s]Extractor Predicting: 119it [01:17,  1.55it/s]Extractor Predicting: 120it [01:18,  1.58it/s]Extractor Predicting: 121it [01:18,  1.57it/s]Extractor Predicting: 122it [01:19,  1.53it/s]Extractor Predicting: 123it [01:20,  1.50it/s]Extractor Predicting: 124it [01:20,  1.43it/s]Extractor Predicting: 125it [01:21,  1.46it/s]Extractor Predicting: 126it [01:22,  1.50it/s]Extractor Predicting: 127it [01:22,  1.51it/s]Extractor Predicting: 128it [01:23,  1.47it/s]Extractor Predicting: 129it [01:24,  1.45it/s]Extractor Predicting: 130it [01:24,  1.49it/s]Extractor Predicting: 131it [01:25,  1.54it/s]Extractor Predicting: 132it [01:26,  1.50it/s]Extractor Predicting: 133it [01:26,  1.48it/s]Extractor Predicting: 134it [01:27,  1.49it/s]Extractor Predicting: 135it [01:28,  1.53it/s]Extractor Predicting: 136it [01:28,  1.54it/s]Extractor Predicting: 137it [01:29,  1.56it/s]Extractor Predicting: 138it [01:30,  1.54it/s]Extractor Predicting: 139it [01:30,  1.54it/s]Extractor Predicting: 140it [01:31,  1.53it/s]Extractor Predicting: 141it [01:31,  1.56it/s]Extractor Predicting: 142it [01:32,  1.53it/s]Extractor Predicting: 143it [01:33,  1.57it/s]Extractor Predicting: 144it [01:33,  1.56it/s]Extractor Predicting: 145it [01:34,  1.57it/s]Extractor Predicting: 146it [01:35,  1.50it/s]Extractor Predicting: 147it [01:35,  1.49it/s]Extractor Predicting: 148it [01:36,  1.50it/s]Extractor Predicting: 149it [01:37,  1.50it/s]Extractor Predicting: 150it [01:37,  1.52it/s]Extractor Predicting: 151it [01:38,  1.49it/s]Extractor Predicting: 152it [01:39,  1.53it/s]Extractor Predicting: 153it [01:39,  1.52it/s]Extractor Predicting: 154it [01:40,  1.52it/s]Extractor Predicting: 155it [01:41,  1.54it/s]Extractor Predicting: 156it [01:41,  1.55it/s]Extractor Predicting: 157it [01:42,  1.55it/s]Extractor Predicting: 158it [01:43,  1.58it/s]Extractor Predicting: 159it [01:43,  1.56it/s]Extractor Predicting: 160it [01:44,  1.57it/s]Extractor Predicting: 161it [01:45,  1.50it/s]Extractor Predicting: 162it [01:45,  1.49it/s]Extractor Predicting: 163it [01:46,  1.52it/s]Extractor Predicting: 164it [01:47,  1.52it/s]Extractor Predicting: 165it [01:47,  1.55it/s]Extractor Predicting: 166it [01:48,  1.50it/s]Extractor Predicting: 167it [01:49,  1.49it/s]Extractor Predicting: 168it [01:49,  1.48it/s]Extractor Predicting: 169it [01:50,  1.45it/s]Extractor Predicting: 170it [01:51,  1.47it/s]Extractor Predicting: 171it [01:51,  1.43it/s]Extractor Predicting: 172it [01:52,  1.46it/s]Extractor Predicting: 173it [01:53,  1.44it/s]Extractor Predicting: 174it [01:53,  1.42it/s]Extractor Predicting: 175it [01:54,  1.47it/s]Extractor Predicting: 176it [01:55,  1.44it/s]Extractor Predicting: 177it [01:56,  1.42it/s]Extractor Predicting: 178it [01:56,  1.41it/s]Extractor Predicting: 179it [01:57,  1.40it/s]Extractor Predicting: 180it [01:58,  1.41it/s]Extractor Predicting: 181it [01:58,  1.41it/s]Extractor Predicting: 182it [01:59,  1.42it/s]Extractor Predicting: 183it [02:00,  1.42it/s]Extractor Predicting: 184it [02:00,  1.43it/s]Extractor Predicting: 185it [02:01,  1.36it/s]Extractor Predicting: 186it [02:02,  1.39it/s]Extractor Predicting: 187it [02:03,  1.46it/s]Extractor Predicting: 187it [02:03,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:21:20,575 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:21:20,603 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:21:20,603 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:21:20,603 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:21:20,603 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 16:21:21,555 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 16:21:21,556 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 16:21:22,271 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 16:21:23,460 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 16:21:23,460 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:21:26,830 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:21:26,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:21:26,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:21:26,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:21:26,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 16:21:27,702 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 16:21:27,703 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 16:21:28,354 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 16:21:28,592 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 16:21:28,592 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.7776669990029911,
  "recall": 0.16036184210526316,
  "score": 0.2658939832964036,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.46it/s]Extractor Predicting: 22it [00:14,  1.45it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.49it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.45it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:23,  1.52it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.48it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.55it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:29,  1.55it/s]Extractor Predicting: 46it [00:30,  1.54it/s]Extractor Predicting: 47it [00:31,  1.47it/s]Extractor Predicting: 48it [00:31,  1.48it/s]Extractor Predicting: 49it [00:32,  1.46it/s]Extractor Predicting: 50it [00:33,  1.48it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.46it/s]Extractor Predicting: 53it [00:35,  1.52it/s]Extractor Predicting: 54it [00:35,  1.53it/s]Extractor Predicting: 55it [00:36,  1.55it/s]Extractor Predicting: 56it [00:36,  1.55it/s]Extractor Predicting: 57it [00:37,  1.48it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:38,  1.53it/s]Extractor Predicting: 60it [00:39,  1.56it/s]Extractor Predicting: 61it [00:40,  1.54it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:42,  1.52it/s]Extractor Predicting: 65it [00:42,  1.53it/s]Extractor Predicting: 66it [00:43,  1.40it/s]Extractor Predicting: 67it [00:44,  1.43it/s]Extractor Predicting: 68it [00:45,  1.38it/s]Extractor Predicting: 69it [00:45,  1.44it/s]Extractor Predicting: 70it [00:46,  1.42it/s]Extractor Predicting: 71it [00:47,  1.46it/s]Extractor Predicting: 72it [00:47,  1.46it/s]Extractor Predicting: 73it [00:48,  1.52it/s]Extractor Predicting: 74it [00:49,  1.54it/s]Extractor Predicting: 75it [00:49,  1.55it/s]Extractor Predicting: 76it [00:50,  1.51it/s]Extractor Predicting: 77it [00:51,  1.54it/s]Extractor Predicting: 78it [00:51,  1.57it/s]Extractor Predicting: 79it [00:52,  1.56it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:53,  1.53it/s]Extractor Predicting: 82it [00:54,  1.54it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:55,  1.50it/s]Extractor Predicting: 85it [00:56,  1.54it/s]Extractor Predicting: 86it [00:56,  1.49it/s]Extractor Predicting: 87it [00:57,  1.54it/s]Extractor Predicting: 88it [00:58,  1.59it/s]Extractor Predicting: 89it [00:58,  1.58it/s]Extractor Predicting: 90it [00:59,  1.55it/s]Extractor Predicting: 91it [01:00,  1.57it/s]Extractor Predicting: 92it [01:00,  1.61it/s]Extractor Predicting: 93it [01:01,  1.59it/s]Extractor Predicting: 94it [01:01,  1.61it/s]Extractor Predicting: 95it [01:02,  1.61it/s]Extractor Predicting: 96it [01:03,  1.59it/s]Extractor Predicting: 97it [01:03,  1.58it/s]Extractor Predicting: 98it [01:04,  1.57it/s]Extractor Predicting: 99it [01:05,  1.56it/s]Extractor Predicting: 100it [01:05,  1.59it/s]Extractor Predicting: 101it [01:06,  1.53it/s]Extractor Predicting: 102it [01:07,  1.55it/s]Extractor Predicting: 103it [01:07,  1.54it/s]Extractor Predicting: 104it [01:08,  1.56it/s]Extractor Predicting: 105it [01:09,  1.53it/s]Extractor Predicting: 106it [01:09,  1.48it/s]Extractor Predicting: 107it [01:10,  1.52it/s]Extractor Predicting: 108it [01:10,  1.54it/s]Extractor Predicting: 109it [01:11,  1.52it/s]Extractor Predicting: 110it [01:12,  1.52it/s]Extractor Predicting: 111it [01:13,  1.49it/s]Extractor Predicting: 112it [01:13,  1.53it/s]Extractor Predicting: 113it [01:14,  1.54it/s]Extractor Predicting: 114it [01:14,  1.56it/s]Extractor Predicting: 115it [01:15,  1.55it/s]Extractor Predicting: 116it [01:16,  1.52it/s]Extractor Predicting: 117it [01:16,  1.54it/s]Extractor Predicting: 118it [01:17,  1.53it/s]Extractor Predicting: 119it [01:18,  1.55it/s]Extractor Predicting: 120it [01:18,  1.53it/s]Extractor Predicting: 121it [01:19,  1.54it/s]Extractor Predicting: 122it [01:20,  1.56it/s]Extractor Predicting: 123it [01:20,  1.56it/s]Extractor Predicting: 124it [01:21,  1.58it/s]Extractor Predicting: 125it [01:22,  1.56it/s]Extractor Predicting: 126it [01:22,  1.56it/s]Extractor Predicting: 127it [01:23,  1.57it/s]Extractor Predicting: 128it [01:24,  1.51it/s]Extractor Predicting: 129it [01:24,  1.49it/s]Extractor Predicting: 130it [01:25,  1.54it/s]Extractor Predicting: 131it [01:25,  1.54it/s]Extractor Predicting: 132it [01:26,  1.53it/s]Extractor Predicting: 133it [01:27,  1.52it/s]Extractor Predicting: 134it [01:27,  1.52it/s]Extractor Predicting: 135it [01:28,  1.53it/s]Extractor Predicting: 136it [01:29,  1.54it/s]Extractor Predicting: 137it [01:29,  1.53it/s]Extractor Predicting: 138it [01:30,  1.49it/s]Extractor Predicting: 139it [01:31,  1.51it/s]Extractor Predicting: 140it [01:31,  1.51it/s]Extractor Predicting: 141it [01:32,  1.51it/s]Extractor Predicting: 142it [01:33,  1.51it/s]Extractor Predicting: 143it [01:33,  1.48it/s]Extractor Predicting: 144it [01:34,  1.49it/s]Extractor Predicting: 145it [01:35,  1.49it/s]Extractor Predicting: 146it [01:35,  1.50it/s]Extractor Predicting: 147it [01:36,  1.50it/s]Extractor Predicting: 148it [01:37,  1.47it/s]Extractor Predicting: 149it [01:37,  1.48it/s]Extractor Predicting: 150it [01:38,  1.46it/s]Extractor Predicting: 151it [01:39,  1.46it/s]Extractor Predicting: 152it [01:40,  1.48it/s]Extractor Predicting: 153it [01:40,  1.50it/s]Extractor Predicting: 154it [01:41,  1.52it/s]Extractor Predicting: 155it [01:41,  1.55it/s]Extractor Predicting: 156it [01:42,  1.53it/s]Extractor Predicting: 157it [01:43,  1.54it/s]Extractor Predicting: 158it [01:43,  1.48it/s]Extractor Predicting: 159it [01:44,  1.49it/s]Extractor Predicting: 160it [01:45,  1.50it/s]Extractor Predicting: 161it [01:45,  1.52it/s]Extractor Predicting: 162it [01:46,  1.52it/s]Extractor Predicting: 163it [01:47,  1.50it/s]Extractor Predicting: 164it [01:47,  1.50it/s]Extractor Predicting: 165it [01:48,  1.53it/s]Extractor Predicting: 166it [01:49,  1.54it/s]Extractor Predicting: 167it [01:49,  1.55it/s]Extractor Predicting: 168it [01:50,  1.47it/s]Extractor Predicting: 169it [01:51,  1.49it/s]Extractor Predicting: 170it [01:51,  1.53it/s]Extractor Predicting: 171it [01:52,  1.59it/s]Extractor Predicting: 172it [01:53,  1.58it/s]Extractor Predicting: 173it [01:53,  1.56it/s]Extractor Predicting: 174it [01:54,  1.54it/s]Extractor Predicting: 175it [01:55,  1.28it/s]Extractor Predicting: 176it [01:56,  1.33it/s]Extractor Predicting: 177it [01:56,  1.39it/s]Extractor Predicting: 178it [01:57,  1.41it/s]Extractor Predicting: 179it [01:58,  1.38it/s]Extractor Predicting: 180it [01:58,  1.40it/s]Extractor Predicting: 181it [01:59,  1.42it/s]Extractor Predicting: 182it [02:00,  1.43it/s]Extractor Predicting: 183it [02:00,  1.44it/s]Extractor Predicting: 184it [02:01,  1.41it/s]Extractor Predicting: 185it [02:02,  1.44it/s]Extractor Predicting: 186it [02:03,  1.46it/s]Extractor Predicting: 187it [02:03,  1.45it/s]Extractor Predicting: 188it [02:04,  1.48it/s]Extractor Predicting: 189it [02:05,  1.47it/s]Extractor Predicting: 190it [02:05,  1.46it/s]Extractor Predicting: 191it [02:06,  1.48it/s]Extractor Predicting: 192it [02:07,  1.48it/s]Extractor Predicting: 193it [02:07,  1.48it/s]Extractor Predicting: 194it [02:08,  1.47it/s]Extractor Predicting: 195it [02:09,  1.52it/s]Extractor Predicting: 196it [02:09,  1.55it/s]Extractor Predicting: 197it [02:10,  1.54it/s]Extractor Predicting: 198it [02:11,  1.54it/s]Extractor Predicting: 199it [02:11,  1.52it/s]Extractor Predicting: 200it [02:12,  1.54it/s]Extractor Predicting: 201it [02:12,  1.53it/s]Extractor Predicting: 202it [02:13,  1.52it/s]Extractor Predicting: 203it [02:14,  1.53it/s]Extractor Predicting: 204it [02:14,  1.51it/s]Extractor Predicting: 205it [02:15,  1.52it/s]Extractor Predicting: 206it [02:16,  1.54it/s]Extractor Predicting: 207it [02:16,  1.53it/s]Extractor Predicting: 208it [02:17,  1.56it/s]Extractor Predicting: 209it [02:18,  1.56it/s]Extractor Predicting: 210it [02:18,  1.54it/s]Extractor Predicting: 211it [02:19,  1.55it/s]Extractor Predicting: 212it [02:20,  1.57it/s]Extractor Predicting: 213it [02:20,  1.54it/s]Extractor Predicting: 214it [02:21,  1.46it/s]Extractor Predicting: 215it [02:22,  1.47it/s]Extractor Predicting: 216it [02:22,  1.48it/s]Extractor Predicting: 217it [02:23,  1.53it/s]Extractor Predicting: 218it [02:24,  1.51it/s]Extractor Predicting: 219it [02:24,  1.49it/s]Extractor Predicting: 220it [02:25,  1.50it/s]Extractor Predicting: 221it [02:26,  1.47it/s]Extractor Predicting: 222it [02:26,  1.48it/s]Extractor Predicting: 223it [02:27,  1.49it/s]Extractor Predicting: 224it [02:28,  1.52it/s]Extractor Predicting: 225it [02:28,  1.52it/s]Extractor Predicting: 226it [02:29,  1.54it/s]Extractor Predicting: 227it [02:30,  1.55it/s]Extractor Predicting: 228it [02:30,  1.56it/s]Extractor Predicting: 229it [02:31,  1.55it/s]Extractor Predicting: 230it [02:32,  1.56it/s]Extractor Predicting: 231it [02:32,  1.57it/s]Extractor Predicting: 232it [02:33,  1.56it/s]Extractor Predicting: 233it [02:33,  1.56it/s]Extractor Predicting: 234it [02:34,  1.53it/s]Extractor Predicting: 235it [02:35,  1.50it/s]Extractor Predicting: 236it [02:35,  1.51it/s]Extractor Predicting: 237it [02:36,  1.51it/s]Extractor Predicting: 238it [02:37,  1.52it/s]Extractor Predicting: 239it [02:37,  1.52it/s]Extractor Predicting: 240it [02:38,  1.50it/s]Extractor Predicting: 241it [02:39,  1.54it/s]Extractor Predicting: 242it [02:39,  1.53it/s]Extractor Predicting: 243it [02:40,  1.57it/s]Extractor Predicting: 244it [02:41,  1.58it/s]Extractor Predicting: 245it [02:41,  1.49it/s]Extractor Predicting: 246it [02:42,  1.53it/s]Extractor Predicting: 247it [02:43,  1.53it/s]Extractor Predicting: 248it [02:43,  1.56it/s]Extractor Predicting: 249it [02:44,  1.56it/s]Extractor Predicting: 250it [02:45,  1.56it/s]Extractor Predicting: 251it [02:45,  1.54it/s]Extractor Predicting: 252it [02:46,  1.57it/s]Extractor Predicting: 253it [02:46,  1.63it/s]Extractor Predicting: 254it [02:47,  1.59it/s]Extractor Predicting: 255it [02:48,  1.55it/s]Extractor Predicting: 256it [02:48,  1.56it/s]Extractor Predicting: 257it [02:49,  1.57it/s]Extractor Predicting: 258it [02:50,  1.58it/s]Extractor Predicting: 259it [02:50,  1.58it/s]Extractor Predicting: 260it [02:51,  1.51it/s]Extractor Predicting: 261it [02:52,  1.51it/s]Extractor Predicting: 262it [02:52,  1.54it/s]Extractor Predicting: 263it [02:53,  1.49it/s]Extractor Predicting: 264it [02:54,  1.54it/s]Extractor Predicting: 265it [02:54,  1.53it/s]Extractor Predicting: 266it [02:55,  1.51it/s]Extractor Predicting: 267it [02:56,  1.50it/s]Extractor Predicting: 268it [02:56,  1.51it/s]Extractor Predicting: 269it [02:57,  1.48it/s]Extractor Predicting: 270it [02:58,  1.50it/s]Extractor Predicting: 271it [02:58,  1.51it/s]Extractor Predicting: 272it [02:59,  1.52it/s]Extractor Predicting: 273it [03:00,  1.51it/s]Extractor Predicting: 274it [03:00,  1.50it/s]Extractor Predicting: 275it [03:01,  1.50it/s]Extractor Predicting: 276it [03:02,  1.54it/s]Extractor Predicting: 277it [03:02,  1.50it/s]Extractor Predicting: 278it [03:03,  1.51it/s]Extractor Predicting: 279it [03:04,  1.50it/s]Extractor Predicting: 280it [03:04,  1.51it/s]Extractor Predicting: 281it [03:05,  1.52it/s]Extractor Predicting: 282it [03:06,  1.31it/s]Extractor Predicting: 283it [03:07,  1.37it/s]Extractor Predicting: 284it [03:07,  1.43it/s]Extractor Predicting: 285it [03:08,  1.47it/s]Extractor Predicting: 286it [03:08,  1.49it/s]Extractor Predicting: 287it [03:09,  1.44it/s]Extractor Predicting: 288it [03:10,  1.46it/s]Extractor Predicting: 289it [03:11,  1.47it/s]Extractor Predicting: 290it [03:11,  1.46it/s]Extractor Predicting: 291it [03:12,  1.47it/s]Extractor Predicting: 292it [03:13,  1.45it/s]Extractor Predicting: 293it [03:13,  1.46it/s]Extractor Predicting: 294it [03:14,  1.44it/s]Extractor Predicting: 295it [03:15,  1.46it/s]Extractor Predicting: 296it [03:15,  1.47it/s]Extractor Predicting: 297it [03:16,  1.44it/s]Extractor Predicting: 298it [03:17,  1.45it/s]Extractor Predicting: 299it [03:17,  1.45it/s]Extractor Predicting: 300it [03:18,  1.42it/s]Extractor Predicting: 301it [03:19,  1.38it/s]Extractor Predicting: 302it [03:20,  1.37it/s]Extractor Predicting: 303it [03:20,  1.40it/s]Extractor Predicting: 304it [03:21,  1.36it/s]Extractor Predicting: 305it [03:22,  1.34it/s]Extractor Predicting: 306it [03:23,  1.40it/s]Extractor Predicting: 307it [03:23,  1.42it/s]Extractor Predicting: 308it [03:24,  1.45it/s]Extractor Predicting: 309it [03:24,  1.52it/s]Extractor Predicting: 310it [03:25,  1.50it/s]Extractor Predicting: 311it [03:26,  1.46it/s]Extractor Predicting: 312it [03:27,  1.46it/s]Extractor Predicting: 313it [03:27,  1.48it/s]Extractor Predicting: 314it [03:28,  1.47it/s]Extractor Predicting: 315it [03:28,  1.54it/s]Extractor Predicting: 316it [03:29,  1.54it/s]Extractor Predicting: 317it [03:30,  1.48it/s]Extractor Predicting: 318it [03:31,  1.49it/s]Extractor Predicting: 319it [03:31,  1.48it/s]Extractor Predicting: 320it [03:32,  1.50it/s]Extractor Predicting: 321it [03:32,  1.53it/s]Extractor Predicting: 322it [03:33,  1.54it/s]Extractor Predicting: 323it [03:34,  1.54it/s]Extractor Predicting: 324it [03:34,  1.56it/s]Extractor Predicting: 325it [03:35,  1.55it/s]Extractor Predicting: 326it [03:36,  1.57it/s]Extractor Predicting: 327it [03:36,  1.56it/s]Extractor Predicting: 328it [03:37,  1.55it/s]Extractor Predicting: 329it [03:38,  1.54it/s]Extractor Predicting: 330it [03:38,  1.53it/s]Extractor Predicting: 331it [03:39,  1.52it/s]Extractor Predicting: 332it [03:40,  1.48it/s]Extractor Predicting: 333it [03:40,  1.49it/s]Extractor Predicting: 334it [03:41,  1.50it/s]Extractor Predicting: 335it [03:42,  1.52it/s]Extractor Predicting: 336it [03:42,  1.54it/s]Extractor Predicting: 337it [03:43,  1.48it/s]Extractor Predicting: 338it [03:44,  1.51it/s]Extractor Predicting: 339it [03:44,  1.52it/s]Extractor Predicting: 340it [03:45,  1.54it/s]Extractor Predicting: 341it [03:46,  1.55it/s]Extractor Predicting: 342it [03:46,  1.48it/s]Extractor Predicting: 343it [03:47,  1.50it/s]Extractor Predicting: 344it [03:48,  1.55it/s]Extractor Predicting: 345it [03:48,  1.58it/s]Extractor Predicting: 346it [03:49,  1.61it/s]Extractor Predicting: 347it [03:49,  1.60it/s]Extractor Predicting: 348it [03:50,  1.59it/s]Extractor Predicting: 349it [03:51,  1.62it/s]Extractor Predicting: 350it [03:51,  1.62it/s]Extractor Predicting: 351it [03:52,  1.62it/s]Extractor Predicting: 352it [03:53,  1.56it/s]Extractor Predicting: 353it [03:53,  1.58it/s]Extractor Predicting: 354it [03:54,  1.60it/s]Extractor Predicting: 355it [03:54,  1.61it/s]Extractor Predicting: 356it [03:55,  1.64it/s]Extractor Predicting: 357it [03:56,  1.62it/s]Extractor Predicting: 358it [03:56,  1.62it/s]Extractor Predicting: 359it [03:57,  1.61it/s]Extractor Predicting: 360it [03:58,  1.54it/s]Extractor Predicting: 361it [03:58,  1.56it/s]Extractor Predicting: 362it [03:59,  1.58it/s]Extractor Predicting: 363it [03:59,  1.61it/s]Extractor Predicting: 364it [04:00,  1.65it/s]Extractor Predicting: 365it [04:01,  1.56it/s]Extractor Predicting: 366it [04:01,  1.54it/s]Extractor Predicting: 367it [04:02,  1.50it/s]Extractor Predicting: 368it [04:03,  1.49it/s]Extractor Predicting: 369it [04:03,  1.53it/s]Extractor Predicting: 370it [04:04,  1.48it/s]Extractor Predicting: 371it [04:05,  1.51it/s]Extractor Predicting: 372it [04:05,  1.53it/s]Extractor Predicting: 373it [04:06,  1.55it/s]Extractor Predicting: 374it [04:07,  1.56it/s]Extractor Predicting: 375it [04:07,  1.51it/s]Extractor Predicting: 376it [04:08,  1.51it/s]Extractor Predicting: 377it [04:09,  1.50it/s]Extractor Predicting: 378it [04:09,  1.51it/s]Extractor Predicting: 379it [04:10,  1.51it/s]Extractor Predicting: 380it [04:11,  1.49it/s]Extractor Predicting: 381it [04:11,  1.53it/s]Extractor Predicting: 382it [04:12,  1.56it/s]Extractor Predicting: 383it [04:12,  1.56it/s]Extractor Predicting: 384it [04:13,  1.58it/s]Extractor Predicting: 385it [04:14,  1.53it/s]Extractor Predicting: 386it [04:14,  1.56it/s]Extractor Predicting: 387it [04:15,  1.57it/s]Extractor Predicting: 388it [04:16,  1.60it/s]Extractor Predicting: 389it [04:16,  1.56it/s]Extractor Predicting: 390it [04:17,  1.50it/s]Extractor Predicting: 391it [04:18,  1.53it/s]Extractor Predicting: 392it [04:18,  1.59it/s]Extractor Predicting: 393it [04:19,  1.61it/s]Extractor Predicting: 394it [04:19,  1.62it/s]Extractor Predicting: 395it [04:20,  1.59it/s]Extractor Predicting: 396it [04:21,  1.62it/s]Extractor Predicting: 397it [04:22,  1.39it/s]Extractor Predicting: 398it [04:22,  1.44it/s]Extractor Predicting: 399it [04:23,  1.48it/s]Extractor Predicting: 400it [04:24,  1.47it/s]Extractor Predicting: 401it [04:24,  1.52it/s]Extractor Predicting: 402it [04:25,  1.56it/s]Extractor Predicting: 403it [04:25,  1.57it/s]Extractor Predicting: 404it [04:26,  1.59it/s]Extractor Predicting: 405it [04:27,  1.56it/s]Extractor Predicting: 406it [04:27,  1.56it/s]Extractor Predicting: 407it [04:28,  1.53it/s]Extractor Predicting: 408it [04:29,  1.55it/s]Extractor Predicting: 409it [04:30,  1.42it/s]Extractor Predicting: 410it [04:30,  1.46it/s]Extractor Predicting: 411it [04:31,  1.48it/s]Extractor Predicting: 412it [04:31,  1.48it/s]Extractor Predicting: 413it [04:32,  1.44it/s]Extractor Predicting: 414it [04:33,  1.42it/s]Extractor Predicting: 415it [04:34,  1.39it/s]Extractor Predicting: 416it [04:34,  1.40it/s]Extractor Predicting: 417it [04:35,  1.39it/s]Extractor Predicting: 418it [04:36,  1.43it/s]Extractor Predicting: 419it [04:36,  1.44it/s]Extractor Predicting: 420it [04:37,  1.42it/s]Extractor Predicting: 421it [04:38,  1.42it/s]Extractor Predicting: 422it [04:39,  1.41it/s]Extractor Predicting: 423it [04:39,  1.45it/s]Extractor Predicting: 424it [04:40,  1.44it/s]Extractor Predicting: 425it [04:41,  1.44it/s]Extractor Predicting: 426it [04:41,  1.43it/s]Extractor Predicting: 427it [04:42,  1.40it/s]Extractor Predicting: 428it [04:43,  1.43it/s]Extractor Predicting: 429it [04:43,  1.46it/s]Extractor Predicting: 430it [04:44,  1.48it/s]Extractor Predicting: 431it [04:45,  1.49it/s]Extractor Predicting: 432it [04:46,  1.44it/s]Extractor Predicting: 433it [04:46,  1.46it/s]Extractor Predicting: 434it [04:47,  1.47it/s]Extractor Predicting: 435it [04:47,  1.49it/s]Extractor Predicting: 436it [04:48,  1.53it/s]Extractor Predicting: 437it [04:49,  1.48it/s]Extractor Predicting: 438it [04:49,  1.51it/s]Extractor Predicting: 439it [04:50,  1.51it/s]Extractor Predicting: 440it [04:51,  1.51it/s]Extractor Predicting: 441it [04:51,  1.50it/s]Extractor Predicting: 442it [04:52,  1.47it/s]Extractor Predicting: 443it [04:53,  1.46it/s]Extractor Predicting: 444it [04:54,  1.47it/s]Extractor Predicting: 445it [04:54,  1.47it/s]Extractor Predicting: 446it [04:55,  1.48it/s]Extractor Predicting: 447it [04:56,  1.49it/s]Extractor Predicting: 448it [04:56,  1.47it/s]Extractor Predicting: 449it [04:57,  1.51it/s]Extractor Predicting: 450it [04:57,  1.54it/s]Extractor Predicting: 451it [04:58,  1.52it/s]Extractor Predicting: 452it [04:59,  1.48it/s]Extractor Predicting: 453it [05:00,  1.48it/s]Extractor Predicting: 454it [05:00,  1.49it/s]Extractor Predicting: 455it [05:01,  1.47it/s]Extractor Predicting: 456it [05:02,  1.49it/s]Extractor Predicting: 457it [05:02,  1.44it/s]Extractor Predicting: 458it [05:03,  1.47it/s]Extractor Predicting: 459it [05:04,  1.47it/s]Extractor Predicting: 460it [05:04,  1.47it/s]Extractor Predicting: 461it [05:05,  1.49it/s]Extractor Predicting: 462it [05:06,  1.47it/s]Extractor Predicting: 463it [05:06,  1.44it/s]Extractor Predicting: 464it [05:07,  1.42it/s]Extractor Predicting: 465it [05:08,  1.42it/s]Extractor Predicting: 466it [05:08,  1.45it/s]Extractor Predicting: 467it [05:09,  1.47it/s]Extractor Predicting: 468it [05:10,  1.49it/s]Extractor Predicting: 469it [05:10,  1.54it/s]Extractor Predicting: 470it [05:11,  1.52it/s]Extractor Predicting: 471it [05:12,  1.49it/s]Extractor Predicting: 472it [05:12,  1.47it/s]Extractor Predicting: 473it [05:13,  1.46it/s]Extractor Predicting: 474it [05:14,  1.46it/s]Extractor Predicting: 475it [05:15,  1.45it/s]Extractor Predicting: 476it [05:15,  1.48it/s]Extractor Predicting: 477it [05:16,  1.45it/s]Extractor Predicting: 478it [05:17,  1.46it/s]Extractor Predicting: 479it [05:17,  1.42it/s]Extractor Predicting: 480it [05:18,  1.39it/s]Extractor Predicting: 481it [05:19,  1.39it/s]Extractor Predicting: 482it [05:20,  1.37it/s]Extractor Predicting: 483it [05:20,  1.36it/s]Extractor Predicting: 484it [05:21,  1.36it/s]Extractor Predicting: 485it [05:22,  1.37it/s]Extractor Predicting: 486it [05:22,  1.40it/s]Extractor Predicting: 487it [05:23,  1.41it/s]Extractor Predicting: 488it [05:24,  1.40it/s]Extractor Predicting: 489it [05:25,  1.43it/s]Extractor Predicting: 490it [05:25,  1.39it/s]Extractor Predicting: 491it [05:26,  1.39it/s]Extractor Predicting: 492it [05:27,  1.39it/s]Extractor Predicting: 493it [05:27,  1.40it/s]Extractor Predicting: 494it [05:28,  1.40it/s]Extractor Predicting: 495it [05:29,  1.40it/s]Extractor Predicting: 496it [05:30,  1.38it/s]Extractor Predicting: 497it [05:30,  1.38it/s]Extractor Predicting: 498it [05:31,  1.40it/s]Extractor Predicting: 499it [05:32,  1.37it/s]Extractor Predicting: 500it [05:32,  1.40it/s]Extractor Predicting: 501it [05:33,  1.40it/s]Extractor Predicting: 502it [05:34,  1.38it/s]Extractor Predicting: 503it [05:35,  1.37it/s]Extractor Predicting: 504it [05:35,  1.35it/s]Extractor Predicting: 505it [05:36,  1.35it/s]Extractor Predicting: 506it [05:37,  1.31it/s]Extractor Predicting: 507it [05:38,  1.35it/s]Extractor Predicting: 508it [05:38,  1.50it/s]Extractor Predicting: 508it [05:38,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:27:28,638 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:27:28,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:27:28,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:27:28,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:27:28,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 16:27:29,632 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 16:27:29,633 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 16:27:30,235 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 16:27:31,434 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 16:27:31,434 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:27:33,217 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:27:33,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:27:33,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:27:33,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:27:33,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 16:27:34,037 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 16:27:34,038 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 16:27:34,412 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 16:27:34,700 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 16:27:34,701 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.18485804416403787,
  "recall": 0.04810770872670553,
  "score": 0.07634681779688621,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.49it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.44it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.44it/s]Extractor Predicting: 10it [00:06,  1.43it/s]Extractor Predicting: 11it [00:07,  1.40it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:11,  1.44it/s]Extractor Predicting: 17it [00:11,  1.46it/s]Extractor Predicting: 18it [00:12,  1.47it/s]Extractor Predicting: 19it [00:13,  1.46it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.43it/s]Extractor Predicting: 23it [00:15,  1.42it/s]Extractor Predicting: 24it [00:16,  1.45it/s]Extractor Predicting: 25it [00:17,  1.47it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:18,  1.44it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:20,  1.42it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:21,  1.48it/s]Extractor Predicting: 32it [00:21,  1.52it/s]Extractor Predicting: 33it [00:22,  1.54it/s]Extractor Predicting: 34it [00:23,  1.55it/s]Extractor Predicting: 35it [00:23,  1.52it/s]Extractor Predicting: 36it [00:24,  1.54it/s]Extractor Predicting: 37it [00:25,  1.52it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.53it/s]Extractor Predicting: 40it [00:27,  1.52it/s]Extractor Predicting: 41it [00:27,  1.43it/s]Extractor Predicting: 42it [00:28,  1.48it/s]Extractor Predicting: 43it [00:29,  1.50it/s]Extractor Predicting: 44it [00:29,  1.55it/s]Extractor Predicting: 45it [00:30,  1.50it/s]Extractor Predicting: 46it [00:31,  1.52it/s]Extractor Predicting: 47it [00:31,  1.52it/s]Extractor Predicting: 48it [00:32,  1.52it/s]Extractor Predicting: 49it [00:33,  1.52it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:34,  1.48it/s]Extractor Predicting: 52it [00:35,  1.48it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:36,  1.45it/s]Extractor Predicting: 55it [00:37,  1.45it/s]Extractor Predicting: 56it [00:37,  1.48it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:39,  1.50it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.47it/s]Extractor Predicting: 61it [00:41,  1.49it/s]Extractor Predicting: 62it [00:41,  1.50it/s]Extractor Predicting: 63it [00:42,  1.51it/s]Extractor Predicting: 64it [00:43,  1.51it/s]Extractor Predicting: 65it [00:44,  1.43it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:45,  1.43it/s]Extractor Predicting: 68it [00:46,  1.42it/s]Extractor Predicting: 69it [00:46,  1.41it/s]Extractor Predicting: 70it [00:47,  1.39it/s]Extractor Predicting: 71it [00:48,  1.40it/s]Extractor Predicting: 72it [00:49,  1.41it/s]Extractor Predicting: 73it [00:49,  1.43it/s]Extractor Predicting: 74it [00:50,  1.40it/s]Extractor Predicting: 75it [00:50,  1.52it/s]Extractor Predicting: 75it [00:50,  1.47it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4321705426356589,
  "recall": 0.056171284634760704,
  "score": 0.09942041908158715,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
