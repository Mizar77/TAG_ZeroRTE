Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_4', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:20<04:47, 20.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:34<03:38, 16.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:52<03:28, 17.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:07<02:58, 16.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:22<02:38, 15.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:37<02:21, 15.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:53<02:04, 15.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:10<01:53, 16.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:24<01:32, 15.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:38<01:15, 15.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:52<00:59, 14.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:08<00:44, 14.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:22<00:29, 14.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:37<00:14, 14.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:57<00:00, 16.40s/it]Generating: 100%|██████████| 15/15 [03:57<00:00, 15.83s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.859375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 100, 'raw': 160}
{'target': 600, 'success': 124, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 180, 'raw': 288}
{'target': 600, 'success': 200, 'raw': 320}
{'target': 600, 'success': 218, 'raw': 352}
{'target': 600, 'success': 240, 'raw': 384}
{'target': 600, 'success': 258, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 307, 'raw': 480}
{'target': 600, 'success': 328, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 374, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 418, 'raw': 640}
{'target': 600, 'success': 440, 'raw': 672}
{'target': 600, 'success': 460, 'raw': 704}
{'target': 600, 'success': 487, 'raw': 736}
{'target': 600, 'success': 506, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 569, 'raw': 864}
{'target': 600, 'success': 595, 'raw': 896}
{'target': 600, 'success': 617, 'raw': 928}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.6648706896551724, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', '(\'2012 MTV Video Music Video Awards\', \'nominated for\', \'\', \'He was in a songwriting competition on " The Simpsons " at the 2012 MTV Video Music Video Awards , winning in one of his four categories .\')'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 402, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 579, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.7271634615384616, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : architect .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 496, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 551, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 594, 'raw': 800}
{'target': 600, 'success': 616, 'raw': 832}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7403846153846154, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a new development project called " Bifurcio " , which was developed by the Swiss developers , EMC , for Bifurcio . Head Entity : Bifurcio , Tail Entity : Ericsson .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.80078125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8152173913043478, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.77375, 'errors': {'', '(\'Pristiniki\', \'member of political party\', \'\', \'In 2005 , she became the first female politician to serve in the parliament of Bulgaria , as first and leader of " Pristiniki " in the new parliament .\')', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : operator . Context : Later in the year , the company built a high - speed commuter railway crossing the Bordeaux via Bordeaux Station to Marseille , France . Head Entity : Marseille , Tail Entity : Ralf Rummel .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 76, 'raw': 128}
{'target': 600, 'success': 92, 'raw': 160}
{'target': 600, 'success': 111, 'raw': 192}
{'target': 600, 'success': 130, 'raw': 224}
{'target': 600, 'success': 144, 'raw': 256}
{'target': 600, 'success': 158, 'raw': 288}
{'target': 600, 'success': 176, 'raw': 320}
{'target': 600, 'success': 191, 'raw': 352}
{'target': 600, 'success': 208, 'raw': 384}
{'target': 600, 'success': 226, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 267, 'raw': 480}
{'target': 600, 'success': 284, 'raw': 512}
{'target': 600, 'success': 298, 'raw': 544}
{'target': 600, 'success': 315, 'raw': 576}
{'target': 600, 'success': 332, 'raw': 608}
{'target': 600, 'success': 349, 'raw': 640}
{'target': 600, 'success': 372, 'raw': 672}
{'target': 600, 'success': 388, 'raw': 704}
{'target': 600, 'success': 406, 'raw': 736}
{'target': 600, 'success': 424, 'raw': 768}
{'target': 600, 'success': 439, 'raw': 800}
{'target': 600, 'success': 454, 'raw': 832}
{'target': 600, 'success': 473, 'raw': 864}
{'target': 600, 'success': 490, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 530, 'raw': 960}
{'target': 600, 'success': 551, 'raw': 992}
{'target': 600, 'success': 567, 'raw': 1024}
{'target': 600, 'success': 586, 'raw': 1056}
{'target': 600, 'success': 608, 'raw': 1088}
{'prompt': 'Relation : position held .', 'success_rate': 0.5588235294117647, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 14871
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14971, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.84s/it]Extractor Estimating: 2it [00:18,  7.64s/it]Extractor Estimating: 3it [00:19,  4.72s/it]Extractor Estimating: 4it [00:19,  3.11s/it]Extractor Estimating: 5it [00:20,  2.19s/it]Extractor Estimating: 6it [00:21,  1.66s/it]Extractor Estimating: 7it [00:21,  1.33s/it]Extractor Estimating: 8it [00:22,  1.09s/it]Extractor Estimating: 9it [00:23,  1.04it/s]Extractor Estimating: 10it [00:23,  1.19it/s]Extractor Estimating: 11it [00:25,  1.29s/it]Extractor Estimating: 12it [00:26,  1.08s/it]Extractor Estimating: 13it [00:27,  1.03it/s]Extractor Estimating: 14it [00:27,  1.14it/s]Extractor Estimating: 15it [00:28,  1.24it/s]Extractor Estimating: 16it [00:29,  1.30it/s]Extractor Estimating: 17it [00:29,  1.40it/s]Extractor Estimating: 18it [00:30,  1.41it/s]Extractor Estimating: 19it [00:31,  1.50it/s]Extractor Estimating: 20it [00:31,  1.51it/s]Extractor Estimating: 21it [00:32,  1.55it/s]Extractor Estimating: 22it [00:32,  1.60it/s]Extractor Estimating: 23it [00:33,  1.57it/s]Extractor Estimating: 24it [00:34,  1.61it/s]Extractor Estimating: 25it [00:34,  1.63it/s]Extractor Estimating: 26it [00:35,  1.58it/s]Extractor Estimating: 27it [00:36,  1.58it/s]Extractor Estimating: 28it [00:36,  1.63it/s]Extractor Estimating: 29it [00:37,  1.65it/s]Extractor Estimating: 30it [00:37,  1.65it/s]Extractor Estimating: 31it [00:38,  1.68it/s]Extractor Estimating: 32it [00:38,  1.70it/s]Extractor Estimating: 33it [00:39,  1.64it/s]Extractor Estimating: 34it [00:40,  1.48it/s]Extractor Estimating: 35it [00:41,  1.56it/s]Extractor Estimating: 36it [00:41,  1.60it/s]Extractor Estimating: 37it [00:42,  1.51it/s]Extractor Estimating: 38it [00:42,  1.55it/s]Extractor Estimating: 39it [00:43,  1.56it/s]Extractor Estimating: 40it [00:44,  1.56it/s]Extractor Estimating: 41it [00:44,  1.54it/s]Extractor Estimating: 42it [00:45,  1.53it/s]Extractor Estimating: 43it [00:46,  1.54it/s]Extractor Estimating: 44it [00:46,  1.49it/s]Extractor Estimating: 45it [00:47,  1.49it/s]Extractor Estimating: 46it [00:48,  1.54it/s]Extractor Estimating: 47it [00:48,  1.56it/s]Extractor Estimating: 48it [00:49,  1.54it/s]Extractor Estimating: 49it [00:50,  1.60it/s]Extractor Estimating: 50it [00:50,  1.57it/s]Extractor Estimating: 51it [00:51,  1.56it/s]Extractor Estimating: 52it [00:52,  1.55it/s]Extractor Estimating: 53it [00:52,  1.55it/s]Extractor Estimating: 54it [00:53,  1.50it/s]Extractor Estimating: 55it [00:54,  1.48it/s]Extractor Estimating: 56it [00:54,  1.49it/s]Extractor Estimating: 57it [00:55,  1.48it/s]Extractor Estimating: 58it [00:56,  1.49it/s]Extractor Estimating: 59it [00:56,  1.47it/s]Extractor Estimating: 60it [00:57,  1.50it/s]Extractor Estimating: 61it [00:58,  1.47it/s]Extractor Estimating: 62it [00:58,  1.49it/s]Extractor Estimating: 63it [00:59,  1.47it/s]Extractor Estimating: 64it [01:00,  1.52it/s]Extractor Estimating: 65it [01:00,  1.48it/s]Extractor Estimating: 66it [01:01,  1.49it/s]Extractor Estimating: 67it [01:02,  1.49it/s]Extractor Estimating: 68it [01:02,  1.55it/s]Extractor Estimating: 69it [01:03,  1.57it/s]Extractor Estimating: 70it [01:03,  1.56it/s]Extractor Estimating: 71it [01:04,  1.53it/s]Extractor Estimating: 72it [01:05,  1.54it/s]Extractor Estimating: 73it [01:06,  1.51it/s]Extractor Estimating: 74it [01:06,  1.57it/s]Extractor Estimating: 75it [01:07,  1.54it/s]Extractor Estimating: 76it [01:07,  1.60it/s]Extractor Estimating: 77it [01:08,  1.62it/s]Extractor Estimating: 78it [01:08,  1.68it/s]Extractor Estimating: 79it [01:09,  1.70it/s]Extractor Estimating: 80it [01:10,  1.75it/s]Extractor Estimating: 81it [01:10,  1.76it/s]Extractor Estimating: 82it [01:11,  1.72it/s]Extractor Estimating: 83it [01:11,  1.73it/s]Extractor Estimating: 84it [01:12,  1.65it/s]Extractor Estimating: 85it [01:13,  1.64it/s]Extractor Estimating: 86it [01:13,  1.65it/s]Extractor Estimating: 87it [01:14,  1.65it/s]Extractor Estimating: 88it [01:14,  1.71it/s]Extractor Estimating: 89it [01:15,  1.72it/s]Extractor Estimating: 90it [01:15,  1.75it/s]Extractor Estimating: 91it [01:16,  1.71it/s]Extractor Estimating: 92it [01:17,  1.74it/s]Extractor Estimating: 93it [01:17,  1.74it/s]Extractor Estimating: 94it [01:18,  1.63it/s]Extractor Estimating: 95it [01:19,  1.56it/s]Extractor Estimating: 96it [01:19,  1.59it/s]Extractor Estimating: 97it [01:20,  1.62it/s]Extractor Estimating: 98it [01:20,  1.65it/s]Extractor Estimating: 99it [01:21,  1.67it/s]Extractor Estimating: 100it [01:22,  1.72it/s]Extractor Estimating: 101it [01:22,  1.74it/s]Extractor Estimating: 102it [01:23,  1.57it/s]Extractor Estimating: 103it [01:23,  1.63it/s]Extractor Estimating: 104it [01:24,  1.70it/s]Extractor Estimating: 105it [01:25,  1.72it/s]Extractor Estimating: 106it [01:25,  1.70it/s]Extractor Estimating: 107it [01:26,  1.77it/s]Extractor Estimating: 108it [01:26,  1.85it/s]Extractor Estimating: 109it [01:27,  1.82it/s]Extractor Estimating: 110it [01:27,  1.84it/s]Extractor Estimating: 111it [01:28,  1.83it/s]Extractor Estimating: 112it [01:28,  1.84it/s]Extractor Estimating: 113it [01:29,  1.79it/s]Extractor Estimating: 114it [01:29,  1.81it/s]Extractor Estimating: 115it [01:30,  1.80it/s]Extractor Estimating: 116it [01:31,  1.80it/s]Extractor Estimating: 117it [01:31,  1.76it/s]Extractor Estimating: 118it [01:32,  1.76it/s]Extractor Estimating: 119it [01:32,  1.70it/s]Extractor Estimating: 120it [01:33,  1.78it/s]Extractor Estimating: 121it [01:33,  1.74it/s]Extractor Estimating: 122it [01:34,  1.65it/s]Extractor Estimating: 123it [01:35,  1.70it/s]Extractor Estimating: 124it [01:35,  1.74it/s]Extractor Estimating: 125it [01:36,  1.72it/s]Extractor Estimating: 126it [01:36,  1.70it/s]Extractor Estimating: 127it [01:37,  1.66it/s]Extractor Estimating: 128it [01:38,  1.70it/s]Extractor Estimating: 129it [01:38,  1.72it/s]Extractor Estimating: 130it [01:39,  1.74it/s]Extractor Estimating: 131it [01:39,  1.72it/s]Extractor Estimating: 132it [01:40,  1.72it/s]Extractor Estimating: 133it [01:41,  1.68it/s]Extractor Estimating: 134it [01:41,  1.72it/s]Extractor Estimating: 135it [01:42,  1.67it/s]Extractor Estimating: 136it [01:42,  1.65it/s]Extractor Estimating: 137it [01:43,  1.65it/s]Extractor Estimating: 138it [01:44,  1.67it/s]Extractor Estimating: 139it [01:44,  1.67it/s]Extractor Estimating: 140it [01:45,  1.67it/s]Extractor Estimating: 141it [01:45,  1.65it/s]Extractor Estimating: 142it [01:46,  1.67it/s]Extractor Estimating: 143it [01:46,  1.72it/s]Extractor Estimating: 144it [01:47,  1.71it/s]Extractor Estimating: 145it [01:48,  1.73it/s]Extractor Estimating: 146it [01:48,  1.74it/s]Extractor Estimating: 147it [01:49,  1.70it/s]Extractor Estimating: 148it [01:50,  1.63it/s]Extractor Estimating: 149it [01:50,  1.62it/s]Extractor Estimating: 150it [01:51,  1.62it/s]Extractor Estimating: 151it [01:51,  1.59it/s]Extractor Estimating: 152it [01:52,  1.57it/s]Extractor Estimating: 153it [01:53,  1.59it/s]Extractor Estimating: 154it [01:53,  1.63it/s]Extractor Estimating: 155it [01:54,  1.66it/s]Extractor Estimating: 156it [01:54,  1.69it/s]Extractor Estimating: 157it [01:55,  1.74it/s]Extractor Estimating: 158it [01:56,  1.66it/s]Extractor Estimating: 159it [01:56,  1.73it/s]Extractor Estimating: 160it [01:57,  1.70it/s]Extractor Estimating: 161it [01:57,  1.67it/s]Extractor Estimating: 162it [01:58,  1.66it/s]Extractor Estimating: 163it [01:59,  1.59it/s]Extractor Estimating: 164it [01:59,  1.62it/s]Extractor Estimating: 165it [02:00,  1.56it/s]Extractor Estimating: 166it [02:01,  1.56it/s]Extractor Estimating: 167it [02:01,  1.61it/s]Extractor Estimating: 168it [02:02,  1.56it/s]Extractor Estimating: 169it [02:02,  1.56it/s]Extractor Estimating: 170it [02:03,  1.63it/s]Extractor Estimating: 171it [02:04,  1.57it/s]Extractor Estimating: 172it [02:04,  1.57it/s]Extractor Estimating: 173it [02:05,  1.56it/s]Extractor Estimating: 174it [02:06,  1.40it/s]Extractor Estimating: 175it [02:06,  1.48it/s]Extractor Estimating: 176it [02:07,  1.45it/s]Extractor Estimating: 177it [02:08,  1.54it/s]Extractor Estimating: 178it [02:08,  1.55it/s]Extractor Estimating: 179it [02:09,  1.62it/s]Extractor Estimating: 180it [02:09,  1.68it/s]Extractor Estimating: 181it [02:10,  1.65it/s]Extractor Estimating: 182it [02:11,  1.64it/s]Extractor Estimating: 183it [02:11,  1.59it/s]Extractor Estimating: 184it [02:12,  1.53it/s]Extractor Estimating: 185it [02:13,  1.56it/s]Extractor Estimating: 186it [02:13,  1.61it/s]Extractor Estimating: 187it [02:14,  1.64it/s]Extractor Estimating: 188it [02:15,  1.61it/s]Extractor Estimating: 189it [02:15,  1.68it/s]Extractor Estimating: 190it [02:16,  1.70it/s]Extractor Estimating: 191it [02:16,  1.64it/s]Extractor Estimating: 192it [02:17,  1.66it/s]Extractor Estimating: 193it [02:18,  1.63it/s]Extractor Estimating: 194it [02:18,  1.60it/s]Extractor Estimating: 195it [02:19,  1.61it/s]Extractor Estimating: 196it [02:19,  1.64it/s]Extractor Estimating: 197it [02:20,  1.62it/s]Extractor Estimating: 198it [02:21,  1.60it/s]Extractor Estimating: 199it [02:21,  1.55it/s]Extractor Estimating: 200it [02:22,  1.63it/s]Extractor Estimating: 201it [02:23,  1.63it/s]Extractor Estimating: 202it [02:23,  1.60it/s]Extractor Estimating: 203it [02:24,  1.62it/s]Extractor Estimating: 204it [02:24,  1.54it/s]Extractor Estimating: 205it [02:25,  1.55it/s]Extractor Estimating: 206it [02:26,  1.60it/s]Extractor Estimating: 207it [02:26,  1.52it/s]Extractor Estimating: 208it [02:27,  1.52it/s]Extractor Estimating: 209it [02:28,  1.51it/s]Extractor Estimating: 210it [02:28,  1.55it/s]Extractor Estimating: 211it [02:29,  1.60it/s]Extractor Estimating: 212it [02:30,  1.57it/s]Extractor Estimating: 213it [02:30,  1.58it/s]Extractor Estimating: 214it [02:31,  1.55it/s]Extractor Estimating: 215it [02:31,  1.61it/s]Extractor Estimating: 216it [02:32,  1.61it/s]Extractor Estimating: 217it [02:33,  1.58it/s]Extractor Estimating: 218it [02:33,  1.61it/s]Extractor Estimating: 219it [02:34,  1.60it/s]Extractor Estimating: 220it [02:35,  1.61it/s]Extractor Estimating: 221it [02:35,  1.57it/s]Extractor Estimating: 222it [02:36,  1.62it/s]Extractor Estimating: 223it [02:36,  1.67it/s]Extractor Estimating: 224it [02:37,  1.68it/s]Extractor Estimating: 225it [02:38,  1.65it/s]Extractor Estimating: 226it [02:38,  1.60it/s]Extractor Estimating: 227it [02:39,  1.59it/s]Extractor Estimating: 228it [02:40,  1.59it/s]Extractor Estimating: 229it [02:40,  1.55it/s]Extractor Estimating: 230it [02:41,  1.52it/s]Extractor Estimating: 231it [02:42,  1.51it/s]Extractor Estimating: 232it [02:42,  1.52it/s]Extractor Estimating: 233it [02:43,  1.53it/s]Extractor Estimating: 234it [02:44,  1.56it/s]Extractor Estimating: 235it [02:44,  1.53it/s]Extractor Estimating: 236it [02:45,  1.51it/s]Extractor Estimating: 237it [02:45,  1.54it/s]Extractor Estimating: 238it [02:46,  1.49it/s]Extractor Estimating: 239it [02:47,  1.45it/s]Extractor Estimating: 240it [02:48,  1.48it/s]Extractor Estimating: 241it [02:48,  1.51it/s]Extractor Estimating: 242it [02:49,  1.52it/s]Extractor Estimating: 243it [02:49,  1.53it/s]Extractor Estimating: 244it [02:50,  1.49it/s]Extractor Estimating: 245it [02:51,  1.44it/s]Extractor Estimating: 246it [02:52,  1.50it/s]Extractor Estimating: 247it [02:52,  1.51it/s]Extractor Estimating: 248it [02:53,  1.35it/s]Extractor Estimating: 249it [02:54,  1.41it/s]Extractor Estimating: 250it [02:54,  1.46it/s]Extractor Estimating: 251it [02:55,  1.56it/s]Extractor Estimating: 252it [02:55,  1.66it/s]Extractor Estimating: 253it [02:56,  1.59it/s]Extractor Estimating: 254it [02:57,  1.67it/s]Extractor Estimating: 255it [02:57,  1.73it/s]Extractor Estimating: 256it [02:58,  1.72it/s]Extractor Estimating: 257it [02:58,  1.76it/s]Extractor Estimating: 258it [02:59,  1.73it/s]Extractor Estimating: 259it [03:00,  1.71it/s]Extractor Estimating: 260it [03:00,  1.63it/s]Extractor Estimating: 261it [03:01,  1.65it/s]Extractor Estimating: 262it [03:01,  1.70it/s]Extractor Estimating: 263it [03:02,  1.69it/s]Extractor Estimating: 264it [03:02,  1.75it/s]Extractor Estimating: 265it [03:03,  1.74it/s]Extractor Estimating: 266it [03:04,  1.82it/s]Extractor Estimating: 267it [03:04,  1.81it/s]Extractor Estimating: 268it [03:05,  1.80it/s]Extractor Estimating: 269it [03:05,  1.80it/s]Extractor Estimating: 270it [03:06,  1.76it/s]Extractor Estimating: 271it [03:06,  1.76it/s]Extractor Estimating: 272it [03:07,  1.76it/s]Extractor Estimating: 273it [03:07,  1.81it/s]Extractor Estimating: 274it [03:08,  1.78it/s]Extractor Estimating: 275it [03:09,  1.79it/s]Extractor Estimating: 276it [03:09,  1.75it/s]Extractor Estimating: 277it [03:10,  1.67it/s]Extractor Estimating: 278it [03:10,  1.64it/s]Extractor Estimating: 279it [03:11,  1.61it/s]Extractor Estimating: 280it [03:12,  1.64it/s]Extractor Estimating: 281it [03:12,  1.65it/s]Extractor Estimating: 282it [03:13,  1.58it/s]Extractor Estimating: 283it [03:14,  1.60it/s]Extractor Estimating: 284it [03:14,  1.55it/s]Extractor Estimating: 285it [03:15,  1.56it/s]Extractor Estimating: 286it [03:16,  1.62it/s]Extractor Estimating: 287it [03:16,  1.63it/s]Extractor Estimating: 288it [03:17,  1.69it/s]Extractor Estimating: 289it [03:17,  1.65it/s]Extractor Estimating: 290it [03:18,  1.68it/s]Extractor Estimating: 291it [03:18,  1.69it/s]Extractor Estimating: 292it [03:19,  1.70it/s]Extractor Estimating: 293it [03:20,  1.67it/s]Extractor Estimating: 294it [03:20,  1.64it/s]Extractor Estimating: 295it [03:21,  1.63it/s]Extractor Estimating: 296it [03:22,  1.63it/s]Extractor Estimating: 297it [03:22,  1.63it/s]Extractor Estimating: 298it [03:23,  1.64it/s]Extractor Estimating: 299it [03:23,  1.64it/s]Extractor Estimating: 300it [03:24,  1.64it/s]Extractor Estimating: 301it [03:25,  1.64it/s]Extractor Estimating: 302it [03:25,  1.60it/s]Extractor Estimating: 303it [03:26,  1.59it/s]Extractor Estimating: 304it [03:26,  1.59it/s]Extractor Estimating: 305it [03:27,  1.59it/s]Extractor Estimating: 306it [03:28,  1.64it/s]Extractor Estimating: 307it [03:28,  1.65it/s]Extractor Estimating: 308it [03:29,  1.66it/s]Extractor Estimating: 309it [03:29,  1.65it/s]Extractor Estimating: 310it [03:30,  1.63it/s]Extractor Estimating: 311it [03:31,  1.58it/s]Extractor Estimating: 312it [03:32,  1.51it/s]Extractor Estimating: 313it [03:32,  1.48it/s]Extractor Estimating: 314it [03:33,  1.53it/s]Extractor Estimating: 315it [03:33,  1.53it/s]Extractor Estimating: 316it [03:34,  1.56it/s]Extractor Estimating: 317it [03:35,  1.55it/s]Extractor Estimating: 318it [03:35,  1.57it/s]Extractor Estimating: 319it [03:36,  1.63it/s]Extractor Estimating: 320it [03:36,  1.67it/s]Extractor Estimating: 321it [03:37,  1.64it/s]Extractor Estimating: 322it [03:38,  1.60it/s]Extractor Estimating: 323it [03:38,  1.59it/s]Extractor Estimating: 324it [03:39,  1.59it/s]Extractor Estimating: 325it [03:40,  1.56it/s]Extractor Estimating: 326it [03:40,  1.59it/s]Extractor Estimating: 327it [03:41,  1.53it/s]Extractor Estimating: 328it [03:42,  1.52it/s]Extractor Estimating: 329it [03:42,  1.50it/s]Extractor Estimating: 330it [03:43,  1.51it/s]Extractor Estimating: 331it [03:44,  1.51it/s]Extractor Estimating: 332it [03:44,  1.59it/s]Extractor Estimating: 333it [03:45,  1.58it/s]Extractor Estimating: 334it [03:45,  1.62it/s]Extractor Estimating: 335it [03:46,  1.58it/s]Extractor Estimating: 336it [03:47,  1.59it/s]Extractor Estimating: 337it [03:48,  1.41it/s]Extractor Estimating: 338it [03:48,  1.46it/s]Extractor Estimating: 339it [03:49,  1.50it/s]Extractor Estimating: 340it [03:50,  1.56it/s]Extractor Estimating: 341it [03:50,  1.58it/s]Extractor Estimating: 342it [03:51,  1.59it/s]Extractor Estimating: 343it [03:51,  1.60it/s]Extractor Estimating: 344it [03:52,  1.56it/s]Extractor Estimating: 345it [03:53,  1.54it/s]Extractor Estimating: 346it [03:53,  1.45it/s]Extractor Estimating: 347it [03:54,  1.48it/s]Extractor Estimating: 348it [03:55,  1.55it/s]Extractor Estimating: 349it [03:55,  1.56it/s]Extractor Estimating: 350it [03:56,  1.54it/s]Extractor Estimating: 351it [03:57,  1.57it/s]Extractor Estimating: 352it [03:57,  1.56it/s]Extractor Estimating: 353it [03:58,  1.61it/s]Extractor Estimating: 354it [03:58,  1.63it/s]Extractor Estimating: 355it [03:59,  1.63it/s]Extractor Estimating: 356it [04:00,  1.61it/s]Extractor Estimating: 357it [04:00,  1.63it/s]Extractor Estimating: 358it [04:01,  1.64it/s]Extractor Estimating: 359it [04:01,  1.66it/s]Extractor Estimating: 360it [04:02,  1.62it/s]Extractor Estimating: 361it [04:03,  1.64it/s]Extractor Estimating: 362it [04:03,  1.67it/s]Extractor Estimating: 363it [04:04,  1.63it/s]Extractor Estimating: 364it [04:05,  1.64it/s]Extractor Estimating: 365it [04:05,  1.64it/s]Extractor Estimating: 366it [04:06,  1.60it/s]Extractor Estimating: 367it [04:06,  1.59it/s]Extractor Estimating: 368it [04:07,  1.62it/s]Extractor Estimating: 369it [04:08,  1.66it/s]Extractor Estimating: 370it [04:08,  1.63it/s]Extractor Estimating: 371it [04:09,  1.62it/s]Extractor Estimating: 372it [04:09,  1.71it/s]Extractor Estimating: 373it [04:10,  1.73it/s]Extractor Estimating: 374it [04:11,  1.74it/s]Extractor Estimating: 375it [04:11,  1.65it/s]Extractor Estimating: 375it [04:11,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7466 mean pseudo reward: 0.9088254036972337
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 27082
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27182, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27182, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.225, loss:1077.6158
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.925, loss:1073.1198
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.932, loss:1068.6726
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 0.927, loss:1002.8050
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 0.923, loss:999.4486
>> valid entity prec:0.5063, rec:0.5476, f1:0.5261
>> valid relation prec:0.1456, rec:0.0324, f1:0.0529
>> valid relation with NER prec:0.1456, rec:0.0324, f1:0.0529
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.137, loss:1006.7746
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.924, loss:944.1431
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.929, loss:998.7545
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 0.925, loss:956.9420
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 0.930, loss:922.0058
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4987, rec:0.5093, f1:0.5039
>> valid relation prec:0.1175, rec:0.0263, f1:0.0430
>> valid relation with NER prec:0.1175, rec:0.0263, f1:0.0430
g_step 1100, step 164, avg_time 2.129, loss:932.6161
g_step 1200, step 264, avg_time 0.923, loss:957.5803
g_step 1300, step 52, avg_time 0.927, loss:901.2109
g_step 1400, step 152, avg_time 0.933, loss:906.4146
g_step 1500, step 252, avg_time 0.935, loss:902.6385
>> valid entity prec:0.5181, rec:0.5873, f1:0.5506
>> valid relation prec:0.1693, rec:0.0464, f1:0.0728
>> valid relation with NER prec:0.1693, rec:0.0464, f1:0.0728
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 40, avg_time 2.124, loss:863.7902
g_step 1700, step 140, avg_time 0.932, loss:859.3490
g_step 1800, step 240, avg_time 0.938, loss:867.4821
g_step 1900, step 28, avg_time 0.923, loss:812.5075
g_step 2000, step 128, avg_time 0.933, loss:792.3626
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5221, rec:0.5201, f1:0.5211
>> valid relation prec:0.2387, rec:0.0501, f1:0.0828
>> valid relation with NER prec:0.2387, rec:0.0501, f1:0.0828
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.128, loss:832.7753
g_step 2200, step 16, avg_time 0.923, loss:809.9296
g_step 2300, step 116, avg_time 0.929, loss:764.4704
g_step 2400, step 216, avg_time 0.930, loss:760.7266
g_step 2500, step 4, avg_time 0.926, loss:793.5574
>> valid entity prec:0.5255, rec:0.5857, f1:0.5540
>> valid relation prec:0.1348, rec:0.0435, f1:0.0658
>> valid relation with NER prec:0.1348, rec:0.0435, f1:0.0658
new max entity f1 on valid!
g_step 2600, step 104, avg_time 2.125, loss:733.7352
g_step 2700, step 204, avg_time 0.927, loss:742.7128
g_step 2800, step 304, avg_time 0.922, loss:747.1624
g_step 2900, step 92, avg_time 0.925, loss:681.8327
g_step 3000, step 192, avg_time 0.928, loss:727.3315
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4884, rec:0.5263, f1:0.5067
>> valid relation prec:0.1193, rec:0.0470, f1:0.0674
>> valid relation with NER prec:0.1193, rec:0.0470, f1:0.0674
g_step 3100, step 292, avg_time 2.125, loss:725.2577
g_step 3200, step 80, avg_time 0.934, loss:673.2534
g_step 3300, step 180, avg_time 0.932, loss:665.5257
g_step 3400, step 280, avg_time 0.921, loss:685.6856
g_step 3500, step 68, avg_time 0.925, loss:663.0775
>> valid entity prec:0.5081, rec:0.4636, f1:0.4848
>> valid relation prec:0.1638, rec:0.0415, f1:0.0662
>> valid relation with NER prec:0.1638, rec:0.0415, f1:0.0662
g_step 3600, step 168, avg_time 2.122, loss:646.5233
g_step 3700, step 268, avg_time 0.933, loss:661.3653
g_step 3800, step 56, avg_time 0.918, loss:642.0685
g_step 3900, step 156, avg_time 0.933, loss:633.0427
g_step 4000, step 256, avg_time 0.933, loss:615.9588
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5104, rec:0.5051, f1:0.5078
>> valid relation prec:0.1343, rec:0.0435, f1:0.0657
>> valid relation with NER prec:0.1343, rec:0.0435, f1:0.0657
g_step 4100, step 44, avg_time 2.123, loss:622.9126
g_step 4200, step 144, avg_time 0.927, loss:603.6704
g_step 4300, step 244, avg_time 0.921, loss:617.3134
g_step 4400, step 32, avg_time 0.932, loss:604.6067
g_step 4500, step 132, avg_time 0.931, loss:573.8035
>> valid entity prec:0.5108, rec:0.4298, f1:0.4668
>> valid relation prec:0.1196, rec:0.0309, f1:0.0491
>> valid relation with NER prec:0.1196, rec:0.0309, f1:0.0491
g_step 4600, step 232, avg_time 2.119, loss:577.5232
g_step 4700, step 20, avg_time 0.930, loss:580.9525
g_step 4800, step 120, avg_time 0.939, loss:539.0554
g_step 4900, step 220, avg_time 0.929, loss:557.6339
g_step 5000, step 8, avg_time 0.923, loss:566.3629
learning rate was adjusted to 0.0008
>> valid entity prec:0.5211, rec:0.4788, f1:0.4991
>> valid relation prec:0.1373, rec:0.0464, f1:0.0693
>> valid relation with NER prec:0.1373, rec:0.0464, f1:0.0693
g_step 5100, step 108, avg_time 2.119, loss:515.9234
g_step 5200, step 208, avg_time 0.940, loss:553.3304
g_step 5300, step 308, avg_time 0.927, loss:549.2781
g_step 5400, step 96, avg_time 0.919, loss:484.5833
g_step 5500, step 196, avg_time 0.934, loss:528.5042
>> valid entity prec:0.5133, rec:0.4369, f1:0.4720
>> valid relation prec:0.1065, rec:0.0366, f1:0.0545
>> valid relation with NER prec:0.1065, rec:0.0366, f1:0.0545
g_step 5600, step 296, avg_time 2.125, loss:529.9951
g_step 5700, step 84, avg_time 0.923, loss:475.5112
g_step 5800, step 184, avg_time 0.933, loss:488.7412
g_step 5900, step 284, avg_time 0.925, loss:527.6776
g_step 6000, step 72, avg_time 0.928, loss:476.1052
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5242, rec:0.4531, f1:0.4861
>> valid relation prec:0.1353, rec:0.0532, f1:0.0764
>> valid relation with NER prec:0.1353, rec:0.0532, f1:0.0764
g_step 6100, step 172, avg_time 2.114, loss:473.6943
g_step 6200, step 272, avg_time 0.924, loss:504.7229
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:29:55 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:29:55 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-29-55_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:29:56 - WARNING - datasets.builder -   Using custom data configuration default-b22159c6a0c7ae69
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-b22159c6a0c7ae69/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  6.34 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:29:56,747 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:29:56,748 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:29:56,749 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:29:56,750 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:29:56,764 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:29:56,770 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:29:56,770 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:29:56,770 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:29:56,770 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:29:56,770 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:29:56,770 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:29:56,906 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:29:59,946 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:29:59,949 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-b22159c6a0c7ae69/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 14:29:59 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x150d9d23c0e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.70ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.64ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.04ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.25ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.38ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.43ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.48ba/s]100%|██████████| 8/8 [00:01<00:00,  5.34ba/s]100%|██████████| 8/8 [00:01<00:00,  4.51ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.03ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.25ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.44ba/s]100%|██████████| 4/4 [00:00<00:00,  4.51ba/s]100%|██████████| 4/4 [00:00<00:00,  4.22ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.06ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.16ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.67ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.98ba/s]100%|██████████| 8/8 [00:00<00:00, 11.38ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.36ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.79ba/s]100%|██████████| 4/4 [00:00<00:00, 12.22ba/s]
[INFO|trainer.py:414] 2023-08-28 14:30:04,120 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:30:04,135 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:30:04,135 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 14:30:04,135 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:30:04,135 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:30:04,135 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:30:04,135 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:30:04,135 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<05:43,  1.70it/s]  0%|          | 2/585 [00:00<04:00,  2.42it/s]  1%|          | 3/585 [00:01<03:27,  2.80it/s]  1%|          | 4/585 [00:01<03:11,  3.03it/s]  1%|          | 5/585 [00:01<03:03,  3.16it/s]  1%|          | 6/585 [00:02<02:58,  3.24it/s]  1%|          | 7/585 [00:02<02:56,  3.28it/s]  1%|▏         | 8/585 [00:02<02:53,  3.32it/s]  2%|▏         | 9/585 [00:02<02:52,  3.34it/s]  2%|▏         | 10/585 [00:03<02:51,  3.36it/s]  2%|▏         | 11/585 [00:03<02:50,  3.37it/s]  2%|▏         | 12/585 [00:03<02:49,  3.38it/s]  2%|▏         | 13/585 [00:04<02:49,  3.38it/s]  2%|▏         | 14/585 [00:04<02:48,  3.39it/s]  3%|▎         | 15/585 [00:04<02:48,  3.39it/s]  3%|▎         | 16/585 [00:04<02:47,  3.39it/s]  3%|▎         | 17/585 [00:05<02:47,  3.39it/s]  3%|▎         | 18/585 [00:05<02:47,  3.38it/s]  3%|▎         | 19/585 [00:05<02:47,  3.38it/s]  3%|▎         | 20/585 [00:06<02:47,  3.38it/s]  4%|▎         | 21/585 [00:06<02:46,  3.39it/s]  4%|▍         | 22/585 [00:06<02:46,  3.39it/s]  4%|▍         | 23/585 [00:07<02:45,  3.39it/s]  4%|▍         | 24/585 [00:07<02:45,  3.39it/s]  4%|▍         | 25/585 [00:07<02:45,  3.39it/s]  4%|▍         | 26/585 [00:07<02:44,  3.39it/s]  5%|▍         | 27/585 [00:08<02:44,  3.39it/s]  5%|▍         | 28/585 [00:08<02:44,  3.39it/s]  5%|▍         | 29/585 [00:08<02:43,  3.39it/s]  5%|▌         | 30/585 [00:09<02:43,  3.39it/s]  5%|▌         | 31/585 [00:09<02:43,  3.39it/s]  5%|▌         | 32/585 [00:09<02:43,  3.38it/s]  6%|▌         | 33/585 [00:10<02:43,  3.38it/s]  6%|▌         | 34/585 [00:10<02:42,  3.38it/s]  6%|▌         | 35/585 [00:10<02:42,  3.38it/s]  6%|▌         | 36/585 [00:10<02:42,  3.38it/s]  6%|▋         | 37/585 [00:11<02:41,  3.39it/s]  6%|▋         | 38/585 [00:11<02:41,  3.39it/s]  7%|▋         | 39/585 [00:11<02:41,  3.39it/s]  7%|▋         | 40/585 [00:12<02:40,  3.39it/s]  7%|▋         | 41/585 [00:12<02:40,  3.39it/s]  7%|▋         | 42/585 [00:12<02:40,  3.39it/s]  7%|▋         | 43/585 [00:12<02:39,  3.39it/s]  8%|▊         | 44/585 [00:13<02:39,  3.39it/s]  8%|▊         | 45/585 [00:13<02:39,  3.39it/s]  8%|▊         | 46/585 [00:13<02:38,  3.39it/s]  8%|▊         | 47/585 [00:14<02:38,  3.39it/s]  8%|▊         | 48/585 [00:14<02:38,  3.39it/s]  8%|▊         | 49/585 [00:14<02:38,  3.37it/s]  9%|▊         | 50/585 [00:15<02:38,  3.38it/s]  9%|▊         | 51/585 [00:15<02:37,  3.39it/s]  9%|▉         | 52/585 [00:15<02:37,  3.39it/s]  9%|▉         | 53/585 [00:15<02:36,  3.39it/s]  9%|▉         | 54/585 [00:16<02:36,  3.40it/s]  9%|▉         | 55/585 [00:16<02:36,  3.39it/s] 10%|▉         | 56/585 [00:16<02:35,  3.40it/s] 10%|▉         | 57/585 [00:17<02:35,  3.40it/s] 10%|▉         | 58/585 [00:17<02:35,  3.40it/s] 10%|█         | 59/585 [00:17<02:35,  3.39it/s] 10%|█         | 60/585 [00:17<02:34,  3.39it/s] 10%|█         | 61/585 [00:18<02:34,  3.40it/s] 11%|█         | 62/585 [00:18<02:34,  3.39it/s] 11%|█         | 63/585 [00:18<02:33,  3.40it/s] 11%|█         | 64/585 [00:19<02:33,  3.39it/s] 11%|█         | 65/585 [00:19<02:33,  3.39it/s] 11%|█▏        | 66/585 [00:19<02:32,  3.39it/s] 11%|█▏        | 67/585 [00:20<02:33,  3.38it/s] 12%|█▏        | 68/585 [00:20<02:32,  3.38it/s] 12%|█▏        | 69/585 [00:20<02:32,  3.39it/s] 12%|█▏        | 70/585 [00:20<02:31,  3.39it/s] 12%|█▏        | 71/585 [00:21<02:31,  3.39it/s] 12%|█▏        | 72/585 [00:21<02:31,  3.39it/s] 12%|█▏        | 73/585 [00:21<02:31,  3.39it/s] 13%|█▎        | 74/585 [00:22<02:30,  3.39it/s] 13%|█▎        | 75/585 [00:22<02:30,  3.39it/s] 13%|█▎        | 76/585 [00:22<02:30,  3.39it/s] 13%|█▎        | 77/585 [00:22<02:29,  3.39it/s] 13%|█▎        | 78/585 [00:23<02:29,  3.39it/s] 14%|█▎        | 79/585 [00:23<02:29,  3.39it/s] 14%|█▎        | 80/585 [00:23<02:29,  3.39it/s] 14%|█▍        | 81/585 [00:24<02:28,  3.39it/s] 14%|█▍        | 82/585 [00:24<02:28,  3.39it/s] 14%|█▍        | 83/585 [00:24<02:28,  3.39it/s] 14%|█▍        | 84/585 [00:25<02:28,  3.37it/s] 15%|█▍        | 85/585 [00:25<02:28,  3.38it/s] 15%|█▍        | 86/585 [00:25<02:27,  3.38it/s] 15%|█▍        | 87/585 [00:25<02:27,  3.38it/s] 15%|█▌        | 88/585 [00:26<02:26,  3.38it/s] 15%|█▌        | 89/585 [00:26<02:26,  3.38it/s] 15%|█▌        | 90/585 [00:26<02:26,  3.38it/s] 16%|█▌        | 91/585 [00:27<02:25,  3.39it/s] 16%|█▌        | 92/585 [00:27<02:25,  3.39it/s] 16%|█▌        | 93/585 [00:27<02:25,  3.39it/s] 16%|█▌        | 94/585 [00:28<02:24,  3.39it/s] 16%|█▌        | 95/585 [00:28<02:24,  3.39it/s] 16%|█▋        | 96/585 [00:28<02:24,  3.39it/s] 17%|█▋        | 97/585 [00:28<02:24,  3.39it/s] 17%|█▋        | 98/585 [00:29<02:23,  3.38it/s] 17%|█▋        | 99/585 [00:29<02:23,  3.38it/s] 17%|█▋        | 100/585 [00:29<02:23,  3.39it/s] 17%|█▋        | 101/585 [00:30<02:23,  3.38it/s] 17%|█▋        | 102/585 [00:30<02:22,  3.38it/s] 18%|█▊        | 103/585 [00:30<02:22,  3.38it/s] 18%|█▊        | 104/585 [00:30<02:22,  3.38it/s] 18%|█▊        | 105/585 [00:31<02:21,  3.38it/s] 18%|█▊        | 106/585 [00:31<02:21,  3.38it/s] 18%|█▊        | 107/585 [00:31<02:21,  3.38it/s] 18%|█▊        | 108/585 [00:32<02:20,  3.38it/s] 19%|█▊        | 109/585 [00:32<02:20,  3.38it/s] 19%|█▉        | 110/585 [00:32<02:20,  3.39it/s] 19%|█▉        | 111/585 [00:33<02:20,  3.38it/s] 19%|█▉        | 112/585 [00:33<02:19,  3.38it/s] 19%|█▉        | 113/585 [00:33<02:19,  3.38it/s] 19%|█▉        | 114/585 [00:33<02:19,  3.39it/s] 20%|█▉        | 115/585 [00:34<02:18,  3.39it/s] 20%|█▉        | 116/585 [00:34<02:18,  3.39it/s] 20%|██        | 117/585 [00:34<02:18,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 14:30:38,996 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:30:38,997 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:30:38,997 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.43it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.16it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.41it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.52it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.87it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.49it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.47it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.27it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.28it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.45it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.42it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.36it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.20it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.07it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.11it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.03it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.06it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.16it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.28it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.31it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.17it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.12it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.04it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.01it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.06it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.13it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.28it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.40it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.18it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.19it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.04it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.98it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.95it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.03it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.15it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.23it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.26it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.19it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.29it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.15it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.04it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.03it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.12it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.19it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.29it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.18it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.20it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.20it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.13it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.06it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.11it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.16it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.17it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.26it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.23it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.16it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.13it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.03it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.09it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.16it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.19it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.11it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.18it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.18it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.18it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.05it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.98it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.91it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.22it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.22it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.25it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.24it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.26it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.19it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.09it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.98it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.04it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.25it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.28it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.19it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.24it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.21it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.17it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.07it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.07it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.17it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.29it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:44<02:18,  3.39it/s]
100%|██████████| 437/437 [00:09<00:00, 44.29it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:30:48,930 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 14:30:48,957 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:30:50,863 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:30:50,875 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:30:50,884 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:51<40:16,  5.17s/it] 20%|██        | 119/585 [00:51<28:49,  3.71s/it] 21%|██        | 120/585 [00:51<20:49,  2.69s/it] 21%|██        | 121/585 [00:52<15:13,  1.97s/it] 21%|██        | 122/585 [00:52<11:19,  1.47s/it] 21%|██        | 123/585 [00:52<08:35,  1.12s/it] 21%|██        | 124/585 [00:53<06:40,  1.15it/s] 21%|██▏       | 125/585 [00:53<05:20,  1.43it/s] 22%|██▏       | 126/585 [00:53<04:24,  1.73it/s] 22%|██▏       | 127/585 [00:54<03:45,  2.03it/s] 22%|██▏       | 128/585 [00:54<03:17,  2.31it/s] 22%|██▏       | 129/585 [00:54<02:58,  2.56it/s] 22%|██▏       | 130/585 [00:54<02:45,  2.75it/s] 22%|██▏       | 131/585 [00:55<02:35,  2.92it/s] 23%|██▎       | 132/585 [00:55<02:28,  3.05it/s] 23%|██▎       | 133/585 [00:55<02:23,  3.14it/s] 23%|██▎       | 134/585 [00:56<02:20,  3.21it/s] 23%|██▎       | 135/585 [00:56<02:17,  3.26it/s] 23%|██▎       | 136/585 [00:56<02:16,  3.30it/s] 23%|██▎       | 137/585 [00:56<02:14,  3.33it/s] 24%|██▎       | 138/585 [00:57<02:13,  3.35it/s] 24%|██▍       | 139/585 [00:57<02:12,  3.36it/s] 24%|██▍       | 140/585 [00:57<02:12,  3.37it/s] 24%|██▍       | 141/585 [00:58<02:12,  3.36it/s] 24%|██▍       | 142/585 [00:58<02:11,  3.37it/s] 24%|██▍       | 143/585 [00:58<02:10,  3.38it/s] 25%|██▍       | 144/585 [00:59<02:10,  3.38it/s] 25%|██▍       | 145/585 [00:59<02:09,  3.39it/s] 25%|██▍       | 146/585 [00:59<02:09,  3.39it/s] 25%|██▌       | 147/585 [00:59<02:09,  3.39it/s] 25%|██▌       | 148/585 [01:00<02:08,  3.39it/s] 25%|██▌       | 149/585 [01:00<02:08,  3.39it/s] 26%|██▌       | 150/585 [01:00<02:08,  3.39it/s] 26%|██▌       | 151/585 [01:01<02:08,  3.39it/s] 26%|██▌       | 152/585 [01:01<02:08,  3.37it/s] 26%|██▌       | 153/585 [01:01<02:07,  3.38it/s] 26%|██▋       | 154/585 [01:01<02:07,  3.38it/s] 26%|██▋       | 155/585 [01:02<02:06,  3.39it/s] 27%|██▋       | 156/585 [01:02<02:06,  3.39it/s] 27%|██▋       | 157/585 [01:02<02:06,  3.39it/s] 27%|██▋       | 158/585 [01:03<02:05,  3.39it/s] 27%|██▋       | 159/585 [01:03<02:05,  3.39it/s] 27%|██▋       | 160/585 [01:03<02:05,  3.39it/s] 28%|██▊       | 161/585 [01:04<02:05,  3.39it/s] 28%|██▊       | 162/585 [01:04<02:04,  3.39it/s] 28%|██▊       | 163/585 [01:04<02:05,  3.37it/s] 28%|██▊       | 164/585 [01:04<02:04,  3.38it/s] 28%|██▊       | 165/585 [01:05<02:04,  3.38it/s] 28%|██▊       | 166/585 [01:05<02:03,  3.38it/s] 29%|██▊       | 167/585 [01:05<02:03,  3.39it/s] 29%|██▊       | 168/585 [01:06<02:02,  3.39it/s] 29%|██▉       | 169/585 [01:06<02:02,  3.39it/s] 29%|██▉       | 170/585 [01:06<02:02,  3.39it/s] 29%|██▉       | 171/585 [01:07<02:02,  3.39it/s] 29%|██▉       | 172/585 [01:07<02:01,  3.39it/s] 30%|██▉       | 173/585 [01:07<02:01,  3.39it/s] 30%|██▉       | 174/585 [01:07<02:01,  3.38it/s] 30%|██▉       | 175/585 [01:08<02:01,  3.38it/s] 30%|███       | 176/585 [01:08<02:00,  3.39it/s] 30%|███       | 177/585 [01:08<02:00,  3.39it/s] 30%|███       | 178/585 [01:09<02:00,  3.39it/s] 31%|███       | 179/585 [01:09<01:59,  3.39it/s] 31%|███       | 180/585 [01:09<01:59,  3.39it/s] 31%|███       | 181/585 [01:09<01:59,  3.39it/s] 31%|███       | 182/585 [01:10<01:58,  3.39it/s] 31%|███▏      | 183/585 [01:10<01:58,  3.39it/s] 31%|███▏      | 184/585 [01:10<01:58,  3.39it/s] 32%|███▏      | 185/585 [01:11<01:58,  3.37it/s] 32%|███▏      | 186/585 [01:11<01:58,  3.38it/s] 32%|███▏      | 187/585 [01:11<01:57,  3.38it/s] 32%|███▏      | 188/585 [01:12<01:57,  3.38it/s] 32%|███▏      | 189/585 [01:12<01:56,  3.39it/s] 32%|███▏      | 190/585 [01:12<01:56,  3.39it/s] 33%|███▎      | 191/585 [01:12<01:56,  3.39it/s] 33%|███▎      | 192/585 [01:13<01:56,  3.39it/s] 33%|███▎      | 193/585 [01:13<01:55,  3.39it/s] 33%|███▎      | 194/585 [01:13<01:55,  3.39it/s] 33%|███▎      | 195/585 [01:14<01:54,  3.39it/s] 34%|███▎      | 196/585 [01:14<01:54,  3.39it/s] 34%|███▎      | 197/585 [01:14<01:54,  3.39it/s] 34%|███▍      | 198/585 [01:14<01:54,  3.39it/s] 34%|███▍      | 199/585 [01:15<01:53,  3.39it/s] 34%|███▍      | 200/585 [01:15<01:53,  3.39it/s] 34%|███▍      | 201/585 [01:15<01:53,  3.38it/s] 35%|███▍      | 202/585 [01:16<01:53,  3.37it/s] 35%|███▍      | 203/585 [01:16<01:52,  3.39it/s] 35%|███▍      | 204/585 [01:16<01:51,  3.40it/s] 35%|███▌      | 205/585 [01:17<01:51,  3.42it/s] 35%|███▌      | 206/585 [01:17<01:50,  3.42it/s] 35%|███▌      | 207/585 [01:17<01:50,  3.43it/s] 36%|███▌      | 208/585 [01:17<01:49,  3.43it/s] 36%|███▌      | 209/585 [01:18<01:49,  3.43it/s] 36%|███▌      | 210/585 [01:18<01:49,  3.43it/s] 36%|███▌      | 211/585 [01:18<01:48,  3.44it/s] 36%|███▌      | 212/585 [01:19<01:48,  3.44it/s] 36%|███▋      | 213/585 [01:19<01:48,  3.43it/s] 37%|███▋      | 214/585 [01:19<01:48,  3.43it/s] 37%|███▋      | 215/585 [01:19<01:47,  3.43it/s] 37%|███▋      | 216/585 [01:20<01:47,  3.44it/s] 37%|███▋      | 217/585 [01:20<01:47,  3.44it/s] 37%|███▋      | 218/585 [01:20<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:21<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:21<01:46,  3.44it/s] 38%|███▊      | 221/585 [01:21<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:21<01:45,  3.44it/s] 38%|███▊      | 223/585 [01:22<01:45,  3.44it/s] 38%|███▊      | 224/585 [01:22<01:45,  3.43it/s] 38%|███▊      | 225/585 [01:22<01:44,  3.43it/s] 39%|███▊      | 226/585 [01:23<01:44,  3.44it/s] 39%|███▉      | 227/585 [01:23<01:44,  3.44it/s] 39%|███▉      | 228/585 [01:23<01:43,  3.44it/s] 39%|███▉      | 229/585 [01:24<01:43,  3.44it/s] 39%|███▉      | 230/585 [01:24<01:43,  3.44it/s] 39%|███▉      | 231/585 [01:24<01:42,  3.44it/s] 40%|███▉      | 232/585 [01:24<01:42,  3.44it/s] 40%|███▉      | 233/585 [01:25<01:42,  3.44it/s] 40%|████      | 234/585 [01:25<01:42,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 14:31:29,662 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:31:29,662 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:31:29,662 >>   Batch size = 8
{'eval_loss': 0.9550086259841919, 'eval_runtime': 9.9056, 'eval_samples_per_second': 352.63, 'eval_steps_per_second': 44.117, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.04it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.84it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.04it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.27it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.74it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.52it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.38it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.23it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.37it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.50it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.38it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.15it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.14it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.06it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.11it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.02it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.95it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.16it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.35it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.17it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.06it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.13it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.04it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.05it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.07it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.20it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.30it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.24it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.20it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.08it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.02it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.01it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.00it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.03it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.22it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.19it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.24it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.13it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.06it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.08it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.08it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.04it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.15it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.21it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.24it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.16it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.15it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.08it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.01it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.09it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.14it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.17it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.23it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.29it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.14it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.09it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.11it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.08it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.14it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.25it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.17it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.19it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.24it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.06it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.03it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.03it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.02it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.11it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.16it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.25it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.23it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.21it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.98it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.05it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.10it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.05it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.11it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.28it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.34it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.30it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.21it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.03it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.09it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.10it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.05it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.12it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.29it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:35<01:42,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 44.29it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:31:39,605 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 14:31:39,621 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:31:41,420 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:31:41,440 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:31:41,451 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:41<29:15,  5.02s/it] 40%|████      | 236/585 [01:41<20:56,  3.60s/it] 41%|████      | 237/585 [01:42<15:07,  2.61s/it] 41%|████      | 238/585 [01:42<11:04,  1.91s/it] 41%|████      | 239/585 [01:42<08:14,  1.43s/it] 41%|████      | 240/585 [01:43<06:15,  1.09s/it] 41%|████      | 241/585 [01:43<04:52,  1.18it/s] 41%|████▏     | 242/585 [01:43<03:54,  1.46it/s] 42%|████▏     | 243/585 [01:43<03:14,  1.76it/s] 42%|████▏     | 244/585 [01:44<02:45,  2.06it/s] 42%|████▏     | 245/585 [01:44<02:25,  2.33it/s] 42%|████▏     | 246/585 [01:44<02:11,  2.58it/s] 42%|████▏     | 247/585 [01:45<02:02,  2.77it/s] 42%|████▏     | 248/585 [01:45<01:55,  2.93it/s] 43%|████▎     | 249/585 [01:45<01:50,  3.05it/s] 43%|████▎     | 250/585 [01:45<01:46,  3.15it/s] 43%|████▎     | 251/585 [01:46<01:43,  3.22it/s] 43%|████▎     | 252/585 [01:46<01:41,  3.27it/s] 43%|████▎     | 253/585 [01:46<01:40,  3.30it/s] 43%|████▎     | 254/585 [01:47<01:39,  3.33it/s] 44%|████▎     | 255/585 [01:47<01:38,  3.35it/s] 44%|████▍     | 256/585 [01:47<01:38,  3.36it/s] 44%|████▍     | 257/585 [01:48<01:37,  3.37it/s] 44%|████▍     | 258/585 [01:48<01:37,  3.36it/s] 44%|████▍     | 259/585 [01:48<01:36,  3.37it/s] 44%|████▍     | 260/585 [01:48<01:36,  3.38it/s] 45%|████▍     | 261/585 [01:49<01:35,  3.38it/s] 45%|████▍     | 262/585 [01:49<01:35,  3.38it/s] 45%|████▍     | 263/585 [01:49<01:35,  3.38it/s] 45%|████▌     | 264/585 [01:50<01:34,  3.39it/s] 45%|████▌     | 265/585 [01:50<01:34,  3.39it/s] 45%|████▌     | 266/585 [01:50<01:34,  3.39it/s] 46%|████▌     | 267/585 [01:50<01:34,  3.38it/s] 46%|████▌     | 268/585 [01:51<01:33,  3.39it/s] 46%|████▌     | 269/585 [01:51<01:35,  3.29it/s] 46%|████▌     | 270/585 [01:51<01:34,  3.32it/s] 46%|████▋     | 271/585 [01:52<01:33,  3.34it/s] 46%|████▋     | 272/585 [01:52<01:33,  3.35it/s] 47%|████▋     | 273/585 [01:52<01:32,  3.36it/s] 47%|████▋     | 274/585 [01:53<01:32,  3.38it/s] 47%|████▋     | 275/585 [01:53<01:31,  3.38it/s] 47%|████▋     | 276/585 [01:53<01:31,  3.38it/s] 47%|████▋     | 277/585 [01:53<01:31,  3.38it/s] 48%|████▊     | 278/585 [01:54<01:30,  3.39it/s] 48%|████▊     | 279/585 [01:54<01:30,  3.39it/s] 48%|████▊     | 280/585 [01:54<01:30,  3.38it/s] 48%|████▊     | 281/585 [01:55<01:29,  3.38it/s] 48%|████▊     | 282/585 [01:55<01:29,  3.38it/s] 48%|████▊     | 283/585 [01:55<01:29,  3.39it/s] 49%|████▊     | 284/585 [01:56<01:28,  3.38it/s] 49%|████▊     | 285/585 [01:56<01:28,  3.39it/s] 49%|████▉     | 286/585 [01:56<01:28,  3.39it/s] 49%|████▉     | 287/585 [01:56<01:27,  3.39it/s] 49%|████▉     | 288/585 [01:57<01:27,  3.39it/s] 49%|████▉     | 289/585 [01:57<01:27,  3.39it/s] 50%|████▉     | 290/585 [01:57<01:27,  3.39it/s] 50%|████▉     | 291/585 [01:58<01:27,  3.38it/s] 50%|████▉     | 292/585 [01:58<01:26,  3.38it/s] 50%|█████     | 293/585 [01:58<01:26,  3.38it/s] 50%|█████     | 294/585 [01:58<01:26,  3.38it/s] 50%|█████     | 295/585 [01:59<01:25,  3.39it/s] 51%|█████     | 296/585 [01:59<01:25,  3.39it/s] 51%|█████     | 297/585 [01:59<01:25,  3.39it/s] 51%|█████     | 298/585 [02:00<01:24,  3.39it/s] 51%|█████     | 299/585 [02:00<01:24,  3.39it/s] 51%|█████▏    | 300/585 [02:00<01:24,  3.39it/s] 51%|█████▏    | 301/585 [02:01<01:23,  3.39it/s] 52%|█████▏    | 302/585 [02:01<01:23,  3.38it/s] 52%|█████▏    | 303/585 [02:01<01:23,  3.39it/s] 52%|█████▏    | 304/585 [02:01<01:22,  3.39it/s] 52%|█████▏    | 305/585 [02:02<01:22,  3.39it/s] 52%|█████▏    | 306/585 [02:02<01:22,  3.39it/s] 52%|█████▏    | 307/585 [02:02<01:22,  3.39it/s] 53%|█████▎    | 308/585 [02:03<01:21,  3.39it/s] 53%|█████▎    | 309/585 [02:03<01:21,  3.39it/s] 53%|█████▎    | 310/585 [02:03<01:21,  3.39it/s] 53%|█████▎    | 311/585 [02:04<01:20,  3.39it/s] 53%|█████▎    | 312/585 [02:04<01:20,  3.39it/s] 54%|█████▎    | 313/585 [02:04<01:20,  3.39it/s] 54%|█████▎    | 314/585 [02:04<01:19,  3.39it/s] 54%|█████▍    | 315/585 [02:05<01:19,  3.39it/s] 54%|█████▍    | 316/585 [02:05<01:19,  3.39it/s] 54%|█████▍    | 317/585 [02:05<01:19,  3.39it/s] 54%|█████▍    | 318/585 [02:06<01:18,  3.39it/s] 55%|█████▍    | 319/585 [02:06<01:18,  3.39it/s] 55%|█████▍    | 320/585 [02:06<01:18,  3.39it/s] 55%|█████▍    | 321/585 [02:06<01:17,  3.39it/s] 55%|█████▌    | 322/585 [02:07<01:17,  3.39it/s] 55%|█████▌    | 323/585 [02:07<01:17,  3.38it/s] 55%|█████▌    | 324/585 [02:07<01:17,  3.38it/s] 56%|█████▌    | 325/585 [02:08<01:16,  3.38it/s] 56%|█████▌    | 326/585 [02:08<01:16,  3.39it/s] 56%|█████▌    | 327/585 [02:08<01:16,  3.39it/s] 56%|█████▌    | 328/585 [02:09<01:15,  3.39it/s] 56%|█████▌    | 329/585 [02:09<01:15,  3.39it/s] 56%|█████▋    | 330/585 [02:09<01:15,  3.39it/s] 57%|█████▋    | 331/585 [02:09<01:15,  3.39it/s] 57%|█████▋    | 332/585 [02:10<01:14,  3.39it/s] 57%|█████▋    | 333/585 [02:10<01:14,  3.39it/s] 57%|█████▋    | 334/585 [02:10<01:14,  3.39it/s] 57%|█████▋    | 335/585 [02:11<01:13,  3.39it/s] 57%|█████▋    | 336/585 [02:11<01:13,  3.39it/s] 58%|█████▊    | 337/585 [02:11<01:13,  3.39it/s] 58%|█████▊    | 338/585 [02:11<01:12,  3.39it/s] 58%|█████▊    | 339/585 [02:12<01:12,  3.39it/s] 58%|█████▊    | 340/585 [02:12<01:12,  3.38it/s] 58%|█████▊    | 341/585 [02:12<01:12,  3.38it/s] 58%|█████▊    | 342/585 [02:13<01:11,  3.38it/s] 59%|█████▊    | 343/585 [02:13<01:11,  3.39it/s] 59%|█████▉    | 344/585 [02:13<01:11,  3.38it/s] 59%|█████▉    | 345/585 [02:14<01:10,  3.38it/s] 59%|█████▉    | 346/585 [02:14<01:10,  3.38it/s] 59%|█████▉    | 347/585 [02:14<01:10,  3.39it/s] 59%|█████▉    | 348/585 [02:14<01:09,  3.39it/s] 60%|█████▉    | 349/585 [02:15<01:09,  3.38it/s] 60%|█████▉    | 350/585 [02:15<01:09,  3.38it/s] 60%|██████    | 351/585 [02:15<01:09,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 14:32:19,994 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:32:19,994 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:32:19,994 >>   Batch size = 8
{'eval_loss': 0.9537877440452576, 'eval_runtime': 9.913, 'eval_samples_per_second': 352.367, 'eval_steps_per_second': 44.084, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.65it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.39it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.91it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.18it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.72it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.48it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.28it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.32it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.33it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.40it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.36it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.14it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.12it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.18it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.03it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.99it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.06it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.23it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.31it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.26it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.09it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.11it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.12it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.06it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.05it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.09it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.10it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.31it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.23it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.11it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.97it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.13it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.06it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.05it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.06it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.17it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.25it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.26it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.10it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.09it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.07it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.02it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.06it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.17it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.20it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.27it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.23it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.08it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.06it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.08it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.00it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.10it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.21it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.27it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.29it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.18it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.17it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.12it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.04it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.91it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.15it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.18it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.27it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.09it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.23it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.20it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.12it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.91it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.91it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.13it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.23it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.21it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.22it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.20it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.20it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.05it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.00it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.99it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.12it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.23it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.20it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.17it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.21it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.07it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.09it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.13it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.07it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:25<01:09,  3.38it/s]
100%|██████████| 437/437 [00:09<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:32:29,927 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 14:32:29,950 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:32:32,032 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:32:32,044 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:32:32,054 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:32<19:58,  5.14s/it] 60%|██████    | 353/585 [02:32<14:15,  3.69s/it] 61%|██████    | 354/585 [02:32<10:16,  2.67s/it] 61%|██████    | 355/585 [02:33<07:30,  1.96s/it] 61%|██████    | 356/585 [02:33<05:34,  1.46s/it] 61%|██████    | 357/585 [02:33<04:13,  1.11s/it] 61%|██████    | 358/585 [02:34<03:16,  1.16it/s] 61%|██████▏   | 359/585 [02:34<02:36,  1.44it/s] 62%|██████▏   | 360/585 [02:34<02:09,  1.74it/s] 62%|██████▏   | 361/585 [02:34<01:49,  2.04it/s] 62%|██████▏   | 362/585 [02:35<01:36,  2.32it/s] 62%|██████▏   | 363/585 [02:35<01:26,  2.56it/s] 62%|██████▏   | 364/585 [02:35<01:20,  2.75it/s] 62%|██████▏   | 365/585 [02:36<01:15,  2.92it/s] 63%|██████▎   | 366/585 [02:36<01:11,  3.04it/s] 63%|██████▎   | 367/585 [02:36<01:09,  3.15it/s] 63%|██████▎   | 368/585 [02:36<01:07,  3.23it/s] 63%|██████▎   | 369/585 [02:37<01:05,  3.29it/s] 63%|██████▎   | 370/585 [02:37<01:04,  3.34it/s] 63%|██████▎   | 371/585 [02:37<01:03,  3.37it/s] 64%|██████▎   | 372/585 [02:38<01:02,  3.39it/s] 64%|██████▍   | 373/585 [02:38<01:02,  3.41it/s] 64%|██████▍   | 374/585 [02:38<01:01,  3.42it/s] 64%|██████▍   | 375/585 [02:39<01:01,  3.41it/s] 64%|██████▍   | 376/585 [02:39<01:01,  3.42it/s] 64%|██████▍   | 377/585 [02:39<01:00,  3.43it/s] 65%|██████▍   | 378/585 [02:39<01:00,  3.43it/s] 65%|██████▍   | 379/585 [02:40<00:59,  3.44it/s] 65%|██████▍   | 380/585 [02:40<00:59,  3.44it/s] 65%|██████▌   | 381/585 [02:40<00:59,  3.44it/s] 65%|██████▌   | 382/585 [02:41<00:59,  3.44it/s] 65%|██████▌   | 383/585 [02:41<00:58,  3.44it/s] 66%|██████▌   | 384/585 [02:41<00:58,  3.44it/s] 66%|██████▌   | 385/585 [02:41<00:58,  3.44it/s] 66%|██████▌   | 386/585 [02:42<00:58,  3.40it/s] 66%|██████▌   | 387/585 [02:42<00:58,  3.41it/s] 66%|██████▋   | 388/585 [02:42<00:57,  3.42it/s] 66%|██████▋   | 389/585 [02:43<00:57,  3.43it/s] 67%|██████▋   | 390/585 [02:43<00:56,  3.43it/s] 67%|██████▋   | 391/585 [02:43<00:56,  3.44it/s] 67%|██████▋   | 392/585 [02:43<00:56,  3.44it/s] 67%|██████▋   | 393/585 [02:44<00:55,  3.44it/s] 67%|██████▋   | 394/585 [02:44<00:55,  3.44it/s] 68%|██████▊   | 395/585 [02:44<00:55,  3.44it/s] 68%|██████▊   | 396/585 [02:45<00:54,  3.44it/s] 68%|██████▊   | 397/585 [02:45<00:54,  3.42it/s] 68%|██████▊   | 398/585 [02:45<00:54,  3.43it/s] 68%|██████▊   | 399/585 [02:46<00:54,  3.43it/s] 68%|██████▊   | 400/585 [02:46<00:53,  3.43it/s] 69%|██████▊   | 401/585 [02:46<00:53,  3.44it/s] 69%|██████▊   | 402/585 [02:46<00:53,  3.44it/s] 69%|██████▉   | 403/585 [02:47<00:52,  3.44it/s] 69%|██████▉   | 404/585 [02:47<00:52,  3.44it/s] 69%|██████▉   | 405/585 [02:47<00:52,  3.44it/s] 69%|██████▉   | 406/585 [02:48<00:52,  3.44it/s] 70%|██████▉   | 407/585 [02:48<00:51,  3.45it/s] 70%|██████▉   | 408/585 [02:48<00:51,  3.42it/s] 70%|██████▉   | 409/585 [02:48<00:51,  3.43it/s] 70%|███████   | 410/585 [02:49<00:50,  3.44it/s] 70%|███████   | 411/585 [02:49<00:50,  3.44it/s] 70%|███████   | 412/585 [02:49<00:50,  3.44it/s] 71%|███████   | 413/585 [02:50<00:50,  3.44it/s] 71%|███████   | 414/585 [02:50<00:49,  3.44it/s] 71%|███████   | 415/585 [02:50<00:49,  3.44it/s] 71%|███████   | 416/585 [02:50<00:49,  3.44it/s] 71%|███████▏  | 417/585 [02:51<00:49,  3.42it/s] 71%|███████▏  | 418/585 [02:51<00:48,  3.43it/s] 72%|███████▏  | 419/585 [02:51<00:50,  3.29it/s] 72%|███████▏  | 420/585 [02:52<00:49,  3.34it/s] 72%|███████▏  | 421/585 [02:52<00:48,  3.37it/s] 72%|███████▏  | 422/585 [02:52<00:48,  3.39it/s] 72%|███████▏  | 423/585 [02:53<00:47,  3.40it/s] 72%|███████▏  | 424/585 [02:53<00:47,  3.41it/s] 73%|███████▎  | 425/585 [02:53<00:46,  3.42it/s] 73%|███████▎  | 426/585 [02:53<00:46,  3.43it/s] 73%|███████▎  | 427/585 [02:54<00:46,  3.43it/s] 73%|███████▎  | 428/585 [02:54<00:45,  3.43it/s] 73%|███████▎  | 429/585 [02:54<00:45,  3.43it/s] 74%|███████▎  | 430/585 [02:55<00:45,  3.41it/s] 74%|███████▎  | 431/585 [02:55<00:45,  3.42it/s] 74%|███████▍  | 432/585 [02:55<00:44,  3.43it/s] 74%|███████▍  | 433/585 [02:55<00:44,  3.43it/s] 74%|███████▍  | 434/585 [02:56<00:43,  3.43it/s] 74%|███████▍  | 435/585 [02:56<00:43,  3.44it/s] 75%|███████▍  | 436/585 [02:56<00:43,  3.44it/s] 75%|███████▍  | 437/585 [02:57<00:43,  3.44it/s] 75%|███████▍  | 438/585 [02:57<00:42,  3.44it/s] 75%|███████▌  | 439/585 [02:57<00:42,  3.44it/s] 75%|███████▌  | 440/585 [02:57<00:42,  3.44it/s] 75%|███████▌  | 441/585 [02:58<00:41,  3.44it/s] 76%|███████▌  | 442/585 [02:58<00:41,  3.42it/s] 76%|███████▌  | 443/585 [02:58<00:41,  3.43it/s] 76%|███████▌  | 444/585 [02:59<00:41,  3.43it/s] 76%|███████▌  | 445/585 [02:59<00:40,  3.43it/s] 76%|███████▌  | 446/585 [02:59<00:40,  3.44it/s] 76%|███████▋  | 447/585 [03:00<00:40,  3.44it/s] 77%|███████▋  | 448/585 [03:00<00:39,  3.44it/s] 77%|███████▋  | 449/585 [03:00<00:39,  3.44it/s] 77%|███████▋  | 450/585 [03:00<00:39,  3.44it/s] 77%|███████▋  | 451/585 [03:01<00:38,  3.44it/s] 77%|███████▋  | 452/585 [03:01<00:38,  3.44it/s] 77%|███████▋  | 453/585 [03:01<00:38,  3.43it/s] 78%|███████▊  | 454/585 [03:02<00:38,  3.43it/s] 78%|███████▊  | 455/585 [03:02<00:37,  3.43it/s] 78%|███████▊  | 456/585 [03:02<00:37,  3.43it/s] 78%|███████▊  | 457/585 [03:02<00:37,  3.44it/s] 78%|███████▊  | 458/585 [03:03<00:36,  3.44it/s] 78%|███████▊  | 459/585 [03:03<00:36,  3.45it/s] 79%|███████▊  | 460/585 [03:03<00:36,  3.44it/s] 79%|███████▉  | 461/585 [03:04<00:36,  3.44it/s] 79%|███████▉  | 462/585 [03:04<00:35,  3.44it/s] 79%|███████▉  | 463/585 [03:04<00:35,  3.44it/s] 79%|███████▉  | 464/585 [03:04<00:35,  3.43it/s] 79%|███████▉  | 465/585 [03:05<00:34,  3.43it/s] 80%|███████▉  | 466/585 [03:05<00:34,  3.43it/s] 80%|███████▉  | 467/585 [03:05<00:34,  3.44it/s] 80%|████████  | 468/585 [03:06<00:34,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 14:33:10,309 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:33:10,309 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:33:10,309 >>   Batch size = 8
{'eval_loss': 0.9548156261444092, 'eval_runtime': 9.9152, 'eval_samples_per_second': 352.289, 'eval_steps_per_second': 44.074, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.32it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.93it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.09it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.23it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.85it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.63it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.41it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.30it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.26it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.44it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.34it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.32it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.17it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.16it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.08it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.08it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.08it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.26it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.16it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.27it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.24it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.14it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.13it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.08it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.96it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.11it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.16it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.28it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.21it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.16it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.10it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.09it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.11it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.09it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.14it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.19it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.29it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.34it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.11it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.10it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.13it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.13it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.08it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.07it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.20it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.25it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.31it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.23it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.09it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.10it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.11it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.03it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.19it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.12it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.16it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.23it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.21it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.08it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.09it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.00it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.08it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.23it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.32it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.34it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.26it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.18it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.17it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.09it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.01it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.08it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.18it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.31it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.24it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.15it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.02it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.09it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.15it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.17it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.99it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.16it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.19it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.21it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.15it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.12it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.15it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.21it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.15it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:16<00:34,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 44.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:33:20,246 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 14:33:20,267 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:33:22,374 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:33:22,391 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:33:22,402 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:22<10:07,  5.24s/it] 80%|████████  | 470/585 [03:23<07:11,  3.75s/it] 81%|████████  | 471/585 [03:23<05:09,  2.72s/it] 81%|████████  | 472/585 [03:23<03:44,  1.99s/it] 81%|████████  | 473/585 [03:24<02:45,  1.48s/it] 81%|████████  | 474/585 [03:24<02:04,  1.12s/it] 81%|████████  | 475/585 [03:24<01:36,  1.14it/s] 81%|████████▏ | 476/585 [03:24<01:16,  1.43it/s] 82%|████████▏ | 477/585 [03:25<01:02,  1.74it/s] 82%|████████▏ | 478/585 [03:25<00:52,  2.04it/s] 82%|████████▏ | 479/585 [03:25<00:45,  2.32it/s] 82%|████████▏ | 480/585 [03:26<00:40,  2.58it/s] 82%|████████▏ | 481/585 [03:26<00:37,  2.78it/s] 82%|████████▏ | 482/585 [03:26<00:34,  2.95it/s] 83%|████████▎ | 483/585 [03:26<00:33,  3.09it/s] 83%|████████▎ | 484/585 [03:27<00:31,  3.18it/s] 83%|████████▎ | 485/585 [03:27<00:30,  3.26it/s] 83%|████████▎ | 486/585 [03:27<00:29,  3.31it/s] 83%|████████▎ | 487/585 [03:28<00:29,  3.36it/s] 83%|████████▎ | 488/585 [03:28<00:28,  3.38it/s] 84%|████████▎ | 489/585 [03:28<00:28,  3.40it/s] 84%|████████▍ | 490/585 [03:29<00:27,  3.42it/s] 84%|████████▍ | 491/585 [03:29<00:27,  3.42it/s] 84%|████████▍ | 492/585 [03:29<00:27,  3.40it/s] 84%|████████▍ | 493/585 [03:29<00:26,  3.42it/s] 84%|████████▍ | 494/585 [03:30<00:26,  3.42it/s] 85%|████████▍ | 495/585 [03:30<00:26,  3.43it/s] 85%|████████▍ | 496/585 [03:30<00:25,  3.43it/s] 85%|████████▍ | 497/585 [03:31<00:25,  3.44it/s] 85%|████████▌ | 498/585 [03:31<00:25,  3.44it/s] 85%|████████▌ | 499/585 [03:31<00:24,  3.44it/s] 85%|████████▌ | 500/585 [03:31<00:24,  3.44it/s]                                                  85%|████████▌ | 500/585 [03:31<00:24,  3.44it/s] 86%|████████▌ | 501/585 [03:32<00:24,  3.44it/s] 86%|████████▌ | 502/585 [03:32<00:24,  3.44it/s] 86%|████████▌ | 503/585 [03:32<00:23,  3.43it/s] 86%|████████▌ | 504/585 [03:33<00:23,  3.43it/s] 86%|████████▋ | 505/585 [03:33<00:23,  3.43it/s] 86%|████████▋ | 506/585 [03:33<00:22,  3.44it/s] 87%|████████▋ | 507/585 [03:33<00:22,  3.44it/s] 87%|████████▋ | 508/585 [03:34<00:22,  3.44it/s] 87%|████████▋ | 509/585 [03:34<00:22,  3.44it/s] 87%|████████▋ | 510/585 [03:34<00:21,  3.44it/s] 87%|████████▋ | 511/585 [03:35<00:21,  3.44it/s] 88%|████████▊ | 512/585 [03:35<00:21,  3.44it/s] 88%|████████▊ | 513/585 [03:35<00:20,  3.45it/s] 88%|████████▊ | 514/585 [03:36<00:20,  3.42it/s] 88%|████████▊ | 515/585 [03:36<00:20,  3.43it/s] 88%|████████▊ | 516/585 [03:36<00:20,  3.43it/s] 88%|████████▊ | 517/585 [03:36<00:19,  3.43it/s] 89%|████████▊ | 518/585 [03:37<00:19,  3.44it/s] 89%|████████▊ | 519/585 [03:37<00:19,  3.44it/s] 89%|████████▉ | 520/585 [03:37<00:18,  3.44it/s] 89%|████████▉ | 521/585 [03:38<00:18,  3.44it/s] 89%|████████▉ | 522/585 [03:38<00:18,  3.44it/s] 89%|████████▉ | 523/585 [03:38<00:18,  3.44it/s] 90%|████████▉ | 524/585 [03:38<00:17,  3.44it/s] 90%|████████▉ | 525/585 [03:39<00:17,  3.43it/s] 90%|████████▉ | 526/585 [03:39<00:17,  3.44it/s] 90%|█████████ | 527/585 [03:39<00:16,  3.44it/s] 90%|█████████ | 528/585 [03:40<00:16,  3.44it/s] 90%|█████████ | 529/585 [03:40<00:16,  3.44it/s] 91%|█████████ | 530/585 [03:40<00:15,  3.44it/s] 91%|█████████ | 531/585 [03:40<00:15,  3.44it/s] 91%|█████████ | 532/585 [03:41<00:15,  3.44it/s] 91%|█████████ | 533/585 [03:41<00:15,  3.44it/s] 91%|█████████▏| 534/585 [03:41<00:14,  3.44it/s] 91%|█████████▏| 535/585 [03:42<00:14,  3.44it/s] 92%|█████████▏| 536/585 [03:42<00:14,  3.44it/s] 92%|█████████▏| 537/585 [03:42<00:13,  3.43it/s] 92%|█████████▏| 538/585 [03:42<00:13,  3.44it/s] 92%|█████████▏| 539/585 [03:43<00:13,  3.44it/s] 92%|█████████▏| 540/585 [03:43<00:13,  3.44it/s] 92%|█████████▏| 541/585 [03:43<00:12,  3.44it/s] 93%|█████████▎| 542/585 [03:44<00:12,  3.44it/s] 93%|█████████▎| 543/585 [03:44<00:12,  3.44it/s] 93%|█████████▎| 544/585 [03:44<00:11,  3.44it/s] 93%|█████████▎| 545/585 [03:45<00:11,  3.45it/s] 93%|█████████▎| 546/585 [03:45<00:11,  3.45it/s] 94%|█████████▎| 547/585 [03:45<00:11,  3.39it/s] 94%|█████████▎| 548/585 [03:45<00:10,  3.40it/s] 94%|█████████▍| 549/585 [03:46<00:10,  3.41it/s] 94%|█████████▍| 550/585 [03:46<00:10,  3.42it/s] 94%|█████████▍| 551/585 [03:46<00:09,  3.43it/s] 94%|█████████▍| 552/585 [03:47<00:09,  3.44it/s] 95%|█████████▍| 553/585 [03:47<00:09,  3.44it/s] 95%|█████████▍| 554/585 [03:47<00:09,  3.44it/s] 95%|█████████▍| 555/585 [03:47<00:08,  3.44it/s] 95%|█████████▌| 556/585 [03:48<00:08,  3.44it/s] 95%|█████████▌| 557/585 [03:48<00:08,  3.44it/s] 95%|█████████▌| 558/585 [03:48<00:07,  3.44it/s] 96%|█████████▌| 559/585 [03:49<00:07,  3.44it/s] 96%|█████████▌| 560/585 [03:49<00:07,  3.44it/s] 96%|█████████▌| 561/585 [03:49<00:06,  3.44it/s] 96%|█████████▌| 562/585 [03:49<00:06,  3.42it/s] 96%|█████████▌| 563/585 [03:50<00:06,  3.43it/s] 96%|█████████▋| 564/585 [03:50<00:06,  3.44it/s] 97%|█████████▋| 565/585 [03:50<00:05,  3.43it/s] 97%|█████████▋| 566/585 [03:51<00:05,  3.44it/s] 97%|█████████▋| 567/585 [03:51<00:05,  3.43it/s] 97%|█████████▋| 568/585 [03:51<00:04,  3.43it/s] 97%|█████████▋| 569/585 [03:52<00:04,  3.33it/s] 97%|█████████▋| 570/585 [03:52<00:04,  3.36it/s] 98%|█████████▊| 571/585 [03:52<00:04,  3.38it/s] 98%|█████████▊| 572/585 [03:52<00:03,  3.40it/s] 98%|█████████▊| 573/585 [03:53<00:03,  3.41it/s] 98%|█████████▊| 574/585 [03:53<00:03,  3.42it/s] 98%|█████████▊| 575/585 [03:53<00:02,  3.43it/s] 98%|█████████▊| 576/585 [03:54<00:02,  3.43it/s] 99%|█████████▊| 577/585 [03:54<00:02,  3.44it/s] 99%|█████████▉| 578/585 [03:54<00:02,  3.44it/s] 99%|█████████▉| 579/585 [03:54<00:01,  3.42it/s] 99%|█████████▉| 580/585 [03:55<00:01,  3.43it/s] 99%|█████████▉| 581/585 [03:55<00:01,  3.43it/s] 99%|█████████▉| 582/585 [03:55<00:00,  3.43it/s]100%|█████████▉| 583/585 [03:56<00:00,  3.44it/s]100%|█████████▉| 584/585 [03:56<00:00,  3.44it/s]100%|██████████| 585/585 [03:56<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 14:34:00,839 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:34:00,839 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:34:00,839 >>   Batch size = 8
{'eval_loss': 0.9579499363899231, 'eval_runtime': 9.9105, 'eval_samples_per_second': 352.453, 'eval_steps_per_second': 44.095, 'epoch': 4.0}
{'loss': 0.8091, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.69it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.06it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.37it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.49it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.87it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.36it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.40it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.20it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.33it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.47it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.36it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.32it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.29it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.15it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.13it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.03it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.11it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.24it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.27it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.31it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.39it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.17it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.08it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.09it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.07it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.05it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.23it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.29it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.24it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.31it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.24it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.11it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.04it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.05it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.11it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.30it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.23it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.20it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.27it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.24it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.14it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.09it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.06it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.14it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.13it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.24it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.24it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.15it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.18it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.06it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.01it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.05it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.05it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.31it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.34it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.37it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.32it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.16it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.01it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.05it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.13it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.15it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.23it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.28it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.36it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.23it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.13it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.95it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.08it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.08it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.21it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.26it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.32it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.25it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.20it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.08it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.04it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.91it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.09it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.26it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.41it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.47it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.44it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.21it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.11it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.09it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.01it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:06<00:00,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 44.01it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:34:10,749 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 14:34:10,774 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:34:12,748 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:34:12,761 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:34:12,773 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:34:17,163 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:34:17,168 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234 (score: 0.9537877440452576).
                                                 100%|██████████| 585/585 [04:15<00:00,  3.44it/s]100%|██████████| 585/585 [04:15<00:00,  2.29it/s]
[INFO|trainer.py:1894] 2023-08-28 14:34:19,175 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 14:34:19,190 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:34:21,273 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:34:21,305 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:34:21,315 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:34:21,509 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:21,509 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:21,509 >>   train_loss               =     0.8035
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:21,509 >>   train_runtime            = 0:04:15.03
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:21,509 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:21,509 >>   train_samples_per_second =    147.038
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:21,510 >>   train_steps_per_second   =      2.294
{'eval_loss': 0.9613750576972961, 'eval_runtime': 9.8865, 'eval_samples_per_second': 353.309, 'eval_steps_per_second': 44.202, 'epoch': 5.0}
{'train_runtime': 255.0365, 'train_samples_per_second': 147.038, 'train_steps_per_second': 2.294, 'train_loss': 0.8035121428660857, 'epoch': 5.0}
08/28/2023 14:34:21 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:34:21,548 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:34:21,548 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:34:21,548 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.42it/s]  3%|▎         | 12/437 [00:00<00:08, 48.70it/s]  4%|▍         | 17/437 [00:00<00:08, 47.05it/s]  5%|▌         | 22/437 [00:00<00:08, 46.32it/s]  6%|▌         | 27/437 [00:00<00:08, 45.76it/s]  7%|▋         | 32/437 [00:00<00:08, 45.45it/s]  8%|▊         | 37/437 [00:00<00:08, 45.36it/s] 10%|▉         | 42/437 [00:00<00:08, 44.94it/s] 11%|█         | 47/437 [00:01<00:08, 44.24it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.02it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.27it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.48it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.54it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.63it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.57it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.70it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.45it/s] 21%|██        | 92/437 [00:02<00:07, 44.10it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.99it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.05it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.19it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.35it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.51it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.70it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.47it/s] 30%|███       | 132/437 [00:02<00:06, 44.34it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.15it/s] 32%|███▏      | 142/437 [00:03<00:06, 44.05it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.00it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.11it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.27it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.44it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.65it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.64it/s] 41%|████      | 177/437 [00:03<00:05, 44.39it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.26it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.12it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.02it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.18it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.33it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.57it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.69it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.61it/s] 51%|█████     | 222/437 [00:04<00:04, 44.44it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.14it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.04it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.03it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.20it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.39it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.56it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.56it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.51it/s] 61%|██████    | 267/437 [00:05<00:03, 44.36it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.14it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.06it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.07it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.16it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.35it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.53it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.55it/s] 70%|███████   | 307/437 [00:06<00:02, 44.49it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.30it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.10it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.04it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.14it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.18it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.46it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.55it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.52it/s] 81%|████████  | 352/437 [00:07<00:01, 44.44it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.17it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.15it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.09it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.15it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.28it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.28it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.48it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.57it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.45it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.24it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.07it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.11it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.11it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.24it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.37it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.38it/s]100%|██████████| 437/437 [00:09<00:00, 44.54it/s]100%|██████████| 437/437 [00:09<00:00, 44.44it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:34:31,400 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:31,400 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:31,400 >>   eval_loss               =     0.9538
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:31,400 >>   eval_runtime            = 0:00:09.85
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:31,400 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:31,400 >>   eval_samples_per_second =     354.57
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:31,400 >>   eval_steps_per_second   =     44.359
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:34:31,400 >>   perplexity              =     2.5955
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:34:37,945 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:34:37,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:34:37,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:34:37,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:34:37,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:34:38,569 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:34:38,569 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:34:39,121 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:34:40,181 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:34:40,181 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:34:43,006 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:34:43,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:34:43,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:34:43,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:34:43,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:34:43,635 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:34:43,636 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:34:44,230 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:34:44,382 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:34:44,383 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.80it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:05,  1.61it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.70it/s]Extractor Predicting: 15it [00:08,  1.68it/s]Extractor Predicting: 16it [00:09,  1.69it/s]Extractor Predicting: 17it [00:10,  1.70it/s]Extractor Predicting: 18it [00:10,  1.69it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:11,  1.65it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:13,  1.64it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:14,  1.66it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.75it/s]Extractor Predicting: 29it [00:17,  1.72it/s]Extractor Predicting: 30it [00:17,  1.67it/s]Extractor Predicting: 31it [00:18,  1.66it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:19,  1.59it/s]Extractor Predicting: 34it [00:20,  1.57it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:21,  1.57it/s]Extractor Predicting: 37it [00:22,  1.56it/s]Extractor Predicting: 38it [00:23,  1.55it/s]Extractor Predicting: 39it [00:23,  1.58it/s]Extractor Predicting: 40it [00:24,  1.58it/s]Extractor Predicting: 41it [00:24,  1.57it/s]Extractor Predicting: 42it [00:25,  1.58it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:26,  1.60it/s]Extractor Predicting: 45it [00:27,  1.62it/s]Extractor Predicting: 46it [00:28,  1.56it/s]Extractor Predicting: 47it [00:28,  1.54it/s]Extractor Predicting: 48it [00:29,  1.54it/s]Extractor Predicting: 49it [00:30,  1.54it/s]Extractor Predicting: 50it [00:30,  1.44it/s]Extractor Predicting: 51it [00:31,  1.47it/s]Extractor Predicting: 52it [00:32,  1.48it/s]Extractor Predicting: 53it [00:32,  1.54it/s]Extractor Predicting: 54it [00:33,  1.54it/s]Extractor Predicting: 55it [00:34,  1.53it/s]Extractor Predicting: 56it [00:34,  1.53it/s]Extractor Predicting: 57it [00:35,  1.52it/s]Extractor Predicting: 58it [00:36,  1.53it/s]Extractor Predicting: 59it [00:36,  1.52it/s]Extractor Predicting: 60it [00:37,  1.52it/s]Extractor Predicting: 61it [00:37,  1.54it/s]Extractor Predicting: 62it [00:38,  1.55it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:39,  1.60it/s]Extractor Predicting: 65it [00:40,  1.58it/s]Extractor Predicting: 66it [00:41,  1.61it/s]Extractor Predicting: 67it [00:41,  1.60it/s]Extractor Predicting: 68it [00:42,  1.59it/s]Extractor Predicting: 69it [00:42,  1.58it/s]Extractor Predicting: 70it [00:43,  1.56it/s]Extractor Predicting: 71it [00:44,  1.57it/s]Extractor Predicting: 72it [00:44,  1.58it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:46,  1.60it/s]Extractor Predicting: 75it [00:46,  1.61it/s]Extractor Predicting: 76it [00:47,  1.58it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:48,  1.57it/s]Extractor Predicting: 79it [00:49,  1.57it/s]Extractor Predicting: 80it [00:49,  1.56it/s]Extractor Predicting: 81it [00:50,  1.61it/s]Extractor Predicting: 82it [00:51,  1.59it/s]Extractor Predicting: 83it [00:51,  1.56it/s]Extractor Predicting: 84it [00:52,  1.58it/s]Extractor Predicting: 85it [00:53,  1.61it/s]Extractor Predicting: 86it [00:53,  1.66it/s]Extractor Predicting: 87it [00:54,  1.70it/s]Extractor Predicting: 88it [00:54,  1.71it/s]Extractor Predicting: 89it [00:55,  1.70it/s]Extractor Predicting: 90it [00:55,  1.71it/s]Extractor Predicting: 91it [00:56,  1.69it/s]Extractor Predicting: 92it [00:57,  1.66it/s]Extractor Predicting: 93it [00:57,  1.70it/s]Extractor Predicting: 94it [00:58,  1.73it/s]Extractor Predicting: 95it [00:58,  1.72it/s]Extractor Predicting: 96it [00:59,  1.73it/s]Extractor Predicting: 97it [01:00,  1.70it/s]Extractor Predicting: 98it [01:00,  1.67it/s]Extractor Predicting: 99it [01:01,  1.63it/s]Extractor Predicting: 100it [01:01,  1.65it/s]Extractor Predicting: 101it [01:02,  1.69it/s]Extractor Predicting: 102it [01:03,  1.67it/s]Extractor Predicting: 103it [01:03,  1.67it/s]Extractor Predicting: 104it [01:04,  1.67it/s]Extractor Predicting: 105it [01:04,  1.71it/s]Extractor Predicting: 106it [01:05,  1.67it/s]Extractor Predicting: 107it [01:06,  1.70it/s]Extractor Predicting: 108it [01:06,  1.69it/s]Extractor Predicting: 109it [01:07,  1.72it/s]Extractor Predicting: 110it [01:07,  1.71it/s]Extractor Predicting: 111it [01:08,  1.68it/s]Extractor Predicting: 112it [01:09,  1.67it/s]Extractor Predicting: 113it [01:09,  1.66it/s]Extractor Predicting: 114it [01:10,  1.61it/s]Extractor Predicting: 115it [01:10,  1.63it/s]Extractor Predicting: 116it [01:11,  1.62it/s]Extractor Predicting: 117it [01:12,  1.61it/s]Extractor Predicting: 118it [01:12,  1.67it/s]Extractor Predicting: 119it [01:13,  1.65it/s]Extractor Predicting: 120it [01:13,  1.67it/s]Extractor Predicting: 121it [01:14,  1.65it/s]Extractor Predicting: 122it [01:15,  1.50it/s]Extractor Predicting: 123it [01:15,  1.53it/s]Extractor Predicting: 124it [01:16,  1.52it/s]Extractor Predicting: 125it [01:17,  1.54it/s]Extractor Predicting: 126it [01:17,  1.56it/s]Extractor Predicting: 127it [01:18,  1.55it/s]Extractor Predicting: 128it [01:19,  1.56it/s]Extractor Predicting: 129it [01:19,  1.53it/s]Extractor Predicting: 130it [01:20,  1.55it/s]Extractor Predicting: 131it [01:21,  1.57it/s]Extractor Predicting: 132it [01:21,  1.60it/s]Extractor Predicting: 133it [01:22,  1.60it/s]Extractor Predicting: 134it [01:22,  1.61it/s]Extractor Predicting: 135it [01:23,  1.60it/s]Extractor Predicting: 136it [01:24,  1.60it/s]Extractor Predicting: 137it [01:24,  1.59it/s]Extractor Predicting: 138it [01:25,  1.59it/s]Extractor Predicting: 139it [01:26,  1.57it/s]Extractor Predicting: 140it [01:26,  1.62it/s]Extractor Predicting: 141it [01:27,  1.59it/s]Extractor Predicting: 142it [01:27,  1.68it/s]Extractor Predicting: 142it [01:27,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:20,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:20,490 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:20,490 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:20,490 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:20,490 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:36:21,098 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:36:21,099 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:36:21,680 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:36:22,714 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:36:22,714 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:25,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:25,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:25,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:25,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:25,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:36:26,215 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:36:26,216 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:36:26,795 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:36:26,975 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:36:26,975 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.21905805038335158,
  "recall": 0.0572573718866304,
  "score": 0.09078529278256924,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.70it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.53it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:10,  1.60it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:15,  1.62it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:19,  1.55it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.66it/s]Extractor Predicting: 39it [00:24,  1.64it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.65it/s]Extractor Predicting: 42it [00:25,  1.63it/s]Extractor Predicting: 43it [00:26,  1.63it/s]Extractor Predicting: 44it [00:27,  1.62it/s]Extractor Predicting: 45it [00:27,  1.61it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:28,  1.63it/s]Extractor Predicting: 48it [00:29,  1.63it/s]Extractor Predicting: 49it [00:30,  1.62it/s]Extractor Predicting: 50it [00:30,  1.60it/s]Extractor Predicting: 51it [00:31,  1.62it/s]Extractor Predicting: 52it [00:32,  1.62it/s]Extractor Predicting: 53it [00:32,  1.61it/s]Extractor Predicting: 54it [00:33,  1.61it/s]Extractor Predicting: 55it [00:33,  1.57it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:35,  1.63it/s]Extractor Predicting: 58it [00:35,  1.65it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 60it [00:36,  1.65it/s]Extractor Predicting: 61it [00:37,  1.65it/s]Extractor Predicting: 62it [00:38,  1.65it/s]Extractor Predicting: 63it [00:38,  1.64it/s]Extractor Predicting: 64it [00:39,  1.60it/s]Extractor Predicting: 65it [00:40,  1.60it/s]Extractor Predicting: 66it [00:40,  1.59it/s]Extractor Predicting: 67it [00:41,  1.61it/s]Extractor Predicting: 68it [00:41,  1.66it/s]Extractor Predicting: 69it [00:42,  1.64it/s]Extractor Predicting: 70it [00:43,  1.64it/s]Extractor Predicting: 71it [00:43,  1.62it/s]Extractor Predicting: 72it [00:44,  1.63it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:45,  1.58it/s]Extractor Predicting: 75it [00:46,  1.59it/s]Extractor Predicting: 76it [00:46,  1.60it/s]Extractor Predicting: 77it [00:47,  1.57it/s]Extractor Predicting: 78it [00:48,  1.57it/s]Extractor Predicting: 79it [00:48,  1.58it/s]Extractor Predicting: 80it [00:49,  1.58it/s]Extractor Predicting: 81it [00:50,  1.60it/s]Extractor Predicting: 82it [00:50,  1.56it/s]Extractor Predicting: 83it [00:51,  1.57it/s]Extractor Predicting: 84it [00:51,  1.58it/s]Extractor Predicting: 85it [00:52,  1.60it/s]Extractor Predicting: 86it [00:53,  1.58it/s]Extractor Predicting: 87it [00:53,  1.59it/s]Extractor Predicting: 88it [00:54,  1.55it/s]Extractor Predicting: 89it [00:55,  1.57it/s]Extractor Predicting: 90it [00:55,  1.60it/s]Extractor Predicting: 91it [00:56,  1.57it/s]Extractor Predicting: 92it [00:57,  1.57it/s]Extractor Predicting: 93it [00:57,  1.54it/s]Extractor Predicting: 94it [00:58,  1.53it/s]Extractor Predicting: 95it [00:59,  1.54it/s]Extractor Predicting: 96it [00:59,  1.54it/s]Extractor Predicting: 97it [01:00,  1.56it/s]Extractor Predicting: 98it [01:00,  1.55it/s]Extractor Predicting: 99it [01:01,  1.56it/s]Extractor Predicting: 100it [01:02,  1.57it/s]Extractor Predicting: 101it [01:02,  1.56it/s]Extractor Predicting: 102it [01:03,  1.55it/s]Extractor Predicting: 103it [01:04,  1.57it/s]Extractor Predicting: 104it [01:04,  1.56it/s]Extractor Predicting: 105it [01:05,  1.55it/s]Extractor Predicting: 106it [01:06,  1.56it/s]Extractor Predicting: 107it [01:06,  1.54it/s]Extractor Predicting: 108it [01:07,  1.39it/s]Extractor Predicting: 109it [01:08,  1.44it/s]Extractor Predicting: 110it [01:08,  1.45it/s]Extractor Predicting: 111it [01:09,  1.47it/s]Extractor Predicting: 112it [01:10,  1.53it/s]Extractor Predicting: 113it [01:10,  1.56it/s]Extractor Predicting: 114it [01:11,  1.57it/s]Extractor Predicting: 115it [01:12,  1.58it/s]Extractor Predicting: 116it [01:12,  1.58it/s]Extractor Predicting: 117it [01:13,  1.60it/s]Extractor Predicting: 118it [01:13,  1.62it/s]Extractor Predicting: 119it [01:14,  1.62it/s]Extractor Predicting: 120it [01:15,  1.63it/s]Extractor Predicting: 121it [01:15,  1.63it/s]Extractor Predicting: 122it [01:16,  1.67it/s]Extractor Predicting: 123it [01:16,  1.66it/s]Extractor Predicting: 124it [01:17,  1.66it/s]Extractor Predicting: 125it [01:18,  1.65it/s]Extractor Predicting: 126it [01:18,  1.62it/s]Extractor Predicting: 127it [01:19,  1.62it/s]Extractor Predicting: 128it [01:20,  1.63it/s]Extractor Predicting: 129it [01:20,  1.60it/s]Extractor Predicting: 130it [01:21,  1.62it/s]Extractor Predicting: 131it [01:21,  1.59it/s]Extractor Predicting: 132it [01:22,  1.60it/s]Extractor Predicting: 133it [01:23,  1.60it/s]Extractor Predicting: 134it [01:23,  1.61it/s]Extractor Predicting: 135it [01:24,  1.65it/s]Extractor Predicting: 136it [01:24,  1.64it/s]Extractor Predicting: 137it [01:25,  1.62it/s]Extractor Predicting: 138it [01:26,  1.63it/s]Extractor Predicting: 139it [01:26,  1.66it/s]Extractor Predicting: 140it [01:27,  1.63it/s]Extractor Predicting: 141it [01:28,  1.61it/s]Extractor Predicting: 142it [01:28,  1.66it/s]Extractor Predicting: 143it [01:29,  1.64it/s]Extractor Predicting: 144it [01:29,  1.61it/s]Extractor Predicting: 145it [01:30,  1.62it/s]Extractor Predicting: 146it [01:31,  1.62it/s]Extractor Predicting: 147it [01:31,  1.59it/s]Extractor Predicting: 148it [01:32,  1.60it/s]Extractor Predicting: 149it [01:33,  1.59it/s]Extractor Predicting: 150it [01:33,  1.60it/s]Extractor Predicting: 151it [01:34,  1.63it/s]Extractor Predicting: 152it [01:34,  1.65it/s]Extractor Predicting: 153it [01:35,  1.64it/s]Extractor Predicting: 154it [01:36,  1.62it/s]Extractor Predicting: 155it [01:36,  1.61it/s]Extractor Predicting: 156it [01:37,  1.60it/s]Extractor Predicting: 157it [01:38,  1.57it/s]Extractor Predicting: 158it [01:38,  1.57it/s]Extractor Predicting: 159it [01:39,  1.57it/s]Extractor Predicting: 160it [01:39,  1.58it/s]Extractor Predicting: 161it [01:40,  1.61it/s]Extractor Predicting: 162it [01:41,  1.62it/s]Extractor Predicting: 163it [01:41,  1.61it/s]Extractor Predicting: 164it [01:42,  1.62it/s]Extractor Predicting: 165it [01:42,  1.59it/s]Extractor Predicting: 166it [01:43,  1.60it/s]Extractor Predicting: 167it [01:44,  1.63it/s]Extractor Predicting: 168it [01:44,  1.62it/s]Extractor Predicting: 169it [01:45,  1.61it/s]Extractor Predicting: 170it [01:46,  1.58it/s]Extractor Predicting: 171it [01:46,  1.55it/s]Extractor Predicting: 172it [01:47,  1.56it/s]Extractor Predicting: 173it [01:48,  1.56it/s]Extractor Predicting: 174it [01:48,  1.52it/s]Extractor Predicting: 175it [01:49,  1.48it/s]Extractor Predicting: 176it [01:50,  1.51it/s]Extractor Predicting: 177it [01:50,  1.51it/s]Extractor Predicting: 178it [01:51,  1.56it/s]Extractor Predicting: 179it [01:52,  1.55it/s]Extractor Predicting: 180it [01:52,  1.60it/s]Extractor Predicting: 181it [01:53,  1.58it/s]Extractor Predicting: 182it [01:53,  1.61it/s]Extractor Predicting: 183it [01:54,  1.60it/s]Extractor Predicting: 184it [01:55,  1.63it/s]Extractor Predicting: 185it [01:55,  1.65it/s]Extractor Predicting: 186it [01:56,  1.66it/s]Extractor Predicting: 187it [01:56,  1.67it/s]Extractor Predicting: 188it [01:57,  1.65it/s]Extractor Predicting: 189it [01:58,  1.65it/s]Extractor Predicting: 190it [01:58,  1.63it/s]Extractor Predicting: 191it [01:59,  1.57it/s]Extractor Predicting: 192it [01:59,  1.60it/s]Extractor Predicting: 193it [02:00,  1.64it/s]Extractor Predicting: 194it [02:01,  1.63it/s]Extractor Predicting: 195it [02:01,  1.63it/s]Extractor Predicting: 196it [02:02,  1.65it/s]Extractor Predicting: 197it [02:02,  1.66it/s]Extractor Predicting: 198it [02:03,  1.62it/s]Extractor Predicting: 199it [02:04,  1.63it/s]Extractor Predicting: 200it [02:04,  1.62it/s]Extractor Predicting: 201it [02:05,  1.47it/s]Extractor Predicting: 202it [02:06,  1.55it/s]Extractor Predicting: 203it [02:06,  1.59it/s]Extractor Predicting: 204it [02:07,  1.60it/s]Extractor Predicting: 205it [02:08,  1.57it/s]Extractor Predicting: 206it [02:08,  1.57it/s]Extractor Predicting: 207it [02:09,  1.61it/s]Extractor Predicting: 208it [02:09,  1.62it/s]Extractor Predicting: 209it [02:10,  1.57it/s]Extractor Predicting: 210it [02:11,  1.56it/s]Extractor Predicting: 211it [02:11,  1.57it/s]Extractor Predicting: 212it [02:12,  1.59it/s]Extractor Predicting: 213it [02:13,  1.61it/s]Extractor Predicting: 214it [02:13,  1.56it/s]Extractor Predicting: 215it [02:14,  1.57it/s]Extractor Predicting: 216it [02:15,  1.60it/s]Extractor Predicting: 217it [02:15,  1.62it/s]Extractor Predicting: 218it [02:16,  1.56it/s]Extractor Predicting: 219it [02:16,  1.57it/s]Extractor Predicting: 220it [02:17,  1.58it/s]Extractor Predicting: 221it [02:18,  1.54it/s]Extractor Predicting: 222it [02:18,  1.54it/s]Extractor Predicting: 223it [02:19,  1.55it/s]Extractor Predicting: 224it [02:20,  1.59it/s]Extractor Predicting: 225it [02:20,  1.59it/s]Extractor Predicting: 226it [02:21,  1.63it/s]Extractor Predicting: 227it [02:21,  1.66it/s]Extractor Predicting: 228it [02:22,  1.63it/s]Extractor Predicting: 229it [02:23,  1.64it/s]Extractor Predicting: 230it [02:23,  1.60it/s]Extractor Predicting: 231it [02:24,  1.59it/s]Extractor Predicting: 232it [02:25,  1.59it/s]Extractor Predicting: 233it [02:25,  1.64it/s]Extractor Predicting: 234it [02:26,  1.62it/s]Extractor Predicting: 235it [02:26,  1.63it/s]Extractor Predicting: 236it [02:27,  1.60it/s]Extractor Predicting: 237it [02:28,  1.59it/s]Extractor Predicting: 238it [02:28,  1.62it/s]Extractor Predicting: 239it [02:29,  1.64it/s]Extractor Predicting: 240it [02:30,  1.62it/s]Extractor Predicting: 241it [02:30,  1.61it/s]Extractor Predicting: 242it [02:31,  1.59it/s]Extractor Predicting: 243it [02:31,  1.56it/s]Extractor Predicting: 244it [02:32,  1.57it/s]Extractor Predicting: 245it [02:33,  1.62it/s]Extractor Predicting: 246it [02:33,  1.59it/s]Extractor Predicting: 247it [02:34,  1.62it/s]Extractor Predicting: 248it [02:35,  1.62it/s]Extractor Predicting: 249it [02:35,  1.62it/s]Extractor Predicting: 250it [02:36,  1.61it/s]Extractor Predicting: 251it [02:36,  1.57it/s]Extractor Predicting: 252it [02:37,  1.58it/s]Extractor Predicting: 253it [02:38,  1.59it/s]Extractor Predicting: 254it [02:38,  1.60it/s]Extractor Predicting: 255it [02:39,  1.60it/s]Extractor Predicting: 256it [02:40,  1.58it/s]Extractor Predicting: 257it [02:40,  1.60it/s]Extractor Predicting: 258it [02:41,  1.59it/s]Extractor Predicting: 259it [02:42,  1.55it/s]Extractor Predicting: 260it [02:42,  1.57it/s]Extractor Predicting: 261it [02:43,  1.59it/s]Extractor Predicting: 262it [02:43,  1.57it/s]Extractor Predicting: 263it [02:44,  1.56it/s]Extractor Predicting: 264it [02:45,  1.56it/s]Extractor Predicting: 265it [02:45,  1.55it/s]Extractor Predicting: 266it [02:46,  1.53it/s]Extractor Predicting: 267it [02:47,  1.51it/s]Extractor Predicting: 268it [02:47,  1.53it/s]Extractor Predicting: 269it [02:48,  1.52it/s]Extractor Predicting: 270it [02:49,  1.51it/s]Extractor Predicting: 271it [02:49,  1.53it/s]Extractor Predicting: 272it [02:50,  1.53it/s]Extractor Predicting: 273it [02:51,  1.53it/s]Extractor Predicting: 274it [02:51,  1.39it/s]Extractor Predicting: 275it [02:52,  1.45it/s]Extractor Predicting: 276it [02:53,  1.48it/s]Extractor Predicting: 277it [02:53,  1.49it/s]Extractor Predicting: 278it [02:54,  1.51it/s]Extractor Predicting: 279it [02:55,  1.51it/s]Extractor Predicting: 280it [02:55,  1.51it/s]Extractor Predicting: 281it [02:56,  1.51it/s]Extractor Predicting: 282it [02:57,  1.54it/s]Extractor Predicting: 283it [02:57,  1.49it/s]Extractor Predicting: 284it [02:58,  1.48it/s]Extractor Predicting: 285it [02:59,  1.48it/s]Extractor Predicting: 286it [02:59,  1.48it/s]Extractor Predicting: 287it [03:00,  1.50it/s]Extractor Predicting: 288it [03:00,  1.99it/s]Extractor Predicting: 288it [03:00,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:35,384 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:35,386 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:35,386 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:35,386 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:35,386 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:39:35,990 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:39:35,991 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:39:36,554 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:39:37,613 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:39:37,615 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:40,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:40,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:40,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:40,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:40,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:39:41,262 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:39:41,263 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:39:41,906 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:39:42,083 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:39:42,083 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5506382978723404,
  "recall": 0.09391784003483815,
  "score": 0.16046626984126985,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:01,  1.94it/s]Extractor Predicting: 3it [00:01,  1.76it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:39:44,175 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:39:44,176 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:39:44,180 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:39:44,180 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:39:44,183 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:39:47,378 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:39:47,385 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:39:47,421 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:39:47,422 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:39:47,439 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:39:47,443 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:39:47,443 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:39:47,443 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:39:47,443 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:39:47,443 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:39:47,443 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8,
  "recall": 0.036036036036036036,
  "score": 0.06896551724137931,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:39:47,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:48,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:48,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:49,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:50,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:50,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:51,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:51,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:52,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:53,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:53,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:54,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:54,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:55,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:56,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:56,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:57,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:57,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:58,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:58,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:59,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:00,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:00,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:12, 13.72s/it][WARNING|generation_utils.py:914] 2023-08-28 14:40:01,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:02,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:02,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:03,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:03,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:04,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:04,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:05,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:06,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:06,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:07,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:07,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:08,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:09,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:09,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:10,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:10,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:11,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:12,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:12,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:13,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:13,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:26<02:53, 13.37s/it][WARNING|generation_utils.py:914] 2023-08-28 14:40:14,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:15,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:16,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:16,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:17,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:17,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:18,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:18,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:19,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:19,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:20,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:21,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:21,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:22,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:22,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:23,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:23,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:24,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:25,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:25,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:26,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:26,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:27,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:28,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:40<02:44, 13.70s/it][WARNING|generation_utils.py:914] 2023-08-28 14:40:28,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:29,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:29,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:30,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:31,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:31,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:32,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:32,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:33,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:34,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:34,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:35,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:36,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:36,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:37,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:38,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:38,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:39,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:39,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:40,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:41,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:41,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:54<02:29, 13.62s/it][WARNING|generation_utils.py:914] 2023-08-28 14:40:42,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:42,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:43,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:43,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:44,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:44,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:45,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:46,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:46,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:47,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:47,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:48,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:48,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:49,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:49,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:50,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:50,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:51,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:52,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:52,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:53,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:53,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:54,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:54,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:07<02:14, 13.43s/it][WARNING|generation_utils.py:914] 2023-08-28 14:40:55,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:55,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:56,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:57,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:57,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:58,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:59,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:40:59,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:00,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:00,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:01,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:02,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:02,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:03,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:04,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:04,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:05,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:05,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:06,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:07,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:07,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:08,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:09,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:22<02:04, 13.80s/it][WARNING|generation_utils.py:914] 2023-08-28 14:41:09,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:10,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:11,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:11,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:12,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:13,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:13,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:14,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:14,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:15,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:16,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:16,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:17,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:18,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:18,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:19,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:19,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:20,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:21,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:21,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:22,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:23,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:35<01:50, 13.83s/it][WARNING|generation_utils.py:914] 2023-08-28 14:41:23,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:24,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:24,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:25,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:26,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:26,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:27,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:28,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:28,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:29,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:29,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:30,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:31,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:31,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:32,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:33,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:33,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:34,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:35,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:35,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:36,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:36,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:37,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:38,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:38,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:39,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:52<01:42, 14.69s/it][WARNING|generation_utils.py:914] 2023-08-28 14:41:40,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:40,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:41,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:41,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:42,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:42,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:43,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:43,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:44,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:44,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:45,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:45,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:46,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:46,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:47,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:48,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:48,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:49,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:49,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:50,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:50,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:51,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:52,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:04<01:23, 13.95s/it][WARNING|generation_utils.py:914] 2023-08-28 14:41:52,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:53,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:53,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:54,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:54,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:55,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:56,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:56,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:57,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:57,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:58,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:59,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:41:59,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:00,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:00,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:01,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:02,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:02,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:03,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:04,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:04,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:05,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:18<01:09, 13.90s/it][WARNING|generation_utils.py:914] 2023-08-28 14:42:06,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:06,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:07,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:08,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:08,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:09,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:09,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:10,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:11,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:11,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:12,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:12,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:13,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:14,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:14,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:15,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:16,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:16,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:17,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:18,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:18,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:31<00:54, 13.61s/it][WARNING|generation_utils.py:914] 2023-08-28 14:42:19,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:19,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:20,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:20,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:21,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:22,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:22,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:23,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:23,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:24,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:25,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:25,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:26,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:26,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:27,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:27,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:28,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:29,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:29,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:30,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:30,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:31,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:32,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:44<00:40, 13.56s/it][WARNING|generation_utils.py:914] 2023-08-28 14:42:32,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:33,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:34,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:34,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:35,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:35,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:36,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:37,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:37,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:38,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:38,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:39,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:40,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:40,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:41,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:41,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:42,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:43,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:43,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:44,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:44,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:45,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:58<00:27, 13.56s/it][WARNING|generation_utils.py:914] 2023-08-28 14:42:46,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:46,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:47,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:47,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:48,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:49,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:49,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:50,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:50,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:51,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:52,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:52,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:53,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:53,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:54,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:54,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:55,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:56,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:56,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:57,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:57,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:58,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:11<00:13, 13.27s/it][WARNING|generation_utils.py:914] 2023-08-28 14:42:58,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:42:59,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:00,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:00,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:01,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:01,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:02,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:03,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:03,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:04,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:04,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:05,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:05,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:06,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:07,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:07,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:08,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:08,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:09,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:10,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:11,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:11,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:12,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:12,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:13,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:14,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:14,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:15,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:15,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:16,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:16,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:17,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:18,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:18,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:19,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:32<00:00, 15.63s/it]Generating: 100%|██████████| 15/15 [03:32<00:00, 14.15s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:26,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:26,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:26,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:26,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:26,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:43:27,010 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:43:27,012 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:43:27,593 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:43:28,667 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:43:28,667 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:31,519 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:31,523 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:31,523 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:31,523 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:31,523 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:43:32,283 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:43:32,284 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:43:32,878 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:43:33,054 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:43:33,054 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8863636363636364, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8111979166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 589, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.796875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : architect . Context : Later in the year , the school built a high school for children of French architecture at the end of the school year , also at the end of the school year . Head Entity : high school for children , Tail Entity : French architects .\n']
['Relation : architect . Context : Later in the year , the school built a high school for children of French architecture at the end of the school year , also at the end of the school year . Head Entity : high school for children , Tail Entity : French architects .\n', "Relation : architect . Context : Erskine was built around 1549 , the only surviving building - built building in the district to feature the house 's original design . Head Entity : constructed , Tail Entity : Erskine .\n"]
['Relation : architect . Context : Later in the year , the school built a high school for children of French architecture at the end of the school year , also at the end of the school year . Head Entity : high school for children , Tail Entity : French architects .\n', "Relation : architect . Context : Erskine was built around 1549 , the only surviving building - built building in the district to feature the house 's original design . Head Entity : constructed , Tail Entity : Erskine .\n", 'Relation : architect . Context : This was the first studio in which the architects had worked on sculptures , including the first of the Daguerreotype works by the 18th - century architect Johann Wolfgang von Goethe . Head Entity : 17th - century , Tail Entity : Johann Wolfgang von Goethe .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : architect .', 'success_rate': 0.8301630434782609, 'errors': {''}}
['Relation : contains administrative territorial entity . Context : The city of Stuttgart is under the rule of the German state of Saxony . Head Entity : Struttgart , Tail Entity : Saxony .\n']
['Relation : contains administrative territorial entity . Context : The city of Stuttgart is under the rule of the German state of Saxony . Head Entity : Struttgart , Tail Entity : Saxony .\n', "Relation : contains administrative territorial entity . Context : Eureka Park is part of the State Forestry Authority 's ( SFA ) Rural Area Plan to manage the conservation and health of Eureka Park . Head Entity : State Forestry Authority , Tail Entity : State in South Carolina .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8536931818181818, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 478, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 622, 'raw': 832}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7475961538461539, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : developer .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : follows .', 'success_rate': 0.8863636363636364, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8678977272727273, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : original broadcaster . Context : Later in the year , the station aired a documentary called The Three Kingdoms , entitled " The Three Kingdoms Chronicles " . Head Entity : The Three Kingdoms Chronicles , Tail Entity : The Three Kingdoms Network .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8849431818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : On 31 March 2014 , the Brazilian national squad returned to their squad for the 2018 Copa Centurio , in a 1–0 win against Real Betis . Head Entity : Marcelo Bamba , Tail Entity : defender .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 35, 'raw': 64}
{'target': 600, 'success': 54, 'raw': 96}
{'target': 600, 'success': 67, 'raw': 128}
{'target': 600, 'success': 85, 'raw': 160}
{'target': 600, 'success': 103, 'raw': 192}
{'target': 600, 'success': 118, 'raw': 224}
{'target': 600, 'success': 134, 'raw': 256}
{'target': 600, 'success': 149, 'raw': 288}
{'target': 600, 'success': 169, 'raw': 320}
{'target': 600, 'success': 187, 'raw': 352}
{'target': 600, 'success': 200, 'raw': 384}
{'target': 600, 'success': 217, 'raw': 416}
{'target': 600, 'success': 239, 'raw': 448}
{'target': 600, 'success': 258, 'raw': 480}
{'target': 600, 'success': 276, 'raw': 512}
{'target': 600, 'success': 296, 'raw': 544}
{'target': 600, 'success': 314, 'raw': 576}
{'target': 600, 'success': 330, 'raw': 608}
{'target': 600, 'success': 347, 'raw': 640}
{'target': 600, 'success': 365, 'raw': 672}
{'target': 600, 'success': 382, 'raw': 704}
{'target': 600, 'success': 402, 'raw': 736}
{'target': 600, 'success': 422, 'raw': 768}
{'target': 600, 'success': 438, 'raw': 800}
{'target': 600, 'success': 450, 'raw': 832}
{'target': 600, 'success': 465, 'raw': 864}
{'target': 600, 'success': 488, 'raw': 896}
{'target': 600, 'success': 502, 'raw': 928}
{'target': 600, 'success': 520, 'raw': 960}
{'target': 600, 'success': 537, 'raw': 992}
{'target': 600, 'success': 555, 'raw': 1024}
{'target': 600, 'success': 572, 'raw': 1056}
{'target': 600, 'success': 590, 'raw': 1088}
{'target': 600, 'success': 611, 'raw': 1120}
{'prompt': 'Relation : position held .', 'success_rate': 0.5455357142857142, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 12840
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12940, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.60it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:01,  1.54it/s]Extractor Estimating: 4it [00:02,  1.62it/s]Extractor Estimating: 5it [00:03,  1.65it/s]Extractor Estimating: 6it [00:03,  1.57it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.63it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.61it/s]Extractor Estimating: 12it [00:07,  1.64it/s]Extractor Estimating: 13it [00:08,  1.63it/s]Extractor Estimating: 14it [00:08,  1.64it/s]Extractor Estimating: 15it [00:09,  1.60it/s]Extractor Estimating: 16it [00:09,  1.61it/s]Extractor Estimating: 17it [00:10,  1.60it/s]Extractor Estimating: 18it [00:11,  1.60it/s]Extractor Estimating: 19it [00:11,  1.66it/s]Extractor Estimating: 20it [00:12,  1.66it/s]Extractor Estimating: 21it [00:13,  1.64it/s]Extractor Estimating: 22it [00:13,  1.52it/s]Extractor Estimating: 23it [00:14,  1.59it/s]Extractor Estimating: 24it [00:14,  1.61it/s]Extractor Estimating: 25it [00:15,  1.67it/s]Extractor Estimating: 26it [00:16,  1.66it/s]Extractor Estimating: 27it [00:16,  1.65it/s]Extractor Estimating: 28it [00:17,  1.64it/s]Extractor Estimating: 29it [00:17,  1.74it/s]Extractor Estimating: 30it [00:18,  1.75it/s]Extractor Estimating: 31it [00:18,  1.78it/s]Extractor Estimating: 32it [00:19,  1.81it/s]Extractor Estimating: 33it [00:20,  1.79it/s]Extractor Estimating: 34it [00:20,  1.83it/s]Extractor Estimating: 35it [00:21,  1.76it/s]Extractor Estimating: 36it [00:21,  1.77it/s]Extractor Estimating: 37it [00:22,  1.73it/s]Extractor Estimating: 38it [00:22,  1.82it/s]Extractor Estimating: 39it [00:23,  1.70it/s]Extractor Estimating: 40it [00:25,  1.04it/s]Extractor Estimating: 41it [00:25,  1.16it/s]Extractor Estimating: 42it [00:26,  1.29it/s]Extractor Estimating: 43it [00:27,  1.37it/s]Extractor Estimating: 44it [00:27,  1.44it/s]Extractor Estimating: 45it [00:28,  1.52it/s]Extractor Estimating: 46it [00:28,  1.56it/s]Extractor Estimating: 47it [00:29,  1.64it/s]Extractor Estimating: 48it [00:30,  1.59it/s]Extractor Estimating: 49it [00:30,  1.63it/s]Extractor Estimating: 50it [00:31,  1.61it/s]Extractor Estimating: 51it [00:31,  1.61it/s]Extractor Estimating: 52it [00:32,  1.54it/s]Extractor Estimating: 53it [00:33,  1.54it/s]Extractor Estimating: 54it [00:33,  1.55it/s]Extractor Estimating: 55it [00:34,  1.60it/s]Extractor Estimating: 56it [00:35,  1.66it/s]Extractor Estimating: 57it [00:35,  1.63it/s]Extractor Estimating: 58it [00:36,  1.63it/s]Extractor Estimating: 59it [00:36,  1.62it/s]Extractor Estimating: 60it [00:37,  1.60it/s]Extractor Estimating: 61it [00:38,  1.60it/s]Extractor Estimating: 62it [00:38,  1.61it/s]Extractor Estimating: 63it [00:39,  1.59it/s]Extractor Estimating: 64it [00:40,  1.59it/s]Extractor Estimating: 65it [00:40,  1.58it/s]Extractor Estimating: 66it [00:41,  1.62it/s]Extractor Estimating: 67it [00:41,  1.61it/s]Extractor Estimating: 68it [00:42,  1.64it/s]Extractor Estimating: 69it [00:43,  1.61it/s]Extractor Estimating: 70it [00:43,  1.60it/s]Extractor Estimating: 71it [00:44,  1.60it/s]Extractor Estimating: 72it [00:45,  1.56it/s]Extractor Estimating: 73it [00:45,  1.52it/s]Extractor Estimating: 74it [00:46,  1.52it/s]Extractor Estimating: 75it [00:47,  1.54it/s]Extractor Estimating: 76it [00:47,  1.60it/s]Extractor Estimating: 77it [00:48,  1.66it/s]Extractor Estimating: 78it [00:48,  1.70it/s]Extractor Estimating: 79it [00:49,  1.66it/s]Extractor Estimating: 80it [00:50,  1.65it/s]Extractor Estimating: 81it [00:50,  1.72it/s]Extractor Estimating: 82it [00:51,  1.74it/s]Extractor Estimating: 83it [00:51,  1.74it/s]Extractor Estimating: 84it [00:52,  1.75it/s]Extractor Estimating: 85it [00:52,  1.79it/s]Extractor Estimating: 86it [00:53,  1.85it/s]Extractor Estimating: 87it [00:53,  1.79it/s]Extractor Estimating: 88it [00:54,  1.60it/s]Extractor Estimating: 89it [00:55,  1.67it/s]Extractor Estimating: 90it [00:55,  1.72it/s]Extractor Estimating: 91it [00:56,  1.78it/s]Extractor Estimating: 92it [00:56,  1.81it/s]Extractor Estimating: 93it [00:57,  1.78it/s]Extractor Estimating: 94it [00:57,  1.78it/s]Extractor Estimating: 95it [00:58,  1.79it/s]Extractor Estimating: 96it [00:59,  1.75it/s]Extractor Estimating: 97it [00:59,  1.82it/s]Extractor Estimating: 98it [01:00,  1.76it/s]Extractor Estimating: 99it [01:00,  1.76it/s]Extractor Estimating: 100it [01:01,  1.81it/s]Extractor Estimating: 101it [01:01,  1.87it/s]Extractor Estimating: 102it [01:02,  1.81it/s]Extractor Estimating: 103it [01:02,  1.85it/s]Extractor Estimating: 104it [01:03,  1.83it/s]Extractor Estimating: 105it [01:04,  1.85it/s]Extractor Estimating: 106it [01:04,  1.87it/s]Extractor Estimating: 107it [01:05,  1.89it/s]Extractor Estimating: 108it [01:05,  1.90it/s]Extractor Estimating: 109it [01:06,  1.93it/s]Extractor Estimating: 110it [01:06,  1.90it/s]Extractor Estimating: 111it [01:07,  2.00it/s]Extractor Estimating: 112it [01:07,  1.97it/s]Extractor Estimating: 113it [01:08,  1.93it/s]Extractor Estimating: 114it [01:08,  1.93it/s]Extractor Estimating: 115it [01:09,  1.87it/s]Extractor Estimating: 116it [01:09,  1.87it/s]Extractor Estimating: 117it [01:10,  1.90it/s]Extractor Estimating: 118it [01:10,  1.91it/s]Extractor Estimating: 119it [01:11,  1.86it/s]Extractor Estimating: 120it [01:11,  1.85it/s]Extractor Estimating: 121it [01:12,  1.85it/s]Extractor Estimating: 122it [01:13,  1.81it/s]Extractor Estimating: 123it [01:13,  1.83it/s]Extractor Estimating: 124it [01:14,  1.86it/s]Extractor Estimating: 125it [01:14,  1.84it/s]Extractor Estimating: 126it [01:15,  1.79it/s]Extractor Estimating: 127it [01:15,  1.76it/s]Extractor Estimating: 128it [01:16,  1.76it/s]Extractor Estimating: 129it [01:16,  1.82it/s]Extractor Estimating: 130it [01:17,  1.75it/s]Extractor Estimating: 131it [01:18,  1.76it/s]Extractor Estimating: 132it [01:18,  1.73it/s]Extractor Estimating: 133it [01:19,  1.71it/s]Extractor Estimating: 134it [01:19,  1.72it/s]Extractor Estimating: 135it [01:20,  1.72it/s]Extractor Estimating: 136it [01:20,  1.74it/s]Extractor Estimating: 137it [01:21,  1.73it/s]Extractor Estimating: 138it [01:22,  1.70it/s]Extractor Estimating: 139it [01:22,  1.72it/s]Extractor Estimating: 140it [01:23,  1.69it/s]Extractor Estimating: 141it [01:23,  1.69it/s]Extractor Estimating: 142it [01:24,  1.71it/s]Extractor Estimating: 143it [01:25,  1.75it/s]Extractor Estimating: 144it [01:25,  1.68it/s]Extractor Estimating: 145it [01:26,  1.63it/s]Extractor Estimating: 146it [01:26,  1.63it/s]Extractor Estimating: 147it [01:27,  1.65it/s]Extractor Estimating: 148it [01:28,  1.62it/s]Extractor Estimating: 149it [01:28,  1.60it/s]Extractor Estimating: 150it [01:29,  1.62it/s]Extractor Estimating: 151it [01:30,  1.63it/s]Extractor Estimating: 152it [01:30,  1.63it/s]Extractor Estimating: 153it [01:31,  1.72it/s]Extractor Estimating: 154it [01:31,  1.57it/s]Extractor Estimating: 155it [01:32,  1.65it/s]Extractor Estimating: 156it [01:33,  1.69it/s]Extractor Estimating: 157it [01:33,  1.63it/s]Extractor Estimating: 158it [01:34,  1.68it/s]Extractor Estimating: 159it [01:34,  1.71it/s]Extractor Estimating: 160it [01:35,  1.77it/s]Extractor Estimating: 161it [01:35,  1.76it/s]Extractor Estimating: 162it [01:36,  1.77it/s]Extractor Estimating: 163it [01:37,  1.78it/s]Extractor Estimating: 164it [01:37,  1.75it/s]Extractor Estimating: 165it [01:38,  1.73it/s]Extractor Estimating: 166it [01:38,  1.76it/s]Extractor Estimating: 167it [01:39,  1.80it/s]Extractor Estimating: 168it [01:39,  1.81it/s]Extractor Estimating: 169it [01:40,  1.77it/s]Extractor Estimating: 170it [01:41,  1.73it/s]Extractor Estimating: 171it [01:41,  1.77it/s]Extractor Estimating: 172it [01:42,  1.70it/s]Extractor Estimating: 173it [01:42,  1.72it/s]Extractor Estimating: 174it [01:43,  1.74it/s]Extractor Estimating: 175it [01:43,  1.64it/s]Extractor Estimating: 176it [01:44,  1.71it/s]Extractor Estimating: 177it [01:45,  1.71it/s]Extractor Estimating: 178it [01:45,  1.67it/s]Extractor Estimating: 179it [01:46,  1.61it/s]Extractor Estimating: 180it [01:46,  1.70it/s]Extractor Estimating: 181it [01:47,  1.63it/s]Extractor Estimating: 182it [01:48,  1.67it/s]Extractor Estimating: 183it [01:48,  1.69it/s]Extractor Estimating: 184it [01:49,  1.70it/s]Extractor Estimating: 185it [01:49,  1.70it/s]Extractor Estimating: 186it [01:50,  1.68it/s]Extractor Estimating: 187it [01:51,  1.71it/s]Extractor Estimating: 188it [01:51,  1.64it/s]Extractor Estimating: 189it [01:52,  1.76it/s]Extractor Estimating: 190it [01:52,  1.75it/s]Extractor Estimating: 191it [01:53,  1.74it/s]Extractor Estimating: 192it [01:53,  1.75it/s]Extractor Estimating: 193it [01:54,  1.76it/s]Extractor Estimating: 194it [01:55,  1.77it/s]Extractor Estimating: 195it [01:55,  1.72it/s]Extractor Estimating: 196it [01:56,  1.76it/s]Extractor Estimating: 197it [01:56,  1.75it/s]Extractor Estimating: 198it [01:57,  1.67it/s]Extractor Estimating: 199it [01:58,  1.71it/s]Extractor Estimating: 200it [01:58,  1.69it/s]Extractor Estimating: 201it [01:59,  1.69it/s]Extractor Estimating: 202it [01:59,  1.70it/s]Extractor Estimating: 203it [02:00,  1.67it/s]Extractor Estimating: 204it [02:01,  1.64it/s]Extractor Estimating: 205it [02:01,  1.64it/s]Extractor Estimating: 206it [02:02,  1.70it/s]Extractor Estimating: 207it [02:02,  1.74it/s]Extractor Estimating: 208it [02:03,  1.75it/s]Extractor Estimating: 209it [02:03,  1.71it/s]Extractor Estimating: 210it [02:04,  1.64it/s]Extractor Estimating: 211it [02:05,  1.62it/s]Extractor Estimating: 212it [02:05,  1.62it/s]Extractor Estimating: 213it [02:06,  1.65it/s]Extractor Estimating: 214it [02:07,  1.67it/s]Extractor Estimating: 215it [02:07,  1.67it/s]Extractor Estimating: 216it [02:08,  1.70it/s]Extractor Estimating: 217it [02:08,  1.70it/s]Extractor Estimating: 218it [02:09,  1.59it/s]Extractor Estimating: 219it [02:10,  1.58it/s]Extractor Estimating: 220it [02:10,  1.63it/s]Extractor Estimating: 221it [02:11,  1.64it/s]Extractor Estimating: 222it [02:11,  1.67it/s]Extractor Estimating: 223it [02:12,  1.63it/s]Extractor Estimating: 224it [02:13,  1.62it/s]Extractor Estimating: 225it [02:13,  1.67it/s]Extractor Estimating: 226it [02:14,  1.63it/s]Extractor Estimating: 227it [02:15,  1.57it/s]Extractor Estimating: 228it [02:15,  1.53it/s]Extractor Estimating: 229it [02:16,  1.57it/s]Extractor Estimating: 230it [02:17,  1.52it/s]Extractor Estimating: 231it [02:17,  1.58it/s]Extractor Estimating: 232it [02:18,  1.59it/s]Extractor Estimating: 233it [02:18,  1.57it/s]Extractor Estimating: 234it [02:19,  1.55it/s]Extractor Estimating: 235it [02:20,  1.56it/s]Extractor Estimating: 236it [02:20,  1.58it/s]Extractor Estimating: 237it [02:21,  1.61it/s]Extractor Estimating: 238it [02:22,  1.58it/s]Extractor Estimating: 239it [02:22,  1.56it/s]Extractor Estimating: 240it [02:23,  1.56it/s]Extractor Estimating: 241it [02:23,  1.55it/s]Extractor Estimating: 242it [02:24,  1.52it/s]Extractor Estimating: 243it [02:25,  1.45it/s]Extractor Estimating: 244it [02:26,  1.50it/s]Extractor Estimating: 245it [02:26,  1.60it/s]Extractor Estimating: 246it [02:27,  1.60it/s]Extractor Estimating: 247it [02:27,  1.56it/s]Extractor Estimating: 248it [02:28,  1.56it/s]Extractor Estimating: 249it [02:29,  1.55it/s]Extractor Estimating: 250it [02:29,  1.47it/s]Extractor Estimating: 251it [02:30,  1.62it/s]Extractor Estimating: 252it [02:30,  1.68it/s]Extractor Estimating: 253it [02:31,  1.70it/s]Extractor Estimating: 254it [02:32,  1.75it/s]Extractor Estimating: 255it [02:32,  1.65it/s]Extractor Estimating: 256it [02:33,  1.78it/s]Extractor Estimating: 257it [02:33,  1.87it/s]Extractor Estimating: 258it [02:34,  1.87it/s]Extractor Estimating: 259it [02:34,  1.79it/s]Extractor Estimating: 260it [02:35,  1.81it/s]Extractor Estimating: 261it [02:35,  1.82it/s]Extractor Estimating: 262it [02:36,  1.85it/s]Extractor Estimating: 263it [02:36,  1.84it/s]Extractor Estimating: 264it [02:37,  1.80it/s]Extractor Estimating: 265it [02:38,  1.85it/s]Extractor Estimating: 266it [02:38,  1.90it/s]Extractor Estimating: 267it [02:39,  1.90it/s]Extractor Estimating: 268it [02:39,  1.94it/s]Extractor Estimating: 269it [02:40,  1.89it/s]Extractor Estimating: 270it [02:40,  1.93it/s]Extractor Estimating: 271it [02:41,  1.92it/s]Extractor Estimating: 272it [02:41,  1.88it/s]Extractor Estimating: 273it [02:42,  1.82it/s]Extractor Estimating: 274it [02:42,  1.80it/s]Extractor Estimating: 275it [02:43,  1.82it/s]Extractor Estimating: 276it [02:43,  1.79it/s]Extractor Estimating: 277it [02:44,  1.76it/s]Extractor Estimating: 278it [02:45,  1.74it/s]Extractor Estimating: 279it [02:45,  1.73it/s]Extractor Estimating: 280it [02:46,  1.75it/s]Extractor Estimating: 281it [02:46,  1.76it/s]Extractor Estimating: 282it [02:47,  1.68it/s]Extractor Estimating: 283it [02:48,  1.71it/s]Extractor Estimating: 284it [02:48,  1.67it/s]Extractor Estimating: 285it [02:49,  1.63it/s]Extractor Estimating: 286it [02:49,  1.63it/s]Extractor Estimating: 287it [02:50,  1.63it/s]Extractor Estimating: 288it [02:51,  1.64it/s]Extractor Estimating: 289it [02:51,  1.66it/s]Extractor Estimating: 290it [02:52,  1.73it/s]Extractor Estimating: 291it [02:52,  1.71it/s]Extractor Estimating: 292it [02:53,  1.70it/s]Extractor Estimating: 293it [02:54,  1.63it/s]Extractor Estimating: 294it [02:54,  1.65it/s]Extractor Estimating: 295it [02:55,  1.62it/s]Extractor Estimating: 296it [02:55,  1.67it/s]Extractor Estimating: 297it [02:56,  1.62it/s]Extractor Estimating: 298it [02:57,  1.64it/s]Extractor Estimating: 299it [02:57,  1.63it/s]Extractor Estimating: 300it [02:58,  1.68it/s]Extractor Estimating: 301it [02:58,  1.69it/s]Extractor Estimating: 302it [02:59,  1.66it/s]Extractor Estimating: 303it [03:00,  1.66it/s]Extractor Estimating: 304it [03:00,  1.68it/s]Extractor Estimating: 305it [03:01,  1.66it/s]Extractor Estimating: 306it [03:01,  1.72it/s]Extractor Estimating: 307it [03:02,  1.66it/s]Extractor Estimating: 308it [03:03,  1.67it/s]Extractor Estimating: 309it [03:03,  1.69it/s]Extractor Estimating: 310it [03:04,  1.70it/s]Extractor Estimating: 311it [03:04,  1.67it/s]Extractor Estimating: 312it [03:05,  1.67it/s]Extractor Estimating: 313it [03:06,  1.72it/s]Extractor Estimating: 314it [03:06,  1.68it/s]Extractor Estimating: 315it [03:07,  1.71it/s]Extractor Estimating: 316it [03:07,  1.75it/s]Extractor Estimating: 317it [03:08,  1.74it/s]Extractor Estimating: 318it [03:08,  1.72it/s]Extractor Estimating: 319it [03:09,  1.71it/s]Extractor Estimating: 320it [03:10,  1.69it/s]Extractor Estimating: 321it [03:10,  1.69it/s]Extractor Estimating: 322it [03:11,  1.60it/s]Extractor Estimating: 323it [03:12,  1.58it/s]Extractor Estimating: 324it [03:12,  1.68it/s]Extractor Estimating: 325it [03:13,  1.64it/s]Extractor Estimating: 326it [03:13,  1.68it/s]Extractor Estimating: 327it [03:14,  1.63it/s]Extractor Estimating: 328it [03:15,  1.59it/s]Extractor Estimating: 329it [03:15,  1.65it/s]Extractor Estimating: 330it [03:16,  1.64it/s]Extractor Estimating: 331it [03:16,  1.64it/s]Extractor Estimating: 332it [03:17,  1.74it/s]Extractor Estimating: 333it [03:18,  1.64it/s]Extractor Estimating: 334it [03:18,  1.66it/s]Extractor Estimating: 335it [03:19,  1.64it/s]Extractor Estimating: 336it [03:19,  1.66it/s]Extractor Estimating: 337it [03:20,  1.68it/s]Extractor Estimating: 338it [03:21,  1.62it/s]Extractor Estimating: 339it [03:21,  1.61it/s]Extractor Estimating: 340it [03:22,  1.67it/s]Extractor Estimating: 341it [03:22,  1.77it/s]Extractor Estimating: 342it [03:23,  1.69it/s]Extractor Estimating: 343it [03:24,  1.71it/s]Extractor Estimating: 344it [03:24,  1.64it/s]Extractor Estimating: 345it [03:25,  1.67it/s]Extractor Estimating: 346it [03:25,  1.63it/s]Extractor Estimating: 347it [03:26,  1.59it/s]Extractor Estimating: 348it [03:27,  1.64it/s]Extractor Estimating: 349it [03:27,  1.71it/s]Extractor Estimating: 350it [03:28,  1.71it/s]Extractor Estimating: 351it [03:28,  1.70it/s]Extractor Estimating: 352it [03:29,  1.72it/s]Extractor Estimating: 353it [03:30,  1.72it/s]Extractor Estimating: 354it [03:30,  1.72it/s]Extractor Estimating: 355it [03:31,  1.69it/s]Extractor Estimating: 356it [03:31,  1.72it/s]Extractor Estimating: 357it [03:32,  1.73it/s]Extractor Estimating: 358it [03:32,  1.70it/s]Extractor Estimating: 359it [03:33,  1.67it/s]Extractor Estimating: 360it [03:34,  1.70it/s]Extractor Estimating: 361it [03:34,  1.67it/s]Extractor Estimating: 362it [03:35,  1.70it/s]Extractor Estimating: 363it [03:35,  1.66it/s]Extractor Estimating: 364it [03:36,  1.60it/s]Extractor Estimating: 365it [03:37,  1.59it/s]Extractor Estimating: 366it [03:37,  1.59it/s]Extractor Estimating: 367it [03:38,  1.62it/s]Extractor Estimating: 368it [03:39,  1.46it/s]Extractor Estimating: 369it [03:39,  1.50it/s]Extractor Estimating: 370it [03:40,  1.55it/s]Extractor Estimating: 371it [03:41,  1.63it/s]Extractor Estimating: 372it [03:41,  1.62it/s]Extractor Estimating: 373it [03:42,  1.64it/s]Extractor Estimating: 374it [03:42,  1.71it/s]Extractor Estimating: 375it [03:43,  1.65it/s]Extractor Estimating: 375it [03:43,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:30,203 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:30,208 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:30,208 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:30,208 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:30,208 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:47:30,597 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:47:30,598 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:47:30,858 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:47:31,956 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:47:31,957 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:33,391 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:33,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:33,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:33,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:33,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:47:33,722 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:47:33,724 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:47:33,995 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:47:34,199 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:47:34,200 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:49:12,158 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:49:12,789 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7502 mean pseudo reward: 0.9337155672805212
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 24886
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24986, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24986, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.959, loss:812.2483
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.937, loss:787.3424
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.939, loss:764.0059
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.938, loss:755.2167
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.934, loss:747.0652
>> valid entity prec:0.5448, rec:0.5596, f1:0.5521
>> valid relation prec:0.1621, rec:0.0527, f1:0.0795
>> valid relation with NER prec:0.1621, rec:0.0527, f1:0.0795
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.133, loss:771.2056
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.938, loss:758.9737
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.944, loss:752.3971
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.942, loss:768.9448
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.932, loss:730.1213
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5439, rec:0.4668, f1:0.5024
>> valid relation prec:0.1621, rec:0.0478, f1:0.0738
>> valid relation with NER prec:0.1621, rec:0.0478, f1:0.0738
g_step 1100, step 161, avg_time 2.145, loss:734.3450
g_step 1200, step 261, avg_time 0.935, loss:779.1412
g_step 1300, step 48, avg_time 0.926, loss:725.1408
g_step 1400, step 148, avg_time 0.930, loss:705.4179
g_step 1500, step 248, avg_time 0.927, loss:739.8426
>> valid entity prec:0.5220, rec:0.5592, f1:0.5400
>> valid relation prec:0.1641, rec:0.0578, f1:0.0855
>> valid relation with NER prec:0.1641, rec:0.0578, f1:0.0855
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.143, loss:671.7893
g_step 1700, step 135, avg_time 0.936, loss:663.8746
g_step 1800, step 235, avg_time 0.937, loss:710.9648
g_step 1900, step 22, avg_time 0.940, loss:666.9413
g_step 2000, step 122, avg_time 0.931, loss:640.9557
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5480, rec:0.5141, f1:0.5305
>> valid relation prec:0.1789, rec:0.0510, f1:0.0793
>> valid relation with NER prec:0.1789, rec:0.0510, f1:0.0793
g_step 2100, step 222, avg_time 2.129, loss:637.5140
g_step 2200, step 9, avg_time 0.941, loss:658.4609
g_step 2300, step 109, avg_time 0.936, loss:612.7475
g_step 2400, step 209, avg_time 0.940, loss:619.1641
g_step 2500, step 309, avg_time 0.928, loss:627.8665
>> valid entity prec:0.4991, rec:0.4662, f1:0.4821
>> valid relation prec:0.1633, rec:0.0515, f1:0.0783
>> valid relation with NER prec:0.1633, rec:0.0515, f1:0.0783
g_step 2600, step 96, avg_time 2.135, loss:571.8772
g_step 2700, step 196, avg_time 0.945, loss:609.1147
g_step 2800, step 296, avg_time 0.931, loss:600.9227
g_step 2900, step 83, avg_time 0.938, loss:560.3681
g_step 3000, step 183, avg_time 0.928, loss:562.2766
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5050, rec:0.5392, f1:0.5215
>> valid relation prec:0.1248, rec:0.0584, f1:0.0796
>> valid relation with NER prec:0.1248, rec:0.0584, f1:0.0796
g_step 3100, step 283, avg_time 2.135, loss:578.8169
g_step 3200, step 70, avg_time 0.936, loss:538.7333
g_step 3300, step 170, avg_time 0.932, loss:548.7772
g_step 3400, step 270, avg_time 0.932, loss:537.6685
g_step 3500, step 57, avg_time 0.935, loss:535.8669
>> valid entity prec:0.5326, rec:0.4983, f1:0.5149
>> valid relation prec:0.1593, rec:0.0472, f1:0.0729
>> valid relation with NER prec:0.1593, rec:0.0472, f1:0.0729
g_step 3600, step 157, avg_time 2.128, loss:494.6875
g_step 3700, step 257, avg_time 0.935, loss:518.5526
g_step 3800, step 44, avg_time 0.929, loss:519.0906
g_step 3900, step 144, avg_time 0.943, loss:485.4036
g_step 4000, step 244, avg_time 0.936, loss:515.5481
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5404, rec:0.5275, f1:0.5338
>> valid relation prec:0.1258, rec:0.0490, f1:0.0705
>> valid relation with NER prec:0.1258, rec:0.0490, f1:0.0705
g_step 4100, step 31, avg_time 2.123, loss:479.2748
g_step 4200, step 131, avg_time 0.943, loss:485.3703
g_step 4300, step 231, avg_time 0.933, loss:483.4192
g_step 4400, step 18, avg_time 0.938, loss:495.7286
g_step 4500, step 118, avg_time 0.943, loss:445.7591
>> valid entity prec:0.5152, rec:0.5376, f1:0.5262
>> valid relation prec:0.1395, rec:0.0673, f1:0.0908
>> valid relation with NER prec:0.1395, rec:0.0673, f1:0.0908
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 218, avg_time 2.117, loss:472.5085
g_step 4700, step 5, avg_time 0.931, loss:486.1983
g_step 4800, step 105, avg_time 0.947, loss:417.1446
g_step 4900, step 205, avg_time 0.922, loss:458.9832
g_step 5000, step 305, avg_time 0.932, loss:467.4503
learning rate was adjusted to 0.0008
>> valid entity prec:0.5112, rec:0.4682, f1:0.4888
>> valid relation prec:0.1110, rec:0.0461, f1:0.0651
>> valid relation with NER prec:0.1110, rec:0.0461, f1:0.0651
g_step 5100, step 92, avg_time 2.130, loss:394.7524
g_step 5200, step 192, avg_time 0.931, loss:440.7128
g_step 5300, step 292, avg_time 0.927, loss:447.6695
g_step 5400, step 79, avg_time 0.915, loss:399.5279
g_step 5500, step 179, avg_time 0.935, loss:415.3401
>> valid entity prec:0.5259, rec:0.5419, f1:0.5338
>> valid relation prec:0.1177, rec:0.0581, f1:0.0778
>> valid relation with NER prec:0.1177, rec:0.0581, f1:0.0778
g_step 5600, step 279, avg_time 2.134, loss:436.5148
g_step 5700, step 66, avg_time 0.932, loss:404.4735
g_step 5800, step 166, avg_time 0.939, loss:399.5124
g_step 5900, step 266, avg_time 0.924, loss:427.5784
g_step 6000, step 53, avg_time 0.931, loss:383.1803
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5416, rec:0.4537, f1:0.4938
>> valid relation prec:0.1170, rec:0.0510, f1:0.0710
>> valid relation with NER prec:0.1170, rec:0.0510, f1:0.0710
g_step 6100, step 153, avg_time 2.135, loss:389.6751
g_step 6200, step 253, avg_time 0.947, loss:385.9174
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:49:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:49:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-49-12_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:49:14 - WARNING - datasets.builder -   Using custom data configuration default-d6b2b25b52d2cdbe
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-d6b2b25b52d2cdbe/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:49:17,284 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:49:17,285 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:49:17,286 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:49:17,287 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:49:17,327 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:49:17,362 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:49:17,362 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:49:17,362 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:49:17,362 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:49:17,362 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:49:17,362 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:49:17,836 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:49:20,954 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:49:20,975 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-d6b2b25b52d2cdbe/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:04,  1.41ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.42ba/s] 38%|███▊      | 3/8 [00:01<00:01,  3.11ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.59ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.90ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.13ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.29ba/s]100%|██████████| 8/8 [00:02<00:00,  5.10ba/s]100%|██████████| 8/8 [00:02<00:00,  3.81ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  3.00ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.69ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.82ba/s]100%|██████████| 4/4 [00:00<00:00,  4.87ba/s]100%|██████████| 4/4 [00:00<00:00,  4.30ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.82ba/s] 38%|███▊      | 3/8 [00:00<00:00,  6.36ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  8.25ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.35ba/s]100%|██████████| 8/8 [00:00<00:00,  8.63ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.52ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.99ba/s]100%|██████████| 4/4 [00:00<00:00,  5.97ba/s]100%|██████████| 4/4 [00:00<00:00,  4.43ba/s]
[INFO|trainer.py:414] 2023-08-28 16:49:33,243 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:49:33,296 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:49:33,296 >>   Num examples = 7528
[INFO|trainer.py:1149] 2023-08-28 16:49:33,296 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:49:33,297 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:49:33,297 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:49:33,297 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:49:33,297 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:59,  3.29it/s]  0%|          | 2/590 [00:00<02:55,  3.36it/s]  1%|          | 3/590 [00:00<02:53,  3.38it/s]  1%|          | 4/590 [00:01<02:52,  3.39it/s]  1%|          | 5/590 [00:01<02:52,  3.39it/s]  1%|          | 6/590 [00:01<02:51,  3.40it/s]  1%|          | 7/590 [00:02<02:51,  3.40it/s]  1%|▏         | 8/590 [00:02<02:51,  3.40it/s]  2%|▏         | 9/590 [00:02<03:07,  3.10it/s]  2%|▏         | 10/590 [00:03<03:02,  3.19it/s]  2%|▏         | 11/590 [00:03<02:58,  3.25it/s]  2%|▏         | 12/590 [00:03<02:55,  3.30it/s]  2%|▏         | 13/590 [00:03<02:53,  3.33it/s]  2%|▏         | 14/590 [00:04<02:51,  3.35it/s]  3%|▎         | 15/590 [00:04<02:50,  3.37it/s]  3%|▎         | 16/590 [00:05<03:48,  2.51it/s]  3%|▎         | 17/590 [00:05<03:30,  2.72it/s]  3%|▎         | 18/590 [00:05<03:17,  2.90it/s]  3%|▎         | 19/590 [00:06<03:08,  3.03it/s]  3%|▎         | 20/590 [00:06<03:01,  3.13it/s]  4%|▎         | 21/590 [00:06<02:56,  3.22it/s]  4%|▎         | 22/590 [00:06<02:53,  3.28it/s]  4%|▍         | 23/590 [00:07<02:50,  3.33it/s]  4%|▍         | 24/590 [00:07<02:48,  3.37it/s]  4%|▍         | 25/590 [00:07<02:46,  3.39it/s]  4%|▍         | 26/590 [00:08<02:49,  3.33it/s]  5%|▍         | 27/590 [00:08<02:47,  3.36it/s]  5%|▍         | 28/590 [00:08<02:45,  3.39it/s]  5%|▍         | 29/590 [00:08<02:44,  3.41it/s]  5%|▌         | 30/590 [00:09<02:43,  3.42it/s]  5%|▌         | 31/590 [00:09<02:43,  3.43it/s]  5%|▌         | 32/590 [00:09<02:42,  3.43it/s]  6%|▌         | 33/590 [00:10<02:41,  3.44it/s]  6%|▌         | 34/590 [00:10<02:41,  3.44it/s]  6%|▌         | 35/590 [00:10<02:41,  3.44it/s]  6%|▌         | 36/590 [00:10<02:40,  3.45it/s]  6%|▋         | 37/590 [00:11<02:40,  3.44it/s]  6%|▋         | 38/590 [00:11<02:40,  3.44it/s]  7%|▋         | 39/590 [00:11<02:40,  3.44it/s]  7%|▋         | 40/590 [00:12<02:39,  3.45it/s]  7%|▋         | 41/590 [00:12<02:39,  3.44it/s]  7%|▋         | 42/590 [00:12<02:39,  3.44it/s]  7%|▋         | 43/590 [00:13<03:09,  2.89it/s]  7%|▋         | 44/590 [00:13<02:59,  3.04it/s]  8%|▊         | 45/590 [00:13<02:53,  3.15it/s]  8%|▊         | 46/590 [00:14<02:48,  3.23it/s]  8%|▊         | 47/590 [00:14<02:44,  3.29it/s]  8%|▊         | 48/590 [00:14<02:42,  3.34it/s]  8%|▊         | 49/590 [00:14<02:40,  3.37it/s]  8%|▊         | 50/590 [00:15<02:39,  3.39it/s]  9%|▊         | 51/590 [00:15<02:38,  3.41it/s]  9%|▉         | 52/590 [00:15<02:37,  3.42it/s]  9%|▉         | 53/590 [00:16<02:36,  3.43it/s]  9%|▉         | 54/590 [00:16<02:36,  3.43it/s]  9%|▉         | 55/590 [00:16<02:35,  3.44it/s]  9%|▉         | 56/590 [00:16<02:35,  3.44it/s] 10%|▉         | 57/590 [00:17<02:35,  3.44it/s] 10%|▉         | 58/590 [00:17<02:34,  3.44it/s] 10%|█         | 59/590 [00:17<02:34,  3.44it/s] 10%|█         | 60/590 [00:18<02:38,  3.35it/s] 10%|█         | 61/590 [00:18<02:36,  3.38it/s] 11%|█         | 62/590 [00:18<02:35,  3.40it/s] 11%|█         | 63/590 [00:19<02:34,  3.41it/s] 11%|█         | 64/590 [00:19<02:33,  3.42it/s] 11%|█         | 65/590 [00:19<02:33,  3.42it/s] 11%|█         | 66/590 [00:19<02:32,  3.43it/s] 11%|█▏        | 67/590 [00:20<02:32,  3.43it/s] 12%|█▏        | 68/590 [00:20<02:31,  3.44it/s] 12%|█▏        | 69/590 [00:20<02:31,  3.44it/s] 12%|█▏        | 70/590 [00:21<02:31,  3.44it/s] 12%|█▏        | 71/590 [00:21<02:30,  3.44it/s] 12%|█▏        | 72/590 [00:21<02:30,  3.44it/s] 12%|█▏        | 73/590 [00:21<02:30,  3.44it/s] 13%|█▎        | 74/590 [00:22<02:30,  3.44it/s] 13%|█▎        | 75/590 [00:22<02:29,  3.44it/s] 13%|█▎        | 76/590 [00:22<02:29,  3.44it/s] 13%|█▎        | 77/590 [00:23<02:29,  3.44it/s] 13%|█▎        | 78/590 [00:23<02:54,  2.93it/s] 13%|█▎        | 79/590 [00:23<02:46,  3.07it/s] 14%|█▎        | 80/590 [00:24<02:40,  3.17it/s] 14%|█▎        | 81/590 [00:24<02:36,  3.25it/s] 14%|█▍        | 82/590 [00:24<02:33,  3.30it/s] 14%|█▍        | 83/590 [00:25<02:31,  3.34it/s] 14%|█▍        | 84/590 [00:25<02:57,  2.84it/s] 14%|█▍        | 85/590 [00:25<02:48,  3.00it/s] 15%|█▍        | 86/590 [00:26<02:41,  3.12it/s] 15%|█▍        | 87/590 [00:26<02:36,  3.21it/s] 15%|█▍        | 88/590 [00:26<02:33,  3.27it/s] 15%|█▌        | 89/590 [00:26<02:30,  3.32it/s] 15%|█▌        | 90/590 [00:27<02:28,  3.36it/s] 15%|█▌        | 91/590 [00:27<02:27,  3.38it/s] 16%|█▌        | 92/590 [00:27<02:26,  3.40it/s] 16%|█▌        | 93/590 [00:28<02:25,  3.41it/s] 16%|█▌        | 94/590 [00:28<02:33,  3.22it/s] 16%|█▌        | 95/590 [00:28<02:30,  3.28it/s] 16%|█▋        | 96/590 [00:29<02:28,  3.33it/s] 16%|█▋        | 97/590 [00:29<02:26,  3.36it/s] 17%|█▋        | 98/590 [00:29<02:25,  3.39it/s] 17%|█▋        | 99/590 [00:29<02:24,  3.40it/s] 17%|█▋        | 100/590 [00:30<02:23,  3.41it/s] 17%|█▋        | 101/590 [00:30<02:22,  3.42it/s] 17%|█▋        | 102/590 [00:30<02:22,  3.43it/s] 17%|█▋        | 103/590 [00:31<02:21,  3.43it/s] 18%|█▊        | 104/590 [00:31<02:21,  3.43it/s] 18%|█▊        | 105/590 [00:31<02:21,  3.44it/s] 18%|█▊        | 106/590 [00:31<02:20,  3.44it/s] 18%|█▊        | 107/590 [00:32<02:20,  3.44it/s] 18%|█▊        | 108/590 [00:32<02:20,  3.44it/s] 18%|█▊        | 109/590 [00:32<02:19,  3.44it/s] 19%|█▊        | 110/590 [00:33<02:19,  3.44it/s] 19%|█▉        | 111/590 [00:33<02:19,  3.44it/s] 19%|█▉        | 112/590 [00:33<02:20,  3.41it/s] 19%|█▉        | 113/590 [00:33<02:19,  3.42it/s] 19%|█▉        | 114/590 [00:34<02:18,  3.43it/s] 19%|█▉        | 115/590 [00:34<02:18,  3.43it/s] 20%|█▉        | 116/590 [00:34<02:18,  3.43it/s] 20%|█▉        | 117/590 [00:35<02:17,  3.44it/s] 20%|██        | 118/590 [00:35<02:04,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 16:50:08,654 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:50:08,654 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 16:50:08,654 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.76it/s][A
  3%|▎         | 12/437 [00:00<00:18, 22.64it/s][A
  4%|▍         | 17/437 [00:00<00:14, 28.41it/s][A
  5%|▌         | 22/437 [00:00<00:12, 32.88it/s][A
  6%|▌         | 27/437 [00:00<00:11, 36.18it/s][A
  7%|▋         | 32/437 [00:00<00:10, 38.59it/s][A
  8%|▊         | 37/437 [00:01<00:09, 40.11it/s][A
 10%|▉         | 42/437 [00:01<00:09, 41.51it/s][A
 11%|█         | 47/437 [00:01<00:09, 42.11it/s][A
 12%|█▏        | 52/437 [00:01<00:10, 36.81it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 38.90it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 40.52it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 41.68it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 42.40it/s][A
 18%|█▊        | 77/437 [00:02<00:08, 43.13it/s][A
 19%|█▉        | 82/437 [00:02<00:08, 43.46it/s][A
 20%|█▉        | 87/437 [00:02<00:07, 43.80it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.51it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.35it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.61it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.88it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.08it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.11it/s][A
 28%|██▊       | 122/437 [00:03<00:07, 44.33it/s][A
 29%|██▉       | 127/437 [00:03<00:06, 44.46it/s][A
 30%|███       | 132/437 [00:03<00:08, 38.10it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 39.94it/s][A
 32%|███▏      | 142/437 [00:04<00:15, 19.05it/s][A
 34%|███▎      | 147/437 [00:04<00:12, 23.38it/s][A
 35%|███▍      | 152/437 [00:04<00:10, 27.33it/s][A
 36%|███▌      | 157/437 [00:04<00:09, 30.91it/s][A
 37%|███▋      | 162/437 [00:04<00:08, 34.07it/s][A
 38%|███▊      | 167/437 [00:04<00:07, 36.65it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 38.75it/s][A
 41%|████      | 177/437 [00:04<00:06, 39.14it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 40.51it/s][A
 43%|████▎     | 187/437 [00:05<00:06, 41.19it/s][A
 44%|████▍     | 192/437 [00:05<00:05, 41.70it/s][A
 45%|████▌     | 197/437 [00:05<00:05, 42.56it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 43.07it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 43.53it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 43.89it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 43.81it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.03it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.00it/s][A
 53%|█████▎    | 232/437 [00:06<00:04, 43.64it/s][A
 54%|█████▍    | 237/437 [00:06<00:04, 43.78it/s][A
 55%|█████▌    | 242/437 [00:06<00:04, 43.96it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 44.21it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 44.22it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.28it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.22it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.21it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.03it/s][A
 63%|██████▎   | 277/437 [00:07<00:03, 43.83it/s][A
 65%|██████▍   | 282/437 [00:07<00:03, 43.84it/s][A
 66%|██████▌   | 287/437 [00:07<00:03, 44.00it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 44.22it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 44.31it/s][A
 69%|██████▉   | 302/437 [00:07<00:04, 29.34it/s][A
 70%|███████   | 307/437 [00:07<00:03, 32.71it/s][A
 71%|███████▏  | 312/437 [00:08<00:03, 35.52it/s][A
 73%|███████▎  | 317/437 [00:08<00:03, 37.88it/s][A
 74%|███████▎  | 322/437 [00:08<00:02, 39.65it/s][A
 75%|███████▍  | 327/437 [00:08<00:02, 41.08it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 42.16it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 38.03it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 39.63it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 40.99it/s][A
 81%|████████  | 352/437 [00:08<00:02, 42.03it/s][A
 82%|████████▏ | 357/437 [00:09<00:01, 42.80it/s][A
 83%|████████▎ | 362/437 [00:09<00:01, 43.35it/s][A
 84%|████████▍ | 367/437 [00:09<00:01, 43.71it/s][A
 85%|████████▌ | 372/437 [00:09<00:01, 43.88it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 43.49it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 43.39it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.49it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.74it/s][A
 91%|█████████ | 397/437 [00:10<00:00, 43.90it/s][A
 92%|█████████▏| 402/437 [00:10<00:00, 44.19it/s][A
 93%|█████████▎| 407/437 [00:10<00:00, 44.33it/s][A
 94%|█████████▍| 412/437 [00:10<00:00, 44.39it/s][A
 95%|█████████▌| 417/437 [00:10<00:00, 44.28it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 43.93it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 43.83it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 43.81it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.99it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:46<02:04,  3.79it/s]
100%|██████████| 437/437 [00:10<00:00, 43.99it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:50:19,961 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-28 16:50:20,151 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:50:25,924 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:50:25,996 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:50:26,029 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [01:10<1:23:55, 10.69s/it] 20%|██        | 120/590 [01:10<59:36,  7.61s/it]   21%|██        | 121/590 [01:11<42:19,  5.41s/it] 21%|██        | 122/590 [01:11<30:14,  3.88s/it] 21%|██        | 123/590 [01:11<21:48,  2.80s/it] 21%|██        | 124/590 [01:11<15:55,  2.05s/it] 21%|██        | 125/590 [01:12<11:48,  1.52s/it] 21%|██▏       | 126/590 [01:12<08:55,  1.15s/it] 22%|██▏       | 127/590 [01:12<06:54,  1.12it/s] 22%|██▏       | 128/590 [01:13<05:30,  1.40it/s] 22%|██▏       | 129/590 [01:13<04:31,  1.70it/s] 22%|██▏       | 130/590 [01:13<03:57,  1.94it/s] 22%|██▏       | 131/590 [01:14<03:26,  2.22it/s] 22%|██▏       | 132/590 [01:14<03:04,  2.48it/s] 23%|██▎       | 133/590 [01:14<02:49,  2.70it/s] 23%|██▎       | 134/590 [01:14<02:38,  2.88it/s] 23%|██▎       | 135/590 [01:15<02:30,  3.02it/s] 23%|██▎       | 136/590 [01:15<02:25,  3.12it/s] 23%|██▎       | 137/590 [01:15<02:21,  3.20it/s] 23%|██▎       | 138/590 [01:16<02:18,  3.26it/s] 24%|██▎       | 139/590 [01:16<02:16,  3.30it/s] 24%|██▎       | 140/590 [01:16<02:15,  3.33it/s] 24%|██▍       | 141/590 [01:17<02:22,  3.16it/s] 24%|██▍       | 142/590 [01:17<02:18,  3.23it/s] 24%|██▍       | 143/590 [01:17<02:16,  3.28it/s] 24%|██▍       | 144/590 [01:17<02:14,  3.31it/s] 25%|██▍       | 145/590 [01:18<02:13,  3.34it/s] 25%|██▍       | 146/590 [01:18<02:12,  3.35it/s] 25%|██▍       | 147/590 [01:18<02:11,  3.37it/s] 25%|██▌       | 148/590 [01:19<02:10,  3.38it/s] 25%|██▌       | 149/590 [01:19<02:10,  3.38it/s] 25%|██▌       | 150/590 [01:19<02:09,  3.39it/s] 26%|██▌       | 151/590 [01:20<02:21,  3.11it/s] 26%|██▌       | 152/590 [01:20<02:17,  3.19it/s] 26%|██▌       | 153/590 [01:20<02:14,  3.25it/s] 26%|██▌       | 154/590 [01:20<02:12,  3.29it/s] 26%|██▋       | 155/590 [01:21<02:10,  3.32it/s] 26%|██▋       | 156/590 [01:21<02:09,  3.35it/s] 27%|██▋       | 157/590 [01:21<02:08,  3.36it/s] 27%|██▋       | 158/590 [01:22<02:08,  3.37it/s] 27%|██▋       | 159/590 [01:22<02:07,  3.38it/s] 27%|██▋       | 160/590 [01:22<02:07,  3.39it/s] 27%|██▋       | 161/590 [01:23<02:09,  3.32it/s] 27%|██▋       | 162/590 [01:23<02:07,  3.34it/s] 28%|██▊       | 163/590 [01:23<02:07,  3.36it/s] 28%|██▊       | 164/590 [01:23<02:06,  3.37it/s] 28%|██▊       | 165/590 [01:24<02:05,  3.38it/s] 28%|██▊       | 166/590 [01:24<02:05,  3.38it/s] 28%|██▊       | 167/590 [01:24<02:04,  3.39it/s] 28%|██▊       | 168/590 [01:25<02:04,  3.39it/s] 29%|██▊       | 169/590 [01:25<02:04,  3.39it/s] 29%|██▉       | 170/590 [01:25<02:03,  3.39it/s] 29%|██▉       | 171/590 [01:26<02:03,  3.39it/s] 29%|██▉       | 172/590 [01:26<02:08,  3.24it/s] 29%|██▉       | 173/590 [01:26<02:06,  3.28it/s] 29%|██▉       | 174/590 [01:26<02:05,  3.31it/s] 30%|██▉       | 175/590 [01:27<02:04,  3.34it/s] 30%|██▉       | 176/590 [01:27<02:03,  3.35it/s] 30%|███       | 177/590 [01:27<02:02,  3.36it/s] 30%|███       | 178/590 [01:28<02:02,  3.38it/s] 30%|███       | 179/590 [01:28<02:01,  3.38it/s] 31%|███       | 180/590 [01:28<02:01,  3.38it/s] 31%|███       | 181/590 [01:29<02:00,  3.39it/s] 31%|███       | 182/590 [01:29<02:00,  3.39it/s] 31%|███       | 183/590 [01:29<02:15,  2.99it/s] 31%|███       | 184/590 [01:30<02:10,  3.10it/s] 31%|███▏      | 185/590 [01:30<02:06,  3.19it/s] 32%|███▏      | 186/590 [01:30<02:04,  3.25it/s] 32%|███▏      | 187/590 [01:30<02:02,  3.29it/s] 32%|███▏      | 188/590 [01:31<02:00,  3.32it/s] 32%|███▏      | 189/590 [01:31<01:59,  3.34it/s] 32%|███▏      | 190/590 [01:31<01:59,  3.36it/s] 32%|███▏      | 191/590 [01:32<01:58,  3.37it/s] 33%|███▎      | 192/590 [01:32<01:57,  3.37it/s] 33%|███▎      | 193/590 [01:32<02:01,  3.27it/s] 33%|███▎      | 194/590 [01:33<01:59,  3.31it/s] 33%|███▎      | 195/590 [01:33<01:58,  3.33it/s] 33%|███▎      | 196/590 [01:33<01:57,  3.35it/s] 33%|███▎      | 197/590 [01:33<01:56,  3.36it/s] 34%|███▎      | 198/590 [01:34<01:56,  3.37it/s] 34%|███▎      | 199/590 [01:34<01:55,  3.38it/s] 34%|███▍      | 200/590 [01:34<01:55,  3.38it/s] 34%|███▍      | 201/590 [01:35<01:55,  3.38it/s] 34%|███▍      | 202/590 [01:35<01:54,  3.39it/s] 34%|███▍      | 203/590 [01:35<01:54,  3.39it/s] 35%|███▍      | 204/590 [01:35<01:54,  3.38it/s] 35%|███▍      | 205/590 [01:36<01:53,  3.38it/s] 35%|███▍      | 206/590 [01:36<02:04,  3.09it/s] 35%|███▌      | 207/590 [01:36<02:00,  3.18it/s] 35%|███▌      | 208/590 [01:37<01:57,  3.24it/s] 35%|███▌      | 209/590 [01:37<01:59,  3.20it/s] 36%|███▌      | 210/590 [01:37<01:56,  3.27it/s] 36%|███▌      | 211/590 [01:38<01:54,  3.32it/s] 36%|███▌      | 212/590 [01:38<01:52,  3.35it/s] 36%|███▌      | 213/590 [01:38<01:51,  3.38it/s] 36%|███▋      | 214/590 [01:39<02:02,  3.08it/s] 36%|███▋      | 215/590 [01:39<02:14,  2.78it/s] 37%|███▋      | 216/590 [01:39<02:06,  2.95it/s] 37%|███▋      | 217/590 [01:40<02:01,  3.08it/s] 37%|███▋      | 218/590 [01:40<01:56,  3.18it/s] 37%|███▋      | 219/590 [01:40<01:54,  3.23it/s] 37%|███▋      | 220/590 [01:41<01:52,  3.29it/s] 37%|███▋      | 221/590 [01:41<01:50,  3.33it/s] 38%|███▊      | 222/590 [01:41<01:49,  3.36it/s] 38%|███▊      | 223/590 [01:41<01:48,  3.39it/s] 38%|███▊      | 224/590 [01:42<01:47,  3.40it/s] 38%|███▊      | 225/590 [01:42<01:46,  3.41it/s] 38%|███▊      | 226/590 [01:42<01:46,  3.42it/s] 38%|███▊      | 227/590 [01:43<01:46,  3.42it/s] 39%|███▊      | 228/590 [01:43<01:45,  3.43it/s] 39%|███▉      | 229/590 [01:43<01:45,  3.43it/s] 39%|███▉      | 230/590 [01:43<01:44,  3.43it/s] 39%|███▉      | 231/590 [01:44<01:44,  3.43it/s] 39%|███▉      | 232/590 [01:44<01:44,  3.44it/s] 39%|███▉      | 233/590 [01:44<01:43,  3.44it/s] 40%|███▉      | 234/590 [01:45<01:43,  3.44it/s] 40%|███▉      | 235/590 [01:45<01:43,  3.44it/s] 40%|████      | 236/590 [01:45<01:37,  3.63it/s][INFO|trainer.py:2140] 2023-08-28 16:51:18,910 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:51:18,910 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 16:51:18,910 >>   Batch size = 8
{'eval_loss': 0.964094877243042, 'eval_runtime': 10.9355, 'eval_samples_per_second': 319.417, 'eval_steps_per_second': 39.961, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.04it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.61it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.74it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.01it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.41it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.78it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.10it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.74it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.91it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.14it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.38it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.38it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.54it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.43it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.23it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.89it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.69it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.78it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.98it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.22it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.42it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.44it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.44it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.12it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.87it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.77it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.78it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.94it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.16it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.28it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.39it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.32it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.24it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.00it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.88it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.82it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.89it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.07it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.37it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.32it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.31it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.14it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.95it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.85it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.91it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.85it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.09it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.24it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.36it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.31it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.10it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.88it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.87it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.92it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.10it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.23it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.33it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.40it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.16it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.01it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.80it/s][A
 71%|███████▏  | 312/437 [00:07<00:03, 40.46it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 41.68it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 42.53it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.16it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.58it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.83it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.83it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.65it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.46it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.56it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.78it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.14it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.26it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.34it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.41it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.25it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.88it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.69it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.82it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.90it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.12it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.33it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.36it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.37it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.27it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.87it/s][A                                                 
                                                 [A 40%|████      | 236/590 [01:55<01:37,  3.63it/s]
100%|██████████| 437/437 [00:09<00:00, 43.87it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:51:29,127 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-28 16:51:29,565 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:51:35,284 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:51:35,360 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:51:35,402 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [02:14<51:35,  8.77s/it] 40%|████      | 238/590 [02:14<36:36,  6.24s/it] 41%|████      | 239/590 [02:14<26:04,  4.46s/it] 41%|████      | 240/590 [02:15<18:42,  3.21s/it] 41%|████      | 241/590 [02:15<13:34,  2.33s/it] 41%|████      | 242/590 [02:15<09:59,  1.72s/it] 41%|████      | 243/590 [02:16<07:29,  1.29s/it] 41%|████▏     | 244/590 [02:16<05:44,  1.01it/s] 42%|████▏     | 245/590 [02:16<04:30,  1.27it/s] 42%|████▏     | 246/590 [02:16<03:39,  1.57it/s] 42%|████▏     | 247/590 [02:17<03:03,  1.87it/s] 42%|████▏     | 248/590 [02:17<02:38,  2.16it/s] 42%|████▏     | 249/590 [02:17<02:20,  2.42it/s] 42%|████▏     | 250/590 [02:18<02:08,  2.65it/s] 43%|████▎     | 251/590 [02:18<01:59,  2.83it/s] 43%|████▎     | 252/590 [02:18<01:53,  2.98it/s] 43%|████▎     | 253/590 [02:18<01:48,  3.10it/s] 43%|████▎     | 254/590 [02:19<01:45,  3.18it/s] 43%|████▎     | 255/590 [02:19<01:43,  3.24it/s] 43%|████▎     | 256/590 [02:19<01:41,  3.29it/s] 44%|████▎     | 257/590 [02:20<01:40,  3.33it/s] 44%|████▎     | 258/590 [02:20<01:38,  3.36it/s] 44%|████▍     | 259/590 [02:20<01:37,  3.39it/s] 44%|████▍     | 260/590 [02:21<01:47,  3.06it/s] 44%|████▍     | 261/590 [02:21<01:43,  3.16it/s] 44%|████▍     | 262/590 [02:21<01:41,  3.24it/s] 45%|████▍     | 263/590 [02:21<01:39,  3.30it/s] 45%|████▍     | 264/590 [02:22<01:37,  3.34it/s] 45%|████▍     | 265/590 [02:22<01:36,  3.37it/s] 45%|████▌     | 266/590 [02:22<01:35,  3.39it/s] 45%|████▌     | 267/590 [02:23<01:34,  3.41it/s] 45%|████▌     | 268/590 [02:23<01:34,  3.42it/s] 46%|████▌     | 269/590 [02:23<01:33,  3.43it/s] 46%|████▌     | 270/590 [02:24<01:33,  3.43it/s] 46%|████▌     | 271/590 [02:24<01:32,  3.43it/s] 46%|████▌     | 272/590 [02:24<01:32,  3.44it/s] 46%|████▋     | 273/590 [02:24<01:32,  3.44it/s] 46%|████▋     | 274/590 [02:25<01:31,  3.44it/s] 47%|████▋     | 275/590 [02:25<01:31,  3.44it/s] 47%|████▋     | 276/590 [02:25<01:31,  3.44it/s] 47%|████▋     | 277/590 [02:26<01:31,  3.44it/s] 47%|████▋     | 278/590 [02:26<01:30,  3.43it/s] 47%|████▋     | 279/590 [02:26<01:31,  3.38it/s] 47%|████▋     | 280/590 [02:26<01:31,  3.40it/s] 48%|████▊     | 281/590 [02:27<01:30,  3.41it/s] 48%|████▊     | 282/590 [02:27<01:30,  3.42it/s] 48%|████▊     | 283/590 [02:27<01:29,  3.43it/s] 48%|████▊     | 284/590 [02:28<01:29,  3.43it/s] 48%|████▊     | 285/590 [02:28<01:28,  3.43it/s] 48%|████▊     | 286/590 [02:28<01:28,  3.43it/s] 49%|████▊     | 287/590 [02:28<01:28,  3.44it/s] 49%|████▉     | 288/590 [02:29<01:27,  3.44it/s] 49%|████▉     | 289/590 [02:29<01:27,  3.44it/s] 49%|████▉     | 290/590 [02:29<01:29,  3.35it/s] 49%|████▉     | 291/590 [02:30<01:28,  3.38it/s] 49%|████▉     | 292/590 [02:30<01:27,  3.40it/s] 50%|████▉     | 293/590 [02:30<01:27,  3.40it/s] 50%|████▉     | 294/590 [02:31<01:26,  3.42it/s] 50%|█████     | 295/590 [02:31<01:26,  3.42it/s] 50%|█████     | 296/590 [02:31<01:25,  3.43it/s] 50%|█████     | 297/590 [02:31<01:25,  3.43it/s] 51%|█████     | 298/590 [02:32<01:25,  3.43it/s] 51%|█████     | 299/590 [02:32<01:24,  3.43it/s] 51%|█████     | 300/590 [02:32<01:24,  3.44it/s] 51%|█████     | 301/590 [02:33<01:28,  3.25it/s] 51%|█████     | 302/590 [02:33<01:27,  3.31it/s] 51%|█████▏    | 303/590 [02:33<01:25,  3.34it/s] 52%|█████▏    | 304/590 [02:34<01:24,  3.37it/s] 52%|█████▏    | 305/590 [02:34<01:24,  3.39it/s] 52%|█████▏    | 306/590 [02:34<01:23,  3.40it/s] 52%|█████▏    | 307/590 [02:34<01:22,  3.42it/s] 52%|█████▏    | 308/590 [02:35<01:22,  3.42it/s] 52%|█████▏    | 309/590 [02:35<01:21,  3.43it/s] 53%|█████▎    | 310/590 [02:35<01:21,  3.43it/s] 53%|█████▎    | 311/590 [02:36<01:21,  3.44it/s] 53%|█████▎    | 312/590 [02:36<01:24,  3.29it/s] 53%|█████▎    | 313/590 [02:36<01:24,  3.28it/s] 53%|█████▎    | 314/590 [02:36<01:22,  3.33it/s] 53%|█████▎    | 315/590 [02:37<01:21,  3.36it/s] 54%|█████▎    | 316/590 [02:37<01:21,  3.38it/s] 54%|█████▎    | 317/590 [02:37<01:20,  3.40it/s] 54%|█████▍    | 318/590 [02:38<01:19,  3.41it/s] 54%|█████▍    | 319/590 [02:38<01:19,  3.42it/s] 54%|█████▍    | 320/590 [02:38<01:18,  3.43it/s] 54%|█████▍    | 321/590 [02:39<01:34,  2.85it/s] 55%|█████▍    | 322/590 [02:39<01:59,  2.24it/s] 55%|█████▍    | 323/590 [02:40<01:46,  2.50it/s] 55%|█████▍    | 324/590 [02:40<01:37,  2.72it/s] 55%|█████▌    | 325/590 [02:40<01:31,  2.90it/s] 55%|█████▌    | 326/590 [02:41<01:26,  3.04it/s] 55%|█████▌    | 327/590 [02:41<01:23,  3.15it/s] 56%|█████▌    | 328/590 [02:41<01:21,  3.23it/s] 56%|█████▌    | 329/590 [02:41<01:19,  3.29it/s] 56%|█████▌    | 330/590 [02:42<01:18,  3.33it/s] 56%|█████▌    | 331/590 [02:42<01:20,  3.22it/s] 56%|█████▋    | 332/590 [02:42<01:18,  3.28it/s] 56%|█████▋    | 333/590 [02:43<01:17,  3.33it/s] 57%|█████▋    | 334/590 [02:43<01:16,  3.36it/s] 57%|█████▋    | 335/590 [02:43<01:15,  3.38it/s] 57%|█████▋    | 336/590 [02:43<01:14,  3.40it/s] 57%|█████▋    | 337/590 [02:44<01:14,  3.40it/s] 57%|█████▋    | 338/590 [02:44<01:13,  3.41it/s] 57%|█████▋    | 339/590 [02:44<01:13,  3.42it/s] 58%|█████▊    | 340/590 [02:45<01:12,  3.43it/s] 58%|█████▊    | 341/590 [02:45<01:12,  3.43it/s] 58%|█████▊    | 342/590 [02:45<01:14,  3.34it/s] 58%|█████▊    | 343/590 [02:46<01:13,  3.37it/s] 58%|█████▊    | 344/590 [02:46<01:12,  3.39it/s] 58%|█████▊    | 345/590 [02:46<01:12,  3.40it/s] 59%|█████▊    | 346/590 [02:46<01:11,  3.41it/s] 59%|█████▉    | 347/590 [02:47<01:11,  3.41it/s] 59%|█████▉    | 348/590 [02:47<01:10,  3.42it/s] 59%|█████▉    | 349/590 [02:47<01:10,  3.42it/s] 59%|█████▉    | 350/590 [02:48<01:10,  3.43it/s] 59%|█████▉    | 351/590 [02:48<01:09,  3.43it/s] 60%|█████▉    | 352/590 [02:48<01:09,  3.43it/s] 60%|█████▉    | 353/590 [02:49<01:14,  3.20it/s] 60%|██████    | 354/590 [02:49<01:05,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 16:52:22,548 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:52:22,548 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 16:52:22,548 >>   Batch size = 8
{'eval_loss': 0.975979745388031, 'eval_runtime': 10.0105, 'eval_samples_per_second': 348.935, 'eval_steps_per_second': 43.654, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.61it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.62it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.86it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.41it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.72it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.35it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.16it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.04it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.17it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.26it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.35it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.45it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.30it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.10it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.06it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.91it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.97it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.08it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.12it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.30it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.31it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.22it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.13it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.01it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.00it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.01it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.17it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.25it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.36it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.23it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.11it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.02it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.90it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.80it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.08it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.06it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.25it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.23it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.20it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.10it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.97it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.92it/s][A
 50%|████▉     | 217/437 [00:05<00:06, 31.47it/s][A
 51%|█████     | 222/437 [00:05<00:06, 34.53it/s][A
 52%|█████▏    | 227/437 [00:05<00:05, 37.04it/s][A
 53%|█████▎    | 232/437 [00:05<00:05, 39.02it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 40.58it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 41.69it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 42.59it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 42.89it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 42.78it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 42.94it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.20it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.54it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.84it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.15it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.39it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.42it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.10it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.77it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.61it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.56it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.90it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.21it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.32it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.36it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.41it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.78it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.73it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.63it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.76it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.99it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.13it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.31it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.43it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.24it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.05it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.83it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.73it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.00it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.03it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.31it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.37it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.32it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.27it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.10it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.91it/s][A                                                 
                                                 [A 60%|██████    | 354/590 [02:59<01:05,  3.59it/s]
100%|██████████| 437/437 [00:10<00:00, 43.91it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:52:33,286 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-28 16:52:33,358 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:52:38,361 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:52:38,437 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:52:38,504 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [03:13<29:15,  7.47s/it] 60%|██████    | 356/590 [03:13<20:49,  5.34s/it] 61%|██████    | 357/590 [03:14<14:51,  3.83s/it] 61%|██████    | 358/590 [03:14<10:41,  2.77s/it] 61%|██████    | 359/590 [03:14<07:47,  2.02s/it] 61%|██████    | 360/590 [03:15<05:46,  1.51s/it] 61%|██████    | 361/590 [03:15<04:21,  1.14s/it] 61%|██████▏   | 362/590 [03:15<03:21,  1.13it/s] 62%|██████▏   | 363/590 [03:15<02:40,  1.41it/s] 62%|██████▏   | 364/590 [03:16<02:11,  1.72it/s] 62%|██████▏   | 365/590 [03:16<01:51,  2.02it/s] 62%|██████▏   | 366/590 [03:16<01:37,  2.31it/s] 62%|██████▏   | 367/590 [03:17<01:27,  2.54it/s] 62%|██████▏   | 368/590 [03:17<01:20,  2.76it/s] 63%|██████▎   | 369/590 [03:17<01:15,  2.93it/s] 63%|██████▎   | 370/590 [03:17<01:11,  3.07it/s] 63%|██████▎   | 371/590 [03:18<01:08,  3.17it/s] 63%|██████▎   | 372/590 [03:18<01:07,  3.25it/s] 63%|██████▎   | 373/590 [03:18<01:05,  3.31it/s] 63%|██████▎   | 374/590 [03:19<01:04,  3.35it/s] 64%|██████▎   | 375/590 [03:19<01:03,  3.38it/s] 64%|██████▎   | 376/590 [03:19<01:03,  3.39it/s] 64%|██████▍   | 377/590 [03:19<01:02,  3.41it/s] 64%|██████▍   | 378/590 [03:20<01:06,  3.21it/s] 64%|██████▍   | 379/590 [03:20<01:04,  3.28it/s] 64%|██████▍   | 380/590 [03:20<01:03,  3.33it/s] 65%|██████▍   | 381/590 [03:21<01:02,  3.36it/s] 65%|██████▍   | 382/590 [03:21<01:01,  3.38it/s] 65%|██████▍   | 383/590 [03:21<01:00,  3.40it/s] 65%|██████▌   | 384/590 [03:22<01:00,  3.41it/s] 65%|██████▌   | 385/590 [03:22<00:59,  3.42it/s] 65%|██████▌   | 386/590 [03:22<00:59,  3.43it/s] 66%|██████▌   | 387/590 [03:22<00:59,  3.43it/s] 66%|██████▌   | 388/590 [03:23<00:58,  3.44it/s] 66%|██████▌   | 389/590 [03:23<01:05,  3.05it/s] 66%|██████▌   | 390/590 [03:23<01:03,  3.16it/s] 66%|██████▋   | 391/590 [03:24<01:01,  3.24it/s] 66%|██████▋   | 392/590 [03:24<01:00,  3.30it/s] 67%|██████▋   | 393/590 [03:24<00:59,  3.34it/s] 67%|██████▋   | 394/590 [03:25<00:58,  3.37it/s] 67%|██████▋   | 395/590 [03:25<00:57,  3.39it/s] 67%|██████▋   | 396/590 [03:25<00:56,  3.40it/s] 67%|██████▋   | 397/590 [03:25<00:56,  3.42it/s] 67%|██████▋   | 398/590 [03:26<00:56,  3.42it/s] 68%|██████▊   | 399/590 [03:26<00:55,  3.43it/s] 68%|██████▊   | 400/590 [03:26<00:55,  3.44it/s] 68%|██████▊   | 401/590 [03:27<00:54,  3.44it/s] 68%|██████▊   | 402/590 [03:27<00:54,  3.44it/s] 68%|██████▊   | 403/590 [03:27<00:54,  3.44it/s] 68%|██████▊   | 404/590 [03:28<01:01,  3.02it/s] 69%|██████▊   | 405/590 [03:28<00:58,  3.14it/s] 69%|██████▉   | 406/590 [03:28<00:57,  3.22it/s] 69%|██████▉   | 407/590 [03:29<00:55,  3.28it/s] 69%|██████▉   | 408/590 [03:29<00:54,  3.33it/s] 69%|██████▉   | 409/590 [03:29<00:53,  3.36it/s] 69%|██████▉   | 410/590 [03:29<00:53,  3.39it/s] 70%|██████▉   | 411/590 [03:30<00:52,  3.40it/s] 70%|██████▉   | 412/590 [03:30<00:52,  3.41it/s] 70%|███████   | 413/590 [03:30<00:51,  3.42it/s] 70%|███████   | 414/590 [03:31<00:51,  3.42it/s] 70%|███████   | 415/590 [03:31<00:51,  3.43it/s] 71%|███████   | 416/590 [03:31<00:50,  3.43it/s] 71%|███████   | 417/590 [03:31<00:50,  3.43it/s] 71%|███████   | 418/590 [03:32<00:50,  3.43it/s] 71%|███████   | 419/590 [03:32<00:49,  3.44it/s] 71%|███████   | 420/590 [03:32<00:49,  3.44it/s] 71%|███████▏  | 421/590 [03:33<00:49,  3.44it/s] 72%|███████▏  | 422/590 [03:33<00:48,  3.44it/s] 72%|███████▏  | 423/590 [03:33<00:48,  3.44it/s] 72%|███████▏  | 424/590 [03:33<00:48,  3.44it/s] 72%|███████▏  | 425/590 [03:34<00:48,  3.43it/s] 72%|███████▏  | 426/590 [03:34<00:47,  3.43it/s] 72%|███████▏  | 427/590 [03:34<00:47,  3.44it/s] 73%|███████▎  | 428/590 [03:35<00:47,  3.44it/s] 73%|███████▎  | 429/590 [03:35<00:46,  3.44it/s] 73%|███████▎  | 430/590 [03:35<00:46,  3.44it/s] 73%|███████▎  | 431/590 [03:36<00:46,  3.44it/s] 73%|███████▎  | 432/590 [03:36<00:45,  3.44it/s] 73%|███████▎  | 433/590 [03:36<00:45,  3.44it/s] 74%|███████▎  | 434/590 [03:36<00:45,  3.43it/s] 74%|███████▎  | 435/590 [03:37<00:45,  3.43it/s] 74%|███████▍  | 436/590 [03:37<00:45,  3.41it/s] 74%|███████▍  | 437/590 [03:37<00:44,  3.42it/s] 74%|███████▍  | 438/590 [03:38<00:44,  3.43it/s] 74%|███████▍  | 439/590 [03:38<00:44,  3.43it/s] 75%|███████▍  | 440/590 [03:38<00:43,  3.43it/s] 75%|███████▍  | 441/590 [03:38<00:43,  3.44it/s] 75%|███████▍  | 442/590 [03:39<00:56,  2.60it/s] 75%|███████▌  | 443/590 [03:39<00:55,  2.65it/s] 75%|███████▌  | 444/590 [03:40<00:51,  2.85it/s] 75%|███████▌  | 445/590 [03:40<00:48,  2.97it/s] 76%|███████▌  | 446/590 [03:40<00:46,  3.09it/s] 76%|███████▌  | 447/590 [03:41<00:44,  3.19it/s] 76%|███████▌  | 448/590 [03:41<00:43,  3.26it/s] 76%|███████▌  | 449/590 [03:41<00:42,  3.31it/s] 76%|███████▋  | 450/590 [03:41<00:41,  3.35it/s] 76%|███████▋  | 451/590 [03:42<00:41,  3.38it/s] 77%|███████▋  | 452/590 [03:42<00:40,  3.39it/s] 77%|███████▋  | 453/590 [03:42<00:40,  3.41it/s] 77%|███████▋  | 454/590 [03:43<00:39,  3.42it/s] 77%|███████▋  | 455/590 [03:43<00:39,  3.43it/s] 77%|███████▋  | 456/590 [03:43<00:40,  3.35it/s] 77%|███████▋  | 457/590 [03:43<00:39,  3.37it/s] 78%|███████▊  | 458/590 [03:44<00:38,  3.39it/s] 78%|███████▊  | 459/590 [03:44<00:38,  3.40it/s] 78%|███████▊  | 460/590 [03:44<00:38,  3.41it/s] 78%|███████▊  | 461/590 [03:45<00:37,  3.42it/s] 78%|███████▊  | 462/590 [03:45<00:37,  3.42it/s] 78%|███████▊  | 463/590 [03:45<00:37,  3.43it/s] 79%|███████▊  | 464/590 [03:46<00:36,  3.43it/s] 79%|███████▉  | 465/590 [03:46<00:36,  3.43it/s] 79%|███████▉  | 466/590 [03:46<00:36,  3.43it/s] 79%|███████▉  | 467/590 [03:46<00:36,  3.36it/s] 79%|███████▉  | 468/590 [03:47<00:36,  3.38it/s] 79%|███████▉  | 469/590 [03:47<00:35,  3.40it/s] 80%|███████▉  | 470/590 [03:47<00:35,  3.41it/s] 80%|███████▉  | 471/590 [03:48<00:34,  3.42it/s] 80%|████████  | 472/590 [03:48<00:31,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 16:53:21,586 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:53:21,586 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 16:53:21,586 >>   Batch size = 8
{'eval_loss': 0.986815869808197, 'eval_runtime': 10.0538, 'eval_samples_per_second': 347.43, 'eval_steps_per_second': 43.466, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.80it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.58it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.87it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.99it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.73it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.40it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.22it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.38it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.32it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.44it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.30it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.19it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.19it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.50it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.61it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.71it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.92it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.07it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.10it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.13it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.96it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.08it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.04it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.06it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.08it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.13it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.22it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.19it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.13it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.15it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.11it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.23it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.98it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.07it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.13it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.21it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.21it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.06it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.14it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.96it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.89it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.02it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.02it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.01it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.10it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.07it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.14it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.12it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.10it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.89it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.97it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.08it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.04it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.19it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.05it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.10it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.19it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.03it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.06it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.06it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.14it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.21it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.14it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.86it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.96it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.01it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.98it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.91it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.14it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.15it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.12it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.96it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.11it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.12it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.05it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.02it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.03it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.10it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.17it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.04it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.02it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.20it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.12it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.98it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.96it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.94it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.07it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:58<00:31,  3.78it/s]
100%|██████████| 437/437 [00:09<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:53:31,518 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-28 16:53:31,542 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:53:39,009 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:53:39,050 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:53:39,066 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [04:16<16:39,  8.54s/it] 80%|████████  | 474/590 [04:16<11:46,  6.09s/it] 81%|████████  | 475/590 [04:16<08:20,  4.35s/it] 81%|████████  | 476/590 [04:17<05:57,  3.14s/it] 81%|████████  | 477/590 [04:17<04:17,  2.28s/it] 81%|████████  | 478/590 [04:17<03:08,  1.69s/it] 81%|████████  | 479/590 [04:17<02:20,  1.27s/it] 81%|████████▏ | 480/590 [04:18<01:47,  1.02it/s] 82%|████████▏ | 481/590 [04:18<01:24,  1.30it/s] 82%|████████▏ | 482/590 [04:18<01:07,  1.59it/s] 82%|████████▏ | 483/590 [04:19<00:56,  1.90it/s] 82%|████████▏ | 484/590 [04:19<00:52,  2.01it/s] 82%|████████▏ | 485/590 [04:19<00:45,  2.30it/s] 82%|████████▏ | 486/590 [04:20<00:40,  2.56it/s] 83%|████████▎ | 487/590 [04:20<00:37,  2.77it/s] 83%|████████▎ | 488/590 [04:20<00:34,  2.94it/s] 83%|████████▎ | 489/590 [04:21<00:32,  3.08it/s] 83%|████████▎ | 490/590 [04:21<00:31,  3.18it/s] 83%|████████▎ | 491/590 [04:21<00:30,  3.25it/s] 83%|████████▎ | 492/590 [04:21<00:29,  3.30it/s] 84%|████████▎ | 493/590 [04:22<00:28,  3.35it/s] 84%|████████▎ | 494/590 [04:22<00:28,  3.36it/s] 84%|████████▍ | 495/590 [04:22<00:28,  3.38it/s] 84%|████████▍ | 496/590 [04:23<00:27,  3.40it/s] 84%|████████▍ | 497/590 [04:23<00:27,  3.41it/s] 84%|████████▍ | 498/590 [04:23<00:26,  3.42it/s] 85%|████████▍ | 499/590 [04:23<00:26,  3.43it/s] 85%|████████▍ | 500/590 [04:24<00:26,  3.43it/s]                                                  85%|████████▍ | 500/590 [04:24<00:26,  3.43it/s] 85%|████████▍ | 501/590 [04:24<00:25,  3.44it/s] 85%|████████▌ | 502/590 [04:24<00:25,  3.44it/s] 85%|████████▌ | 503/590 [04:25<00:25,  3.44it/s] 85%|████████▌ | 504/590 [04:25<00:24,  3.44it/s] 86%|████████▌ | 505/590 [04:25<00:25,  3.38it/s] 86%|████████▌ | 506/590 [04:26<00:24,  3.40it/s] 86%|████████▌ | 507/590 [04:26<00:24,  3.41it/s] 86%|████████▌ | 508/590 [04:26<00:23,  3.42it/s] 86%|████████▋ | 509/590 [04:26<00:23,  3.42it/s] 86%|████████▋ | 510/590 [04:27<00:23,  3.43it/s] 87%|████████▋ | 511/590 [04:27<00:23,  3.43it/s] 87%|████████▋ | 512/590 [04:27<00:22,  3.43it/s] 87%|████████▋ | 513/590 [04:28<00:22,  3.44it/s] 87%|████████▋ | 514/590 [04:28<00:22,  3.44it/s] 87%|████████▋ | 515/590 [04:28<00:21,  3.44it/s] 87%|████████▋ | 516/590 [04:28<00:21,  3.44it/s] 88%|████████▊ | 517/590 [04:29<00:21,  3.44it/s] 88%|████████▊ | 518/590 [04:29<00:20,  3.43it/s] 88%|████████▊ | 519/590 [04:29<00:20,  3.44it/s] 88%|████████▊ | 520/590 [04:30<00:20,  3.44it/s] 88%|████████▊ | 521/590 [04:30<00:20,  3.44it/s] 88%|████████▊ | 522/590 [04:30<00:19,  3.44it/s] 89%|████████▊ | 523/590 [04:30<00:19,  3.44it/s] 89%|████████▉ | 524/590 [04:31<00:19,  3.44it/s] 89%|████████▉ | 525/590 [04:31<00:18,  3.44it/s] 89%|████████▉ | 526/590 [04:31<00:18,  3.44it/s] 89%|████████▉ | 527/590 [04:32<00:18,  3.44it/s] 89%|████████▉ | 528/590 [04:32<00:18,  3.44it/s] 90%|████████▉ | 529/590 [04:32<00:18,  3.32it/s] 90%|████████▉ | 530/590 [04:33<00:17,  3.35it/s] 90%|█████████ | 531/590 [04:33<00:17,  3.38it/s] 90%|█████████ | 532/590 [04:33<00:17,  3.39it/s] 90%|█████████ | 533/590 [04:33<00:16,  3.41it/s] 91%|█████████ | 534/590 [04:34<00:16,  3.42it/s] 91%|█████████ | 535/590 [04:34<00:16,  3.42it/s] 91%|█████████ | 536/590 [04:34<00:15,  3.42it/s] 91%|█████████ | 537/590 [04:35<00:15,  3.43it/s] 91%|█████████ | 538/590 [04:35<00:15,  3.43it/s] 91%|█████████▏| 539/590 [04:35<00:14,  3.44it/s] 92%|█████████▏| 540/590 [04:35<00:14,  3.35it/s] 92%|█████████▏| 541/590 [04:36<00:14,  3.38it/s] 92%|█████████▏| 542/590 [04:36<00:14,  3.40it/s] 92%|█████████▏| 543/590 [04:36<00:13,  3.41it/s] 92%|█████████▏| 544/590 [04:37<00:13,  3.36it/s] 92%|█████████▏| 545/590 [04:37<00:13,  3.38it/s] 93%|█████████▎| 546/590 [04:37<00:12,  3.40it/s] 93%|█████████▎| 547/590 [04:37<00:12,  3.41it/s] 93%|█████████▎| 548/590 [04:38<00:12,  3.42it/s] 93%|█████████▎| 549/590 [04:38<00:11,  3.43it/s] 93%|█████████▎| 550/590 [04:38<00:11,  3.43it/s] 93%|█████████▎| 551/590 [04:39<00:11,  3.42it/s] 94%|█████████▎| 552/590 [04:39<00:11,  3.31it/s] 94%|█████████▎| 553/590 [04:40<00:16,  2.30it/s] 94%|█████████▍| 554/590 [04:40<00:14,  2.55it/s] 94%|█████████▍| 555/590 [04:40<00:12,  2.77it/s] 94%|█████████▍| 556/590 [04:41<00:11,  2.94it/s] 94%|█████████▍| 557/590 [04:41<00:10,  3.07it/s] 95%|█████████▍| 558/590 [04:41<00:10,  3.17it/s] 95%|█████████▍| 559/590 [04:41<00:09,  3.25it/s] 95%|█████████▍| 560/590 [04:42<00:09,  3.26it/s] 95%|█████████▌| 561/590 [04:42<00:08,  3.32it/s] 95%|█████████▌| 562/590 [04:42<00:08,  3.35it/s] 95%|█████████▌| 563/590 [04:43<00:08,  3.37it/s] 96%|█████████▌| 564/590 [04:43<00:07,  3.39it/s] 96%|█████████▌| 565/590 [04:43<00:07,  3.41it/s] 96%|█████████▌| 566/590 [04:44<00:07,  3.42it/s] 96%|█████████▌| 567/590 [04:44<00:06,  3.42it/s] 96%|█████████▋| 568/590 [04:44<00:06,  3.43it/s] 96%|█████████▋| 569/590 [04:44<00:06,  3.44it/s] 97%|█████████▋| 570/590 [04:45<00:05,  3.43it/s] 97%|█████████▋| 571/590 [04:45<00:05,  3.42it/s] 97%|█████████▋| 572/590 [04:45<00:05,  3.43it/s] 97%|█████████▋| 573/590 [04:46<00:04,  3.43it/s] 97%|█████████▋| 574/590 [04:46<00:04,  3.43it/s] 97%|█████████▋| 575/590 [04:46<00:04,  3.44it/s] 98%|█████████▊| 576/590 [04:46<00:04,  3.44it/s] 98%|█████████▊| 577/590 [04:47<00:03,  3.44it/s] 98%|█████████▊| 578/590 [04:47<00:03,  3.44it/s] 98%|█████████▊| 579/590 [04:47<00:03,  3.44it/s] 98%|█████████▊| 580/590 [04:48<00:02,  3.44it/s] 98%|█████████▊| 581/590 [04:48<00:02,  3.44it/s] 99%|█████████▊| 582/590 [04:48<00:02,  3.43it/s] 99%|█████████▉| 583/590 [04:48<00:02,  3.44it/s] 99%|█████████▉| 584/590 [04:49<00:01,  3.44it/s] 99%|█████████▉| 585/590 [04:49<00:01,  3.44it/s] 99%|█████████▉| 586/590 [04:49<00:01,  3.44it/s] 99%|█████████▉| 587/590 [04:50<00:00,  3.44it/s]100%|█████████▉| 588/590 [04:50<00:00,  3.44it/s]100%|█████████▉| 589/590 [04:50<00:00,  3.44it/s]100%|██████████| 590/590 [04:50<00:00,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 16:54:24,219 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:54:24,219 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 16:54:24,219 >>   Batch size = 8
{'eval_loss': 0.9942631721496582, 'eval_runtime': 9.9154, 'eval_samples_per_second': 352.279, 'eval_steps_per_second': 44.073, 'epoch': 4.0}
{'loss': 0.7054, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.10it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.87it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.12it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.33it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.97it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.61it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.44it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.14it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.16it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.31it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.29it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.26it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.17it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.16it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.10it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.91it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.78it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.82it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.06it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.12it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.14it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.05it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.00it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.91it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.78it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.88it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.95it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.12it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.22it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.35it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.65it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.68it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.59it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.39it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.59it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.68it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.03it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.85it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.95it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.96it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.93it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.84it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.90it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.42it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.14it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.36it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.08it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.26it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.03it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.91it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.00it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.98it/s][A
 61%|██████    | 267/437 [00:06<00:04, 42.46it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.15it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.61it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.92it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.02it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.95it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.81it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.83it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.64it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.89it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.11it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.16it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.29it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.30it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.22it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.11it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.92it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.83it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.02it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.21it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.28it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.32it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.32it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.18it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.00it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.86it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.84it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.00it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.04it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.13it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.28it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.24it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.07it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.99it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.86it/s][A                                                 
                                                 [A100%|██████████| 590/590 [05:00<00:00,  3.80it/s]
100%|██████████| 437/437 [00:09<00:00, 43.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:54:34,571 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-28 16:54:37,286 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:54:40,890 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:54:40,940 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:54:40,948 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 16:54:55,703 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 16:54:55,731 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118 (score: 0.964094877243042).
                                                 100%|██████████| 590/590 [05:39<00:00,  3.80it/s]100%|██████████| 590/590 [05:39<00:00,  1.74it/s]
[INFO|trainer.py:1894] 2023-08-28 16:55:12,355 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 16:55:12,408 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:55:19,066 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:55:19,087 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:55:19,101 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:55:19,304 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:19,304 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:19,304 >>   train_loss               =      0.701
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:19,304 >>   train_runtime            = 0:05:39.03
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:19,304 >>   train_samples            =       7528
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:19,304 >>   train_samples_per_second =    111.021
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:19,304 >>   train_steps_per_second   =       1.74
{'eval_loss': 0.9989626407623291, 'eval_runtime': 9.9323, 'eval_samples_per_second': 351.681, 'eval_steps_per_second': 43.998, 'epoch': 5.0}
{'train_runtime': 339.0364, 'train_samples_per_second': 111.021, 'train_steps_per_second': 1.74, 'train_loss': 0.7009575084104376, 'epoch': 5.0}
08/28/2023 16:55:19 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 16:55:19,340 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:55:19,340 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 16:55:19,340 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.02it/s]  3%|▎         | 12/437 [00:00<00:08, 48.49it/s]  4%|▍         | 17/437 [00:00<00:08, 47.19it/s]  5%|▌         | 22/437 [00:00<00:08, 46.23it/s]  6%|▌         | 27/437 [00:00<00:08, 45.86it/s]  7%|▋         | 32/437 [00:00<00:08, 45.62it/s]  8%|▊         | 37/437 [00:00<00:08, 45.53it/s] 10%|▉         | 42/437 [00:00<00:08, 45.06it/s] 11%|█         | 47/437 [00:01<00:08, 44.48it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.34it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.34it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.42it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.54it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.64it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.79it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.82it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.57it/s] 21%|██        | 92/437 [00:02<00:07, 44.27it/s] 22%|██▏       | 97/437 [00:02<00:07, 44.08it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.09it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.32it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.50it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.63it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.71it/s] 29%|██▉       | 127/437 [00:02<00:07, 40.25it/s] 30%|███       | 132/437 [00:02<00:07, 41.64it/s] 31%|███▏      | 137/437 [00:03<00:07, 42.48it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.02it/s] 34%|███▎      | 147/437 [00:03<00:06, 43.25it/s] 35%|███▍      | 152/437 [00:03<00:06, 43.69it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.08it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.26it/s] 38%|███▊      | 167/437 [00:03<00:06, 43.86it/s] 39%|███▉      | 172/437 [00:03<00:06, 43.75it/s] 41%|████      | 177/437 [00:03<00:05, 44.19it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.22it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.43it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.40it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.38it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.52it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.30it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.14it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.13it/s] 51%|█████     | 222/437 [00:05<00:04, 44.16it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.35it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.35it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.42it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.47it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.28it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.35it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.15it/s] 60%|█████▉    | 262/437 [00:05<00:03, 43.86it/s] 61%|██████    | 267/437 [00:06<00:03, 43.97it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.17it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.32it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.16it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.36it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.36it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.26it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.12it/s] 70%|███████   | 307/437 [00:06<00:02, 43.98it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.16it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.37it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.46it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.36it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.39it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.30it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.24it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.13it/s] 81%|████████  | 352/437 [00:07<00:01, 44.01it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.25it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.26it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.22it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.32it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.37it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.25it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.09it/s] 90%|████████▉ | 392/437 [00:08<00:01, 43.95it/s] 91%|█████████ | 397/437 [00:09<00:00, 40.06it/s] 92%|█████████▏| 402/437 [00:09<00:00, 41.42it/s] 93%|█████████▎| 407/437 [00:09<00:00, 42.34it/s] 94%|█████████▍| 412/437 [00:09<00:00, 43.07it/s] 95%|█████████▌| 417/437 [00:09<00:00, 43.52it/s] 97%|█████████▋| 422/437 [00:09<00:00, 43.96it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.26it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.18it/s]100%|██████████| 437/437 [00:09<00:00, 43.81it/s]100%|██████████| 437/437 [00:09<00:00, 44.11it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:55:29,265 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:29,265 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:29,265 >>   eval_loss               =     0.9641
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:29,265 >>   eval_runtime            = 0:00:09.92
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:29,265 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:29,265 >>   eval_samples_per_second =    351.947
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:29,265 >>   eval_steps_per_second   =     44.031
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:55:29,265 >>   perplexity              =     2.6224
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:55:39,083 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:55:39,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:55:39,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:55:39,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:55:39,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:55:39,399 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:55:39,401 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:55:40,088 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:55:41,193 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:55:41,194 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:55:42,824 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:55:42,855 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:55:42,855 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:55:42,855 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:55:42,855 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:55:43,390 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:55:43,391 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:55:43,680 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:55:43,870 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:55:43,871 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:08,  1.69it/s]Extractor Predicting: 16it [00:09,  1.70it/s]Extractor Predicting: 17it [00:10,  1.71it/s]Extractor Predicting: 18it [00:10,  1.70it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.66it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:13,  1.65it/s]Extractor Predicting: 24it [00:14,  1.69it/s]Extractor Predicting: 25it [00:14,  1.66it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:16,  1.73it/s]Extractor Predicting: 28it [00:16,  1.77it/s]Extractor Predicting: 29it [00:17,  1.73it/s]Extractor Predicting: 30it [00:17,  1.69it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:19,  1.60it/s]Extractor Predicting: 34it [00:20,  1.57it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:21,  1.58it/s]Extractor Predicting: 37it [00:22,  1.57it/s]Extractor Predicting: 38it [00:22,  1.56it/s]Extractor Predicting: 39it [00:23,  1.59it/s]Extractor Predicting: 40it [00:24,  1.59it/s]Extractor Predicting: 41it [00:24,  1.58it/s]Extractor Predicting: 42it [00:25,  1.57it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:26,  1.61it/s]Extractor Predicting: 45it [00:27,  1.63it/s]Extractor Predicting: 46it [00:27,  1.59it/s]Extractor Predicting: 47it [00:28,  1.56it/s]Extractor Predicting: 48it [00:29,  1.57it/s]Extractor Predicting: 49it [00:29,  1.56it/s]Extractor Predicting: 50it [00:30,  1.56it/s]Extractor Predicting: 51it [00:31,  1.57it/s]Extractor Predicting: 52it [00:31,  1.56it/s]Extractor Predicting: 53it [00:32,  1.60it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:33,  1.56it/s]Extractor Predicting: 56it [00:34,  1.56it/s]Extractor Predicting: 57it [00:35,  1.53it/s]Extractor Predicting: 58it [00:35,  1.53it/s]Extractor Predicting: 59it [00:36,  1.52it/s]Extractor Predicting: 60it [00:37,  1.51it/s]Extractor Predicting: 61it [00:37,  1.54it/s]Extractor Predicting: 62it [00:38,  1.43it/s]Extractor Predicting: 63it [00:39,  1.48it/s]Extractor Predicting: 64it [00:39,  1.53it/s]Extractor Predicting: 65it [00:40,  1.51it/s]Extractor Predicting: 66it [00:40,  1.55it/s]Extractor Predicting: 67it [00:41,  1.55it/s]Extractor Predicting: 68it [00:42,  1.55it/s]Extractor Predicting: 69it [00:42,  1.55it/s]Extractor Predicting: 70it [00:43,  1.54it/s]Extractor Predicting: 71it [00:44,  1.56it/s]Extractor Predicting: 72it [00:44,  1.58it/s]Extractor Predicting: 73it [00:45,  1.57it/s]Extractor Predicting: 74it [00:46,  1.60it/s]Extractor Predicting: 75it [00:46,  1.62it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:47,  1.59it/s]Extractor Predicting: 78it [00:48,  1.57it/s]Extractor Predicting: 79it [00:49,  1.57it/s]Extractor Predicting: 80it [00:49,  1.56it/s]Extractor Predicting: 81it [00:50,  1.61it/s]Extractor Predicting: 82it [00:51,  1.59it/s]Extractor Predicting: 83it [00:51,  1.57it/s]Extractor Predicting: 84it [00:52,  1.58it/s]Extractor Predicting: 85it [00:52,  1.61it/s]Extractor Predicting: 86it [00:53,  1.67it/s]Extractor Predicting: 87it [00:54,  1.71it/s]Extractor Predicting: 88it [00:54,  1.71it/s]Extractor Predicting: 89it [00:55,  1.71it/s]Extractor Predicting: 90it [00:55,  1.71it/s]Extractor Predicting: 91it [00:56,  1.69it/s]Extractor Predicting: 92it [00:57,  1.67it/s]Extractor Predicting: 93it [00:57,  1.68it/s]Extractor Predicting: 94it [00:58,  1.71it/s]Extractor Predicting: 95it [00:58,  1.71it/s]Extractor Predicting: 96it [00:59,  1.72it/s]Extractor Predicting: 97it [00:59,  1.70it/s]Extractor Predicting: 98it [01:00,  1.66it/s]Extractor Predicting: 99it [01:01,  1.63it/s]Extractor Predicting: 100it [01:01,  1.66it/s]Extractor Predicting: 101it [01:02,  1.69it/s]Extractor Predicting: 102it [01:03,  1.67it/s]Extractor Predicting: 103it [01:03,  1.68it/s]Extractor Predicting: 104it [01:04,  1.68it/s]Extractor Predicting: 105it [01:04,  1.71it/s]Extractor Predicting: 106it [01:05,  1.68it/s]Extractor Predicting: 107it [01:05,  1.70it/s]Extractor Predicting: 108it [01:06,  1.70it/s]Extractor Predicting: 109it [01:07,  1.73it/s]Extractor Predicting: 110it [01:07,  1.72it/s]Extractor Predicting: 111it [01:08,  1.70it/s]Extractor Predicting: 112it [01:08,  1.69it/s]Extractor Predicting: 113it [01:09,  1.67it/s]Extractor Predicting: 114it [01:10,  1.63it/s]Extractor Predicting: 115it [01:10,  1.64it/s]Extractor Predicting: 116it [01:11,  1.62it/s]Extractor Predicting: 117it [01:12,  1.61it/s]Extractor Predicting: 118it [01:12,  1.67it/s]Extractor Predicting: 119it [01:13,  1.65it/s]Extractor Predicting: 120it [01:13,  1.67it/s]Extractor Predicting: 121it [01:14,  1.66it/s]Extractor Predicting: 122it [01:15,  1.64it/s]Extractor Predicting: 123it [01:15,  1.63it/s]Extractor Predicting: 124it [01:16,  1.57it/s]Extractor Predicting: 125it [01:16,  1.58it/s]Extractor Predicting: 126it [01:17,  1.59it/s]Extractor Predicting: 127it [01:18,  1.57it/s]Extractor Predicting: 128it [01:18,  1.57it/s]Extractor Predicting: 129it [01:19,  1.53it/s]Extractor Predicting: 130it [01:20,  1.55it/s]Extractor Predicting: 131it [01:20,  1.57it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.60it/s]Extractor Predicting: 134it [01:22,  1.60it/s]Extractor Predicting: 135it [01:23,  1.59it/s]Extractor Predicting: 136it [01:23,  1.59it/s]Extractor Predicting: 137it [01:24,  1.58it/s]Extractor Predicting: 138it [01:25,  1.59it/s]Extractor Predicting: 139it [01:25,  1.58it/s]Extractor Predicting: 140it [01:26,  1.62it/s]Extractor Predicting: 141it [01:27,  1.60it/s]Extractor Predicting: 142it [01:27,  1.68it/s]Extractor Predicting: 142it [01:27,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:57:19,898 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:57:19,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:57:19,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:57:19,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:57:19,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:57:20,206 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:57:20,207 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:57:20,467 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:57:21,510 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:57:21,511 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:57:23,959 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:57:23,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:57:23,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:57:23,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:57:23,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:57:24,599 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:57:24,601 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:57:25,197 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:57:25,361 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:57:25,361 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.20696324951644102,
  "recall": 0.061265387918694535,
  "score": 0.09454384802297329,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:05,  1.70it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.70it/s]Extractor Predicting: 13it [00:07,  1.55it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:10,  1.60it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:14,  1.62it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.55it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:17,  1.57it/s]Extractor Predicting: 30it [00:18,  1.54it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:19,  1.54it/s]Extractor Predicting: 33it [00:20,  1.56it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:24,  1.63it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.63it/s]Extractor Predicting: 44it [00:27,  1.61it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.62it/s]Extractor Predicting: 47it [00:29,  1.63it/s]Extractor Predicting: 48it [00:29,  1.63it/s]Extractor Predicting: 49it [00:30,  1.62it/s]Extractor Predicting: 50it [00:30,  1.59it/s]Extractor Predicting: 51it [00:31,  1.62it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:32,  1.60it/s]Extractor Predicting: 54it [00:33,  1.61it/s]Extractor Predicting: 55it [00:34,  1.45it/s]Extractor Predicting: 56it [00:34,  1.50it/s]Extractor Predicting: 57it [00:35,  1.55it/s]Extractor Predicting: 58it [00:36,  1.60it/s]Extractor Predicting: 59it [00:36,  1.60it/s]Extractor Predicting: 60it [00:37,  1.60it/s]Extractor Predicting: 61it [00:37,  1.61it/s]Extractor Predicting: 62it [00:38,  1.61it/s]Extractor Predicting: 63it [00:39,  1.63it/s]Extractor Predicting: 64it [00:39,  1.59it/s]Extractor Predicting: 65it [00:40,  1.59it/s]Extractor Predicting: 66it [00:41,  1.58it/s]Extractor Predicting: 67it [00:41,  1.61it/s]Extractor Predicting: 68it [00:42,  1.63it/s]Extractor Predicting: 69it [00:42,  1.62it/s]Extractor Predicting: 70it [00:43,  1.63it/s]Extractor Predicting: 71it [00:44,  1.61it/s]Extractor Predicting: 72it [00:44,  1.62it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:45,  1.59it/s]Extractor Predicting: 75it [00:46,  1.60it/s]Extractor Predicting: 76it [00:47,  1.57it/s]Extractor Predicting: 77it [00:47,  1.55it/s]Extractor Predicting: 78it [00:48,  1.57it/s]Extractor Predicting: 79it [00:49,  1.58it/s]Extractor Predicting: 80it [00:49,  1.58it/s]Extractor Predicting: 81it [00:50,  1.60it/s]Extractor Predicting: 82it [00:51,  1.56it/s]Extractor Predicting: 83it [00:51,  1.58it/s]Extractor Predicting: 84it [00:52,  1.59it/s]Extractor Predicting: 85it [00:52,  1.61it/s]Extractor Predicting: 86it [00:53,  1.58it/s]Extractor Predicting: 87it [00:54,  1.59it/s]Extractor Predicting: 88it [00:54,  1.55it/s]Extractor Predicting: 89it [00:55,  1.57it/s]Extractor Predicting: 90it [00:56,  1.60it/s]Extractor Predicting: 91it [00:56,  1.57it/s]Extractor Predicting: 92it [00:57,  1.57it/s]Extractor Predicting: 93it [00:58,  1.55it/s]Extractor Predicting: 94it [00:58,  1.53it/s]Extractor Predicting: 95it [00:59,  1.54it/s]Extractor Predicting: 96it [01:00,  1.54it/s]Extractor Predicting: 97it [01:00,  1.56it/s]Extractor Predicting: 98it [01:01,  1.55it/s]Extractor Predicting: 99it [01:01,  1.56it/s]Extractor Predicting: 100it [01:02,  1.55it/s]Extractor Predicting: 101it [01:03,  1.55it/s]Extractor Predicting: 102it [01:03,  1.55it/s]Extractor Predicting: 103it [01:04,  1.57it/s]Extractor Predicting: 104it [01:05,  1.56it/s]Extractor Predicting: 105it [01:05,  1.55it/s]Extractor Predicting: 106it [01:06,  1.56it/s]Extractor Predicting: 107it [01:07,  1.54it/s]Extractor Predicting: 108it [01:07,  1.53it/s]Extractor Predicting: 109it [01:08,  1.55it/s]Extractor Predicting: 110it [01:09,  1.53it/s]Extractor Predicting: 111it [01:09,  1.52it/s]Extractor Predicting: 112it [01:10,  1.57it/s]Extractor Predicting: 113it [01:10,  1.60it/s]Extractor Predicting: 114it [01:11,  1.60it/s]Extractor Predicting: 115it [01:12,  1.60it/s]Extractor Predicting: 116it [01:12,  1.53it/s]Extractor Predicting: 117it [01:13,  1.57it/s]Extractor Predicting: 118it [01:14,  1.60it/s]Extractor Predicting: 119it [01:14,  1.60it/s]Extractor Predicting: 120it [01:15,  1.61it/s]Extractor Predicting: 121it [01:15,  1.62it/s]Extractor Predicting: 122it [01:16,  1.67it/s]Extractor Predicting: 123it [01:17,  1.66it/s]Extractor Predicting: 124it [01:17,  1.66it/s]Extractor Predicting: 125it [01:18,  1.65it/s]Extractor Predicting: 126it [01:18,  1.63it/s]Extractor Predicting: 127it [01:19,  1.63it/s]Extractor Predicting: 128it [01:20,  1.64it/s]Extractor Predicting: 129it [01:20,  1.61it/s]Extractor Predicting: 130it [01:21,  1.63it/s]Extractor Predicting: 131it [01:22,  1.60it/s]Extractor Predicting: 132it [01:22,  1.62it/s]Extractor Predicting: 133it [01:23,  1.60it/s]Extractor Predicting: 134it [01:23,  1.61it/s]Extractor Predicting: 135it [01:24,  1.65it/s]Extractor Predicting: 136it [01:25,  1.64it/s]Extractor Predicting: 137it [01:25,  1.47it/s]Extractor Predicting: 138it [01:26,  1.51it/s]Extractor Predicting: 139it [01:27,  1.58it/s]Extractor Predicting: 140it [01:27,  1.58it/s]Extractor Predicting: 141it [01:28,  1.57it/s]Extractor Predicting: 142it [01:28,  1.62it/s]Extractor Predicting: 143it [01:29,  1.62it/s]Extractor Predicting: 144it [01:30,  1.60it/s]Extractor Predicting: 145it [01:30,  1.62it/s]Extractor Predicting: 146it [01:31,  1.62it/s]Extractor Predicting: 147it [01:32,  1.59it/s]Extractor Predicting: 148it [01:32,  1.59it/s]Extractor Predicting: 149it [01:33,  1.57it/s]Extractor Predicting: 150it [01:34,  1.60it/s]Extractor Predicting: 151it [01:34,  1.63it/s]Extractor Predicting: 152it [01:35,  1.64it/s]Extractor Predicting: 153it [01:35,  1.63it/s]Extractor Predicting: 154it [01:36,  1.61it/s]Extractor Predicting: 155it [01:37,  1.60it/s]Extractor Predicting: 156it [01:37,  1.59it/s]Extractor Predicting: 157it [01:38,  1.55it/s]Extractor Predicting: 158it [01:39,  1.55it/s]Extractor Predicting: 159it [01:39,  1.56it/s]Extractor Predicting: 160it [01:40,  1.57it/s]Extractor Predicting: 161it [01:40,  1.60it/s]Extractor Predicting: 162it [01:41,  1.62it/s]Extractor Predicting: 163it [01:42,  1.61it/s]Extractor Predicting: 164it [01:42,  1.62it/s]Extractor Predicting: 165it [01:43,  1.59it/s]Extractor Predicting: 166it [01:44,  1.57it/s]Extractor Predicting: 167it [01:44,  1.61it/s]Extractor Predicting: 168it [01:45,  1.60it/s]Extractor Predicting: 169it [01:45,  1.60it/s]Extractor Predicting: 170it [01:46,  1.58it/s]Extractor Predicting: 171it [01:47,  1.54it/s]Extractor Predicting: 172it [01:47,  1.55it/s]Extractor Predicting: 173it [01:48,  1.56it/s]Extractor Predicting: 174it [01:49,  1.51it/s]Extractor Predicting: 175it [01:49,  1.47it/s]Extractor Predicting: 176it [01:50,  1.50it/s]Extractor Predicting: 177it [01:51,  1.51it/s]Extractor Predicting: 178it [01:51,  1.55it/s]Extractor Predicting: 179it [01:52,  1.56it/s]Extractor Predicting: 180it [01:53,  1.60it/s]Extractor Predicting: 181it [01:53,  1.57it/s]Extractor Predicting: 182it [01:54,  1.60it/s]Extractor Predicting: 183it [01:54,  1.60it/s]Extractor Predicting: 184it [01:55,  1.63it/s]Extractor Predicting: 185it [01:56,  1.66it/s]Extractor Predicting: 186it [01:56,  1.66it/s]Extractor Predicting: 187it [01:57,  1.67it/s]Extractor Predicting: 188it [01:57,  1.65it/s]Extractor Predicting: 189it [01:58,  1.66it/s]Extractor Predicting: 190it [01:59,  1.61it/s]Extractor Predicting: 191it [01:59,  1.56it/s]Extractor Predicting: 192it [02:00,  1.59it/s]Extractor Predicting: 193it [02:01,  1.63it/s]Extractor Predicting: 194it [02:01,  1.63it/s]Extractor Predicting: 195it [02:02,  1.63it/s]Extractor Predicting: 196it [02:02,  1.65it/s]Extractor Predicting: 197it [02:03,  1.66it/s]Extractor Predicting: 198it [02:04,  1.62it/s]Extractor Predicting: 199it [02:04,  1.62it/s]Extractor Predicting: 200it [02:05,  1.62it/s]Extractor Predicting: 201it [02:05,  1.63it/s]Extractor Predicting: 202it [02:06,  1.66it/s]Extractor Predicting: 203it [02:07,  1.67it/s]Extractor Predicting: 204it [02:07,  1.65it/s]Extractor Predicting: 205it [02:08,  1.60it/s]Extractor Predicting: 206it [02:09,  1.57it/s]Extractor Predicting: 207it [02:09,  1.60it/s]Extractor Predicting: 208it [02:10,  1.62it/s]Extractor Predicting: 209it [02:10,  1.57it/s]Extractor Predicting: 210it [02:11,  1.56it/s]Extractor Predicting: 211it [02:12,  1.57it/s]Extractor Predicting: 212it [02:12,  1.60it/s]Extractor Predicting: 213it [02:13,  1.61it/s]Extractor Predicting: 214it [02:14,  1.57it/s]Extractor Predicting: 215it [02:14,  1.57it/s]Extractor Predicting: 216it [02:15,  1.60it/s]Extractor Predicting: 217it [02:15,  1.61it/s]Extractor Predicting: 218it [02:16,  1.55it/s]Extractor Predicting: 219it [02:17,  1.56it/s]Extractor Predicting: 220it [02:17,  1.57it/s]Extractor Predicting: 221it [02:18,  1.54it/s]Extractor Predicting: 222it [02:19,  1.37it/s]Extractor Predicting: 223it [02:20,  1.42it/s]Extractor Predicting: 224it [02:20,  1.49it/s]Extractor Predicting: 225it [02:21,  1.51it/s]Extractor Predicting: 226it [02:21,  1.58it/s]Extractor Predicting: 227it [02:22,  1.62it/s]Extractor Predicting: 228it [02:23,  1.59it/s]Extractor Predicting: 229it [02:23,  1.61it/s]Extractor Predicting: 230it [02:24,  1.57it/s]Extractor Predicting: 231it [02:25,  1.57it/s]Extractor Predicting: 232it [02:25,  1.58it/s]Extractor Predicting: 233it [02:26,  1.63it/s]Extractor Predicting: 234it [02:26,  1.61it/s]Extractor Predicting: 235it [02:27,  1.62it/s]Extractor Predicting: 236it [02:28,  1.59it/s]Extractor Predicting: 237it [02:28,  1.58it/s]Extractor Predicting: 238it [02:29,  1.61it/s]Extractor Predicting: 239it [02:30,  1.62it/s]Extractor Predicting: 240it [02:30,  1.61it/s]Extractor Predicting: 241it [02:31,  1.61it/s]Extractor Predicting: 242it [02:31,  1.59it/s]Extractor Predicting: 243it [02:32,  1.56it/s]Extractor Predicting: 244it [02:33,  1.57it/s]Extractor Predicting: 245it [02:33,  1.63it/s]Extractor Predicting: 246it [02:34,  1.60it/s]Extractor Predicting: 247it [02:35,  1.61it/s]Extractor Predicting: 248it [02:35,  1.62it/s]Extractor Predicting: 249it [02:36,  1.61it/s]Extractor Predicting: 250it [02:36,  1.60it/s]Extractor Predicting: 251it [02:37,  1.57it/s]Extractor Predicting: 252it [02:38,  1.58it/s]Extractor Predicting: 253it [02:38,  1.59it/s]Extractor Predicting: 254it [02:39,  1.61it/s]Extractor Predicting: 255it [02:40,  1.60it/s]Extractor Predicting: 256it [02:40,  1.59it/s]Extractor Predicting: 257it [02:41,  1.61it/s]Extractor Predicting: 258it [02:41,  1.60it/s]Extractor Predicting: 259it [02:42,  1.56it/s]Extractor Predicting: 260it [02:43,  1.58it/s]Extractor Predicting: 261it [02:43,  1.60it/s]Extractor Predicting: 262it [02:44,  1.58it/s]Extractor Predicting: 263it [02:45,  1.56it/s]Extractor Predicting: 264it [02:45,  1.56it/s]Extractor Predicting: 265it [02:46,  1.54it/s]Extractor Predicting: 266it [02:47,  1.52it/s]Extractor Predicting: 267it [02:47,  1.50it/s]Extractor Predicting: 268it [02:48,  1.52it/s]Extractor Predicting: 269it [02:49,  1.52it/s]Extractor Predicting: 270it [02:49,  1.51it/s]Extractor Predicting: 271it [02:50,  1.52it/s]Extractor Predicting: 272it [02:51,  1.53it/s]Extractor Predicting: 273it [02:51,  1.52it/s]Extractor Predicting: 274it [02:52,  1.53it/s]Extractor Predicting: 275it [02:53,  1.57it/s]Extractor Predicting: 276it [02:53,  1.56it/s]Extractor Predicting: 277it [02:54,  1.54it/s]Extractor Predicting: 278it [02:54,  1.55it/s]Extractor Predicting: 279it [02:55,  1.53it/s]Extractor Predicting: 280it [02:56,  1.53it/s]Extractor Predicting: 281it [02:56,  1.52it/s]Extractor Predicting: 282it [02:57,  1.54it/s]Extractor Predicting: 283it [02:58,  1.50it/s]Extractor Predicting: 284it [02:58,  1.48it/s]Extractor Predicting: 285it [02:59,  1.48it/s]Extractor Predicting: 286it [03:00,  1.47it/s]Extractor Predicting: 287it [03:00,  1.50it/s]Extractor Predicting: 288it [03:01,  1.99it/s]Extractor Predicting: 288it [03:01,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:00:35,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:00:35,827 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:00:35,827 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:00:35,827 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:00:35,827 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:00:36,481 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:00:36,482 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:00:37,040 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:00:38,099 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:00:38,099 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:00:41,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:00:41,187 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:00:41,187 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:00:41,187 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:00:41,187 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:00:41,854 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:00:41,855 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:00:42,455 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:00:42,813 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:00:42,813 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5260915867944622,
  "recall": 0.1434170416606184,
  "score": 0.22539066955629064,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:01,  1.92it/s]Extractor Predicting: 3it [00:01,  1.76it/s]
[INFO|configuration_utils.py:515] 2023-08-28 17:00:45,433 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:00:45,434 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:00:45,448 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:00:45,449 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 17:00:45,454 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:00:54,497 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 17:00:54,716 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 17:00:55,061 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:00:55,062 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:00:55,113 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:00:55,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:00:55,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:00:55,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:00:55,178 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:00:55,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:00:55,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5555555555555556,
  "recall": 0.04504504504504504,
  "score": 0.08333333333333334,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 17:00:55,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:00:56,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:00:56,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:00:57,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:00:58,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:00:58,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:00:59,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:00:59,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:00,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:01,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:01,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:02,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:02,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:03,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:04,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:04,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:05,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:06,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:06,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:07,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:07,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:08,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:07, 13.37s/it][WARNING|generation_utils.py:914] 2023-08-28 17:01:09,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:09,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:10,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:10,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:11,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:11,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:12,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:12,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:13,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:13,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:14,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:14,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:15,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:16,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:16,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:17,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:17,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:18,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:18,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:19,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:20,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:25<02:41, 12.43s/it][WARNING|generation_utils.py:914] 2023-08-28 17:01:20,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:21,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:21,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:22,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:23,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:23,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:24,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:24,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:25,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:25,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:26,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:27,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:27,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:28,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:28,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:29,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:30,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:30,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:31,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:31,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:32,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:33,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:33,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:34,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:39<02:38, 13.21s/it][WARNING|generation_utils.py:914] 2023-08-28 17:01:34,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:35,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:36,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:36,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:37,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:37,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:38,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:39,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:39,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:40,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:40,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:41,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:42,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:42,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:43,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:43,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:44,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:45,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:45,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:46,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:46,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:51<02:22, 12.98s/it][WARNING|generation_utils.py:914] 2023-08-28 17:01:47,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:48,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:48,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:49,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:49,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:50,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:50,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:51,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:52,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:52,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:53,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:54,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:54,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:55,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:56,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:56,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:57,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:57,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:58,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:58,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:59,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:01:59,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:00,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:05<02:11, 13.14s/it][WARNING|generation_utils.py:914] 2023-08-28 17:02:00,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:01,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:02,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:02,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:03,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:04,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:04,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:05,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:05,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:06,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:07,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:07,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:08,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:08,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:09,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:10,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:10,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:11,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:12,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:12,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:13,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:13,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:18<01:59, 13.25s/it][WARNING|generation_utils.py:914] 2023-08-28 17:02:14,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:15,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:15,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:16,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:17,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:17,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:18,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:19,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:19,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:20,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:21,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:22,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:22,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:23,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:23,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:24,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:24,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:25,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:26,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:26,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:27,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:32<01:46, 13.33s/it][WARNING|generation_utils.py:914] 2023-08-28 17:02:27,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:28,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:29,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:29,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:30,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:31,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:32,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:33,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:33,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:34,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:35,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:36,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:36,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:37,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:37,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:38,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:39,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:39,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:40,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:40,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:41,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:41,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:42,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:43,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:48<01:38, 14.10s/it][WARNING|generation_utils.py:914] 2023-08-28 17:02:43,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:44,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:44,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:45,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:45,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:46,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:46,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:47,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:47,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:48,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:49,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:49,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:50,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:50,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:51,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:51,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:52,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:52,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:53,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:53,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:54,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:59<01:19, 13.20s/it][WARNING|generation_utils.py:914] 2023-08-28 17:02:54,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:55,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:55,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:56,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:57,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:57,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:58,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:59,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:02:59,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:00,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:00,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:01,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:02,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:02,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:03,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:03,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:04,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:05,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:05,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:06,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:06,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:11<01:05, 13.03s/it][WARNING|generation_utils.py:914] 2023-08-28 17:03:07,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:08,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:08,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:09,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:09,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:10,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:10,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:11,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:11,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:12,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:13,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:13,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:14,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:14,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:15,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:15,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:16,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:17,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:17,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:18,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:23<00:50, 12.55s/it][WARNING|generation_utils.py:914] 2023-08-28 17:03:19,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:19,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:20,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:20,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:21,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:21,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:22,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:23,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:23,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:24,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:24,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:25,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:25,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:26,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:26,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:27,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:28,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:29,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:29,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:30,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:30,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:31,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:32,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:37<00:38, 12.91s/it][WARNING|generation_utils.py:914] 2023-08-28 17:03:32,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:33,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:33,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:34,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:35,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:35,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:36,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:37,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:37,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:38,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:38,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:39,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:40,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:40,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:41,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:41,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:42,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:43,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:43,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:44,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:45,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:45,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:50<00:26, 13.15s/it][WARNING|generation_utils.py:914] 2023-08-28 17:03:46,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:47,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:47,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:48,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:48,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:49,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:50,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:51,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:51,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:52,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:52,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:53,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:54,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:54,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:55,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:55,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:56,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:57,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:57,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:58,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:58,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:03<00:13, 13.09s/it][WARNING|generation_utils.py:914] 2023-08-28 17:03:59,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:03:59,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:00,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:01,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:01,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:02,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:02,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:03,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:04,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:04,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:05,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:05,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:06,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:07,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:07,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:08,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:08,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:09,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:10,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:10,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:11,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:11,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:12,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:12,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:13,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:14,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:14,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:04:15,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:20<00:00, 14.09s/it]Generating: 100%|██████████| 15/15 [03:20<00:00, 13.35s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:04:24,021 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:04:24,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:04:24,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:04:24,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:04:24,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:04:24,800 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:04:24,801 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:04:25,811 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:04:26,878 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:04:26,878 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:04:29,845 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:04:29,856 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:04:29,856 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:04:29,856 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:04:29,856 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:04:30,516 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:04:30,517 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:04:31,234 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:04:31,470 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:04:31,470 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8522727272727273, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9300595238095238, 'errors': {''}}
['Relation : nominated for . Context : Later in the year , the band announced that they would be releasing their fourth studio album , " I Am Not So Old " . Head Entity : I Am Not So Old , Tail Entity : Bizarro .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 628, 'raw': 768}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8177083333333334, 'errors': {'', '(\'Gremlins\', \'nominated for\', \'\', \'From the film " Gremlins " .\')', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : place served by transport hub . Context : Later in the year , the railway built the station 's first railway station in Bishkek , while others such as Rakhmat Kosh , Krasnodar and Krasnodar were also built here . Head Entity : Krasnodar , Tail Entity : Tashkent .\n"]
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.8396739130434783, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : architect .', 'success_rate': 0.8892045454545454, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8046875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.8958333333333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Green', 'member of political party', '', 'Following a failed 2010 federal election , the government returned to power on 24 August 2014 , after which the left , the Greens , became the third party .')"}}
['Relation : operator . Context : Later in 2008 , the operator of the New River railway station at Dandridge , New South Wales , announced that it was closing the station once and for all . Head Entity : New River rail station , Tail Entity : Dandridge , New South Wales .\n']
['Relation : operator . Context : Later in 2008 , the operator of the New River railway station at Dandridge , New South Wales , announced that it was closing the station once and for all . Head Entity : New River rail station , Tail Entity : Dandridge , New South Wales .\n', 'Relation : operator . Context : Energiewende was built and operating under the Energiewende - Fuel Energiewende System ( FEES ) . Head Entity : Fuel Energiewende - Fuel Energiewende System , Tail Entity : Energiewende .\n']
['Relation : operator . Context : Later in 2008 , the operator of the New River railway station at Dandridge , New South Wales , announced that it was closing the station once and for all . Head Entity : New River rail station , Tail Entity : Dandridge , New South Wales .\n', 'Relation : operator . Context : Energiewende was built and operating under the Energiewende - Fuel Energiewende System ( FEES ) . Head Entity : Fuel Energiewende - Fuel Energiewende System , Tail Entity : Energiewende .\n', 'Relation : operator . Context : This was the first phase of the new Lusaka station line connecting Warsaw with the mainland of Czechoslovakia . Head Entity : Lusaka station line , Tail Entity : Polish .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 272, 'raw': 384}
{'target': 600, 'success': 293, 'raw': 416}
{'target': 600, 'success': 314, 'raw': 448}
{'target': 600, 'success': 336, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 377, 'raw': 544}
{'target': 600, 'success': 396, 'raw': 576}
{'target': 600, 'success': 416, 'raw': 608}
{'target': 600, 'success': 439, 'raw': 640}
{'target': 600, 'success': 458, 'raw': 672}
{'target': 600, 'success': 479, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 519, 'raw': 768}
{'target': 600, 'success': 542, 'raw': 800}
{'target': 600, 'success': 562, 'raw': 832}
{'target': 600, 'success': 585, 'raw': 864}
{'target': 600, 'success': 606, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6763392857142857, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 11326
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11426, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.50it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:02,  1.51it/s]Extractor Estimating: 4it [00:02,  1.53it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.58it/s]Extractor Estimating: 7it [00:04,  1.62it/s]Extractor Estimating: 8it [00:05,  1.64it/s]Extractor Estimating: 9it [00:05,  1.64it/s]Extractor Estimating: 10it [00:06,  1.65it/s]Extractor Estimating: 11it [00:06,  1.73it/s]Extractor Estimating: 12it [00:07,  1.70it/s]Extractor Estimating: 13it [00:07,  1.70it/s]Extractor Estimating: 14it [00:08,  1.70it/s]Extractor Estimating: 15it [00:09,  1.73it/s]Extractor Estimating: 16it [00:09,  1.74it/s]Extractor Estimating: 17it [00:10,  1.76it/s]Extractor Estimating: 18it [00:10,  1.75it/s]Extractor Estimating: 19it [00:11,  1.72it/s]Extractor Estimating: 20it [00:12,  1.65it/s]Extractor Estimating: 21it [00:12,  1.63it/s]Extractor Estimating: 22it [00:13,  1.66it/s]Extractor Estimating: 23it [00:13,  1.69it/s]Extractor Estimating: 24it [00:14,  1.71it/s]Extractor Estimating: 25it [00:14,  1.74it/s]Extractor Estimating: 26it [00:15,  1.76it/s]Extractor Estimating: 27it [00:16,  1.73it/s]Extractor Estimating: 28it [00:16,  1.79it/s]Extractor Estimating: 29it [00:17,  1.86it/s]Extractor Estimating: 30it [00:17,  1.87it/s]Extractor Estimating: 31it [00:18,  1.90it/s]Extractor Estimating: 32it [00:18,  1.85it/s]Extractor Estimating: 33it [00:19,  1.88it/s]Extractor Estimating: 34it [00:19,  1.91it/s]Extractor Estimating: 35it [00:20,  1.96it/s]Extractor Estimating: 36it [00:20,  1.94it/s]Extractor Estimating: 37it [00:21,  1.98it/s]Extractor Estimating: 38it [00:21,  1.95it/s]Extractor Estimating: 39it [00:22,  1.88it/s]Extractor Estimating: 40it [00:22,  1.76it/s]Extractor Estimating: 41it [00:23,  1.77it/s]Extractor Estimating: 42it [00:24,  1.81it/s]Extractor Estimating: 43it [00:24,  1.82it/s]Extractor Estimating: 44it [00:25,  1.83it/s]Extractor Estimating: 45it [00:25,  1.87it/s]Extractor Estimating: 46it [00:26,  1.76it/s]Extractor Estimating: 47it [00:27,  1.65it/s]Extractor Estimating: 48it [00:27,  1.61it/s]Extractor Estimating: 49it [00:28,  1.63it/s]Extractor Estimating: 50it [00:28,  1.64it/s]Extractor Estimating: 51it [00:29,  1.58it/s]Extractor Estimating: 52it [00:30,  1.59it/s]Extractor Estimating: 53it [00:30,  1.60it/s]Extractor Estimating: 54it [00:31,  1.55it/s]Extractor Estimating: 55it [00:32,  1.51it/s]Extractor Estimating: 56it [00:32,  1.53it/s]Extractor Estimating: 57it [00:33,  1.56it/s]Extractor Estimating: 58it [00:34,  1.57it/s]Extractor Estimating: 59it [00:34,  1.62it/s]Extractor Estimating: 60it [00:35,  1.61it/s]Extractor Estimating: 61it [00:35,  1.63it/s]Extractor Estimating: 62it [00:36,  1.61it/s]Extractor Estimating: 63it [00:37,  1.60it/s]Extractor Estimating: 64it [00:37,  1.53it/s]Extractor Estimating: 65it [00:38,  1.58it/s]Extractor Estimating: 66it [00:39,  1.56it/s]Extractor Estimating: 67it [00:39,  1.60it/s]Extractor Estimating: 68it [00:40,  1.56it/s]Extractor Estimating: 69it [00:40,  1.60it/s]Extractor Estimating: 70it [00:41,  1.60it/s]Extractor Estimating: 71it [00:42,  1.62it/s]Extractor Estimating: 72it [00:42,  1.66it/s]Extractor Estimating: 73it [00:43,  1.62it/s]Extractor Estimating: 74it [00:44,  1.60it/s]Extractor Estimating: 75it [00:44,  1.58it/s]Extractor Estimating: 76it [00:45,  1.64it/s]Extractor Estimating: 77it [00:45,  1.70it/s]Extractor Estimating: 78it [00:46,  1.80it/s]Extractor Estimating: 79it [00:46,  1.81it/s]Extractor Estimating: 80it [00:47,  1.84it/s]Extractor Estimating: 81it [00:47,  1.84it/s]Extractor Estimating: 82it [00:48,  1.88it/s]Extractor Estimating: 83it [00:48,  1.90it/s]Extractor Estimating: 84it [00:49,  1.87it/s]Extractor Estimating: 85it [00:49,  1.89it/s]Extractor Estimating: 86it [00:50,  1.90it/s]Extractor Estimating: 87it [00:51,  1.85it/s]Extractor Estimating: 88it [00:51,  1.85it/s]Extractor Estimating: 89it [00:52,  1.83it/s]Extractor Estimating: 90it [00:52,  1.83it/s]Extractor Estimating: 91it [00:53,  1.79it/s]Extractor Estimating: 92it [00:53,  1.85it/s]Extractor Estimating: 93it [00:54,  1.82it/s]Extractor Estimating: 94it [00:54,  1.83it/s]Extractor Estimating: 95it [00:55,  1.82it/s]Extractor Estimating: 96it [00:55,  1.85it/s]Extractor Estimating: 97it [00:56,  1.82it/s]Extractor Estimating: 98it [00:57,  1.81it/s]Extractor Estimating: 99it [00:57,  1.78it/s]Extractor Estimating: 100it [00:58,  1.79it/s]Extractor Estimating: 101it [00:58,  1.84it/s]Extractor Estimating: 102it [00:59,  1.84it/s]Extractor Estimating: 103it [00:59,  1.84it/s]Extractor Estimating: 104it [01:00,  1.82it/s]Extractor Estimating: 105it [01:00,  1.82it/s]Extractor Estimating: 106it [01:01,  1.86it/s]Extractor Estimating: 107it [01:01,  1.87it/s]Extractor Estimating: 108it [01:02,  1.84it/s]Extractor Estimating: 109it [01:03,  1.83it/s]Extractor Estimating: 110it [01:03,  1.86it/s]Extractor Estimating: 111it [01:04,  1.89it/s]Extractor Estimating: 112it [01:04,  1.90it/s]Extractor Estimating: 113it [01:05,  1.85it/s]Extractor Estimating: 114it [01:05,  1.85it/s]Extractor Estimating: 115it [01:06,  1.82it/s]Extractor Estimating: 116it [01:06,  1.91it/s]Extractor Estimating: 117it [01:07,  1.87it/s]Extractor Estimating: 118it [01:07,  1.93it/s]Extractor Estimating: 119it [01:08,  1.91it/s]Extractor Estimating: 120it [01:08,  1.87it/s]Extractor Estimating: 121it [01:09,  1.87it/s]Extractor Estimating: 122it [01:09,  1.88it/s]Extractor Estimating: 123it [01:10,  1.86it/s]Extractor Estimating: 124it [01:11,  1.86it/s]Extractor Estimating: 125it [01:11,  1.93it/s]Extractor Estimating: 126it [01:12,  1.85it/s]Extractor Estimating: 127it [01:12,  1.68it/s]Extractor Estimating: 128it [01:13,  1.68it/s]Extractor Estimating: 129it [01:13,  1.73it/s]Extractor Estimating: 130it [01:14,  1.78it/s]Extractor Estimating: 131it [01:15,  1.80it/s]Extractor Estimating: 132it [01:15,  1.77it/s]Extractor Estimating: 133it [01:16,  1.76it/s]Extractor Estimating: 134it [01:16,  1.79it/s]Extractor Estimating: 135it [01:17,  1.86it/s]Extractor Estimating: 136it [01:18,  1.64it/s]Extractor Estimating: 137it [01:18,  1.55it/s]Extractor Estimating: 138it [01:19,  1.58it/s]Extractor Estimating: 139it [01:19,  1.63it/s]Extractor Estimating: 140it [01:20,  1.73it/s]Extractor Estimating: 141it [01:20,  1.75it/s]Extractor Estimating: 142it [01:21,  1.71it/s]Extractor Estimating: 143it [01:22,  1.71it/s]Extractor Estimating: 144it [01:22,  1.75it/s]Extractor Estimating: 145it [01:23,  1.75it/s]Extractor Estimating: 146it [01:23,  1.76it/s]Extractor Estimating: 147it [01:24,  1.80it/s]Extractor Estimating: 148it [01:24,  1.75it/s]Extractor Estimating: 149it [01:25,  1.76it/s]Extractor Estimating: 150it [01:26,  1.76it/s]Extractor Estimating: 151it [01:26,  1.75it/s]Extractor Estimating: 152it [01:27,  1.76it/s]Extractor Estimating: 153it [01:27,  1.75it/s]Extractor Estimating: 154it [01:28,  1.74it/s]Extractor Estimating: 155it [01:28,  1.80it/s]Extractor Estimating: 156it [01:29,  1.86it/s]Extractor Estimating: 157it [01:29,  1.82it/s]Extractor Estimating: 158it [01:30,  1.80it/s]Extractor Estimating: 159it [01:31,  1.82it/s]Extractor Estimating: 160it [01:31,  1.80it/s]Extractor Estimating: 161it [01:32,  1.80it/s]Extractor Estimating: 162it [01:32,  1.69it/s]Extractor Estimating: 163it [01:33,  1.69it/s]Extractor Estimating: 164it [01:34,  1.73it/s]Extractor Estimating: 165it [01:34,  1.75it/s]Extractor Estimating: 166it [01:35,  1.82it/s]Extractor Estimating: 167it [01:35,  1.80it/s]Extractor Estimating: 168it [01:36,  1.82it/s]Extractor Estimating: 169it [01:36,  1.76it/s]Extractor Estimating: 170it [01:37,  1.80it/s]Extractor Estimating: 171it [01:37,  1.76it/s]Extractor Estimating: 172it [01:38,  1.81it/s]Extractor Estimating: 173it [01:38,  1.84it/s]Extractor Estimating: 174it [01:39,  1.84it/s]Extractor Estimating: 175it [01:40,  1.81it/s]Extractor Estimating: 176it [01:40,  1.71it/s]Extractor Estimating: 177it [01:41,  1.74it/s]Extractor Estimating: 178it [01:41,  1.75it/s]Extractor Estimating: 179it [01:42,  1.79it/s]Extractor Estimating: 180it [01:42,  1.78it/s]Extractor Estimating: 181it [01:43,  1.71it/s]Extractor Estimating: 182it [01:44,  1.67it/s]Extractor Estimating: 183it [01:44,  1.70it/s]Extractor Estimating: 184it [01:45,  1.72it/s]Extractor Estimating: 185it [01:45,  1.77it/s]Extractor Estimating: 186it [01:46,  1.73it/s]Extractor Estimating: 187it [01:47,  1.74it/s]Extractor Estimating: 188it [01:47,  1.77it/s]Extractor Estimating: 189it [01:48,  1.72it/s]Extractor Estimating: 190it [01:48,  1.75it/s]Extractor Estimating: 191it [01:49,  1.78it/s]Extractor Estimating: 192it [01:49,  1.83it/s]Extractor Estimating: 193it [01:50,  1.86it/s]Extractor Estimating: 194it [01:50,  1.85it/s]Extractor Estimating: 195it [01:51,  1.80it/s]Extractor Estimating: 196it [01:51,  1.83it/s]Extractor Estimating: 197it [01:52,  1.81it/s]Extractor Estimating: 198it [01:53,  1.73it/s]Extractor Estimating: 199it [01:53,  1.68it/s]Extractor Estimating: 200it [01:54,  1.70it/s]Extractor Estimating: 201it [01:55,  1.69it/s]Extractor Estimating: 202it [01:55,  1.70it/s]Extractor Estimating: 203it [01:56,  1.65it/s]Extractor Estimating: 204it [01:56,  1.71it/s]Extractor Estimating: 205it [01:57,  1.67it/s]Extractor Estimating: 206it [01:57,  1.70it/s]Extractor Estimating: 207it [01:58,  1.66it/s]Extractor Estimating: 208it [01:59,  1.67it/s]Extractor Estimating: 209it [01:59,  1.71it/s]Extractor Estimating: 210it [02:00,  1.72it/s]Extractor Estimating: 211it [02:00,  1.68it/s]Extractor Estimating: 212it [02:01,  1.73it/s]Extractor Estimating: 213it [02:02,  1.72it/s]Extractor Estimating: 214it [02:02,  1.78it/s]Extractor Estimating: 215it [02:03,  1.71it/s]Extractor Estimating: 216it [02:03,  1.74it/s]Extractor Estimating: 217it [02:04,  1.70it/s]Extractor Estimating: 218it [02:05,  1.68it/s]Extractor Estimating: 219it [02:05,  1.64it/s]Extractor Estimating: 220it [02:06,  1.64it/s]Extractor Estimating: 221it [02:07,  1.51it/s]Extractor Estimating: 222it [02:07,  1.52it/s]Extractor Estimating: 223it [02:08,  1.58it/s]Extractor Estimating: 224it [02:08,  1.59it/s]Extractor Estimating: 225it [02:09,  1.61it/s]Extractor Estimating: 226it [02:10,  1.57it/s]Extractor Estimating: 227it [02:10,  1.58it/s]Extractor Estimating: 228it [02:11,  1.58it/s]Extractor Estimating: 229it [02:12,  1.58it/s]Extractor Estimating: 230it [02:12,  1.55it/s]Extractor Estimating: 231it [02:13,  1.55it/s]Extractor Estimating: 232it [02:14,  1.48it/s]Extractor Estimating: 233it [02:14,  1.48it/s]Extractor Estimating: 234it [02:15,  1.45it/s]Extractor Estimating: 235it [02:16,  1.50it/s]Extractor Estimating: 236it [02:16,  1.53it/s]Extractor Estimating: 237it [02:17,  1.48it/s]Extractor Estimating: 238it [02:18,  1.51it/s]Extractor Estimating: 239it [02:18,  1.53it/s]Extractor Estimating: 240it [02:19,  1.53it/s]Extractor Estimating: 241it [02:20,  1.50it/s]Extractor Estimating: 242it [02:20,  1.50it/s]Extractor Estimating: 243it [02:21,  1.55it/s]Extractor Estimating: 244it [02:22,  1.53it/s]Extractor Estimating: 245it [02:22,  1.49it/s]Extractor Estimating: 246it [02:23,  1.51it/s]Extractor Estimating: 247it [02:24,  1.54it/s]Extractor Estimating: 248it [02:24,  1.59it/s]Extractor Estimating: 249it [02:25,  1.57it/s]Extractor Estimating: 250it [02:25,  1.57it/s]Extractor Estimating: 251it [02:26,  1.74it/s]Extractor Estimating: 252it [02:26,  1.85it/s]Extractor Estimating: 253it [02:27,  1.88it/s]Extractor Estimating: 254it [02:27,  1.97it/s]Extractor Estimating: 255it [02:28,  2.04it/s]Extractor Estimating: 256it [02:28,  2.02it/s]Extractor Estimating: 257it [02:29,  2.03it/s]Extractor Estimating: 258it [02:29,  2.09it/s]Extractor Estimating: 259it [02:30,  2.05it/s]Extractor Estimating: 260it [02:30,  2.04it/s]Extractor Estimating: 261it [02:31,  2.09it/s]Extractor Estimating: 262it [02:31,  2.09it/s]Extractor Estimating: 263it [02:32,  2.09it/s]Extractor Estimating: 264it [02:32,  2.09it/s]Extractor Estimating: 265it [02:33,  2.06it/s]Extractor Estimating: 266it [02:33,  1.99it/s]Extractor Estimating: 267it [02:34,  2.06it/s]Extractor Estimating: 268it [02:34,  2.00it/s]Extractor Estimating: 269it [02:35,  2.01it/s]Extractor Estimating: 270it [02:35,  2.05it/s]Extractor Estimating: 271it [02:35,  2.04it/s]Extractor Estimating: 272it [02:36,  2.02it/s]Extractor Estimating: 273it [02:37,  2.02it/s]Extractor Estimating: 274it [02:37,  2.07it/s]Extractor Estimating: 275it [02:37,  2.13it/s]Extractor Estimating: 276it [02:38,  1.98it/s]Extractor Estimating: 277it [02:39,  1.89it/s]Extractor Estimating: 278it [02:39,  1.81it/s]Extractor Estimating: 279it [02:40,  1.78it/s]Extractor Estimating: 280it [02:40,  1.68it/s]Extractor Estimating: 281it [02:41,  1.66it/s]Extractor Estimating: 282it [02:42,  1.69it/s]Extractor Estimating: 283it [02:42,  1.73it/s]Extractor Estimating: 284it [02:43,  1.78it/s]Extractor Estimating: 285it [02:43,  1.69it/s]Extractor Estimating: 286it [02:44,  1.72it/s]Extractor Estimating: 287it [02:44,  1.72it/s]Extractor Estimating: 288it [02:45,  1.69it/s]Extractor Estimating: 289it [02:46,  1.70it/s]Extractor Estimating: 290it [02:46,  1.71it/s]Extractor Estimating: 291it [02:47,  1.75it/s]Extractor Estimating: 292it [02:47,  1.71it/s]Extractor Estimating: 293it [02:48,  1.76it/s]Extractor Estimating: 294it [02:49,  1.71it/s]Extractor Estimating: 295it [02:49,  1.62it/s]Extractor Estimating: 296it [02:50,  1.67it/s]Extractor Estimating: 297it [02:50,  1.62it/s]Extractor Estimating: 298it [02:51,  1.65it/s]Extractor Estimating: 299it [02:52,  1.63it/s]Extractor Estimating: 300it [02:52,  1.67it/s]Extractor Estimating: 301it [02:53,  1.52it/s]Extractor Estimating: 302it [02:54,  1.58it/s]Extractor Estimating: 303it [02:54,  1.59it/s]Extractor Estimating: 304it [02:55,  1.70it/s]Extractor Estimating: 305it [02:55,  1.69it/s]Extractor Estimating: 306it [02:56,  1.68it/s]Extractor Estimating: 307it [02:57,  1.68it/s]Extractor Estimating: 308it [02:57,  1.67it/s]Extractor Estimating: 309it [02:58,  1.69it/s]Extractor Estimating: 310it [02:58,  1.72it/s]Extractor Estimating: 311it [02:59,  1.76it/s]Extractor Estimating: 312it [02:59,  1.70it/s]Extractor Estimating: 313it [03:00,  1.66it/s]Extractor Estimating: 314it [03:01,  1.71it/s]Extractor Estimating: 315it [03:01,  1.76it/s]Extractor Estimating: 316it [03:02,  1.80it/s]Extractor Estimating: 317it [03:02,  1.70it/s]Extractor Estimating: 318it [03:03,  1.66it/s]Extractor Estimating: 319it [03:04,  1.66it/s]Extractor Estimating: 320it [03:04,  1.63it/s]Extractor Estimating: 321it [03:05,  1.62it/s]Extractor Estimating: 322it [03:05,  1.65it/s]Extractor Estimating: 323it [03:06,  1.68it/s]Extractor Estimating: 324it [03:07,  1.73it/s]Extractor Estimating: 325it [03:07,  1.75it/s]Extractor Estimating: 326it [03:08,  1.77it/s]Extractor Estimating: 327it [03:08,  1.65it/s]Extractor Estimating: 328it [03:09,  1.65it/s]Extractor Estimating: 329it [03:10,  1.64it/s]Extractor Estimating: 330it [03:10,  1.61it/s]Extractor Estimating: 331it [03:11,  1.53it/s]Extractor Estimating: 332it [03:12,  1.56it/s]Extractor Estimating: 333it [03:12,  1.57it/s]Extractor Estimating: 334it [03:13,  1.61it/s]Extractor Estimating: 335it [03:13,  1.66it/s]Extractor Estimating: 336it [03:14,  1.54it/s]Extractor Estimating: 337it [03:15,  1.60it/s]Extractor Estimating: 338it [03:15,  1.61it/s]Extractor Estimating: 339it [03:16,  1.65it/s]Extractor Estimating: 340it [03:16,  1.66it/s]Extractor Estimating: 341it [03:17,  1.67it/s]Extractor Estimating: 342it [03:18,  1.68it/s]Extractor Estimating: 343it [03:18,  1.63it/s]Extractor Estimating: 344it [03:19,  1.69it/s]Extractor Estimating: 345it [03:19,  1.67it/s]Extractor Estimating: 346it [03:20,  1.62it/s]Extractor Estimating: 347it [03:21,  1.60it/s]Extractor Estimating: 348it [03:21,  1.64it/s]Extractor Estimating: 349it [03:22,  1.61it/s]Extractor Estimating: 350it [03:23,  1.58it/s]Extractor Estimating: 351it [03:23,  1.63it/s]Extractor Estimating: 352it [03:24,  1.61it/s]Extractor Estimating: 353it [03:24,  1.64it/s]Extractor Estimating: 354it [03:25,  1.67it/s]Extractor Estimating: 355it [03:26,  1.70it/s]Extractor Estimating: 356it [03:26,  1.69it/s]Extractor Estimating: 357it [03:27,  1.68it/s]Extractor Estimating: 358it [03:27,  1.59it/s]Extractor Estimating: 359it [03:28,  1.62it/s]Extractor Estimating: 360it [03:29,  1.67it/s]Extractor Estimating: 361it [03:29,  1.69it/s]Extractor Estimating: 362it [03:30,  1.72it/s]Extractor Estimating: 363it [03:30,  1.70it/s]Extractor Estimating: 364it [03:31,  1.68it/s]Extractor Estimating: 365it [03:31,  1.72it/s]Extractor Estimating: 366it [03:32,  1.68it/s]Extractor Estimating: 367it [03:33,  1.75it/s]Extractor Estimating: 368it [03:33,  1.76it/s]Extractor Estimating: 369it [03:34,  1.70it/s]Extractor Estimating: 370it [03:34,  1.68it/s]Extractor Estimating: 371it [03:35,  1.71it/s]Extractor Estimating: 372it [03:36,  1.71it/s]Extractor Estimating: 373it [03:36,  1.72it/s]Extractor Estimating: 374it [03:37,  1.68it/s]Extractor Estimating: 375it [03:37,  1.70it/s]Extractor Estimating: 375it [03:37,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:08:26,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:08:26,997 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:08:26,997 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:08:26,997 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:08:26,997 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:08:27,467 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:08:27,468 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:08:27,818 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:08:28,906 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:08:28,907 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:08:30,241 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:08:30,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:08:30,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:08:30,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:08:30,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:08:30,623 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:08:30,624 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:08:30,901 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:08:31,073 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:08:31,074 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:10:29,406 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:10:29,434 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7509 mean pseudo reward: 0.9425990499345169
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 22568
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22668, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22668, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.929, loss:701.3148
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.934, loss:705.0461
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.941, loss:701.4214
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.938, loss:687.6061
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.939, loss:665.1444
>> valid entity prec:0.5363, rec:0.5554, f1:0.5457
>> valid relation prec:0.2068, rec:0.0676, f1:0.1019
>> valid relation with NER prec:0.2068, rec:0.0676, f1:0.1019
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.138, loss:692.2298
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.948, loss:654.5123
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.943, loss:671.7940
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.938, loss:720.5187
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.951, loss:657.6805
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5463, rec:0.5037, f1:0.5241
>> valid relation prec:0.1864, rec:0.0604, f1:0.0912
>> valid relation with NER prec:0.1864, rec:0.0604, f1:0.0912
g_step 1100, step 161, avg_time 2.126, loss:653.5998
g_step 1200, step 261, avg_time 0.939, loss:707.3504
g_step 1300, step 48, avg_time 0.938, loss:639.7779
g_step 1400, step 148, avg_time 0.929, loss:638.8204
g_step 1500, step 248, avg_time 0.941, loss:661.1606
>> valid entity prec:0.5195, rec:0.5290, f1:0.5242
>> valid relation prec:0.1920, rec:0.0561, f1:0.0868
>> valid relation with NER prec:0.1920, rec:0.0561, f1:0.0868
g_step 1600, step 35, avg_time 2.136, loss:616.7347
g_step 1700, step 135, avg_time 0.942, loss:592.3445
g_step 1800, step 235, avg_time 0.944, loss:613.1887
g_step 1900, step 22, avg_time 0.933, loss:598.5552
g_step 2000, step 122, avg_time 0.932, loss:572.5516
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5800, rec:0.4471, f1:0.5049
>> valid relation prec:0.1541, rec:0.0487, f1:0.0740
>> valid relation with NER prec:0.1541, rec:0.0487, f1:0.0740
g_step 2100, step 222, avg_time 2.146, loss:582.3574
g_step 2200, step 9, avg_time 0.946, loss:585.3351
g_step 2300, step 109, avg_time 0.953, loss:553.1022
g_step 2400, step 209, avg_time 0.942, loss:540.7675
g_step 2500, step 309, avg_time 0.930, loss:566.4029
>> valid entity prec:0.5065, rec:0.5359, f1:0.5208
>> valid relation prec:0.1254, rec:0.0444, f1:0.0656
>> valid relation with NER prec:0.1254, rec:0.0444, f1:0.0656
g_step 2600, step 96, avg_time 2.135, loss:527.5549
g_step 2700, step 196, avg_time 0.944, loss:513.5901
g_step 2800, step 296, avg_time 0.943, loss:547.0979
g_step 2900, step 83, avg_time 0.921, loss:496.1146
g_step 3000, step 183, avg_time 0.948, loss:484.3242
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5356, rec:0.4810, f1:0.5068
>> valid relation prec:0.1351, rec:0.0475, f1:0.0703
>> valid relation with NER prec:0.1351, rec:0.0475, f1:0.0703
g_step 3100, step 283, avg_time 2.121, loss:515.1710
g_step 3200, step 70, avg_time 0.941, loss:475.2011
g_step 3300, step 170, avg_time 0.945, loss:472.8191
g_step 3400, step 270, avg_time 0.930, loss:477.9220
g_step 3500, step 57, avg_time 0.941, loss:461.9798
>> valid entity prec:0.5315, rec:0.4979, f1:0.5142
>> valid relation prec:0.1237, rec:0.0481, f1:0.0693
>> valid relation with NER prec:0.1237, rec:0.0481, f1:0.0693
g_step 3600, step 157, avg_time 2.140, loss:434.3134
g_step 3700, step 257, avg_time 0.939, loss:472.1952
g_step 3800, step 44, avg_time 0.931, loss:450.0968
g_step 3900, step 144, avg_time 0.950, loss:434.8385
g_step 4000, step 244, avg_time 0.936, loss:436.7775
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5384, rec:0.4707, f1:0.5022
>> valid relation prec:0.1443, rec:0.0667, f1:0.0912
>> valid relation with NER prec:0.1443, rec:0.0667, f1:0.0912
g_step 4100, step 31, avg_time 2.136, loss:455.5622
g_step 4200, step 131, avg_time 0.931, loss:394.5058
g_step 4300, step 231, avg_time 0.941, loss:418.5579
g_step 4400, step 18, avg_time 0.941, loss:417.3593
g_step 4500, step 118, avg_time 0.940, loss:403.0583
>> valid entity prec:0.5180, rec:0.5370, f1:0.5273
>> valid relation prec:0.1284, rec:0.0595, f1:0.0814
>> valid relation with NER prec:0.1284, rec:0.0595, f1:0.0814
g_step 4600, step 218, avg_time 2.143, loss:406.3275
g_step 4700, step 5, avg_time 0.935, loss:428.2727
g_step 4800, step 105, avg_time 0.943, loss:363.8284
g_step 4900, step 205, avg_time 0.939, loss:391.2588
g_step 5000, step 305, avg_time 0.930, loss:405.3981
learning rate was adjusted to 0.0008
>> valid entity prec:0.5166, rec:0.4405, f1:0.4755
>> valid relation prec:0.1630, rec:0.0595, f1:0.0872
>> valid relation with NER prec:0.1630, rec:0.0595, f1:0.0872
g_step 5100, step 92, avg_time 2.148, loss:363.1760
g_step 5200, step 192, avg_time 0.937, loss:372.3474
g_step 5300, step 292, avg_time 0.929, loss:392.3699
g_step 5400, step 79, avg_time 0.938, loss:347.0410
g_step 5500, step 179, avg_time 0.943, loss:361.9329
>> valid entity prec:0.5333, rec:0.4739, f1:0.5018
>> valid relation prec:0.1284, rec:0.0555, f1:0.0775
>> valid relation with NER prec:0.1284, rec:0.0555, f1:0.0775
g_step 5600, step 279, avg_time 2.139, loss:382.7733
g_step 5700, step 66, avg_time 0.930, loss:349.6333
g_step 5800, step 166, avg_time 0.946, loss:343.7060
g_step 5900, step 266, avg_time 0.937, loss:372.7616
g_step 6000, step 53, avg_time 0.941, loss:338.2433
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5303, rec:0.4988, f1:0.5141
>> valid relation prec:0.1264, rec:0.0598, f1:0.0812
>> valid relation with NER prec:0.1264, rec:0.0598, f1:0.0812
g_step 6100, step 153, avg_time 2.137, loss:332.3152
g_step 6200, step 253, avg_time 0.933, loss:332.3149
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:10:29 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:10:29 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-10-29_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:10:30 - WARNING - datasets.builder -   Using custom data configuration default-f7eb6e5a7473d8c2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f7eb6e5a7473d8c2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:10:31,665 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:10:31,666 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:10:31,667 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:10:31,668 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:10:31,749 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:10:31,862 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:10:31,862 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:10:31,862 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:10:31,862 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:10:31,862 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:10:31,863 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:10:32,434 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:10:35,623 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:10:35,625 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f7eb6e5a7473d8c2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.37ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.15ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.44ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.58ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.63ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.70ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.75ba/s]100%|██████████| 8/8 [00:01<00:00,  5.57ba/s]100%|██████████| 8/8 [00:01<00:00,  4.85ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.82ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.16ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.28ba/s]100%|██████████| 4/4 [00:00<00:00,  5.36ba/s]100%|██████████| 4/4 [00:00<00:00,  4.83ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.48ba/s] 38%|███▊      | 3/8 [00:00<00:00, 11.00ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 11.14ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 11.14ba/s]100%|██████████| 8/8 [00:00<00:00, 11.76ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.42ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.10ba/s]100%|██████████| 4/4 [00:00<00:00, 11.34ba/s]
[INFO|trainer.py:414] 2023-08-28 19:10:40,006 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:10:40,035 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:10:40,035 >>   Num examples = 7518
[INFO|trainer.py:1149] 2023-08-28 19:10:40,035 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:10:40,035 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:10:40,036 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:10:40,036 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:10:40,036 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:53,  3.36it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:49,  3.43it/s]  1%|          | 5/585 [00:01<02:48,  3.44it/s]  1%|          | 6/585 [00:01<02:48,  3.44it/s]  1%|          | 7/585 [00:02<02:48,  3.43it/s]  1%|▏         | 8/585 [00:02<02:48,  3.42it/s]  2%|▏         | 9/585 [00:02<02:49,  3.41it/s]  2%|▏         | 10/585 [00:02<02:48,  3.41it/s]  2%|▏         | 11/585 [00:03<02:48,  3.40it/s]  2%|▏         | 12/585 [00:03<02:48,  3.40it/s]  2%|▏         | 13/585 [00:03<02:48,  3.40it/s]  2%|▏         | 14/585 [00:04<02:48,  3.39it/s]  3%|▎         | 15/585 [00:04<02:47,  3.40it/s]  3%|▎         | 16/585 [00:04<02:47,  3.40it/s]  3%|▎         | 17/585 [00:04<02:47,  3.39it/s]  3%|▎         | 18/585 [00:05<02:48,  3.36it/s]  3%|▎         | 19/585 [00:05<02:48,  3.37it/s]  3%|▎         | 20/585 [00:05<02:47,  3.37it/s]  4%|▎         | 21/585 [00:06<02:46,  3.38it/s]  4%|▍         | 22/585 [00:06<02:46,  3.39it/s]  4%|▍         | 23/585 [00:06<02:46,  3.38it/s]  4%|▍         | 24/585 [00:07<02:45,  3.39it/s]  4%|▍         | 25/585 [00:07<02:45,  3.39it/s]  4%|▍         | 26/585 [00:07<02:44,  3.39it/s]  5%|▍         | 27/585 [00:07<02:44,  3.39it/s]  5%|▍         | 28/585 [00:08<02:44,  3.39it/s]  5%|▍         | 29/585 [00:08<02:47,  3.31it/s]  5%|▌         | 30/585 [00:08<02:46,  3.34it/s]  5%|▌         | 31/585 [00:09<02:45,  3.35it/s]  5%|▌         | 32/585 [00:09<02:44,  3.36it/s]  6%|▌         | 33/585 [00:09<02:43,  3.37it/s]  6%|▌         | 34/585 [00:10<02:43,  3.38it/s]  6%|▌         | 35/585 [00:10<02:42,  3.38it/s]  6%|▌         | 36/585 [00:10<02:41,  3.39it/s]  6%|▋         | 37/585 [00:10<02:40,  3.40it/s]  6%|▋         | 38/585 [00:11<02:40,  3.42it/s]  7%|▋         | 39/585 [00:11<02:39,  3.42it/s]  7%|▋         | 40/585 [00:11<02:40,  3.40it/s]  7%|▋         | 41/585 [00:12<02:39,  3.42it/s]  7%|▋         | 42/585 [00:12<02:38,  3.42it/s]  7%|▋         | 43/585 [00:12<02:38,  3.43it/s]  8%|▊         | 44/585 [00:12<02:37,  3.43it/s]  8%|▊         | 45/585 [00:13<02:37,  3.43it/s]  8%|▊         | 46/585 [00:13<02:36,  3.43it/s]  8%|▊         | 47/585 [00:13<02:36,  3.44it/s]  8%|▊         | 48/585 [00:14<02:36,  3.44it/s]  8%|▊         | 49/585 [00:14<02:35,  3.44it/s]  9%|▊         | 50/585 [00:14<02:35,  3.44it/s]  9%|▊         | 51/585 [00:14<02:35,  3.42it/s]  9%|▉         | 52/585 [00:15<02:35,  3.43it/s]  9%|▉         | 53/585 [00:15<02:35,  3.43it/s]  9%|▉         | 54/585 [00:15<02:34,  3.43it/s]  9%|▉         | 55/585 [00:16<02:34,  3.43it/s] 10%|▉         | 56/585 [00:16<02:34,  3.43it/s] 10%|▉         | 57/585 [00:16<02:33,  3.44it/s] 10%|▉         | 58/585 [00:17<02:33,  3.44it/s] 10%|█         | 59/585 [00:17<02:32,  3.44it/s] 10%|█         | 60/585 [00:17<02:32,  3.43it/s] 10%|█         | 61/585 [00:17<02:32,  3.43it/s] 11%|█         | 62/585 [00:18<02:34,  3.38it/s] 11%|█         | 63/585 [00:18<02:33,  3.39it/s] 11%|█         | 64/585 [00:18<02:32,  3.41it/s] 11%|█         | 65/585 [00:19<02:32,  3.41it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.42it/s] 11%|█▏        | 67/585 [00:19<02:31,  3.43it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 69/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 70/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.44it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.43it/s] 13%|█▎        | 76/585 [00:22<02:28,  3.43it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.43it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.44it/s] 14%|█▎        | 79/585 [00:23<02:27,  3.44it/s] 14%|█▎        | 80/585 [00:23<02:27,  3.42it/s] 14%|█▍        | 81/585 [00:23<02:27,  3.42it/s] 14%|█▍        | 82/585 [00:24<02:26,  3.43it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.43it/s] 14%|█▍        | 84/585 [00:24<02:26,  3.43it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 86/585 [00:25<02:25,  3.43it/s] 15%|█▍        | 87/585 [00:25<02:25,  3.43it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.44it/s] 15%|█▌        | 89/585 [00:26<02:24,  3.43it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.44it/s] 16%|█▌        | 91/585 [00:26<02:27,  3.35it/s] 16%|█▌        | 92/585 [00:26<02:26,  3.38it/s] 16%|█▌        | 93/585 [00:27<02:24,  3.39it/s] 16%|█▌        | 94/585 [00:27<02:24,  3.41it/s] 16%|█▌        | 95/585 [00:27<02:23,  3.41it/s] 16%|█▋        | 96/585 [00:28<02:22,  3.42it/s] 17%|█▋        | 97/585 [00:28<02:22,  3.42it/s] 17%|█▋        | 98/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 99/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 101/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 102/585 [00:29<02:35,  3.10it/s] 18%|█▊        | 103/585 [00:30<02:31,  3.19it/s] 18%|█▊        | 104/585 [00:30<02:27,  3.26it/s] 18%|█▊        | 105/585 [00:30<02:24,  3.31it/s] 18%|█▊        | 106/585 [00:31<02:23,  3.35it/s] 18%|█▊        | 107/585 [00:31<02:21,  3.37it/s] 18%|█▊        | 108/585 [00:31<02:20,  3.39it/s] 19%|█▊        | 109/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 110/585 [00:32<02:19,  3.41it/s] 19%|█▉        | 111/585 [00:32<02:18,  3.42it/s] 19%|█▉        | 112/585 [00:32<02:24,  3.28it/s] 19%|█▉        | 113/585 [00:33<02:22,  3.32it/s] 19%|█▉        | 114/585 [00:33<02:20,  3.36it/s] 20%|█▉        | 115/585 [00:33<02:19,  3.38it/s] 20%|█▉        | 116/585 [00:34<02:18,  3.40it/s] 20%|██        | 117/585 [00:34<02:17,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 19:11:14,572 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:11:14,572 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:11:14,572 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.31it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.66it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.08it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.29it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.83it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.48it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.24it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.11it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.97it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.19it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 41.22it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 42.25it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 42.84it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.19it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.32it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.21it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.70it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.86it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.55it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.87it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.04it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.17it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.22it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.11it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.95it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.08it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.95it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.94it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.04it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.14it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.23it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.19it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.10it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.99it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.98it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.95it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.03it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.85it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.97it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.10it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.00it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.99it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.95it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.86it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.94it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.03it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.10it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.27it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.15it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.11it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.99it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.97it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.96it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.01it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.09it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.15it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.25it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.14it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.95it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.95it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.00it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.95it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.99it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.97it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 42.35it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.08it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.42it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.69it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.74it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.87it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.85it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.94it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.83it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.83it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.94it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.10it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.23it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.24it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.03it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.00it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.97it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.93it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.85it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.99it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.09it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.23it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.28it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.28it/s][A 20%|██        | 117/585 [00:44<02:17,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:11:24,578 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 19:11:24,610 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:11:27,147 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:11:27,161 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:11:27,168 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:55<51:01,  6.56s/it] 20%|██        | 119/585 [00:55<36:26,  4.69s/it] 21%|██        | 120/585 [00:56<26:08,  3.37s/it] 21%|██        | 121/585 [00:56<18:56,  2.45s/it] 21%|██        | 122/585 [00:56<13:54,  1.80s/it] 21%|██        | 123/585 [00:57<10:24,  1.35s/it] 21%|██        | 124/585 [00:57<07:56,  1.03s/it] 21%|██▏       | 125/585 [00:57<06:13,  1.23it/s] 22%|██▏       | 126/585 [00:57<05:01,  1.52it/s] 22%|██▏       | 127/585 [00:58<04:11,  1.82it/s] 22%|██▏       | 128/585 [00:58<03:36,  2.12it/s] 22%|██▏       | 129/585 [00:58<03:14,  2.34it/s] 22%|██▏       | 130/585 [00:59<02:56,  2.58it/s] 22%|██▏       | 131/585 [00:59<02:43,  2.78it/s] 23%|██▎       | 132/585 [00:59<02:34,  2.94it/s] 23%|██▎       | 133/585 [01:00<02:27,  3.06it/s] 23%|██▎       | 134/585 [01:00<02:23,  3.15it/s] 23%|██▎       | 135/585 [01:00<02:19,  3.22it/s] 23%|██▎       | 136/585 [01:00<02:17,  3.27it/s] 23%|██▎       | 137/585 [01:01<02:15,  3.30it/s] 24%|██▎       | 138/585 [01:01<02:14,  3.33it/s] 24%|██▍       | 139/585 [01:01<02:13,  3.35it/s] 24%|██▍       | 140/585 [01:02<02:16,  3.26it/s] 24%|██▍       | 141/585 [01:02<02:14,  3.30it/s] 24%|██▍       | 142/585 [01:02<02:13,  3.33it/s] 24%|██▍       | 143/585 [01:03<02:12,  3.34it/s] 25%|██▍       | 144/585 [01:03<02:11,  3.35it/s] 25%|██▍       | 145/585 [01:03<02:10,  3.37it/s] 25%|██▍       | 146/585 [01:03<02:10,  3.37it/s] 25%|██▌       | 147/585 [01:04<02:09,  3.38it/s] 25%|██▌       | 148/585 [01:04<02:09,  3.38it/s] 25%|██▌       | 149/585 [01:04<02:08,  3.38it/s] 26%|██▌       | 150/585 [01:05<02:08,  3.38it/s] 26%|██▌       | 151/585 [01:05<02:08,  3.37it/s] 26%|██▌       | 152/585 [01:05<02:08,  3.37it/s] 26%|██▌       | 153/585 [01:06<02:07,  3.38it/s] 26%|██▋       | 154/585 [01:06<02:07,  3.38it/s] 26%|██▋       | 155/585 [01:06<02:07,  3.38it/s] 27%|██▋       | 156/585 [01:06<02:06,  3.38it/s] 27%|██▋       | 157/585 [01:07<02:06,  3.38it/s] 27%|██▋       | 158/585 [01:07<02:06,  3.38it/s] 27%|██▋       | 159/585 [01:07<02:05,  3.39it/s] 27%|██▋       | 160/585 [01:08<02:05,  3.39it/s] 28%|██▊       | 161/585 [01:08<02:05,  3.39it/s] 28%|██▊       | 162/585 [01:08<02:10,  3.24it/s] 28%|██▊       | 163/585 [01:09<02:08,  3.28it/s] 28%|██▊       | 164/585 [01:09<02:07,  3.31it/s] 28%|██▊       | 165/585 [01:09<02:05,  3.33it/s] 28%|██▊       | 166/585 [01:09<02:05,  3.35it/s] 29%|██▊       | 167/585 [01:10<02:04,  3.36it/s] 29%|██▊       | 168/585 [01:10<02:03,  3.36it/s] 29%|██▉       | 169/585 [01:10<02:03,  3.37it/s] 29%|██▉       | 170/585 [01:11<02:02,  3.38it/s] 29%|██▉       | 171/585 [01:11<02:02,  3.38it/s] 29%|██▉       | 172/585 [01:11<02:02,  3.38it/s] 30%|██▉       | 173/585 [01:11<02:02,  3.35it/s] 30%|██▉       | 174/585 [01:12<02:02,  3.36it/s] 30%|██▉       | 175/585 [01:12<02:01,  3.37it/s] 30%|███       | 176/585 [01:12<02:01,  3.37it/s] 30%|███       | 177/585 [01:13<02:00,  3.38it/s] 30%|███       | 178/585 [01:13<02:00,  3.38it/s] 31%|███       | 179/585 [01:13<02:00,  3.38it/s] 31%|███       | 180/585 [01:14<01:59,  3.38it/s] 31%|███       | 181/585 [01:14<01:59,  3.38it/s] 31%|███       | 182/585 [01:14<01:59,  3.38it/s] 31%|███▏      | 183/585 [01:14<01:59,  3.38it/s] 31%|███▏      | 184/585 [01:15<01:59,  3.36it/s] 32%|███▏      | 185/585 [01:15<01:58,  3.37it/s] 32%|███▏      | 186/585 [01:15<01:58,  3.38it/s] 32%|███▏      | 187/585 [01:16<01:57,  3.38it/s] 32%|███▏      | 188/585 [01:16<01:57,  3.38it/s] 32%|███▏      | 189/585 [01:16<01:57,  3.38it/s] 32%|███▏      | 190/585 [01:17<01:56,  3.38it/s] 33%|███▎      | 191/585 [01:17<01:56,  3.38it/s] 33%|███▎      | 192/585 [01:17<01:56,  3.38it/s] 33%|███▎      | 193/585 [01:17<01:55,  3.38it/s] 33%|███▎      | 194/585 [01:18<01:55,  3.38it/s] 33%|███▎      | 195/585 [01:18<01:59,  3.26it/s] 34%|███▎      | 196/585 [01:18<01:57,  3.30it/s] 34%|███▎      | 197/585 [01:19<01:56,  3.34it/s] 34%|███▍      | 198/585 [01:19<01:54,  3.37it/s] 34%|███▍      | 199/585 [01:19<01:53,  3.39it/s] 34%|███▍      | 200/585 [01:19<01:53,  3.40it/s] 34%|███▍      | 201/585 [01:20<01:52,  3.41it/s] 35%|███▍      | 202/585 [01:20<01:52,  3.42it/s] 35%|███▍      | 203/585 [01:20<01:51,  3.42it/s] 35%|███▍      | 204/585 [01:21<01:51,  3.43it/s] 35%|███▌      | 205/585 [01:21<01:50,  3.43it/s] 35%|███▌      | 206/585 [01:21<01:51,  3.39it/s] 35%|███▌      | 207/585 [01:22<01:51,  3.40it/s] 36%|███▌      | 208/585 [01:22<01:50,  3.41it/s] 36%|███▌      | 209/585 [01:22<01:50,  3.42it/s] 36%|███▌      | 210/585 [01:22<01:49,  3.42it/s] 36%|███▌      | 211/585 [01:23<01:49,  3.43it/s] 36%|███▌      | 212/585 [01:23<01:48,  3.43it/s] 36%|███▋      | 213/585 [01:23<01:48,  3.43it/s] 37%|███▋      | 214/585 [01:24<01:47,  3.44it/s] 37%|███▋      | 215/585 [01:24<01:47,  3.44it/s] 37%|███▋      | 216/585 [01:24<01:47,  3.43it/s] 37%|███▋      | 217/585 [01:24<01:49,  3.37it/s] 37%|███▋      | 218/585 [01:25<01:48,  3.39it/s] 37%|███▋      | 219/585 [01:25<01:47,  3.40it/s] 38%|███▊      | 220/585 [01:25<01:46,  3.41it/s] 38%|███▊      | 221/585 [01:26<01:46,  3.42it/s] 38%|███▊      | 222/585 [01:26<01:45,  3.43it/s] 38%|███▊      | 223/585 [01:26<01:45,  3.43it/s] 38%|███▊      | 224/585 [01:26<01:45,  3.43it/s] 38%|███▊      | 225/585 [01:27<01:44,  3.43it/s] 39%|███▊      | 226/585 [01:27<01:44,  3.43it/s] 39%|███▉      | 227/585 [01:27<01:44,  3.43it/s] 39%|███▉      | 228/585 [01:28<01:44,  3.43it/s] 39%|███▉      | 229/585 [01:28<01:43,  3.43it/s] 39%|███▉      | 230/585 [01:28<01:43,  3.43it/s] 39%|███▉      | 231/585 [01:29<01:43,  3.43it/s] 40%|███▉      | 232/585 [01:29<01:42,  3.43it/s] 40%|███▉      | 233/585 [01:29<01:42,  3.43it/s] 40%|████      | 234/585 [01:29<01:42,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 19:12:10,083 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:12:10,084 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:12:10,084 >>   Batch size = 8
{'eval_loss': 0.9836576581001282, 'eval_runtime': 9.954, 'eval_samples_per_second': 350.916, 'eval_steps_per_second': 43.902, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.95it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.33it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.74it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.98it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.69it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.39it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.40it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.35it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.49it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.41it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.21it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.90it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.93it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.98it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.03it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.95it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.14it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.31it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.23it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.06it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.90it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.88it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.85it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.01it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.09it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.19it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.22it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.19it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.98it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.02it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.94it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.92it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.02it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.12it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.09it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.17it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.07it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.99it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.10it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.05it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.96it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.06it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.13it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.08it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.01it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.10it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.07it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.96it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.92it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.98it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.12it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.16it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.83it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.03it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.18it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.95it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.89it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.97it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.02it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.00it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.89it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.04it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.18it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.24it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.15it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.99it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.98it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.06it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.99it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.98it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.07it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.21it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.25it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.13it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.00it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.03it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.02it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.90it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.84it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.07it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.26it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.16it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.11it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.04it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.98it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.86it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.94it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.94it/s][A 40%|████      | 234/585 [01:39<01:42,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:12:20,026 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 19:12:20,082 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:12:23,394 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:12:23,536 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:12:23,629 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:51<38:43,  6.64s/it] 40%|████      | 236/585 [01:51<27:33,  4.74s/it] 41%|████      | 237/585 [01:51<19:45,  3.41s/it] 41%|████      | 238/585 [01:52<14:17,  2.47s/it] 41%|████      | 239/585 [01:52<10:29,  1.82s/it] 41%|████      | 240/585 [01:52<07:49,  1.36s/it] 41%|████      | 241/585 [01:53<05:58,  1.04s/it] 41%|████▏     | 242/585 [01:53<04:40,  1.22it/s] 42%|████▏     | 243/585 [01:53<03:46,  1.51it/s] 42%|████▏     | 244/585 [01:54<03:07,  1.81it/s] 42%|████▏     | 245/585 [01:54<02:41,  2.11it/s] 42%|████▏     | 246/585 [01:54<02:22,  2.37it/s] 42%|████▏     | 247/585 [01:54<02:10,  2.58it/s] 42%|████▏     | 248/585 [01:55<02:01,  2.77it/s] 43%|████▎     | 249/585 [01:55<02:09,  2.60it/s] 43%|████▎     | 250/585 [01:55<01:59,  2.80it/s] 43%|████▎     | 251/585 [01:56<01:53,  2.95it/s] 43%|████▎     | 252/585 [01:56<01:48,  3.07it/s] 43%|████▎     | 253/585 [01:56<01:45,  3.16it/s] 43%|████▎     | 254/585 [01:57<01:42,  3.22it/s] 44%|████▎     | 255/585 [01:57<01:40,  3.27it/s] 44%|████▍     | 256/585 [01:57<01:39,  3.30it/s] 44%|████▍     | 257/585 [01:58<01:38,  3.31it/s] 44%|████▍     | 258/585 [01:58<01:38,  3.33it/s] 44%|████▍     | 259/585 [01:58<01:37,  3.35it/s] 44%|████▍     | 260/585 [01:58<01:36,  3.36it/s] 45%|████▍     | 261/585 [01:59<01:36,  3.37it/s] 45%|████▍     | 262/585 [01:59<01:35,  3.38it/s] 45%|████▍     | 263/585 [01:59<01:35,  3.38it/s] 45%|████▌     | 264/585 [02:00<01:35,  3.38it/s] 45%|████▌     | 265/585 [02:00<01:34,  3.38it/s] 45%|████▌     | 266/585 [02:00<01:34,  3.38it/s] 46%|████▌     | 267/585 [02:00<01:33,  3.38it/s] 46%|████▌     | 268/585 [02:01<01:34,  3.36it/s] 46%|████▌     | 269/585 [02:01<01:33,  3.37it/s] 46%|████▌     | 270/585 [02:01<01:33,  3.37it/s] 46%|████▋     | 271/585 [02:02<01:32,  3.38it/s] 46%|████▋     | 272/585 [02:02<01:32,  3.38it/s] 47%|████▋     | 273/585 [02:02<01:32,  3.38it/s] 47%|████▋     | 274/585 [02:03<01:31,  3.39it/s] 47%|████▋     | 275/585 [02:03<01:31,  3.39it/s] 47%|████▋     | 276/585 [02:03<01:31,  3.39it/s] 47%|████▋     | 277/585 [02:03<01:30,  3.39it/s] 48%|████▊     | 278/585 [02:04<01:30,  3.39it/s] 48%|████▊     | 279/585 [02:04<01:30,  3.37it/s] 48%|████▊     | 280/585 [02:04<01:30,  3.37it/s] 48%|████▊     | 281/585 [02:05<01:30,  3.37it/s] 48%|████▊     | 282/585 [02:05<01:29,  3.38it/s] 48%|████▊     | 283/585 [02:05<01:29,  3.38it/s] 49%|████▊     | 284/585 [02:06<01:29,  3.38it/s] 49%|████▊     | 285/585 [02:06<01:28,  3.38it/s] 49%|████▉     | 286/585 [02:06<01:28,  3.38it/s] 49%|████▉     | 287/585 [02:06<01:28,  3.38it/s] 49%|████▉     | 288/585 [02:07<01:27,  3.38it/s] 49%|████▉     | 289/585 [02:07<01:27,  3.38it/s] 50%|████▉     | 290/585 [02:07<01:28,  3.35it/s] 50%|████▉     | 291/585 [02:08<01:27,  3.36it/s] 50%|████▉     | 292/585 [02:08<01:27,  3.36it/s] 50%|█████     | 293/585 [02:08<01:26,  3.38it/s] 50%|█████     | 294/585 [02:08<01:25,  3.40it/s] 50%|█████     | 295/585 [02:09<01:25,  3.41it/s] 51%|█████     | 296/585 [02:09<01:24,  3.41it/s] 51%|█████     | 297/585 [02:09<01:24,  3.42it/s] 51%|█████     | 298/585 [02:10<01:23,  3.43it/s] 51%|█████     | 299/585 [02:10<01:23,  3.42it/s] 51%|█████▏    | 300/585 [02:10<01:23,  3.43it/s] 51%|█████▏    | 301/585 [02:11<01:27,  3.25it/s] 52%|█████▏    | 302/585 [02:11<01:25,  3.31it/s] 52%|█████▏    | 303/585 [02:11<01:24,  3.34it/s] 52%|█████▏    | 304/585 [02:11<01:23,  3.37it/s] 52%|█████▏    | 305/585 [02:12<01:22,  3.39it/s] 52%|█████▏    | 306/585 [02:12<01:21,  3.40it/s] 52%|█████▏    | 307/585 [02:12<01:21,  3.41it/s] 53%|█████▎    | 308/585 [02:13<01:21,  3.42it/s] 53%|█████▎    | 309/585 [02:13<01:20,  3.42it/s] 53%|█████▎    | 310/585 [02:13<01:20,  3.43it/s] 53%|█████▎    | 311/585 [02:13<01:19,  3.43it/s] 53%|█████▎    | 312/585 [02:14<01:20,  3.40it/s] 54%|█████▎    | 313/585 [02:14<01:19,  3.41it/s] 54%|█████▎    | 314/585 [02:14<01:19,  3.42it/s] 54%|█████▍    | 315/585 [02:15<01:18,  3.42it/s] 54%|█████▍    | 316/585 [02:15<01:18,  3.42it/s] 54%|█████▍    | 317/585 [02:15<01:18,  3.43it/s] 54%|█████▍    | 318/585 [02:16<01:17,  3.43it/s] 55%|█████▍    | 319/585 [02:16<01:17,  3.43it/s] 55%|█████▍    | 320/585 [02:16<01:17,  3.43it/s] 55%|█████▍    | 321/585 [02:16<01:16,  3.43it/s] 55%|█████▌    | 322/585 [02:17<01:16,  3.43it/s] 55%|█████▌    | 323/585 [02:17<01:16,  3.42it/s] 55%|█████▌    | 324/585 [02:17<01:16,  3.42it/s] 56%|█████▌    | 325/585 [02:18<01:15,  3.43it/s] 56%|█████▌    | 326/585 [02:18<01:15,  3.43it/s] 56%|█████▌    | 327/585 [02:18<01:15,  3.43it/s] 56%|█████▌    | 328/585 [02:18<01:14,  3.43it/s] 56%|█████▌    | 329/585 [02:19<01:14,  3.43it/s] 56%|█████▋    | 330/585 [02:19<01:14,  3.43it/s] 57%|█████▋    | 331/585 [02:19<01:14,  3.43it/s] 57%|█████▋    | 332/585 [02:20<01:13,  3.43it/s] 57%|█████▋    | 333/585 [02:20<01:13,  3.43it/s] 57%|█████▋    | 334/585 [02:20<01:17,  3.22it/s] 57%|█████▋    | 335/585 [02:21<01:16,  3.28it/s] 57%|█████▋    | 336/585 [02:21<01:14,  3.32it/s] 58%|█████▊    | 337/585 [02:21<01:13,  3.35it/s] 58%|█████▊    | 338/585 [02:21<01:13,  3.37it/s] 58%|█████▊    | 339/585 [02:22<01:12,  3.39it/s] 58%|█████▊    | 340/585 [02:22<01:12,  3.40it/s] 58%|█████▊    | 341/585 [02:22<01:11,  3.41it/s] 58%|█████▊    | 342/585 [02:23<01:11,  3.42it/s] 59%|█████▊    | 343/585 [02:23<01:10,  3.42it/s] 59%|█████▉    | 344/585 [02:23<01:10,  3.43it/s] 59%|█████▉    | 345/585 [02:23<01:10,  3.41it/s] 59%|█████▉    | 346/585 [02:24<01:10,  3.41it/s] 59%|█████▉    | 347/585 [02:24<01:09,  3.42it/s] 59%|█████▉    | 348/585 [02:24<01:09,  3.43it/s] 60%|█████▉    | 349/585 [02:25<01:08,  3.43it/s] 60%|█████▉    | 350/585 [02:25<01:08,  3.43it/s] 60%|██████    | 351/585 [02:25<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 19:13:05,885 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:13:05,885 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:13:05,885 >>   Batch size = 8
{'eval_loss': 0.9976267218589783, 'eval_runtime': 9.918, 'eval_samples_per_second': 352.187, 'eval_steps_per_second': 44.061, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.96it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.52it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.79it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.14it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.63it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.51it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.29it/s][A
 10%|▉         | 42/437 [00:00<00:10, 38.61it/s][A
 11%|█         | 47/437 [00:01<00:09, 40.27it/s][A
 12%|█▏        | 52/437 [00:01<00:09, 41.60it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 42.45it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.09it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.55it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.68it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.68it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.40it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 43.32it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.55it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.92it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.96it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.26it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.37it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.34it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.02it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.60it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.63it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.72it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.95it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.16it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.25it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.38it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.38it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.98it/s][A
 39%|███▉      | 172/437 [00:04<00:08, 32.51it/s][A
 41%|████      | 177/437 [00:04<00:07, 35.39it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 37.76it/s][A
 43%|████▎     | 187/437 [00:04<00:06, 39.60it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 40.95it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 42.06it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 42.81it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.04it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 42.96it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 43.00it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.09it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.46it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.75it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.06it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.26it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.32it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.09it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.88it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 43.74it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.62it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.89it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.17it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.21it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.37it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.32it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 44.11it/s][A
 69%|██████▉   | 302/437 [00:07<00:05, 23.75it/s][A
 70%|███████   | 307/437 [00:07<00:04, 27.65it/s][A
 71%|███████▏  | 312/437 [00:07<00:04, 31.16it/s][A
 73%|███████▎  | 317/437 [00:07<00:03, 34.28it/s][A
 74%|███████▎  | 322/437 [00:07<00:03, 36.84it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 38.80it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 40.45it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 41.43it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 41.74it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 42.29it/s][A
 81%|████████  | 352/437 [00:08<00:01, 42.82it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.21it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.65it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.98it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.05it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 44.07it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 43.91it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.68it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.61it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.78it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.97it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.16it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.35it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.23it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 42.81it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 43.15it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 43.27it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.46it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 43.46it/s][A 60%|██████    | 351/585 [02:36<01:08,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:13:16,459 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 19:13:16,489 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:13:20,166 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:13:20,217 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:13:20,247 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:48<27:08,  6.99s/it] 60%|██████    | 353/585 [02:48<19:16,  4.98s/it] 61%|██████    | 354/585 [02:48<13:46,  3.58s/it] 61%|██████    | 355/585 [02:49<09:56,  2.59s/it] 61%|██████    | 356/585 [02:49<07:15,  1.90s/it] 61%|██████    | 357/585 [02:49<05:23,  1.42s/it] 61%|██████    | 358/585 [02:50<04:05,  1.08s/it] 61%|██████▏   | 359/585 [02:50<03:11,  1.18it/s] 62%|██████▏   | 360/585 [02:50<02:33,  1.47it/s] 62%|██████▏   | 361/585 [02:51<02:06,  1.77it/s] 62%|██████▏   | 362/585 [02:51<01:48,  2.06it/s] 62%|██████▏   | 363/585 [02:51<01:34,  2.34it/s] 62%|██████▏   | 364/585 [02:51<01:27,  2.52it/s] 62%|██████▏   | 365/585 [02:52<01:20,  2.74it/s] 63%|██████▎   | 366/585 [02:52<01:15,  2.92it/s] 63%|██████▎   | 367/585 [02:52<01:11,  3.06it/s] 63%|██████▎   | 368/585 [02:53<01:08,  3.16it/s] 63%|██████▎   | 369/585 [02:53<01:06,  3.24it/s] 63%|██████▎   | 370/585 [02:53<01:05,  3.30it/s] 63%|██████▎   | 371/585 [02:53<01:04,  3.34it/s] 64%|██████▎   | 372/585 [02:54<01:03,  3.37it/s] 64%|██████▍   | 373/585 [02:54<01:02,  3.39it/s] 64%|██████▍   | 374/585 [02:54<01:01,  3.41it/s] 64%|██████▍   | 375/585 [02:55<01:01,  3.40it/s] 64%|██████▍   | 376/585 [02:55<01:03,  3.29it/s] 64%|██████▍   | 377/585 [02:55<01:10,  2.95it/s] 65%|██████▍   | 378/585 [02:56<01:07,  3.08it/s] 65%|██████▍   | 379/585 [02:56<01:04,  3.18it/s] 65%|██████▍   | 380/585 [02:56<01:02,  3.26it/s] 65%|██████▌   | 381/585 [02:57<01:01,  3.31it/s] 65%|██████▌   | 382/585 [02:57<01:00,  3.34it/s] 65%|██████▌   | 383/585 [02:57<00:59,  3.37it/s] 66%|██████▌   | 384/585 [02:57<00:59,  3.39it/s] 66%|██████▌   | 385/585 [02:58<00:59,  3.39it/s] 66%|██████▌   | 386/585 [02:58<00:58,  3.41it/s] 66%|██████▌   | 387/585 [02:58<00:58,  3.41it/s] 66%|██████▋   | 388/585 [02:59<00:57,  3.42it/s] 66%|██████▋   | 389/585 [02:59<00:57,  3.43it/s] 67%|██████▋   | 390/585 [02:59<00:56,  3.43it/s] 67%|██████▋   | 391/585 [02:59<00:56,  3.43it/s] 67%|██████▋   | 392/585 [03:00<00:56,  3.43it/s] 67%|██████▋   | 393/585 [03:00<00:55,  3.43it/s] 67%|██████▋   | 394/585 [03:00<00:55,  3.43it/s] 68%|██████▊   | 395/585 [03:01<00:55,  3.44it/s] 68%|██████▊   | 396/585 [03:01<00:59,  3.16it/s] 68%|██████▊   | 397/585 [03:01<00:58,  3.24it/s] 68%|██████▊   | 398/585 [03:02<00:56,  3.29it/s] 68%|██████▊   | 399/585 [03:02<00:55,  3.33it/s] 68%|██████▊   | 400/585 [03:02<00:54,  3.37it/s] 69%|██████▊   | 401/585 [03:02<00:54,  3.38it/s] 69%|██████▊   | 402/585 [03:03<00:53,  3.40it/s] 69%|██████▉   | 403/585 [03:03<00:53,  3.41it/s] 69%|██████▉   | 404/585 [03:03<00:52,  3.42it/s] 69%|██████▉   | 405/585 [03:04<00:52,  3.42it/s] 69%|██████▉   | 406/585 [03:04<00:52,  3.43it/s] 70%|██████▉   | 407/585 [03:04<00:54,  3.25it/s] 70%|██████▉   | 408/585 [03:05<00:53,  3.31it/s] 70%|██████▉   | 409/585 [03:05<00:52,  3.35it/s] 70%|███████   | 410/585 [03:05<00:51,  3.37it/s] 70%|███████   | 411/585 [03:05<00:51,  3.39it/s] 70%|███████   | 412/585 [03:06<00:50,  3.40it/s] 71%|███████   | 413/585 [03:06<00:50,  3.41it/s] 71%|███████   | 414/585 [03:06<00:50,  3.42it/s] 71%|███████   | 415/585 [03:07<00:49,  3.42it/s] 71%|███████   | 416/585 [03:07<00:49,  3.43it/s] 71%|███████▏  | 417/585 [03:07<00:49,  3.43it/s] 71%|███████▏  | 418/585 [03:07<00:49,  3.38it/s] 72%|███████▏  | 419/585 [03:08<00:48,  3.39it/s] 72%|███████▏  | 420/585 [03:08<00:48,  3.40it/s] 72%|███████▏  | 421/585 [03:08<00:48,  3.41it/s] 72%|███████▏  | 422/585 [03:09<00:47,  3.42it/s] 72%|███████▏  | 423/585 [03:09<00:47,  3.42it/s] 72%|███████▏  | 424/585 [03:09<00:46,  3.43it/s] 73%|███████▎  | 425/585 [03:09<00:46,  3.43it/s] 73%|███████▎  | 426/585 [03:10<00:46,  3.43it/s] 73%|███████▎  | 427/585 [03:10<00:46,  3.43it/s] 73%|███████▎  | 428/585 [03:10<00:45,  3.43it/s] 73%|███████▎  | 429/585 [03:11<00:45,  3.43it/s] 74%|███████▎  | 430/585 [03:11<00:45,  3.43it/s] 74%|███████▎  | 431/585 [03:11<00:44,  3.43it/s] 74%|███████▍  | 432/585 [03:12<00:44,  3.43it/s] 74%|███████▍  | 433/585 [03:12<00:44,  3.42it/s] 74%|███████▍  | 434/585 [03:12<00:44,  3.42it/s] 74%|███████▍  | 435/585 [03:12<00:43,  3.43it/s] 75%|███████▍  | 436/585 [03:13<00:43,  3.43it/s] 75%|███████▍  | 437/585 [03:13<00:43,  3.43it/s] 75%|███████▍  | 438/585 [03:13<00:42,  3.43it/s] 75%|███████▌  | 439/585 [03:14<00:42,  3.43it/s] 75%|███████▌  | 440/585 [03:14<00:42,  3.43it/s] 75%|███████▌  | 441/585 [03:14<00:41,  3.44it/s] 76%|███████▌  | 442/585 [03:14<00:41,  3.43it/s] 76%|███████▌  | 443/585 [03:15<00:41,  3.43it/s] 76%|███████▌  | 444/585 [03:15<00:44,  3.16it/s] 76%|███████▌  | 445/585 [03:15<00:43,  3.23it/s] 76%|███████▌  | 446/585 [03:16<00:42,  3.29it/s] 76%|███████▋  | 447/585 [03:16<00:41,  3.33it/s] 77%|███████▋  | 448/585 [03:16<00:40,  3.36it/s] 77%|███████▋  | 449/585 [03:17<00:40,  3.38it/s] 77%|███████▋  | 450/585 [03:17<00:39,  3.39it/s] 77%|███████▋  | 451/585 [03:17<00:39,  3.41it/s] 77%|███████▋  | 452/585 [03:17<00:38,  3.41it/s] 77%|███████▋  | 453/585 [03:18<00:38,  3.42it/s] 78%|███████▊  | 454/585 [03:18<00:39,  3.36it/s] 78%|███████▊  | 455/585 [03:18<00:38,  3.38it/s] 78%|███████▊  | 456/585 [03:19<00:37,  3.40it/s] 78%|███████▊  | 457/585 [03:19<00:37,  3.41it/s] 78%|███████▊  | 458/585 [03:19<00:37,  3.41it/s] 78%|███████▊  | 459/585 [03:20<00:36,  3.42it/s] 79%|███████▊  | 460/585 [03:20<00:36,  3.43it/s] 79%|███████▉  | 461/585 [03:20<00:36,  3.42it/s] 79%|███████▉  | 462/585 [03:20<00:35,  3.42it/s] 79%|███████▉  | 463/585 [03:21<00:35,  3.43it/s] 79%|███████▉  | 464/585 [03:21<00:35,  3.42it/s] 79%|███████▉  | 465/585 [03:21<00:35,  3.39it/s] 80%|███████▉  | 466/585 [03:22<00:34,  3.40it/s] 80%|███████▉  | 467/585 [03:22<00:34,  3.41it/s] 80%|████████  | 468/585 [03:22<00:34,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 19:14:02,818 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:14:02,818 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:14:02,818 >>   Batch size = 8
{'eval_loss': 1.0161430835723877, 'eval_runtime': 10.4246, 'eval_samples_per_second': 335.073, 'eval_steps_per_second': 41.92, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.71it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.09it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.36it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.31it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.81it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.49it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.11it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.05it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.18it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.34it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.34it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.21it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.19it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.99it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.97it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.07it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.53it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.76it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.06it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.03it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.07it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.99it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.99it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.78it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.74it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.98it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.11it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.17it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.28it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.16it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.03it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.02it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.89it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.88it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.06it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.14it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.23it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.26it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.08it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.01it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.96it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.80it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.69it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.82it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.07it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.18it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.23it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.17it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.14it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.98it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.90it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.93it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.06it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.18it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.27it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.23it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.18it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.11it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.94it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.79it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.94it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.90it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.25it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.35it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.29it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.21it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.99it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.85it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.77it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.88it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.06it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.29it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.25it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.28it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.21it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.97it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.78it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.84it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.89it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.08it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.20it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.23it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.22it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.10it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.90it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.86it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.83it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:32<00:34,  3.41it/s]
100%|██████████| 437/437 [00:09<00:00, 43.83it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:14:12,755 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 19:14:12,892 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:14:20,471 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:14:20,596 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:14:20,634 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:55<19:12,  9.93s/it] 80%|████████  | 470/585 [03:55<13:29,  7.04s/it] 81%|████████  | 471/585 [03:55<09:32,  5.02s/it] 81%|████████  | 472/585 [03:56<06:50,  3.63s/it] 81%|████████  | 473/585 [03:56<04:54,  2.63s/it] 81%|████████  | 474/585 [03:56<03:34,  1.93s/it] 81%|████████  | 475/585 [03:56<02:38,  1.44s/it] 81%|████████▏ | 476/585 [03:57<01:59,  1.10s/it] 82%|████████▏ | 477/585 [03:57<01:32,  1.17it/s] 82%|████████▏ | 478/585 [03:57<01:13,  1.46it/s] 82%|████████▏ | 479/585 [03:58<01:00,  1.76it/s] 82%|████████▏ | 480/585 [03:58<00:51,  2.04it/s] 82%|████████▏ | 481/585 [03:58<00:44,  2.32it/s] 82%|████████▏ | 482/585 [03:59<00:40,  2.56it/s] 83%|████████▎ | 483/585 [03:59<00:36,  2.77it/s] 83%|████████▎ | 484/585 [03:59<00:34,  2.93it/s] 83%|████████▎ | 485/585 [03:59<00:32,  3.05it/s] 83%|████████▎ | 486/585 [04:00<00:31,  3.14it/s] 83%|████████▎ | 487/585 [04:00<00:30,  3.21it/s] 83%|████████▎ | 488/585 [04:00<00:29,  3.27it/s] 84%|████████▎ | 489/585 [04:01<00:29,  3.30it/s] 84%|████████▍ | 490/585 [04:01<00:28,  3.33it/s] 84%|████████▍ | 491/585 [04:01<00:28,  3.34it/s] 84%|████████▍ | 492/585 [04:01<00:27,  3.35it/s] 84%|████████▍ | 493/585 [04:02<00:27,  3.37it/s] 84%|████████▍ | 494/585 [04:02<00:27,  3.37it/s] 85%|████████▍ | 495/585 [04:02<00:26,  3.38it/s] 85%|████████▍ | 496/585 [04:03<00:26,  3.38it/s] 85%|████████▍ | 497/585 [04:03<00:26,  3.38it/s] 85%|████████▌ | 498/585 [04:03<00:25,  3.38it/s] 85%|████████▌ | 499/585 [04:04<00:25,  3.39it/s] 85%|████████▌ | 500/585 [04:04<00:25,  3.40it/s]                                                  85%|████████▌ | 500/585 [04:04<00:25,  3.40it/s] 86%|████████▌ | 501/585 [04:04<00:24,  3.41it/s] 86%|████████▌ | 502/585 [04:04<00:25,  3.31it/s] 86%|████████▌ | 503/585 [04:05<00:24,  3.35it/s] 86%|████████▌ | 504/585 [04:05<00:23,  3.38it/s] 86%|████████▋ | 505/585 [04:05<00:23,  3.40it/s] 86%|████████▋ | 506/585 [04:06<00:23,  3.41it/s] 87%|████████▋ | 507/585 [04:06<00:22,  3.42it/s] 87%|████████▋ | 508/585 [04:06<00:22,  3.42it/s] 87%|████████▋ | 509/585 [04:06<00:22,  3.43it/s] 87%|████████▋ | 510/585 [04:07<00:21,  3.43it/s] 87%|████████▋ | 511/585 [04:07<00:21,  3.43it/s] 88%|████████▊ | 512/585 [04:07<00:21,  3.43it/s] 88%|████████▊ | 513/585 [04:08<00:21,  3.42it/s] 88%|████████▊ | 514/585 [04:08<00:20,  3.43it/s] 88%|████████▊ | 515/585 [04:08<00:20,  3.43it/s] 88%|████████▊ | 516/585 [04:09<00:20,  3.43it/s] 88%|████████▊ | 517/585 [04:09<00:19,  3.43it/s] 89%|████████▊ | 518/585 [04:09<00:19,  3.43it/s] 89%|████████▊ | 519/585 [04:09<00:19,  3.44it/s] 89%|████████▉ | 520/585 [04:10<00:18,  3.44it/s] 89%|████████▉ | 521/585 [04:10<00:18,  3.44it/s] 89%|████████▉ | 522/585 [04:10<00:18,  3.44it/s] 89%|████████▉ | 523/585 [04:11<00:18,  3.44it/s] 90%|████████▉ | 524/585 [04:11<00:17,  3.44it/s] 90%|████████▉ | 525/585 [04:11<00:17,  3.44it/s] 90%|████████▉ | 526/585 [04:11<00:17,  3.44it/s] 90%|█████████ | 527/585 [04:12<00:16,  3.44it/s] 90%|█████████ | 528/585 [04:12<00:16,  3.44it/s] 90%|█████████ | 529/585 [04:12<00:16,  3.44it/s] 91%|█████████ | 530/585 [04:13<00:16,  3.43it/s] 91%|█████████ | 531/585 [04:13<00:15,  3.43it/s] 91%|█████████ | 532/585 [04:13<00:15,  3.43it/s] 91%|█████████ | 533/585 [04:13<00:15,  3.42it/s] 91%|█████████▏| 534/585 [04:14<00:14,  3.42it/s] 91%|█████████▏| 535/585 [04:14<00:14,  3.43it/s] 92%|█████████▏| 536/585 [04:14<00:14,  3.43it/s] 92%|█████████▏| 537/585 [04:15<00:13,  3.43it/s] 92%|█████████▏| 538/585 [04:15<00:13,  3.43it/s] 92%|█████████▏| 539/585 [04:15<00:13,  3.43it/s] 92%|█████████▏| 540/585 [04:16<00:13,  3.43it/s] 92%|█████████▏| 541/585 [04:16<00:12,  3.43it/s] 93%|█████████▎| 542/585 [04:16<00:12,  3.43it/s] 93%|█████████▎| 543/585 [04:16<00:12,  3.44it/s] 93%|█████████▎| 544/585 [04:17<00:12,  3.41it/s] 93%|█████████▎| 545/585 [04:17<00:11,  3.42it/s] 93%|█████████▎| 546/585 [04:17<00:11,  3.43it/s] 94%|█████████▎| 547/585 [04:18<00:11,  3.43it/s] 94%|█████████▎| 548/585 [04:18<00:10,  3.43it/s] 94%|█████████▍| 549/585 [04:18<00:10,  3.43it/s] 94%|█████████▍| 550/585 [04:18<00:10,  3.43it/s] 94%|█████████▍| 551/585 [04:19<00:09,  3.43it/s] 94%|█████████▍| 552/585 [04:19<00:09,  3.44it/s] 95%|█████████▍| 553/585 [04:19<00:09,  3.43it/s] 95%|█████████▍| 554/585 [04:20<00:09,  3.44it/s] 95%|█████████▍| 555/585 [04:20<00:08,  3.41it/s] 95%|█████████▌| 556/585 [04:20<00:08,  3.41it/s] 95%|█████████▌| 557/585 [04:20<00:08,  3.42it/s] 95%|█████████▌| 558/585 [04:21<00:07,  3.42it/s] 96%|█████████▌| 559/585 [04:21<00:07,  3.43it/s] 96%|█████████▌| 560/585 [04:21<00:07,  3.42it/s] 96%|█████████▌| 561/585 [04:22<00:07,  3.42it/s] 96%|█████████▌| 562/585 [04:22<00:06,  3.43it/s] 96%|█████████▌| 563/585 [04:22<00:06,  3.43it/s] 96%|█████████▋| 564/585 [04:23<00:06,  3.43it/s] 97%|█████████▋| 565/585 [04:23<00:05,  3.43it/s] 97%|█████████▋| 566/585 [04:23<00:06,  3.13it/s] 97%|█████████▋| 567/585 [04:23<00:05,  3.22it/s] 97%|█████████▋| 568/585 [04:24<00:05,  3.29it/s] 97%|█████████▋| 569/585 [04:24<00:04,  3.33it/s] 97%|█████████▋| 570/585 [04:24<00:04,  3.36it/s] 98%|█████████▊| 571/585 [04:25<00:04,  3.38it/s] 98%|█████████▊| 572/585 [04:25<00:03,  3.39it/s] 98%|█████████▊| 573/585 [04:25<00:03,  3.41it/s] 98%|█████████▊| 574/585 [04:26<00:03,  3.41it/s] 98%|█████████▊| 575/585 [04:26<00:02,  3.42it/s] 98%|█████████▊| 576/585 [04:26<00:02,  3.41it/s] 99%|█████████▊| 577/585 [04:26<00:02,  3.42it/s] 99%|█████████▉| 578/585 [04:27<00:02,  3.43it/s] 99%|█████████▉| 579/585 [04:27<00:01,  3.42it/s] 99%|█████████▉| 580/585 [04:27<00:01,  3.43it/s] 99%|█████████▉| 581/585 [04:28<00:01,  3.43it/s] 99%|█████████▉| 582/585 [04:28<00:00,  3.43it/s]100%|█████████▉| 583/585 [04:28<00:00,  3.43it/s]100%|█████████▉| 584/585 [04:28<00:00,  3.43it/s]100%|██████████| 585/585 [04:29<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 19:15:09,256 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:15:09,256 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:15:09,256 >>   Batch size = 8
{'eval_loss': 1.0203757286071777, 'eval_runtime': 9.9204, 'eval_samples_per_second': 352.104, 'eval_steps_per_second': 44.051, 'epoch': 4.0}
{'loss': 0.615, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.25it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.51it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.87it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.16it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.72it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.57it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.52it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.29it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.43it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.37it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.13it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.14it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.05it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.98it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.13it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.22it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.12it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.19it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.20it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.04it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.02it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.09it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.02it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.16it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.24it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.26it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.22it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.17it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 42.87it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.30it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.52it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.62it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.91it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.12it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.08it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.04it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.85it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.95it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.02it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.94it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.12it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.23it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.24it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.10it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.08it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.97it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.98it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.98it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.03it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.01it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.14it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.15it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.96it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.97it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.93it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.82it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.93it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.05it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.08it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.19it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.05it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.04it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.94it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.02it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.97it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.02it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.20it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.25it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.22it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.18it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.02it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.08it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.97it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.07it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.05it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.16it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.24it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.21it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.02it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.87it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.95it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.02it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.04it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.10it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.16it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.27it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.11it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.11it/s][A100%|██████████| 585/585 [04:39<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:15:19,187 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 19:15:19,213 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:15:24,529 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:15:25,266 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:15:25,287 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:15:36,338 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:15:36,341 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117 (score: 0.9836576581001282).
                                                 100%|██████████| 585/585 [04:59<00:00,  3.43it/s]100%|██████████| 585/585 [04:59<00:00,  1.95it/s]
[INFO|trainer.py:1894] 2023-08-28 19:15:39,816 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 19:15:39,832 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:15:43,841 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:15:43,864 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:15:43,872 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:15:44,083 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:44,083 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:44,083 >>   train_loss               =     0.6112
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:44,083 >>   train_runtime            = 0:04:59.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:44,083 >>   train_samples            =       7518
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:44,083 >>   train_samples_per_second =    125.395
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:44,083 >>   train_steps_per_second   =      1.951
{'eval_loss': 1.0256036520004272, 'eval_runtime': 9.9161, 'eval_samples_per_second': 352.255, 'eval_steps_per_second': 44.07, 'epoch': 5.0}
{'train_runtime': 299.7725, 'train_samples_per_second': 125.395, 'train_steps_per_second': 1.951, 'train_loss': 0.6111559484758947, 'epoch': 5.0}
08/28/2023 19:15:44 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:15:44,125 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:15:44,126 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:15:44,126 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.65it/s]  3%|▎         | 12/437 [00:00<00:08, 48.55it/s]  4%|▍         | 17/437 [00:00<00:08, 46.98it/s]  5%|▌         | 22/437 [00:00<00:08, 46.32it/s]  6%|▌         | 27/437 [00:00<00:08, 45.77it/s]  7%|▋         | 32/437 [00:00<00:08, 45.55it/s]  8%|▊         | 37/437 [00:00<00:08, 45.42it/s] 10%|▉         | 42/437 [00:00<00:08, 44.96it/s] 11%|█         | 47/437 [00:01<00:08, 44.25it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.04it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.05it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.24it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.22it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.40it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.53it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.53it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.32it/s] 21%|██        | 92/437 [00:02<00:07, 44.04it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.81it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.02it/s] 24%|██▍       | 107/437 [00:02<00:07, 43.97it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.22it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.25it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.37it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.30it/s] 30%|███       | 132/437 [00:02<00:06, 44.22it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.04it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.91it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.08it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.24it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.30it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.32it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.33it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.25it/s] 41%|████      | 177/437 [00:03<00:05, 44.18it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.04it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.01it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.12it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.26it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.27it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.34it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.32it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.19it/s] 51%|█████     | 222/437 [00:04<00:04, 44.09it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.01it/s] 53%|█████▎    | 232/437 [00:05<00:04, 43.90it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.09it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.09it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.27it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.36it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.23it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.21it/s] 61%|██████    | 267/437 [00:06<00:03, 44.04it/s] 62%|██████▏   | 272/437 [00:06<00:03, 43.98it/s] 63%|██████▎   | 277/437 [00:06<00:03, 43.96it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.07it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.24it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.13it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.28it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.30it/s] 70%|███████   | 307/437 [00:06<00:02, 44.13it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.04it/s] 73%|███████▎  | 317/437 [00:07<00:02, 43.94it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.02it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.22it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.24it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.31it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.26it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.09it/s] 81%|████████  | 352/437 [00:07<00:01, 44.05it/s] 82%|████████▏ | 357/437 [00:08<00:01, 43.93it/s] 83%|████████▎ | 362/437 [00:08<00:01, 43.98it/s] 84%|████████▍ | 367/437 [00:08<00:01, 43.97it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.14it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.25it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.26it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.24it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.17it/s] 91%|█████████ | 397/437 [00:08<00:00, 43.93it/s] 92%|█████████▏| 402/437 [00:09<00:00, 43.84it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.06it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.07it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.26it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.26it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.25it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.26it/s]100%|██████████| 437/437 [00:09<00:00, 44.19it/s]100%|██████████| 437/437 [00:09<00:00, 44.29it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:15:54,013 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:54,013 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:54,013 >>   eval_loss               =     0.9837
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:54,013 >>   eval_runtime            = 0:00:09.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:54,013 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:54,013 >>   eval_samples_per_second =    353.278
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:54,013 >>   eval_steps_per_second   =     44.198
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:54,013 >>   perplexity              =     2.6742
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:01,252 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:01,260 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:01,260 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:01,260 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:01,260 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:16:01,594 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:16:01,595 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:16:01,867 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:16:02,916 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:16:02,916 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:05,083 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:05,106 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:05,106 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:05,106 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:16:05,106 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:16:05,775 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:16:05,776 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:16:06,527 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:16:06,865 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:16:06,865 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.76it/s]Extractor Predicting: 2it [00:01,  1.81it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.51it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:11,  1.67it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.60it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:17,  1.75it/s]Extractor Predicting: 29it [00:17,  1.72it/s]Extractor Predicting: 30it [00:18,  1.68it/s]Extractor Predicting: 31it [00:18,  1.66it/s]Extractor Predicting: 32it [00:19,  1.64it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:20,  1.57it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:22,  1.56it/s]Extractor Predicting: 38it [00:23,  1.55it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:24,  1.57it/s]Extractor Predicting: 41it [00:25,  1.57it/s]Extractor Predicting: 42it [00:25,  1.59it/s]Extractor Predicting: 43it [00:26,  1.59it/s]Extractor Predicting: 44it [00:27,  1.61it/s]Extractor Predicting: 45it [00:27,  1.63it/s]Extractor Predicting: 46it [00:28,  1.58it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:29,  1.57it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:30,  1.57it/s]Extractor Predicting: 51it [00:31,  1.58it/s]Extractor Predicting: 52it [00:32,  1.56it/s]Extractor Predicting: 53it [00:32,  1.60it/s]Extractor Predicting: 54it [00:33,  1.57it/s]Extractor Predicting: 55it [00:34,  1.55it/s]Extractor Predicting: 56it [00:34,  1.56it/s]Extractor Predicting: 57it [00:35,  1.53it/s]Extractor Predicting: 58it [00:36,  1.53it/s]Extractor Predicting: 59it [00:36,  1.52it/s]Extractor Predicting: 60it [00:37,  1.52it/s]Extractor Predicting: 61it [00:38,  1.54it/s]Extractor Predicting: 62it [00:38,  1.54it/s]Extractor Predicting: 63it [00:39,  1.57it/s]Extractor Predicting: 64it [00:39,  1.59it/s]Extractor Predicting: 65it [00:40,  1.56it/s]Extractor Predicting: 66it [00:41,  1.60it/s]Extractor Predicting: 67it [00:41,  1.59it/s]Extractor Predicting: 68it [00:42,  1.58it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:43,  1.55it/s]Extractor Predicting: 71it [00:44,  1.56it/s]Extractor Predicting: 72it [00:45,  1.58it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:46,  1.61it/s]Extractor Predicting: 75it [00:46,  1.62it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:48,  1.57it/s]Extractor Predicting: 79it [00:49,  1.57it/s]Extractor Predicting: 80it [00:50,  1.56it/s]Extractor Predicting: 81it [00:50,  1.60it/s]Extractor Predicting: 82it [00:51,  1.59it/s]Extractor Predicting: 83it [00:52,  1.56it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:53,  1.60it/s]Extractor Predicting: 86it [00:53,  1.66it/s]Extractor Predicting: 87it [00:54,  1.70it/s]Extractor Predicting: 88it [00:54,  1.70it/s]Extractor Predicting: 89it [00:55,  1.55it/s]Extractor Predicting: 90it [00:56,  1.60it/s]Extractor Predicting: 91it [00:56,  1.61it/s]Extractor Predicting: 92it [00:57,  1.56it/s]Extractor Predicting: 93it [00:58,  1.62it/s]Extractor Predicting: 94it [00:58,  1.66it/s]Extractor Predicting: 95it [00:59,  1.67it/s]Extractor Predicting: 96it [00:59,  1.69it/s]Extractor Predicting: 97it [01:00,  1.67it/s]Extractor Predicting: 98it [01:01,  1.63it/s]Extractor Predicting: 99it [01:01,  1.61it/s]Extractor Predicting: 100it [01:02,  1.64it/s]Extractor Predicting: 101it [01:02,  1.68it/s]Extractor Predicting: 102it [01:03,  1.66it/s]Extractor Predicting: 103it [01:04,  1.62it/s]Extractor Predicting: 104it [01:04,  1.63it/s]Extractor Predicting: 105it [01:05,  1.68it/s]Extractor Predicting: 106it [01:05,  1.66it/s]Extractor Predicting: 107it [01:06,  1.68it/s]Extractor Predicting: 108it [01:07,  1.68it/s]Extractor Predicting: 109it [01:07,  1.72it/s]Extractor Predicting: 110it [01:08,  1.70it/s]Extractor Predicting: 111it [01:08,  1.69it/s]Extractor Predicting: 112it [01:09,  1.67it/s]Extractor Predicting: 113it [01:10,  1.67it/s]Extractor Predicting: 114it [01:10,  1.61it/s]Extractor Predicting: 115it [01:11,  1.63it/s]Extractor Predicting: 116it [01:12,  1.62it/s]Extractor Predicting: 117it [01:12,  1.61it/s]Extractor Predicting: 118it [01:13,  1.67it/s]Extractor Predicting: 119it [01:13,  1.64it/s]Extractor Predicting: 120it [01:14,  1.66it/s]Extractor Predicting: 121it [01:15,  1.64it/s]Extractor Predicting: 122it [01:15,  1.63it/s]Extractor Predicting: 123it [01:16,  1.63it/s]Extractor Predicting: 124it [01:16,  1.59it/s]Extractor Predicting: 125it [01:17,  1.59it/s]Extractor Predicting: 126it [01:18,  1.60it/s]Extractor Predicting: 127it [01:18,  1.57it/s]Extractor Predicting: 128it [01:19,  1.57it/s]Extractor Predicting: 129it [01:20,  1.53it/s]Extractor Predicting: 130it [01:20,  1.55it/s]Extractor Predicting: 131it [01:21,  1.57it/s]Extractor Predicting: 132it [01:22,  1.58it/s]Extractor Predicting: 133it [01:22,  1.58it/s]Extractor Predicting: 134it [01:23,  1.59it/s]Extractor Predicting: 135it [01:23,  1.59it/s]Extractor Predicting: 136it [01:24,  1.59it/s]Extractor Predicting: 137it [01:25,  1.55it/s]Extractor Predicting: 138it [01:25,  1.56it/s]Extractor Predicting: 139it [01:26,  1.56it/s]Extractor Predicting: 140it [01:27,  1.61it/s]Extractor Predicting: 141it [01:27,  1.58it/s]Extractor Predicting: 142it [01:28,  1.65it/s]Extractor Predicting: 142it [01:28,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:45,705 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:45,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:45,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:45,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:45,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:17:46,328 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:17:46,329 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:17:46,942 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:17:48,018 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:17:48,018 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:51,019 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:51,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:51,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:51,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:51,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:17:51,797 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:17:51,798 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:17:52,377 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:17:52,548 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:17:52,549 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2637913741223671,
  "recall": 0.07529344403091898,
  "score": 0.11714922048997772,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:05,  1.70it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:10,  1.62it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.62it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:17,  1.58it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:19,  1.55it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:20,  1.57it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:24,  1.62it/s]Extractor Predicting: 40it [00:24,  1.63it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.62it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.61it/s]Extractor Predicting: 47it [00:28,  1.62it/s]Extractor Predicting: 48it [00:29,  1.62it/s]Extractor Predicting: 49it [00:30,  1.61it/s]Extractor Predicting: 50it [00:30,  1.58it/s]Extractor Predicting: 51it [00:31,  1.61it/s]Extractor Predicting: 52it [00:32,  1.62it/s]Extractor Predicting: 53it [00:32,  1.61it/s]Extractor Predicting: 54it [00:33,  1.61it/s]Extractor Predicting: 55it [00:33,  1.57it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:35,  1.57it/s]Extractor Predicting: 58it [00:35,  1.61it/s]Extractor Predicting: 59it [00:36,  1.62it/s]Extractor Predicting: 60it [00:37,  1.63it/s]Extractor Predicting: 61it [00:37,  1.62it/s]Extractor Predicting: 62it [00:38,  1.62it/s]Extractor Predicting: 63it [00:38,  1.63it/s]Extractor Predicting: 64it [00:39,  1.59it/s]Extractor Predicting: 65it [00:40,  1.59it/s]Extractor Predicting: 66it [00:40,  1.59it/s]Extractor Predicting: 67it [00:41,  1.61it/s]Extractor Predicting: 68it [00:41,  1.66it/s]Extractor Predicting: 69it [00:42,  1.65it/s]Extractor Predicting: 70it [00:43,  1.64it/s]Extractor Predicting: 71it [00:44,  1.48it/s]Extractor Predicting: 72it [00:44,  1.52it/s]Extractor Predicting: 73it [00:45,  1.50it/s]Extractor Predicting: 74it [00:45,  1.53it/s]Extractor Predicting: 75it [00:46,  1.55it/s]Extractor Predicting: 76it [00:47,  1.56it/s]Extractor Predicting: 77it [00:47,  1.54it/s]Extractor Predicting: 78it [00:48,  1.56it/s]Extractor Predicting: 79it [00:49,  1.56it/s]Extractor Predicting: 80it [00:49,  1.57it/s]Extractor Predicting: 81it [00:50,  1.58it/s]Extractor Predicting: 82it [00:51,  1.54it/s]Extractor Predicting: 83it [00:51,  1.56it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:52,  1.59it/s]Extractor Predicting: 86it [00:53,  1.57it/s]Extractor Predicting: 87it [00:54,  1.56it/s]Extractor Predicting: 88it [00:54,  1.53it/s]Extractor Predicting: 89it [00:55,  1.55it/s]Extractor Predicting: 90it [00:56,  1.58it/s]Extractor Predicting: 91it [00:56,  1.56it/s]Extractor Predicting: 92it [00:57,  1.55it/s]Extractor Predicting: 93it [00:58,  1.53it/s]Extractor Predicting: 94it [00:58,  1.51it/s]Extractor Predicting: 95it [00:59,  1.53it/s]Extractor Predicting: 96it [01:00,  1.53it/s]Extractor Predicting: 97it [01:00,  1.54it/s]Extractor Predicting: 98it [01:01,  1.54it/s]Extractor Predicting: 99it [01:02,  1.55it/s]Extractor Predicting: 100it [01:02,  1.56it/s]Extractor Predicting: 101it [01:03,  1.56it/s]Extractor Predicting: 102it [01:03,  1.55it/s]Extractor Predicting: 103it [01:04,  1.57it/s]Extractor Predicting: 104it [01:05,  1.56it/s]Extractor Predicting: 105it [01:05,  1.55it/s]Extractor Predicting: 106it [01:06,  1.56it/s]Extractor Predicting: 107it [01:07,  1.54it/s]Extractor Predicting: 108it [01:07,  1.53it/s]Extractor Predicting: 109it [01:08,  1.54it/s]Extractor Predicting: 110it [01:09,  1.51it/s]Extractor Predicting: 111it [01:09,  1.51it/s]Extractor Predicting: 112it [01:10,  1.56it/s]Extractor Predicting: 113it [01:11,  1.59it/s]Extractor Predicting: 114it [01:11,  1.60it/s]Extractor Predicting: 115it [01:12,  1.59it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:13,  1.61it/s]Extractor Predicting: 118it [01:14,  1.62it/s]Extractor Predicting: 119it [01:14,  1.61it/s]Extractor Predicting: 120it [01:15,  1.62it/s]Extractor Predicting: 121it [01:16,  1.62it/s]Extractor Predicting: 122it [01:16,  1.67it/s]Extractor Predicting: 123it [01:17,  1.65it/s]Extractor Predicting: 124it [01:17,  1.65it/s]Extractor Predicting: 125it [01:18,  1.64it/s]Extractor Predicting: 126it [01:19,  1.62it/s]Extractor Predicting: 127it [01:19,  1.62it/s]Extractor Predicting: 128it [01:20,  1.64it/s]Extractor Predicting: 129it [01:20,  1.61it/s]Extractor Predicting: 130it [01:21,  1.62it/s]Extractor Predicting: 131it [01:22,  1.60it/s]Extractor Predicting: 132it [01:22,  1.60it/s]Extractor Predicting: 133it [01:23,  1.59it/s]Extractor Predicting: 134it [01:24,  1.61it/s]Extractor Predicting: 135it [01:24,  1.63it/s]Extractor Predicting: 136it [01:25,  1.63it/s]Extractor Predicting: 137it [01:25,  1.61it/s]Extractor Predicting: 138it [01:26,  1.62it/s]Extractor Predicting: 139it [01:27,  1.66it/s]Extractor Predicting: 140it [01:27,  1.63it/s]Extractor Predicting: 141it [01:28,  1.61it/s]Extractor Predicting: 142it [01:28,  1.66it/s]Extractor Predicting: 143it [01:29,  1.65it/s]Extractor Predicting: 144it [01:30,  1.62it/s]Extractor Predicting: 145it [01:30,  1.63it/s]Extractor Predicting: 146it [01:31,  1.62it/s]Extractor Predicting: 147it [01:32,  1.59it/s]Extractor Predicting: 148it [01:32,  1.59it/s]Extractor Predicting: 149it [01:33,  1.58it/s]Extractor Predicting: 150it [01:33,  1.61it/s]Extractor Predicting: 151it [01:34,  1.63it/s]Extractor Predicting: 152it [01:35,  1.64it/s]Extractor Predicting: 153it [01:35,  1.64it/s]Extractor Predicting: 154it [01:36,  1.61it/s]Extractor Predicting: 155it [01:36,  1.59it/s]Extractor Predicting: 156it [01:37,  1.58it/s]Extractor Predicting: 157it [01:38,  1.56it/s]Extractor Predicting: 158it [01:38,  1.57it/s]Extractor Predicting: 159it [01:39,  1.41it/s]Extractor Predicting: 160it [01:40,  1.45it/s]Extractor Predicting: 161it [01:41,  1.51it/s]Extractor Predicting: 162it [01:41,  1.55it/s]Extractor Predicting: 163it [01:42,  1.55it/s]Extractor Predicting: 164it [01:42,  1.57it/s]Extractor Predicting: 165it [01:43,  1.53it/s]Extractor Predicting: 166it [01:44,  1.56it/s]Extractor Predicting: 167it [01:44,  1.59it/s]Extractor Predicting: 168it [01:45,  1.58it/s]Extractor Predicting: 169it [01:46,  1.58it/s]Extractor Predicting: 170it [01:46,  1.55it/s]Extractor Predicting: 171it [01:47,  1.52it/s]Extractor Predicting: 172it [01:48,  1.53it/s]Extractor Predicting: 173it [01:48,  1.54it/s]Extractor Predicting: 174it [01:49,  1.50it/s]Extractor Predicting: 175it [01:50,  1.46it/s]Extractor Predicting: 176it [01:50,  1.49it/s]Extractor Predicting: 177it [01:51,  1.50it/s]Extractor Predicting: 178it [01:52,  1.55it/s]Extractor Predicting: 179it [01:52,  1.54it/s]Extractor Predicting: 180it [01:53,  1.57it/s]Extractor Predicting: 181it [01:53,  1.56it/s]Extractor Predicting: 182it [01:54,  1.59it/s]Extractor Predicting: 183it [01:55,  1.59it/s]Extractor Predicting: 184it [01:55,  1.63it/s]Extractor Predicting: 185it [01:56,  1.63it/s]Extractor Predicting: 186it [01:57,  1.64it/s]Extractor Predicting: 187it [01:57,  1.65it/s]Extractor Predicting: 188it [01:58,  1.64it/s]Extractor Predicting: 189it [01:58,  1.64it/s]Extractor Predicting: 190it [01:59,  1.61it/s]Extractor Predicting: 191it [02:00,  1.56it/s]Extractor Predicting: 192it [02:00,  1.59it/s]Extractor Predicting: 193it [02:01,  1.63it/s]Extractor Predicting: 194it [02:01,  1.63it/s]Extractor Predicting: 195it [02:02,  1.62it/s]Extractor Predicting: 196it [02:03,  1.64it/s]Extractor Predicting: 197it [02:03,  1.65it/s]Extractor Predicting: 198it [02:04,  1.62it/s]Extractor Predicting: 199it [02:05,  1.62it/s]Extractor Predicting: 200it [02:05,  1.62it/s]Extractor Predicting: 201it [02:06,  1.63it/s]Extractor Predicting: 202it [02:06,  1.66it/s]Extractor Predicting: 203it [02:07,  1.65it/s]Extractor Predicting: 204it [02:08,  1.63it/s]Extractor Predicting: 205it [02:08,  1.59it/s]Extractor Predicting: 206it [02:09,  1.59it/s]Extractor Predicting: 207it [02:09,  1.62it/s]Extractor Predicting: 208it [02:10,  1.61it/s]Extractor Predicting: 209it [02:11,  1.56it/s]Extractor Predicting: 210it [02:11,  1.55it/s]Extractor Predicting: 211it [02:12,  1.56it/s]Extractor Predicting: 212it [02:13,  1.59it/s]Extractor Predicting: 213it [02:13,  1.60it/s]Extractor Predicting: 214it [02:14,  1.56it/s]Extractor Predicting: 215it [02:15,  1.56it/s]Extractor Predicting: 216it [02:15,  1.60it/s]Extractor Predicting: 217it [02:16,  1.61it/s]Extractor Predicting: 218it [02:16,  1.54it/s]Extractor Predicting: 219it [02:17,  1.56it/s]Extractor Predicting: 220it [02:18,  1.57it/s]Extractor Predicting: 221it [02:18,  1.54it/s]Extractor Predicting: 222it [02:19,  1.54it/s]Extractor Predicting: 223it [02:20,  1.53it/s]Extractor Predicting: 224it [02:20,  1.56it/s]Extractor Predicting: 225it [02:21,  1.56it/s]Extractor Predicting: 226it [02:22,  1.61it/s]Extractor Predicting: 227it [02:22,  1.64it/s]Extractor Predicting: 228it [02:23,  1.61it/s]Extractor Predicting: 229it [02:23,  1.62it/s]Extractor Predicting: 230it [02:24,  1.59it/s]Extractor Predicting: 231it [02:25,  1.58it/s]Extractor Predicting: 232it [02:25,  1.59it/s]Extractor Predicting: 233it [02:26,  1.62it/s]Extractor Predicting: 234it [02:27,  1.61it/s]Extractor Predicting: 235it [02:27,  1.62it/s]Extractor Predicting: 236it [02:28,  1.59it/s]Extractor Predicting: 237it [02:28,  1.58it/s]Extractor Predicting: 238it [02:29,  1.60it/s]Extractor Predicting: 239it [02:30,  1.61it/s]Extractor Predicting: 240it [02:30,  1.61it/s]Extractor Predicting: 241it [02:31,  1.59it/s]Extractor Predicting: 242it [02:32,  1.58it/s]Extractor Predicting: 243it [02:32,  1.54it/s]Extractor Predicting: 244it [02:33,  1.56it/s]Extractor Predicting: 245it [02:33,  1.62it/s]Extractor Predicting: 246it [02:34,  1.59it/s]Extractor Predicting: 247it [02:35,  1.61it/s]Extractor Predicting: 248it [02:35,  1.61it/s]Extractor Predicting: 249it [02:36,  1.61it/s]Extractor Predicting: 250it [02:37,  1.44it/s]Extractor Predicting: 251it [02:38,  1.43it/s]Extractor Predicting: 252it [02:38,  1.47it/s]Extractor Predicting: 253it [02:39,  1.51it/s]Extractor Predicting: 254it [02:39,  1.55it/s]Extractor Predicting: 255it [02:40,  1.56it/s]Extractor Predicting: 256it [02:41,  1.55it/s]Extractor Predicting: 257it [02:41,  1.57it/s]Extractor Predicting: 258it [02:42,  1.57it/s]Extractor Predicting: 259it [02:43,  1.53it/s]Extractor Predicting: 260it [02:43,  1.56it/s]Extractor Predicting: 261it [02:44,  1.58it/s]Extractor Predicting: 262it [02:44,  1.56it/s]Extractor Predicting: 263it [02:45,  1.55it/s]Extractor Predicting: 264it [02:46,  1.55it/s]Extractor Predicting: 265it [02:46,  1.54it/s]Extractor Predicting: 266it [02:47,  1.52it/s]Extractor Predicting: 267it [02:48,  1.50it/s]Extractor Predicting: 268it [02:48,  1.52it/s]Extractor Predicting: 269it [02:49,  1.53it/s]Extractor Predicting: 270it [02:50,  1.52it/s]Extractor Predicting: 271it [02:50,  1.51it/s]Extractor Predicting: 272it [02:51,  1.53it/s]Extractor Predicting: 273it [02:52,  1.52it/s]Extractor Predicting: 274it [02:52,  1.52it/s]Extractor Predicting: 275it [02:53,  1.56it/s]Extractor Predicting: 276it [02:54,  1.56it/s]Extractor Predicting: 277it [02:54,  1.54it/s]Extractor Predicting: 278it [02:55,  1.54it/s]Extractor Predicting: 279it [02:56,  1.55it/s]Extractor Predicting: 280it [02:56,  1.53it/s]Extractor Predicting: 281it [02:57,  1.52it/s]Extractor Predicting: 282it [02:58,  1.54it/s]Extractor Predicting: 283it [02:58,  1.50it/s]Extractor Predicting: 284it [02:59,  1.48it/s]Extractor Predicting: 285it [03:00,  1.48it/s]Extractor Predicting: 286it [03:00,  1.46it/s]Extractor Predicting: 287it [03:01,  1.49it/s]Extractor Predicting: 288it [03:01,  1.97it/s]Extractor Predicting: 288it [03:01,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:21:01,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:21:01,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:21:01,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:21:01,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:21:01,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:21:02,413 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:21:02,436 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:21:02,706 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:21:03,768 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:21:03,768 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:21:05,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:21:05,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:21:05,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:21:05,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:21:05,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:21:06,084 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:21:06,085 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:21:06,383 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:21:06,584 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:21:06,584 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4817351598173516,
  "recall": 0.15314269124691537,
  "score": 0.23240444982927635,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:01,  1.90it/s]Extractor Predicting: 3it [00:01,  1.72it/s]
[INFO|configuration_utils.py:515] 2023-08-28 19:21:08,891 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:21:08,892 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:21:08,897 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:21:08,897 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 19:21:08,898 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:21:14,866 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 19:21:14,877 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 19:21:14,917 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:21:14,918 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:21:14,934 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:14,950 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:14,950 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:14,950 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:14,950 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:14,950 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:14,950 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.38461538461538464,
  "recall": 0.04504504504504504,
  "score": 0.08064516129032258,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 19:21:15,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:15,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:16,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:16,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:17,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:18,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:18,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:19,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:19,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:20,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:21,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:21,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:22,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:22,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:23,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:23,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:24,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:24,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:25,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:26,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:26,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<02:55, 12.51s/it][WARNING|generation_utils.py:914] 2023-08-28 19:21:27,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:28,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:28,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:30,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:30,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:31,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:31,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:32,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:33,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:33,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:34,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:34,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:35,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:35,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:36,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:36,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:37,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:37,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:38,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:39,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:39,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:25<02:44, 12.62s/it][WARNING|generation_utils.py:914] 2023-08-28 19:21:40,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:41,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:41,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:42,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:42,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:43,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:43,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:44,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:45,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:45,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:46,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:46,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:47,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:47,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:48,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:49,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:49,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:50,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:50,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:51,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:52,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:52,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:38<02:32, 12.74s/it][WARNING|generation_utils.py:914] 2023-08-28 19:21:53,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:53,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:54,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:54,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:55,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:56,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:56,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:57,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:58,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:58,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:59,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:59,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:00,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:00,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:01,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:02,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:02,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:03,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:03,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:04,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:04,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:50<02:17, 12.47s/it][WARNING|generation_utils.py:914] 2023-08-28 19:22:05,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:05,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:06,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:06,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:07,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:08,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:08,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:09,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:09,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:10,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:10,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:11,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:11,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:12,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:12,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:13,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:13,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:14,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:15,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:15,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:16,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:16,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:02<02:02, 12.27s/it][WARNING|generation_utils.py:914] 2023-08-28 19:22:17,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:18,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:18,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:19,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:19,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:20,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:21,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:21,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:22,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:23,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:23,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:24,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:25,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:25,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:26,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:26,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:27,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:27,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:28,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:29,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:29,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:15<01:53, 12.66s/it][WARNING|generation_utils.py:914] 2023-08-28 19:22:30,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:31,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:32,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:32,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:33,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:33,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:34,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:34,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:35,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:36,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:36,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:37,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:38,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:38,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:39,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:39,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:40,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:41,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:41,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:42,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:43,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:43,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:29<01:44, 13.04s/it][WARNING|generation_utils.py:914] 2023-08-28 19:22:44,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:45,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:45,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:46,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:46,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:47,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:48,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:48,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:49,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:50,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:50,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:51,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:51,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:52,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:53,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:53,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:54,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:54,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:55,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:56,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:56,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:57,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:57,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:58,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:43<01:34, 13.47s/it][WARNING|generation_utils.py:914] 2023-08-28 19:22:58,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:59,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:59,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:00,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:00,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:01,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:02,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:02,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:03,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:03,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:04,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:04,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:05,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:05,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:06,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:06,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:07,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:07,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:08,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:08,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:09,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:54<01:16, 12.68s/it][WARNING|generation_utils.py:914] 2023-08-28 19:23:09,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:10,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:10,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:11,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:12,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:12,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:13,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:13,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:14,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:15,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:15,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:16,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:16,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:17,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:17,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:18,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:19,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:19,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:20,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:20,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:06<01:01, 12.36s/it][WARNING|generation_utils.py:914] 2023-08-28 19:23:21,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:22,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:22,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:23,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:23,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:24,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:24,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:25,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:25,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:26,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:26,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:27,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:28,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:28,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:29,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:29,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:30,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:30,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:31,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:31,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:17<00:47, 11.92s/it][WARNING|generation_utils.py:914] 2023-08-28 19:23:32,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:33,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:33,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:34,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:34,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:35,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:36,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:36,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:37,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:37,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:38,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:39,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:39,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:40,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:40,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:41,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:42,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:42,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:43,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:43,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:44,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:44,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:30<00:36, 12.29s/it][WARNING|generation_utils.py:914] 2023-08-28 19:23:45,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:46,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:46,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:47,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:47,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:48,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:49,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:49,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:50,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:51,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:51,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:52,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:52,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:53,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:53,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:54,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:55,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:55,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:56,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:56,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:57,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:42<00:24, 12.28s/it][WARNING|generation_utils.py:914] 2023-08-28 19:23:57,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:58,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:58,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:59,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:59,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:00,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:01,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:01,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:02,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:02,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:03,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:04,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:04,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:05,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:05,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:06,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:06,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:07,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:07,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:08,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:53<00:11, 11.96s/it][WARNING|generation_utils.py:914] 2023-08-28 19:24:09,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:09,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:10,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:10,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:11,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:12,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:12,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:13,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:13,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:14,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:15,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:15,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:16,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:16,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:17,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:18,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:18,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:19,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:19,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:20,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:21,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:21,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:22,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:23,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:23,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:24,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:24,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:25,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:10<00:00, 13.41s/it]Generating: 100%|██████████| 15/15 [03:10<00:00, 12.71s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:24:33,330 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:24:33,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:24:33,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:24:33,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:24:33,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:24:34,134 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:24:34,134 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:24:34,700 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:24:35,776 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:24:35,776 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:24:37,111 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:24:37,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:24:37,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:24:37,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:24:37,114 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:24:37,948 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:24:37,952 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:24:38,225 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:24:38,417 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:24:38,417 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8735795454545454, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.8735795454545454, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : architect .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8792613636363636, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7981770833333334, 'errors': {'', "('Amaranthus', 'country of origin', '', 'It is home to a number of extinct species of plant species , including the cactus in the family Amaranthus .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8536931818181818, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : operator .', 'success_rate': 0.9032738095238095, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 236, 'raw': 352}
{'target': 600, 'success': 256, 'raw': 384}
{'target': 600, 'success': 278, 'raw': 416}
{'target': 600, 'success': 298, 'raw': 448}
{'target': 600, 'success': 320, 'raw': 480}
{'target': 600, 'success': 343, 'raw': 512}
{'target': 600, 'success': 363, 'raw': 544}
{'target': 600, 'success': 387, 'raw': 576}
{'target': 600, 'success': 406, 'raw': 608}
{'target': 600, 'success': 431, 'raw': 640}
{'target': 600, 'success': 451, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 502, 'raw': 736}
{'target': 600, 'success': 524, 'raw': 768}
{'target': 600, 'success': 547, 'raw': 800}
{'target': 600, 'success': 573, 'raw': 832}
{'target': 600, 'success': 596, 'raw': 864}
{'target': 600, 'success': 617, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6886160714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 10108
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10208, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.61it/s]Extractor Estimating: 2it [00:01,  1.44it/s]Extractor Estimating: 3it [00:01,  1.54it/s]Extractor Estimating: 4it [00:02,  1.70it/s]Extractor Estimating: 5it [00:03,  1.69it/s]Extractor Estimating: 6it [00:03,  1.70it/s]Extractor Estimating: 7it [00:04,  1.74it/s]Extractor Estimating: 8it [00:04,  1.80it/s]Extractor Estimating: 9it [00:05,  1.80it/s]Extractor Estimating: 10it [00:05,  1.79it/s]Extractor Estimating: 11it [00:06,  1.80it/s]Extractor Estimating: 12it [00:07,  1.72it/s]Extractor Estimating: 13it [00:07,  1.72it/s]Extractor Estimating: 14it [00:08,  1.72it/s]Extractor Estimating: 15it [00:08,  1.71it/s]Extractor Estimating: 16it [00:09,  1.72it/s]Extractor Estimating: 17it [00:09,  1.72it/s]Extractor Estimating: 18it [00:10,  1.77it/s]Extractor Estimating: 19it [00:11,  1.75it/s]Extractor Estimating: 20it [00:11,  1.79it/s]Extractor Estimating: 21it [00:12,  1.69it/s]Extractor Estimating: 22it [00:12,  1.71it/s]Extractor Estimating: 23it [00:13,  1.62it/s]Extractor Estimating: 24it [00:14,  1.60it/s]Extractor Estimating: 25it [00:14,  1.65it/s]Extractor Estimating: 26it [00:15,  1.62it/s]Extractor Estimating: 27it [00:15,  1.63it/s]Extractor Estimating: 28it [00:16,  1.57it/s]Extractor Estimating: 29it [00:17,  1.62it/s]Extractor Estimating: 30it [00:17,  1.69it/s]Extractor Estimating: 31it [00:18,  1.69it/s]Extractor Estimating: 32it [00:18,  1.75it/s]Extractor Estimating: 33it [00:19,  1.80it/s]Extractor Estimating: 34it [00:19,  1.79it/s]Extractor Estimating: 35it [00:20,  1.82it/s]Extractor Estimating: 36it [00:21,  1.78it/s]Extractor Estimating: 37it [00:21,  1.76it/s]Extractor Estimating: 38it [00:22,  1.82it/s]Extractor Estimating: 39it [00:22,  1.85it/s]Extractor Estimating: 40it [00:23,  1.88it/s]Extractor Estimating: 41it [00:23,  1.90it/s]Extractor Estimating: 42it [00:24,  1.90it/s]Extractor Estimating: 43it [00:24,  1.89it/s]Extractor Estimating: 44it [00:25,  1.87it/s]Extractor Estimating: 45it [00:25,  1.88it/s]Extractor Estimating: 46it [00:26,  1.82it/s]Extractor Estimating: 47it [00:26,  1.84it/s]Extractor Estimating: 48it [00:27,  1.87it/s]Extractor Estimating: 49it [00:28,  1.84it/s]Extractor Estimating: 50it [00:28,  1.77it/s]Extractor Estimating: 51it [00:29,  1.64it/s]Extractor Estimating: 52it [00:29,  1.62it/s]Extractor Estimating: 53it [00:30,  1.59it/s]Extractor Estimating: 54it [00:31,  1.67it/s]Extractor Estimating: 55it [00:31,  1.64it/s]Extractor Estimating: 56it [00:32,  1.65it/s]Extractor Estimating: 57it [00:33,  1.60it/s]Extractor Estimating: 58it [00:33,  1.63it/s]Extractor Estimating: 59it [00:34,  1.67it/s]Extractor Estimating: 60it [00:34,  1.58it/s]Extractor Estimating: 61it [00:35,  1.60it/s]Extractor Estimating: 62it [00:36,  1.55it/s]Extractor Estimating: 63it [00:36,  1.59it/s]Extractor Estimating: 64it [00:37,  1.57it/s]Extractor Estimating: 65it [00:38,  1.55it/s]Extractor Estimating: 66it [00:38,  1.58it/s]Extractor Estimating: 67it [00:39,  1.59it/s]Extractor Estimating: 68it [00:40,  1.56it/s]Extractor Estimating: 69it [00:40,  1.56it/s]Extractor Estimating: 70it [00:41,  1.58it/s]Extractor Estimating: 71it [00:41,  1.60it/s]Extractor Estimating: 72it [00:42,  1.57it/s]Extractor Estimating: 73it [00:43,  1.59it/s]Extractor Estimating: 74it [00:43,  1.49it/s]Extractor Estimating: 75it [00:44,  1.53it/s]Extractor Estimating: 76it [00:45,  1.64it/s]Extractor Estimating: 77it [00:45,  1.72it/s]Extractor Estimating: 78it [00:46,  1.76it/s]Extractor Estimating: 79it [00:46,  1.77it/s]Extractor Estimating: 80it [00:47,  1.86it/s]Extractor Estimating: 81it [00:47,  1.88it/s]Extractor Estimating: 82it [00:48,  1.78it/s]Extractor Estimating: 83it [00:48,  1.83it/s]Extractor Estimating: 84it [00:49,  1.86it/s]Extractor Estimating: 85it [00:49,  1.93it/s]Extractor Estimating: 86it [00:50,  1.85it/s]Extractor Estimating: 87it [00:50,  1.90it/s]Extractor Estimating: 88it [00:51,  1.96it/s]Extractor Estimating: 89it [00:51,  1.93it/s]Extractor Estimating: 90it [00:52,  1.98it/s]Extractor Estimating: 91it [00:52,  1.87it/s]Extractor Estimating: 92it [00:53,  1.95it/s]Extractor Estimating: 93it [00:53,  1.90it/s]Extractor Estimating: 94it [00:54,  1.95it/s]Extractor Estimating: 95it [00:55,  1.92it/s]Extractor Estimating: 96it [00:55,  1.95it/s]Extractor Estimating: 97it [00:56,  1.92it/s]Extractor Estimating: 98it [00:56,  1.95it/s]Extractor Estimating: 99it [00:57,  1.95it/s]Extractor Estimating: 100it [00:57,  2.03it/s]Extractor Estimating: 101it [00:58,  2.01it/s]Extractor Estimating: 102it [00:58,  2.02it/s]Extractor Estimating: 103it [00:59,  1.98it/s]Extractor Estimating: 104it [00:59,  1.90it/s]Extractor Estimating: 105it [01:00,  1.92it/s]Extractor Estimating: 106it [01:00,  1.89it/s]Extractor Estimating: 107it [01:01,  1.83it/s]Extractor Estimating: 108it [01:01,  1.85it/s]Extractor Estimating: 109it [01:02,  1.87it/s]Extractor Estimating: 110it [01:02,  1.83it/s]Extractor Estimating: 111it [01:03,  1.91it/s]Extractor Estimating: 112it [01:03,  1.93it/s]Extractor Estimating: 113it [01:04,  1.69it/s]Extractor Estimating: 114it [01:05,  1.73it/s]Extractor Estimating: 115it [01:05,  1.74it/s]Extractor Estimating: 116it [01:06,  1.73it/s]Extractor Estimating: 117it [01:06,  1.81it/s]Extractor Estimating: 118it [01:07,  1.90it/s]Extractor Estimating: 119it [01:07,  1.89it/s]Extractor Estimating: 120it [01:08,  1.87it/s]Extractor Estimating: 121it [01:08,  1.82it/s]Extractor Estimating: 122it [01:09,  1.82it/s]Extractor Estimating: 123it [01:09,  1.90it/s]Extractor Estimating: 124it [01:10,  1.88it/s]Extractor Estimating: 125it [01:10,  1.93it/s]Extractor Estimating: 126it [01:11,  1.86it/s]Extractor Estimating: 127it [01:12,  1.92it/s]Extractor Estimating: 128it [01:12,  1.89it/s]Extractor Estimating: 129it [01:13,  1.84it/s]Extractor Estimating: 130it [01:13,  1.87it/s]Extractor Estimating: 131it [01:14,  1.92it/s]Extractor Estimating: 132it [01:14,  1.91it/s]Extractor Estimating: 133it [01:15,  1.90it/s]Extractor Estimating: 134it [01:15,  1.84it/s]Extractor Estimating: 135it [01:16,  1.89it/s]Extractor Estimating: 136it [01:16,  1.85it/s]Extractor Estimating: 137it [01:17,  1.91it/s]Extractor Estimating: 138it [01:17,  1.81it/s]Extractor Estimating: 139it [01:18,  1.76it/s]Extractor Estimating: 140it [01:19,  1.82it/s]Extractor Estimating: 141it [01:19,  1.83it/s]Extractor Estimating: 142it [01:20,  1.81it/s]Extractor Estimating: 143it [01:20,  1.83it/s]Extractor Estimating: 144it [01:21,  1.84it/s]Extractor Estimating: 145it [01:21,  1.87it/s]Extractor Estimating: 146it [01:22,  1.82it/s]Extractor Estimating: 147it [01:22,  1.85it/s]Extractor Estimating: 148it [01:23,  1.88it/s]Extractor Estimating: 149it [01:23,  1.86it/s]Extractor Estimating: 150it [01:24,  1.90it/s]Extractor Estimating: 151it [01:25,  1.83it/s]Extractor Estimating: 152it [01:25,  1.76it/s]Extractor Estimating: 153it [01:26,  1.84it/s]Extractor Estimating: 154it [01:26,  1.84it/s]Extractor Estimating: 155it [01:27,  1.83it/s]Extractor Estimating: 156it [01:27,  1.86it/s]Extractor Estimating: 157it [01:28,  1.91it/s]Extractor Estimating: 158it [01:28,  1.87it/s]Extractor Estimating: 159it [01:29,  1.91it/s]Extractor Estimating: 160it [01:29,  1.87it/s]Extractor Estimating: 161it [01:30,  1.88it/s]Extractor Estimating: 162it [01:30,  1.86it/s]Extractor Estimating: 163it [01:31,  1.86it/s]Extractor Estimating: 164it [01:32,  1.85it/s]Extractor Estimating: 165it [01:32,  1.85it/s]Extractor Estimating: 166it [01:33,  1.80it/s]Extractor Estimating: 167it [01:33,  1.80it/s]Extractor Estimating: 168it [01:34,  1.85it/s]Extractor Estimating: 169it [01:34,  1.84it/s]Extractor Estimating: 170it [01:35,  1.82it/s]Extractor Estimating: 171it [01:35,  1.81it/s]Extractor Estimating: 172it [01:36,  1.73it/s]Extractor Estimating: 173it [01:37,  1.69it/s]Extractor Estimating: 174it [01:37,  1.73it/s]Extractor Estimating: 175it [01:38,  1.76it/s]Extractor Estimating: 176it [01:38,  1.81it/s]Extractor Estimating: 177it [01:39,  1.79it/s]Extractor Estimating: 178it [01:39,  1.86it/s]Extractor Estimating: 179it [01:40,  1.90it/s]Extractor Estimating: 180it [01:40,  1.88it/s]Extractor Estimating: 181it [01:41,  1.81it/s]Extractor Estimating: 182it [01:42,  1.83it/s]Extractor Estimating: 183it [01:42,  1.82it/s]Extractor Estimating: 184it [01:43,  1.80it/s]Extractor Estimating: 185it [01:43,  1.80it/s]Extractor Estimating: 186it [01:44,  1.85it/s]Extractor Estimating: 187it [01:44,  1.80it/s]Extractor Estimating: 188it [01:45,  1.79it/s]Extractor Estimating: 189it [01:45,  1.77it/s]Extractor Estimating: 190it [01:46,  1.78it/s]Extractor Estimating: 191it [01:47,  1.77it/s]Extractor Estimating: 192it [01:47,  1.75it/s]Extractor Estimating: 193it [01:48,  1.72it/s]Extractor Estimating: 194it [01:48,  1.73it/s]Extractor Estimating: 195it [01:49,  1.80it/s]Extractor Estimating: 196it [01:49,  1.81it/s]Extractor Estimating: 197it [01:50,  1.75it/s]Extractor Estimating: 198it [01:51,  1.74it/s]Extractor Estimating: 199it [01:51,  1.71it/s]Extractor Estimating: 200it [01:52,  1.73it/s]Extractor Estimating: 201it [01:52,  1.74it/s]Extractor Estimating: 202it [01:53,  1.75it/s]Extractor Estimating: 203it [01:53,  1.74it/s]Extractor Estimating: 204it [01:54,  1.57it/s]Extractor Estimating: 205it [01:55,  1.61it/s]Extractor Estimating: 206it [01:55,  1.59it/s]Extractor Estimating: 207it [01:56,  1.65it/s]Extractor Estimating: 208it [01:57,  1.67it/s]Extractor Estimating: 209it [01:57,  1.73it/s]Extractor Estimating: 210it [01:58,  1.74it/s]Extractor Estimating: 211it [01:58,  1.66it/s]Extractor Estimating: 212it [01:59,  1.70it/s]Extractor Estimating: 213it [02:00,  1.68it/s]Extractor Estimating: 214it [02:00,  1.71it/s]Extractor Estimating: 215it [02:01,  1.67it/s]Extractor Estimating: 216it [02:01,  1.70it/s]Extractor Estimating: 217it [02:02,  1.64it/s]Extractor Estimating: 218it [02:02,  1.69it/s]Extractor Estimating: 219it [02:03,  1.74it/s]Extractor Estimating: 220it [02:04,  1.73it/s]Extractor Estimating: 221it [02:04,  1.73it/s]Extractor Estimating: 222it [02:05,  1.68it/s]Extractor Estimating: 223it [02:05,  1.70it/s]Extractor Estimating: 224it [02:06,  1.67it/s]Extractor Estimating: 225it [02:07,  1.66it/s]Extractor Estimating: 226it [02:07,  1.63it/s]Extractor Estimating: 227it [02:08,  1.67it/s]Extractor Estimating: 228it [02:08,  1.66it/s]Extractor Estimating: 229it [02:09,  1.62it/s]Extractor Estimating: 230it [02:10,  1.67it/s]Extractor Estimating: 231it [02:10,  1.59it/s]Extractor Estimating: 232it [02:11,  1.57it/s]Extractor Estimating: 233it [02:12,  1.61it/s]Extractor Estimating: 234it [02:12,  1.54it/s]Extractor Estimating: 235it [02:13,  1.55it/s]Extractor Estimating: 236it [02:14,  1.60it/s]Extractor Estimating: 237it [02:14,  1.65it/s]Extractor Estimating: 238it [02:15,  1.66it/s]Extractor Estimating: 239it [02:15,  1.67it/s]Extractor Estimating: 240it [02:16,  1.60it/s]Extractor Estimating: 241it [02:17,  1.56it/s]Extractor Estimating: 242it [02:17,  1.60it/s]Extractor Estimating: 243it [02:18,  1.57it/s]Extractor Estimating: 244it [02:19,  1.56it/s]Extractor Estimating: 245it [02:19,  1.58it/s]Extractor Estimating: 246it [02:20,  1.54it/s]Extractor Estimating: 247it [02:21,  1.51it/s]Extractor Estimating: 248it [02:21,  1.49it/s]Extractor Estimating: 249it [02:22,  1.48it/s]Extractor Estimating: 250it [02:23,  1.50it/s]Extractor Estimating: 251it [02:23,  1.61it/s]Extractor Estimating: 252it [02:24,  1.77it/s]Extractor Estimating: 253it [02:24,  1.84it/s]Extractor Estimating: 254it [02:24,  1.92it/s]Extractor Estimating: 255it [02:25,  2.02it/s]Extractor Estimating: 256it [02:25,  2.07it/s]Extractor Estimating: 257it [02:26,  2.09it/s]Extractor Estimating: 258it [02:26,  2.10it/s]Extractor Estimating: 259it [02:27,  2.09it/s]Extractor Estimating: 260it [02:27,  2.11it/s]Extractor Estimating: 261it [02:28,  2.13it/s]Extractor Estimating: 262it [02:28,  2.03it/s]Extractor Estimating: 263it [02:29,  2.12it/s]Extractor Estimating: 264it [02:29,  2.15it/s]Extractor Estimating: 265it [02:30,  2.19it/s]Extractor Estimating: 266it [02:30,  2.18it/s]Extractor Estimating: 267it [02:30,  2.23it/s]Extractor Estimating: 268it [02:31,  2.20it/s]Extractor Estimating: 269it [02:31,  2.18it/s]Extractor Estimating: 270it [02:32,  2.21it/s]Extractor Estimating: 271it [02:32,  2.24it/s]Extractor Estimating: 272it [02:33,  2.17it/s]Extractor Estimating: 273it [02:33,  2.11it/s]Extractor Estimating: 274it [02:34,  2.16it/s]Extractor Estimating: 275it [02:34,  2.13it/s]Extractor Estimating: 276it [02:35,  1.92it/s]Extractor Estimating: 277it [02:35,  1.89it/s]Extractor Estimating: 278it [02:36,  1.81it/s]Extractor Estimating: 279it [02:37,  1.77it/s]Extractor Estimating: 280it [02:37,  1.74it/s]Extractor Estimating: 281it [02:38,  1.73it/s]Extractor Estimating: 282it [02:38,  1.73it/s]Extractor Estimating: 283it [02:39,  1.72it/s]Extractor Estimating: 284it [02:40,  1.64it/s]Extractor Estimating: 285it [02:40,  1.69it/s]Extractor Estimating: 286it [02:41,  1.63it/s]Extractor Estimating: 287it [02:41,  1.59it/s]Extractor Estimating: 288it [02:42,  1.61it/s]Extractor Estimating: 289it [02:43,  1.62it/s]Extractor Estimating: 290it [02:43,  1.58it/s]Extractor Estimating: 291it [02:44,  1.62it/s]Extractor Estimating: 292it [02:45,  1.63it/s]Extractor Estimating: 293it [02:45,  1.66it/s]Extractor Estimating: 294it [02:46,  1.63it/s]Extractor Estimating: 295it [02:47,  1.49it/s]Extractor Estimating: 296it [02:47,  1.58it/s]Extractor Estimating: 297it [02:48,  1.49it/s]Extractor Estimating: 298it [02:48,  1.56it/s]Extractor Estimating: 299it [02:49,  1.62it/s]Extractor Estimating: 300it [02:50,  1.62it/s]Extractor Estimating: 301it [02:50,  1.68it/s]Extractor Estimating: 302it [02:51,  1.68it/s]Extractor Estimating: 303it [02:51,  1.73it/s]Extractor Estimating: 304it [02:52,  1.75it/s]Extractor Estimating: 305it [02:53,  1.68it/s]Extractor Estimating: 306it [02:53,  1.72it/s]Extractor Estimating: 307it [02:54,  1.74it/s]Extractor Estimating: 308it [02:54,  1.74it/s]Extractor Estimating: 309it [02:55,  1.69it/s]Extractor Estimating: 310it [02:55,  1.72it/s]Extractor Estimating: 311it [02:56,  1.72it/s]Extractor Estimating: 312it [02:57,  1.69it/s]Extractor Estimating: 313it [02:57,  1.71it/s]Extractor Estimating: 314it [02:58,  1.67it/s]Extractor Estimating: 315it [02:58,  1.76it/s]Extractor Estimating: 316it [02:59,  1.78it/s]Extractor Estimating: 317it [02:59,  1.78it/s]Extractor Estimating: 318it [03:00,  1.86it/s]Extractor Estimating: 319it [03:00,  1.83it/s]Extractor Estimating: 320it [03:01,  1.76it/s]Extractor Estimating: 321it [03:02,  1.73it/s]Extractor Estimating: 322it [03:02,  1.76it/s]Extractor Estimating: 323it [03:03,  1.75it/s]Extractor Estimating: 324it [03:03,  1.75it/s]Extractor Estimating: 325it [03:04,  1.71it/s]Extractor Estimating: 326it [03:05,  1.75it/s]Extractor Estimating: 327it [03:05,  1.72it/s]Extractor Estimating: 328it [03:06,  1.58it/s]Extractor Estimating: 329it [03:06,  1.60it/s]Extractor Estimating: 330it [03:07,  1.61it/s]Extractor Estimating: 331it [03:08,  1.62it/s]Extractor Estimating: 332it [03:08,  1.66it/s]Extractor Estimating: 333it [03:09,  1.66it/s]Extractor Estimating: 334it [03:09,  1.65it/s]Extractor Estimating: 335it [03:10,  1.67it/s]Extractor Estimating: 336it [03:11,  1.65it/s]Extractor Estimating: 337it [03:11,  1.70it/s]Extractor Estimating: 338it [03:12,  1.73it/s]Extractor Estimating: 339it [03:12,  1.69it/s]Extractor Estimating: 340it [03:13,  1.67it/s]Extractor Estimating: 341it [03:14,  1.67it/s]Extractor Estimating: 342it [03:14,  1.68it/s]Extractor Estimating: 343it [03:15,  1.74it/s]Extractor Estimating: 344it [03:15,  1.76it/s]Extractor Estimating: 345it [03:16,  1.71it/s]Extractor Estimating: 346it [03:16,  1.76it/s]Extractor Estimating: 347it [03:17,  1.74it/s]Extractor Estimating: 348it [03:18,  1.74it/s]Extractor Estimating: 349it [03:18,  1.77it/s]Extractor Estimating: 350it [03:19,  1.68it/s]Extractor Estimating: 351it [03:19,  1.72it/s]Extractor Estimating: 352it [03:20,  1.71it/s]Extractor Estimating: 353it [03:20,  1.75it/s]Extractor Estimating: 354it [03:21,  1.73it/s]Extractor Estimating: 355it [03:22,  1.72it/s]Extractor Estimating: 356it [03:22,  1.75it/s]Extractor Estimating: 357it [03:23,  1.73it/s]Extractor Estimating: 358it [03:23,  1.73it/s]Extractor Estimating: 359it [03:24,  1.67it/s]Extractor Estimating: 360it [03:25,  1.67it/s]Extractor Estimating: 361it [03:25,  1.68it/s]Extractor Estimating: 362it [03:26,  1.68it/s]Extractor Estimating: 363it [03:26,  1.71it/s]Extractor Estimating: 364it [03:27,  1.77it/s]Extractor Estimating: 365it [03:28,  1.70it/s]Extractor Estimating: 366it [03:28,  1.74it/s]Extractor Estimating: 367it [03:29,  1.70it/s]Extractor Estimating: 368it [03:29,  1.65it/s]Extractor Estimating: 369it [03:30,  1.70it/s]Extractor Estimating: 370it [03:30,  1.70it/s]Extractor Estimating: 371it [03:31,  1.75it/s]Extractor Estimating: 372it [03:32,  1.77it/s]Extractor Estimating: 373it [03:32,  1.83it/s]Extractor Estimating: 374it [03:33,  1.83it/s]Extractor Estimating: 375it [03:33,  1.78it/s]Extractor Estimating: 375it [03:33,  1.75it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:26,533 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:26,546 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:26,546 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:26,546 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:26,546 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:28:27,181 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:28:27,181 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:28:27,763 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:28:28,896 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:28:28,897 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:32,048 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:32,053 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:32,053 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:32,053 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:32,053 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:28:32,735 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:28:32,736 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:28:33,308 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:28:33,476 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:28:33,476 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:30:32,712 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:30:32,845 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7493 mean pseudo reward: 0.9352670737890482
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 19722
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19822, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19822, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.929, loss:621.1691
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.957, loss:588.5503
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.943, loss:580.8833
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.939, loss:578.4810
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.955, loss:556.9884
>> valid entity prec:0.5088, rec:0.5370, f1:0.5225
>> valid relation prec:0.1537, rec:0.0701, f1:0.0963
>> valid relation with NER prec:0.1537, rec:0.0701, f1:0.0963
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.131, loss:590.1800
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.927, loss:525.3005
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.939, loss:550.6130
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.948, loss:581.4992
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.932, loss:572.6233
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5065, rec:0.5049, f1:0.5057
>> valid relation prec:0.1672, rec:0.0624, f1:0.0909
>> valid relation with NER prec:0.1672, rec:0.0624, f1:0.0909
g_step 1100, step 161, avg_time 2.131, loss:551.4604
g_step 1200, step 261, avg_time 0.942, loss:565.2024
g_step 1300, step 48, avg_time 0.942, loss:546.1974
g_step 1400, step 148, avg_time 0.937, loss:512.5376
g_step 1500, step 248, avg_time 0.945, loss:535.8576
>> valid entity prec:0.5490, rec:0.5035, f1:0.5253
>> valid relation prec:0.1858, rec:0.0658, f1:0.0972
>> valid relation with NER prec:0.1858, rec:0.0658, f1:0.0972
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.185, loss:531.7395
g_step 1700, step 135, avg_time 0.938, loss:496.6372
g_step 1800, step 235, avg_time 0.947, loss:502.7231
g_step 1900, step 22, avg_time 0.931, loss:470.5358
g_step 2000, step 122, avg_time 0.929, loss:485.4131
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5090, rec:0.5101, f1:0.5096
>> valid relation prec:0.1554, rec:0.0535, f1:0.0796
>> valid relation with NER prec:0.1554, rec:0.0535, f1:0.0796
g_step 2100, step 222, avg_time 2.141, loss:469.3369
g_step 2200, step 9, avg_time 0.943, loss:466.5389
g_step 2300, step 109, avg_time 0.933, loss:436.1031
g_step 2400, step 209, avg_time 0.948, loss:451.1846
g_step 2500, step 309, avg_time 0.926, loss:459.1118
>> valid entity prec:0.4969, rec:0.5147, f1:0.5056
>> valid relation prec:0.1270, rec:0.0578, f1:0.0795
>> valid relation with NER prec:0.1270, rec:0.0578, f1:0.0795
g_step 2600, step 96, avg_time 2.139, loss:405.9833
g_step 2700, step 196, avg_time 0.928, loss:412.3311
g_step 2800, step 296, avg_time 0.936, loss:452.5340
g_step 2900, step 83, avg_time 0.943, loss:378.1572
g_step 3000, step 183, avg_time 0.927, loss:406.6260
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5217, rec:0.4883, f1:0.5045
>> valid relation prec:0.1238, rec:0.0452, f1:0.0663
>> valid relation with NER prec:0.1238, rec:0.0452, f1:0.0663
g_step 3100, step 283, avg_time 2.148, loss:429.8647
g_step 3200, step 70, avg_time 0.929, loss:388.1161
g_step 3300, step 170, avg_time 0.943, loss:395.7722
g_step 3400, step 270, avg_time 0.936, loss:402.1966
g_step 3500, step 57, avg_time 0.934, loss:354.8067
>> valid entity prec:0.5287, rec:0.5127, f1:0.5206
>> valid relation prec:0.1561, rec:0.0739, f1:0.1003
>> valid relation with NER prec:0.1561, rec:0.0739, f1:0.1003
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 157, avg_time 2.146, loss:370.2128
g_step 3700, step 257, avg_time 0.937, loss:385.0809
g_step 3800, step 44, avg_time 0.940, loss:354.5597
g_step 3900, step 144, avg_time 0.955, loss:374.3640
g_step 4000, step 244, avg_time 0.928, loss:365.4340
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5391, rec:0.4285, f1:0.4775
>> valid relation prec:0.1470, rec:0.0498, f1:0.0744
>> valid relation with NER prec:0.1470, rec:0.0498, f1:0.0744
g_step 4100, step 31, avg_time 2.129, loss:359.6327
g_step 4200, step 131, avg_time 0.933, loss:339.2453
g_step 4300, step 231, avg_time 0.958, loss:359.1323
g_step 4400, step 18, avg_time 0.932, loss:343.8041
g_step 4500, step 118, avg_time 0.922, loss:315.7379
>> valid entity prec:0.4938, rec:0.5267, f1:0.5097
>> valid relation prec:0.1384, rec:0.0678, f1:0.0910
>> valid relation with NER prec:0.1384, rec:0.0678, f1:0.0910
g_step 4600, step 218, avg_time 2.131, loss:337.9753
g_step 4700, step 5, avg_time 0.949, loss:338.5899
g_step 4800, step 105, avg_time 0.940, loss:308.7123
g_step 4900, step 205, avg_time 0.945, loss:327.3888
g_step 5000, step 305, avg_time 0.928, loss:334.5173
learning rate was adjusted to 0.0008
>> valid entity prec:0.5404, rec:0.5137, f1:0.5267
>> valid relation prec:0.1524, rec:0.0601, f1:0.0862
>> valid relation with NER prec:0.1524, rec:0.0601, f1:0.0862
new max entity f1 on valid!
g_step 5100, step 92, avg_time 2.146, loss:291.7505
g_step 5200, step 192, avg_time 0.949, loss:302.1937
g_step 5300, step 292, avg_time 0.930, loss:331.9034
g_step 5400, step 79, avg_time 0.932, loss:285.3433
g_step 5500, step 179, avg_time 0.950, loss:299.8027
>> valid entity prec:0.5197, rec:0.4941, f1:0.5066
>> valid relation prec:0.1261, rec:0.0598, f1:0.0811
>> valid relation with NER prec:0.1261, rec:0.0598, f1:0.0811
g_step 5600, step 279, avg_time 2.125, loss:300.1068
g_step 5700, step 66, avg_time 0.916, loss:280.9403
g_step 5800, step 166, avg_time 0.948, loss:282.8690
g_step 5900, step 266, avg_time 0.931, loss:291.8588
g_step 6000, step 53, avg_time 0.934, loss:271.8607
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5221, rec:0.4607, f1:0.4895
>> valid relation prec:0.1575, rec:0.0673, f1:0.0943
>> valid relation with NER prec:0.1575, rec:0.0673, f1:0.0943
g_step 6100, step 153, avg_time 2.136, loss:272.8347
g_step 6200, step 253, avg_time 0.931, loss:276.5919
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:30:32 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:30:32 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-30-32_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:30:34 - WARNING - datasets.builder -   Using custom data configuration default-215c539b5761792a
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-215c539b5761792a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:30:34,946 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:30:34,947 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:30:34,948 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:30:34,949 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:30:35,209 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:35,228 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:35,228 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:35,228 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:35,228 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:35,228 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:30:35,228 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:30:35,441 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:30:38,575 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:30:38,585 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-215c539b5761792a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.17ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.06ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.46ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.79ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.14ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.39ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.55ba/s]100%|██████████| 8/8 [00:01<00:00,  5.44ba/s]100%|██████████| 8/8 [00:01<00:00,  4.57ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.04ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.27ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.35ba/s]100%|██████████| 4/4 [00:00<00:00,  5.42ba/s]100%|██████████| 4/4 [00:00<00:00,  4.92ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.41ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.13ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.54ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.85ba/s]100%|██████████| 8/8 [00:00<00:00, 11.25ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.59ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 11.01ba/s]100%|██████████| 4/4 [00:00<00:00, 12.49ba/s]
[INFO|trainer.py:414] 2023-08-28 21:30:42,790 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:30:42,828 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:30:42,828 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-28 21:30:42,828 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:30:42,828 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:30:42,828 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:30:42,829 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:30:42,829 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.34it/s]  0%|          | 2/585 [00:00<02:51,  3.39it/s]  1%|          | 3/585 [00:00<02:50,  3.42it/s]  1%|          | 4/585 [00:01<02:49,  3.43it/s]  1%|          | 5/585 [00:01<02:49,  3.41it/s]  1%|          | 6/585 [00:01<02:50,  3.40it/s]  1%|          | 7/585 [00:02<02:49,  3.40it/s]  1%|▏         | 8/585 [00:02<02:50,  3.38it/s]  2%|▏         | 9/585 [00:02<02:50,  3.38it/s]  2%|▏         | 10/585 [00:02<02:50,  3.38it/s]  2%|▏         | 11/585 [00:03<02:49,  3.39it/s]  2%|▏         | 12/585 [00:03<02:48,  3.39it/s]  2%|▏         | 13/585 [00:03<02:48,  3.39it/s]  2%|▏         | 14/585 [00:04<02:48,  3.39it/s]  3%|▎         | 15/585 [00:04<02:48,  3.39it/s]  3%|▎         | 16/585 [00:04<02:47,  3.39it/s]  3%|▎         | 17/585 [00:05<02:47,  3.39it/s]  3%|▎         | 18/585 [00:05<02:47,  3.39it/s]  3%|▎         | 19/585 [00:05<02:47,  3.39it/s]  3%|▎         | 20/585 [00:05<02:46,  3.39it/s]  4%|▎         | 21/585 [00:06<02:46,  3.39it/s]  4%|▍         | 22/585 [00:06<02:46,  3.39it/s]  4%|▍         | 23/585 [00:06<02:45,  3.39it/s]  4%|▍         | 24/585 [00:07<02:44,  3.41it/s]  4%|▍         | 25/585 [00:07<02:43,  3.42it/s]  4%|▍         | 26/585 [00:07<02:49,  3.30it/s]  5%|▍         | 27/585 [00:07<02:47,  3.34it/s]  5%|▍         | 28/585 [00:08<02:45,  3.37it/s]  5%|▍         | 29/585 [00:08<02:44,  3.39it/s]  5%|▌         | 30/585 [00:08<02:43,  3.40it/s]  5%|▌         | 31/585 [00:09<02:42,  3.41it/s]  5%|▌         | 32/585 [00:09<02:41,  3.42it/s]  6%|▌         | 33/585 [00:09<02:41,  3.43it/s]  6%|▌         | 34/585 [00:10<02:40,  3.43it/s]  6%|▌         | 35/585 [00:10<02:40,  3.43it/s]  6%|▌         | 36/585 [00:10<02:39,  3.44it/s]  6%|▋         | 37/585 [00:10<02:39,  3.44it/s]  6%|▋         | 38/585 [00:11<02:39,  3.44it/s]  7%|▋         | 39/585 [00:11<02:38,  3.44it/s]  7%|▋         | 40/585 [00:11<02:38,  3.44it/s]  7%|▋         | 41/585 [00:12<02:38,  3.44it/s]  7%|▋         | 42/585 [00:12<02:37,  3.44it/s]  7%|▋         | 43/585 [00:12<02:37,  3.44it/s]  8%|▊         | 44/585 [00:12<02:38,  3.41it/s]  8%|▊         | 45/585 [00:13<02:37,  3.42it/s]  8%|▊         | 46/585 [00:13<02:37,  3.43it/s]  8%|▊         | 47/585 [00:13<02:36,  3.43it/s]  8%|▊         | 48/585 [00:14<02:36,  3.43it/s]  8%|▊         | 49/585 [00:14<02:36,  3.43it/s]  9%|▊         | 50/585 [00:14<02:35,  3.44it/s]  9%|▊         | 51/585 [00:14<02:35,  3.44it/s]  9%|▉         | 52/585 [00:15<02:34,  3.44it/s]  9%|▉         | 53/585 [00:15<02:34,  3.44it/s]  9%|▉         | 54/585 [00:15<02:34,  3.44it/s]  9%|▉         | 55/585 [00:16<02:34,  3.44it/s] 10%|▉         | 56/585 [00:16<02:33,  3.44it/s] 10%|▉         | 57/585 [00:16<02:33,  3.44it/s] 10%|▉         | 58/585 [00:17<02:33,  3.44it/s] 10%|█         | 59/585 [00:17<02:32,  3.44it/s] 10%|█         | 60/585 [00:17<02:32,  3.44it/s] 10%|█         | 61/585 [00:17<02:34,  3.40it/s] 11%|█         | 62/585 [00:18<02:33,  3.41it/s] 11%|█         | 63/585 [00:18<02:32,  3.42it/s] 11%|█         | 64/585 [00:18<02:31,  3.43it/s] 11%|█         | 65/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 69/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.43it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.43it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.44it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 76/585 [00:22<02:28,  3.44it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.43it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.44it/s] 14%|█▎        | 79/585 [00:23<02:28,  3.41it/s] 14%|█▎        | 80/585 [00:23<02:27,  3.41it/s] 14%|█▍        | 81/585 [00:23<02:27,  3.42it/s] 14%|█▍        | 82/585 [00:24<02:26,  3.43it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.43it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 86/585 [00:25<02:25,  3.44it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.44it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.43it/s] 15%|█▌        | 89/585 [00:26<02:24,  3.43it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.43it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.43it/s] 16%|█▌        | 92/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 93/585 [00:27<02:23,  3.44it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.44it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.44it/s] 16%|█▋        | 96/585 [00:28<02:22,  3.44it/s] 17%|█▋        | 97/585 [00:28<02:23,  3.40it/s] 17%|█▋        | 98/585 [00:28<02:22,  3.41it/s] 17%|█▋        | 99/585 [00:28<02:22,  3.42it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 101/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 102/585 [00:29<02:20,  3.43it/s] 18%|█▊        | 103/585 [00:30<02:20,  3.44it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.44it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.44it/s] 18%|█▊        | 106/585 [00:30<02:19,  3.44it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.44it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.44it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.44it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.44it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 113/585 [00:33<02:17,  3.44it/s] 19%|█▉        | 114/585 [00:33<02:23,  3.29it/s] 20%|█▉        | 115/585 [00:33<02:21,  3.33it/s] 20%|█▉        | 116/585 [00:33<02:19,  3.36it/s] 20%|██        | 117/585 [00:34<02:18,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 21:31:17,109 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:31:17,109 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:31:17,109 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.72it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.83it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.04it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.20it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.78it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.52it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.39it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.26it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.19it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.41it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.39it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.13it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.11it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.99it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.97it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.07it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.07it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.14it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.22it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.31it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.19it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.11it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.98it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.97it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.06it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.14it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.13it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.27it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.26it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.14it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.10it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.95it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.92it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 39.37it/s][A
 41%|████      | 177/437 [00:04<00:06, 40.84it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 41.97it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 42.69it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.29it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.65it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.91it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.79it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.42it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.42it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.75it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.96it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.22it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.31it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.38it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.20it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.87it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.55it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 43.41it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.80it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.98it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.27it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.46it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.55it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.31it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.99it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.63it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.58it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.81it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.04it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.29it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.35it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.45it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.23it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.92it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.72it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.72it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.77it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.11it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.30it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.43it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.42it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.13it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.91it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.64it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.59it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.92it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.19it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.34it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.41it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.39it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.18it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.91it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.86it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:44<02:18,  3.38it/s]
100%|██████████| 437/437 [00:09<00:00, 43.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:31:27,159 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 21:31:27,215 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:31:33,147 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:31:33,194 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:31:33,205 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:04<1:11:18,  9.16s/it] 20%|██        | 119/585 [01:04<50:36,  6.52s/it]   21%|██        | 120/585 [01:04<36:02,  4.65s/it] 21%|██        | 121/585 [01:05<25:51,  3.34s/it] 21%|██        | 122/585 [01:05<18:44,  2.43s/it] 21%|██        | 123/585 [01:05<13:46,  1.79s/it] 21%|██        | 124/585 [01:05<10:18,  1.34s/it] 21%|██▏       | 125/585 [01:06<07:52,  1.03s/it] 22%|██▏       | 126/585 [01:06<06:10,  1.24it/s] 22%|██▏       | 127/585 [01:06<04:59,  1.53it/s] 22%|██▏       | 128/585 [01:07<04:09,  1.83it/s] 22%|██▏       | 129/585 [01:07<03:38,  2.09it/s] 22%|██▏       | 130/585 [01:07<03:12,  2.36it/s] 22%|██▏       | 131/585 [01:08<02:54,  2.60it/s] 23%|██▎       | 132/585 [01:08<02:42,  2.79it/s] 23%|██▎       | 133/585 [01:08<02:33,  2.95it/s] 23%|██▎       | 134/585 [01:08<02:26,  3.07it/s] 23%|██▎       | 135/585 [01:09<02:22,  3.16it/s] 23%|██▎       | 136/585 [01:09<02:19,  3.23it/s] 23%|██▎       | 137/585 [01:09<02:16,  3.27it/s] 24%|██▎       | 138/585 [01:10<02:15,  3.30it/s] 24%|██▍       | 139/585 [01:10<02:14,  3.33it/s] 24%|██▍       | 140/585 [01:10<02:22,  3.13it/s] 24%|██▍       | 141/585 [01:11<02:18,  3.20it/s] 24%|██▍       | 142/585 [01:11<02:16,  3.25it/s] 24%|██▍       | 143/585 [01:11<02:14,  3.29it/s] 25%|██▍       | 144/585 [01:11<02:12,  3.32it/s] 25%|██▍       | 145/585 [01:12<02:11,  3.34it/s] 25%|██▍       | 146/585 [01:12<02:11,  3.35it/s] 25%|██▌       | 147/585 [01:12<02:10,  3.36it/s] 25%|██▌       | 148/585 [01:13<02:09,  3.37it/s] 25%|██▌       | 149/585 [01:13<02:09,  3.37it/s] 26%|██▌       | 150/585 [01:13<02:08,  3.37it/s] 26%|██▌       | 151/585 [01:13<02:08,  3.37it/s] 26%|██▌       | 152/585 [01:14<02:10,  3.32it/s] 26%|██▌       | 153/585 [01:14<02:09,  3.34it/s] 26%|██▋       | 154/585 [01:14<02:08,  3.35it/s] 26%|██▋       | 155/585 [01:15<02:07,  3.37it/s] 27%|██▋       | 156/585 [01:15<02:07,  3.37it/s] 27%|██▋       | 157/585 [01:15<02:06,  3.38it/s] 27%|██▋       | 158/585 [01:16<02:06,  3.38it/s] 27%|██▋       | 159/585 [01:16<02:06,  3.38it/s] 27%|██▋       | 160/585 [01:16<02:05,  3.38it/s] 28%|██▊       | 161/585 [01:16<02:05,  3.39it/s] 28%|██▊       | 162/585 [01:17<02:04,  3.39it/s] 28%|██▊       | 163/585 [01:17<02:05,  3.37it/s] 28%|██▊       | 164/585 [01:17<02:04,  3.37it/s] 28%|██▊       | 165/585 [01:18<02:04,  3.38it/s] 28%|██▊       | 166/585 [01:18<02:03,  3.38it/s] 29%|██▊       | 167/585 [01:18<02:03,  3.38it/s] 29%|██▊       | 168/585 [01:19<02:03,  3.38it/s] 29%|██▉       | 169/585 [01:19<02:02,  3.38it/s] 29%|██▉       | 170/585 [01:19<02:02,  3.38it/s] 29%|██▉       | 171/585 [01:19<02:02,  3.39it/s] 29%|██▉       | 172/585 [01:20<02:02,  3.38it/s] 30%|██▉       | 173/585 [01:20<02:01,  3.39it/s] 30%|██▉       | 174/585 [01:20<02:01,  3.39it/s] 30%|██▉       | 175/585 [01:21<02:00,  3.40it/s] 30%|███       | 176/585 [01:21<01:59,  3.41it/s] 30%|███       | 177/585 [01:21<01:59,  3.42it/s] 30%|███       | 178/585 [01:21<01:58,  3.42it/s] 31%|███       | 179/585 [01:22<01:58,  3.43it/s] 31%|███       | 180/585 [01:22<01:58,  3.43it/s] 31%|███       | 181/585 [01:22<01:57,  3.43it/s] 31%|███       | 182/585 [01:23<01:57,  3.43it/s] 31%|███▏      | 183/585 [01:23<01:57,  3.43it/s] 31%|███▏      | 184/585 [01:23<01:56,  3.43it/s] 32%|███▏      | 185/585 [01:24<02:08,  3.12it/s] 32%|███▏      | 186/585 [01:24<02:04,  3.21it/s] 32%|███▏      | 187/585 [01:24<02:01,  3.27it/s] 32%|███▏      | 188/585 [01:24<01:59,  3.32it/s] 32%|███▏      | 189/585 [01:25<01:58,  3.35it/s] 32%|███▏      | 190/585 [01:25<01:56,  3.38it/s] 33%|███▎      | 191/585 [01:25<01:56,  3.39it/s] 33%|███▎      | 192/585 [01:26<01:55,  3.41it/s] 33%|███▎      | 193/585 [01:26<01:54,  3.41it/s] 33%|███▎      | 194/585 [01:26<01:54,  3.42it/s] 33%|███▎      | 195/585 [01:27<01:58,  3.29it/s] 34%|███▎      | 196/585 [01:27<01:56,  3.34it/s] 34%|███▎      | 197/585 [01:27<01:55,  3.37it/s] 34%|███▍      | 198/585 [01:27<01:54,  3.38it/s] 34%|███▍      | 199/585 [01:28<01:53,  3.40it/s] 34%|███▍      | 200/585 [01:28<01:53,  3.41it/s] 34%|███▍      | 201/585 [01:28<01:52,  3.41it/s] 35%|███▍      | 202/585 [01:29<01:52,  3.42it/s] 35%|███▍      | 203/585 [01:29<01:51,  3.43it/s] 35%|███▍      | 204/585 [01:29<01:51,  3.43it/s] 35%|███▌      | 205/585 [01:29<01:50,  3.43it/s] 35%|███▌      | 206/585 [01:30<01:51,  3.40it/s] 35%|███▌      | 207/585 [01:30<01:50,  3.42it/s] 36%|███▌      | 208/585 [01:30<01:50,  3.42it/s] 36%|███▌      | 209/585 [01:31<01:49,  3.42it/s] 36%|███▌      | 210/585 [01:31<01:49,  3.43it/s] 36%|███▌      | 211/585 [01:31<02:10,  2.87it/s] 36%|███▌      | 212/585 [01:32<02:03,  3.01it/s] 36%|███▋      | 213/585 [01:32<01:58,  3.13it/s] 37%|███▋      | 214/585 [01:32<01:55,  3.22it/s] 37%|███▋      | 215/585 [01:33<01:52,  3.28it/s] 37%|███▋      | 216/585 [01:33<01:51,  3.32it/s] 37%|███▋      | 217/585 [01:33<01:49,  3.35it/s] 37%|███▋      | 218/585 [01:33<01:48,  3.37it/s] 37%|███▋      | 219/585 [01:34<01:47,  3.39it/s] 38%|███▊      | 220/585 [01:34<01:47,  3.40it/s] 38%|███▊      | 221/585 [01:34<01:58,  3.06it/s] 38%|███▊      | 222/585 [01:35<01:54,  3.17it/s] 38%|███▊      | 223/585 [01:35<01:51,  3.24it/s] 38%|███▊      | 224/585 [01:35<01:49,  3.29it/s] 38%|███▊      | 225/585 [01:36<01:48,  3.33it/s] 39%|███▊      | 226/585 [01:36<01:46,  3.36it/s] 39%|███▉      | 227/585 [01:36<01:45,  3.38it/s] 39%|███▉      | 228/585 [01:36<01:45,  3.40it/s] 39%|███▉      | 229/585 [01:37<01:44,  3.41it/s] 39%|███▉      | 230/585 [01:37<01:43,  3.42it/s] 39%|███▉      | 231/585 [01:37<01:43,  3.42it/s] 40%|███▉      | 232/585 [01:38<01:43,  3.42it/s] 40%|███▉      | 233/585 [01:38<01:42,  3.43it/s] 40%|████      | 234/585 [01:38<01:42,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 21:32:21,589 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:32:21,589 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:32:21,589 >>   Batch size = 8
{'eval_loss': 1.0159136056900024, 'eval_runtime': 9.9659, 'eval_samples_per_second': 350.494, 'eval_steps_per_second': 43.849, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.85it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.37it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.52it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.91it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.57it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.50it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.38it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.15it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.06it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.17it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.07it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.96it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.91it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.02it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.05it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.03it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.99it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.03it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.00it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.99it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.91it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.91it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.01it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.05it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.04it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.12it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.04it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 40.80it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 41.81it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 42.47it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 42.90it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.25it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.52it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.75it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.84it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.47it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.62it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.77it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.92it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.93it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.97it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.07it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.03it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.87it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.71it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.73it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.91it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.91it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.00it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.03it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.14it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.98it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.88it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.84it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.78it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.87it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.01it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.10it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.15it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.08it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.92it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.74it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.78it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.81it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.89it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.04it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.13it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.13it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.09it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.81it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.75it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.71it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.78it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.82it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.98it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.10it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.16it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.03it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.80it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.83it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.81it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.82it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.89it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.03it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.13it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.95it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.05it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.05it/s][A 40%|████      | 234/585 [01:48<01:42,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:32:31,628 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 21:32:31,710 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:32:36,156 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:32:36,301 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:32:36,362 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:02<43:00,  7.37s/it] 40%|████      | 236/585 [02:03<30:47,  5.29s/it] 41%|████      | 237/585 [02:03<22:00,  3.79s/it] 41%|████      | 238/585 [02:03<15:52,  2.74s/it] 41%|████      | 239/585 [02:03<11:35,  2.01s/it] 41%|████      | 240/585 [02:04<08:36,  1.50s/it] 41%|████      | 241/585 [02:04<06:30,  1.14s/it] 41%|████▏     | 242/585 [02:04<05:03,  1.13it/s] 42%|████▏     | 243/585 [02:05<04:01,  1.41it/s] 42%|████▏     | 244/585 [02:05<03:19,  1.71it/s] 42%|████▏     | 245/585 [02:05<02:49,  2.01it/s] 42%|████▏     | 246/585 [02:06<02:28,  2.29it/s] 42%|████▏     | 247/585 [02:06<02:13,  2.53it/s] 42%|████▏     | 248/585 [02:06<02:03,  2.74it/s] 43%|████▎     | 249/585 [02:06<01:55,  2.90it/s] 43%|████▎     | 250/585 [02:07<01:50,  3.03it/s] 43%|████▎     | 251/585 [02:07<01:46,  3.13it/s] 43%|████▎     | 252/585 [02:07<01:43,  3.21it/s] 43%|████▎     | 253/585 [02:08<01:41,  3.26it/s] 43%|████▎     | 254/585 [02:08<01:40,  3.29it/s] 44%|████▎     | 255/585 [02:08<01:39,  3.32it/s] 44%|████▍     | 256/585 [02:08<01:38,  3.34it/s] 44%|████▍     | 257/585 [02:09<01:40,  3.26it/s] 44%|████▍     | 258/585 [02:09<01:39,  3.30it/s] 44%|████▍     | 259/585 [02:09<01:38,  3.32it/s] 44%|████▍     | 260/585 [02:10<01:37,  3.34it/s] 45%|████▍     | 261/585 [02:10<01:36,  3.35it/s] 45%|████▍     | 262/585 [02:10<01:35,  3.36it/s] 45%|████▍     | 263/585 [02:11<01:35,  3.37it/s] 45%|████▌     | 264/585 [02:11<01:35,  3.37it/s] 45%|████▌     | 265/585 [02:11<01:34,  3.38it/s] 45%|████▌     | 266/585 [02:11<01:34,  3.38it/s] 46%|████▌     | 267/585 [02:12<01:34,  3.38it/s] 46%|████▌     | 268/585 [02:12<01:44,  3.03it/s] 46%|████▌     | 269/585 [02:12<01:41,  3.12it/s] 46%|████▌     | 270/585 [02:13<01:38,  3.20it/s] 46%|████▋     | 271/585 [02:13<01:36,  3.25it/s] 46%|████▋     | 272/585 [02:13<01:35,  3.29it/s] 47%|████▋     | 273/585 [02:14<01:34,  3.31it/s] 47%|████▋     | 274/585 [02:14<01:33,  3.34it/s] 47%|████▋     | 275/585 [02:14<01:32,  3.35it/s] 47%|████▋     | 276/585 [02:15<01:31,  3.36it/s] 47%|████▋     | 277/585 [02:15<01:31,  3.37it/s] 48%|████▊     | 278/585 [02:15<01:32,  3.31it/s] 48%|████▊     | 279/585 [02:15<01:31,  3.33it/s] 48%|████▊     | 280/585 [02:16<01:31,  3.35it/s] 48%|████▊     | 281/585 [02:16<01:30,  3.36it/s] 48%|████▊     | 282/585 [02:16<01:30,  3.36it/s] 48%|████▊     | 283/585 [02:17<01:29,  3.37it/s] 49%|████▊     | 284/585 [02:17<01:29,  3.37it/s] 49%|████▊     | 285/585 [02:17<01:28,  3.38it/s] 49%|████▉     | 286/585 [02:18<01:28,  3.38it/s] 49%|████▉     | 287/585 [02:18<01:28,  3.38it/s] 49%|████▉     | 288/585 [02:18<01:27,  3.38it/s] 49%|████▉     | 289/585 [02:18<01:27,  3.39it/s] 50%|████▉     | 290/585 [02:19<01:27,  3.38it/s] 50%|████▉     | 291/585 [02:19<01:26,  3.38it/s] 50%|████▉     | 292/585 [02:19<01:26,  3.38it/s] 50%|█████     | 293/585 [02:20<01:26,  3.38it/s] 50%|█████     | 294/585 [02:20<01:25,  3.39it/s] 50%|█████     | 295/585 [02:20<01:25,  3.39it/s] 51%|█████     | 296/585 [02:20<01:25,  3.38it/s] 51%|█████     | 297/585 [02:21<01:28,  3.27it/s] 51%|█████     | 298/585 [02:21<01:26,  3.30it/s] 51%|█████     | 299/585 [02:21<01:25,  3.33it/s] 51%|█████▏    | 300/585 [02:22<01:25,  3.35it/s] 51%|█████▏    | 301/585 [02:22<01:24,  3.36it/s] 52%|█████▏    | 302/585 [02:22<01:24,  3.36it/s] 52%|█████▏    | 303/585 [02:23<01:23,  3.37it/s] 52%|█████▏    | 304/585 [02:23<01:23,  3.37it/s] 52%|█████▏    | 305/585 [02:23<01:22,  3.38it/s] 52%|█████▏    | 306/585 [02:23<01:22,  3.38it/s] 52%|█████▏    | 307/585 [02:24<01:22,  3.38it/s] 53%|█████▎    | 308/585 [02:24<01:31,  3.01it/s] 53%|█████▎    | 309/585 [02:24<01:28,  3.12it/s] 53%|█████▎    | 310/585 [02:25<01:26,  3.19it/s] 53%|█████▎    | 311/585 [02:25<01:24,  3.25it/s] 53%|█████▎    | 312/585 [02:25<01:23,  3.29it/s] 54%|█████▎    | 313/585 [02:26<01:22,  3.31it/s] 54%|█████▎    | 314/585 [02:26<01:21,  3.34it/s] 54%|█████▍    | 315/585 [02:26<01:20,  3.35it/s] 54%|█████▍    | 316/585 [02:27<01:20,  3.36it/s] 54%|█████▍    | 317/585 [02:27<01:19,  3.36it/s] 54%|█████▍    | 318/585 [02:27<01:19,  3.34it/s] 55%|█████▍    | 319/585 [02:27<01:19,  3.35it/s] 55%|█████▍    | 320/585 [02:28<01:18,  3.36it/s] 55%|█████▍    | 321/585 [02:28<01:18,  3.37it/s] 55%|█████▌    | 322/585 [02:28<01:18,  3.37it/s] 55%|█████▌    | 323/585 [02:29<01:17,  3.37it/s] 55%|█████▌    | 324/585 [02:29<01:17,  3.37it/s] 56%|█████▌    | 325/585 [02:29<01:17,  3.37it/s] 56%|█████▌    | 326/585 [02:29<01:16,  3.38it/s] 56%|█████▌    | 327/585 [02:30<01:16,  3.38it/s] 56%|█████▌    | 328/585 [02:30<01:16,  3.38it/s] 56%|█████▌    | 329/585 [02:31<01:29,  2.87it/s] 56%|█████▋    | 330/585 [02:31<01:24,  3.01it/s] 57%|█████▋    | 331/585 [02:31<01:21,  3.11it/s] 57%|█████▋    | 332/585 [02:31<01:19,  3.19it/s] 57%|█████▋    | 333/585 [02:32<01:17,  3.24it/s] 57%|█████▋    | 334/585 [02:32<01:16,  3.27it/s] 57%|█████▋    | 335/585 [02:32<01:15,  3.30it/s] 57%|█████▋    | 336/585 [02:33<01:14,  3.32it/s] 58%|█████▊    | 337/585 [02:33<01:14,  3.34it/s] 58%|█████▊    | 338/585 [02:33<01:13,  3.35it/s] 58%|█████▊    | 339/585 [02:34<01:16,  3.21it/s] 58%|█████▊    | 340/585 [02:34<01:15,  3.26it/s] 58%|█████▊    | 341/585 [02:34<01:14,  3.29it/s] 58%|█████▊    | 342/585 [02:34<01:13,  3.32it/s] 59%|█████▊    | 343/585 [02:35<01:12,  3.34it/s] 59%|█████▉    | 344/585 [02:35<01:11,  3.35it/s] 59%|█████▉    | 345/585 [02:35<01:11,  3.36it/s] 59%|█████▉    | 346/585 [02:36<01:11,  3.36it/s] 59%|█████▉    | 347/585 [02:36<01:10,  3.37it/s] 59%|█████▉    | 348/585 [02:36<01:10,  3.37it/s] 60%|█████▉    | 349/585 [02:37<01:14,  3.17it/s] 60%|█████▉    | 350/585 [02:37<01:12,  3.23it/s] 60%|██████    | 351/585 [02:37<01:11,  3.27it/s][INFO|trainer.py:2140] 2023-08-28 21:33:20,553 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:33:20,553 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:33:20,553 >>   Batch size = 8
{'eval_loss': 1.0274814367294312, 'eval_runtime': 9.979, 'eval_samples_per_second': 350.036, 'eval_steps_per_second': 43.792, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.36it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.03it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.26it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.29it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.63it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.15it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.16it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.98it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.02it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.13it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.22it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.17it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.13it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.97it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.80it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.73it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.81it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.91it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.00it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.05it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.12it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.94it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.90it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.82it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.68it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.87it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.93it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.06it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.17it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.13it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.05it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.78it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.78it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.84it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.85it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.98it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.04it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.05it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.08it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.88it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.81it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.71it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.86it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.91it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.99it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.12it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.49it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.54it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.57it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.53it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.67it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 43.68it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.75it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.86it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.96it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.03it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.03it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.86it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.80it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.79it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.89it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.91it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.88it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.02it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.03it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.95it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.90it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.80it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.87it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.83it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.99it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.05it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.07it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.05it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.96it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.79it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.80it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.83it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.87it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.95it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.89it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.00it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.04it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.89it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.82it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.70it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.90it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.90it/s][A 60%|██████    | 351/585 [02:47<01:11,  3.27it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:33:30,581 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 21:33:30,674 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:33:34,576 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:33:34,616 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:33:34,632 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:00<27:06,  6.98s/it] 60%|██████    | 353/585 [03:00<19:14,  4.98s/it] 61%|██████    | 354/585 [03:00<13:45,  3.57s/it] 61%|██████    | 355/585 [03:01<09:55,  2.59s/it] 61%|██████    | 356/585 [03:01<07:14,  1.90s/it] 61%|██████    | 357/585 [03:01<05:22,  1.42s/it] 61%|██████    | 358/585 [03:01<04:04,  1.08s/it] 61%|██████▏   | 359/585 [03:02<03:10,  1.19it/s] 62%|██████▏   | 360/585 [03:02<02:32,  1.48it/s] 62%|██████▏   | 361/585 [03:02<02:05,  1.78it/s] 62%|██████▏   | 362/585 [03:03<01:47,  2.08it/s] 62%|██████▏   | 363/585 [03:03<01:33,  2.36it/s] 62%|██████▏   | 364/585 [03:03<01:25,  2.59it/s] 62%|██████▏   | 365/585 [03:04<01:18,  2.80it/s] 63%|██████▎   | 366/585 [03:04<01:13,  2.97it/s] 63%|██████▎   | 367/585 [03:04<01:10,  3.09it/s] 63%|██████▎   | 368/585 [03:04<01:08,  3.19it/s] 63%|██████▎   | 369/585 [03:05<01:06,  3.26it/s] 63%|██████▎   | 370/585 [03:05<01:04,  3.31it/s] 63%|██████▎   | 371/585 [03:05<01:03,  3.35it/s] 64%|██████▎   | 372/585 [03:06<01:03,  3.37it/s] 64%|██████▍   | 373/585 [03:06<01:02,  3.39it/s] 64%|██████▍   | 374/585 [03:06<01:02,  3.40it/s] 64%|██████▍   | 375/585 [03:06<01:04,  3.28it/s] 64%|██████▍   | 376/585 [03:07<01:02,  3.33it/s] 64%|██████▍   | 377/585 [03:07<01:01,  3.36it/s] 65%|██████▍   | 378/585 [03:07<01:01,  3.38it/s] 65%|██████▍   | 379/585 [03:08<01:00,  3.40it/s] 65%|██████▍   | 380/585 [03:08<01:00,  3.41it/s] 65%|██████▌   | 381/585 [03:08<00:59,  3.42it/s] 65%|██████▌   | 382/585 [03:09<00:59,  3.42it/s] 65%|██████▌   | 383/585 [03:09<00:58,  3.43it/s] 66%|██████▌   | 384/585 [03:09<00:58,  3.42it/s] 66%|██████▌   | 385/585 [03:09<00:58,  3.43it/s] 66%|██████▌   | 386/585 [03:10<00:58,  3.38it/s] 66%|██████▌   | 387/585 [03:10<00:58,  3.40it/s] 66%|██████▋   | 388/585 [03:10<00:57,  3.41it/s] 66%|██████▋   | 389/585 [03:11<00:57,  3.41it/s] 67%|██████▋   | 390/585 [03:11<00:57,  3.42it/s] 67%|██████▋   | 391/585 [03:11<00:56,  3.42it/s] 67%|██████▋   | 392/585 [03:11<00:56,  3.43it/s] 67%|██████▋   | 393/585 [03:12<00:55,  3.43it/s] 67%|██████▋   | 394/585 [03:12<00:55,  3.43it/s] 68%|██████▊   | 395/585 [03:12<00:55,  3.43it/s] 68%|██████▊   | 396/585 [03:13<00:55,  3.43it/s] 68%|██████▊   | 397/585 [03:13<00:58,  3.24it/s] 68%|██████▊   | 398/585 [03:13<00:56,  3.30it/s] 68%|██████▊   | 399/585 [03:14<00:55,  3.33it/s] 68%|██████▊   | 400/585 [03:14<00:54,  3.36it/s] 69%|██████▊   | 401/585 [03:14<00:54,  3.38it/s] 69%|██████▊   | 402/585 [03:14<00:53,  3.40it/s] 69%|██████▉   | 403/585 [03:15<00:53,  3.41it/s] 69%|██████▉   | 404/585 [03:15<00:52,  3.42it/s] 69%|██████▉   | 405/585 [03:15<00:52,  3.42it/s] 69%|██████▉   | 406/585 [03:16<00:52,  3.43it/s] 70%|██████▉   | 407/585 [03:16<00:51,  3.43it/s] 70%|██████▉   | 408/585 [03:16<00:52,  3.40it/s] 70%|██████▉   | 409/585 [03:16<00:51,  3.41it/s] 70%|███████   | 410/585 [03:17<00:51,  3.42it/s] 70%|███████   | 411/585 [03:17<00:50,  3.42it/s] 70%|███████   | 412/585 [03:17<00:50,  3.42it/s] 71%|███████   | 413/585 [03:18<00:50,  3.43it/s] 71%|███████   | 414/585 [03:18<00:49,  3.43it/s] 71%|███████   | 415/585 [03:18<00:49,  3.43it/s] 71%|███████   | 416/585 [03:19<00:49,  3.43it/s] 71%|███████▏  | 417/585 [03:19<00:48,  3.43it/s] 71%|███████▏  | 418/585 [03:19<00:48,  3.43it/s] 72%|███████▏  | 419/585 [03:19<00:48,  3.43it/s] 72%|███████▏  | 420/585 [03:20<00:48,  3.43it/s] 72%|███████▏  | 421/585 [03:20<00:47,  3.43it/s] 72%|███████▏  | 422/585 [03:20<00:47,  3.43it/s] 72%|███████▏  | 423/585 [03:21<00:47,  3.43it/s] 72%|███████▏  | 424/585 [03:21<00:46,  3.43it/s] 73%|███████▎  | 425/585 [03:21<00:46,  3.43it/s] 73%|███████▎  | 426/585 [03:21<00:46,  3.43it/s] 73%|███████▎  | 427/585 [03:22<00:46,  3.43it/s] 73%|███████▎  | 428/585 [03:22<00:47,  3.31it/s] 73%|███████▎  | 429/585 [03:22<00:46,  3.34it/s] 74%|███████▎  | 430/585 [03:23<00:46,  3.37it/s] 74%|███████▎  | 431/585 [03:23<00:45,  3.39it/s] 74%|███████▍  | 432/585 [03:23<00:45,  3.40it/s] 74%|███████▍  | 433/585 [03:23<00:44,  3.41it/s] 74%|███████▍  | 434/585 [03:24<00:44,  3.42it/s] 74%|███████▍  | 435/585 [03:24<00:43,  3.42it/s] 75%|███████▍  | 436/585 [03:24<00:43,  3.42it/s] 75%|███████▍  | 437/585 [03:25<00:43,  3.42it/s] 75%|███████▍  | 438/585 [03:25<00:42,  3.42it/s] 75%|███████▌  | 439/585 [03:25<00:42,  3.40it/s] 75%|███████▌  | 440/585 [03:26<00:42,  3.41it/s] 75%|███████▌  | 441/585 [03:26<00:42,  3.42it/s] 76%|███████▌  | 442/585 [03:26<00:41,  3.42it/s] 76%|███████▌  | 443/585 [03:26<00:41,  3.42it/s] 76%|███████▌  | 444/585 [03:27<00:41,  3.42it/s] 76%|███████▌  | 445/585 [03:27<00:40,  3.42it/s] 76%|███████▌  | 446/585 [03:27<00:40,  3.43it/s] 76%|███████▋  | 447/585 [03:28<00:40,  3.43it/s] 77%|███████▋  | 448/585 [03:28<00:39,  3.43it/s] 77%|███████▋  | 449/585 [03:28<00:39,  3.43it/s] 77%|███████▋  | 450/585 [03:28<00:39,  3.41it/s] 77%|███████▋  | 451/585 [03:29<00:39,  3.42it/s] 77%|███████▋  | 452/585 [03:29<00:38,  3.42it/s] 77%|███████▋  | 453/585 [03:29<00:38,  3.42it/s] 78%|███████▊  | 454/585 [03:30<00:38,  3.43it/s] 78%|███████▊  | 455/585 [03:30<00:37,  3.42it/s] 78%|███████▊  | 456/585 [03:30<00:37,  3.42it/s] 78%|███████▊  | 457/585 [03:31<00:37,  3.43it/s] 78%|███████▊  | 458/585 [03:31<00:37,  3.43it/s] 78%|███████▊  | 459/585 [03:31<00:36,  3.43it/s] 79%|███████▊  | 460/585 [03:31<00:36,  3.43it/s] 79%|███████▉  | 461/585 [03:32<00:36,  3.40it/s] 79%|███████▉  | 462/585 [03:32<00:36,  3.41it/s] 79%|███████▉  | 463/585 [03:32<00:35,  3.42it/s] 79%|███████▉  | 464/585 [03:33<00:35,  3.42it/s] 79%|███████▉  | 465/585 [03:33<00:35,  3.43it/s] 80%|███████▉  | 466/585 [03:33<00:34,  3.43it/s] 80%|███████▉  | 467/585 [03:33<00:34,  3.43it/s] 80%|████████  | 468/585 [03:34<00:34,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 21:34:17,094 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:34:17,094 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:34:17,094 >>   Batch size = 8
{'eval_loss': 1.0445281267166138, 'eval_runtime': 9.9624, 'eval_samples_per_second': 350.619, 'eval_steps_per_second': 43.865, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.40it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.77it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.97it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.16it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.71it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.37it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.47it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.62it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.99it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.14it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.17it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.05it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.97it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.97it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.84it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.91it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.13it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.26it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.30it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.20it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.12it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.10it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.92it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.94it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.95it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.12it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.14it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.31it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.27it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.15it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.05it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.99it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.92it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.97it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.13it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.29it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.27it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.19it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.08it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.06it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.98it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.98it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.08it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.28it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.42it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.31it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.21it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.05it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.94it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.93it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.96it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.00it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.26it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.30it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.33it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.14it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.05it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.03it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.95it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.96it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.73it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.08it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.24it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.21it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.19it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.02it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.91it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.92it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.88it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.06it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.22it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.39it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.28it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.20it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.12it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.02it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.92it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.91it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.11it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.34it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.38it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.31it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.03it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.03it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.05it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.89it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.14it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.14it/s][A 80%|████████  | 468/585 [03:44<00:34,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:34:27,037 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:34:27,059 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:34:37,739 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:34:37,767 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:34:37,820 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:05<18:32,  9.59s/it] 80%|████████  | 470/585 [04:05<13:02,  6.80s/it] 81%|████████  | 471/585 [04:06<09:13,  4.85s/it] 81%|████████  | 472/585 [04:06<06:33,  3.48s/it] 81%|████████  | 473/585 [04:06<04:43,  2.53s/it] 81%|████████  | 474/585 [04:06<03:26,  1.86s/it] 81%|████████  | 475/585 [04:07<02:32,  1.39s/it] 81%|████████▏ | 476/585 [04:07<01:55,  1.06s/it] 82%|████████▏ | 477/585 [04:07<01:29,  1.20it/s] 82%|████████▏ | 478/585 [04:08<01:11,  1.49it/s] 82%|████████▏ | 479/585 [04:08<00:59,  1.79it/s] 82%|████████▏ | 480/585 [04:08<00:50,  2.08it/s] 82%|████████▏ | 481/585 [04:09<00:44,  2.32it/s] 82%|████████▏ | 482/585 [04:09<00:40,  2.57it/s] 83%|████████▎ | 483/585 [04:09<00:36,  2.77it/s] 83%|████████▎ | 484/585 [04:09<00:34,  2.93it/s] 83%|████████▎ | 485/585 [04:10<00:32,  3.05it/s] 83%|████████▎ | 486/585 [04:10<00:31,  3.15it/s] 83%|████████▎ | 487/585 [04:10<00:30,  3.21it/s] 83%|████████▎ | 488/585 [04:11<00:29,  3.26it/s] 84%|████████▎ | 489/585 [04:11<00:29,  3.30it/s] 84%|████████▍ | 490/585 [04:11<00:28,  3.33it/s] 84%|████████▍ | 491/585 [04:12<00:28,  3.34it/s] 84%|████████▍ | 492/585 [04:12<00:27,  3.33it/s] 84%|████████▍ | 493/585 [04:12<00:27,  3.35it/s] 84%|████████▍ | 494/585 [04:12<00:27,  3.36it/s] 85%|████████▍ | 495/585 [04:13<00:26,  3.36it/s] 85%|████████▍ | 496/585 [04:13<00:26,  3.37it/s] 85%|████████▍ | 497/585 [04:13<00:26,  3.38it/s] 85%|████████▌ | 498/585 [04:14<00:25,  3.38it/s] 85%|████████▌ | 499/585 [04:14<00:25,  3.38it/s] 85%|████████▌ | 500/585 [04:14<00:25,  3.39it/s]                                                  85%|████████▌ | 500/585 [04:14<00:25,  3.39it/s] 86%|████████▌ | 501/585 [04:14<00:24,  3.39it/s] 86%|████████▌ | 502/585 [04:15<00:24,  3.38it/s] 86%|████████▌ | 503/585 [04:15<00:24,  3.37it/s] 86%|████████▌ | 504/585 [04:15<00:24,  3.37it/s] 86%|████████▋ | 505/585 [04:16<00:23,  3.38it/s] 86%|████████▋ | 506/585 [04:16<00:23,  3.38it/s] 87%|████████▋ | 507/585 [04:16<00:23,  3.38it/s] 87%|████████▋ | 508/585 [04:17<00:22,  3.38it/s] 87%|████████▋ | 509/585 [04:17<00:22,  3.38it/s] 87%|████████▋ | 510/585 [04:17<00:22,  3.38it/s] 87%|████████▋ | 511/585 [04:17<00:21,  3.39it/s] 88%|████████▊ | 512/585 [04:18<00:21,  3.39it/s] 88%|████████▊ | 513/585 [04:18<00:21,  3.38it/s] 88%|████████▊ | 514/585 [04:18<00:20,  3.38it/s] 88%|████████▊ | 515/585 [04:19<00:20,  3.36it/s] 88%|████████▊ | 516/585 [04:19<00:20,  3.37it/s] 88%|████████▊ | 517/585 [04:19<00:20,  3.37it/s] 89%|████████▊ | 518/585 [04:20<00:19,  3.38it/s] 89%|████████▊ | 519/585 [04:20<00:19,  3.38it/s] 89%|████████▉ | 520/585 [04:20<00:19,  3.38it/s] 89%|████████▉ | 521/585 [04:20<00:18,  3.38it/s] 89%|████████▉ | 522/585 [04:21<00:18,  3.38it/s] 89%|████████▉ | 523/585 [04:21<00:18,  3.38it/s] 90%|████████▉ | 524/585 [04:21<00:18,  3.38it/s] 90%|████████▉ | 525/585 [04:22<00:17,  3.38it/s] 90%|████████▉ | 526/585 [04:22<00:17,  3.37it/s] 90%|█████████ | 527/585 [04:22<00:17,  3.38it/s] 90%|█████████ | 528/585 [04:22<00:16,  3.38it/s] 90%|█████████ | 529/585 [04:23<00:16,  3.38it/s] 91%|█████████ | 530/585 [04:23<00:16,  3.38it/s] 91%|█████████ | 531/585 [04:23<00:15,  3.38it/s] 91%|█████████ | 532/585 [04:24<00:15,  3.38it/s] 91%|█████████ | 533/585 [04:24<00:15,  3.38it/s] 91%|█████████▏| 534/585 [04:24<00:15,  3.38it/s] 91%|█████████▏| 535/585 [04:25<00:14,  3.38it/s] 92%|█████████▏| 536/585 [04:25<00:14,  3.38it/s] 92%|█████████▏| 537/585 [04:25<00:14,  3.25it/s] 92%|█████████▏| 538/585 [04:25<00:14,  3.29it/s] 92%|█████████▏| 539/585 [04:26<00:13,  3.32it/s] 92%|█████████▏| 540/585 [04:26<00:13,  3.34it/s] 92%|█████████▏| 541/585 [04:26<00:13,  3.36it/s] 93%|█████████▎| 542/585 [04:27<00:12,  3.37it/s] 93%|█████████▎| 543/585 [04:27<00:12,  3.39it/s] 93%|█████████▎| 544/585 [04:27<00:12,  3.41it/s] 93%|█████████▎| 545/585 [04:28<00:11,  3.41it/s] 93%|█████████▎| 546/585 [04:28<00:11,  3.42it/s] 94%|█████████▎| 547/585 [04:28<00:11,  3.42it/s] 94%|█████████▎| 548/585 [04:28<00:10,  3.42it/s] 94%|█████████▍| 549/585 [04:29<00:10,  3.42it/s] 94%|█████████▍| 550/585 [04:29<00:10,  3.43it/s] 94%|█████████▍| 551/585 [04:29<00:09,  3.43it/s] 94%|█████████▍| 552/585 [04:30<00:09,  3.43it/s] 95%|█████████▍| 553/585 [04:30<00:09,  3.43it/s] 95%|█████████▍| 554/585 [04:30<00:09,  3.42it/s] 95%|█████████▍| 555/585 [04:30<00:08,  3.42it/s] 95%|█████████▌| 556/585 [04:31<00:08,  3.43it/s] 95%|█████████▌| 557/585 [04:31<00:08,  3.42it/s] 95%|█████████▌| 558/585 [04:31<00:07,  3.42it/s] 96%|█████████▌| 559/585 [04:32<00:07,  3.40it/s] 96%|█████████▌| 560/585 [04:32<00:07,  3.41it/s] 96%|█████████▌| 561/585 [04:32<00:07,  3.42it/s] 96%|█████████▌| 562/585 [04:33<00:06,  3.42it/s] 96%|█████████▌| 563/585 [04:33<00:06,  3.43it/s] 96%|█████████▋| 564/585 [04:33<00:06,  3.43it/s] 97%|█████████▋| 565/585 [04:33<00:05,  3.43it/s] 97%|█████████▋| 566/585 [04:34<00:05,  3.43it/s] 97%|█████████▋| 567/585 [04:34<00:05,  3.43it/s] 97%|█████████▋| 568/585 [04:34<00:04,  3.43it/s] 97%|█████████▋| 569/585 [04:35<00:04,  3.43it/s] 97%|█████████▋| 570/585 [04:35<00:04,  3.40it/s] 98%|█████████▊| 571/585 [04:35<00:04,  3.41it/s] 98%|█████████▊| 572/585 [04:35<00:03,  3.42it/s] 98%|█████████▊| 573/585 [04:36<00:03,  3.42it/s] 98%|█████████▊| 574/585 [04:36<00:03,  3.42it/s] 98%|█████████▊| 575/585 [04:36<00:02,  3.43it/s] 98%|█████████▊| 576/585 [04:37<00:02,  3.43it/s] 99%|█████████▊| 577/585 [04:37<00:02,  3.43it/s] 99%|█████████▉| 578/585 [04:37<00:02,  3.43it/s] 99%|█████████▉| 579/585 [04:37<00:01,  3.43it/s] 99%|█████████▉| 580/585 [04:38<00:01,  3.43it/s] 99%|█████████▉| 581/585 [04:38<00:01,  3.32it/s] 99%|█████████▉| 582/585 [04:38<00:00,  3.35it/s]100%|█████████▉| 583/585 [04:39<00:00,  3.38it/s]100%|█████████▉| 584/585 [04:39<00:00,  3.39it/s]100%|██████████| 585/585 [04:39<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 21:35:22,580 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:35:22,580 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:35:22,580 >>   Batch size = 8
{'eval_loss': 1.059427261352539, 'eval_runtime': 9.9249, 'eval_samples_per_second': 351.944, 'eval_steps_per_second': 44.031, 'epoch': 4.0}
{'loss': 0.5187, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.60it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.81it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.03it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.02it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.51it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.21it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.10it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.18it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.30it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.34it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.18it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.06it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.96it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.77it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.68it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.80it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.97it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.04it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.17it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.15it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.90it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.92it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.88it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.83it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.85it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.79it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.91it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.07it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.07it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.98it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.84it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.86it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.81it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.93it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.95it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.11it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.11it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.14it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.95it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.88it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.64it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.75it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.79it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.96it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.15it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.08it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.05it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.94it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.87it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.82it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.79it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.90it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.01it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.03it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.19it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.06it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.91it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.91it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.80it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.83it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.95it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.05it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.08it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.14it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.01it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.90it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.72it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.78it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.91it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.95it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.11it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.14it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.14it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.96it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.85it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.70it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.71it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.77it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.95it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.12it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.13it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.16it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.04it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.85it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.67it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 41.87it/s][A
100%|██████████| 437/437 [00:09<00:00, 42.55it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:49<00:00,  3.40it/s]
100%|██████████| 437/437 [00:09<00:00, 42.55it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:35:32,560 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 21:35:32,591 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:35:37,138 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:35:37,201 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:35:37,225 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:35:46,792 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:35:46,797 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117 (score: 1.0159136056900024).
                                                 100%|██████████| 585/585 [05:09<00:00,  3.40it/s]100%|██████████| 585/585 [05:09<00:00,  1.89it/s]
[INFO|trainer.py:1894] 2023-08-28 21:35:51,962 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 21:35:51,994 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:35:56,216 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:35:56,337 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:35:56,348 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:35:56,612 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:56,613 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:56,613 >>   train_loss               =     0.5151
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:56,613 >>   train_runtime            = 0:05:09.05
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:56,613 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:56,613 >>   train_samples_per_second =    121.321
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:56,613 >>   train_steps_per_second   =      1.893
{'eval_loss': 1.0607737302780151, 'eval_runtime': 9.9563, 'eval_samples_per_second': 350.833, 'eval_steps_per_second': 43.892, 'epoch': 5.0}
{'train_runtime': 309.0555, 'train_samples_per_second': 121.321, 'train_steps_per_second': 1.893, 'train_loss': 0.5150716341458834, 'epoch': 5.0}
08/28/2023 21:35:56 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:35:56,720 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:35:56,720 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:35:56,720 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.52it/s]  3%|▎         | 12/437 [00:00<00:08, 48.47it/s]  4%|▍         | 17/437 [00:00<00:08, 47.10it/s]  5%|▌         | 22/437 [00:00<00:08, 46.22it/s]  6%|▌         | 27/437 [00:00<00:08, 45.85it/s]  7%|▋         | 32/437 [00:00<00:08, 45.62it/s]  8%|▊         | 37/437 [00:00<00:08, 45.27it/s] 10%|▉         | 42/437 [00:00<00:08, 44.90it/s] 11%|█         | 47/437 [00:01<00:08, 44.37it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.20it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.21it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.33it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.50it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.66it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.64it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.60it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.39it/s] 21%|██        | 92/437 [00:02<00:07, 43.95it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.97it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.12it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.30it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.28it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.57it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.72it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.54it/s] 30%|███       | 132/437 [00:02<00:06, 44.27it/s] 31%|███▏      | 137/437 [00:03<00:06, 43.98it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.97it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.09it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.22it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.42it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.48it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.59it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.45it/s] 41%|████      | 177/437 [00:03<00:05, 44.14it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.06it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.00it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.07it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.21it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.43it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.53it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.34it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.22it/s] 51%|█████     | 222/437 [00:04<00:04, 44.14it/s] 52%|█████▏    | 227/437 [00:05<00:04, 43.94it/s] 53%|█████▎    | 232/437 [00:05<00:04, 43.94it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.07it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.16it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.41it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.47it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.50it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.20it/s] 61%|██████    | 267/437 [00:05<00:03, 44.14it/s] 62%|██████▏   | 272/437 [00:06<00:03, 43.98it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.01it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.11it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.20it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.35it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.39it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.37it/s] 70%|███████   | 307/437 [00:06<00:02, 44.30it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.07it/s] 73%|███████▎  | 317/437 [00:07<00:02, 43.95it/s] 74%|███████▎  | 322/437 [00:07<00:02, 43.99it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.05it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.18it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.32it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.38it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.37it/s] 81%|████████  | 352/437 [00:07<00:01, 44.21it/s] 82%|████████▏ | 357/437 [00:08<00:01, 43.99it/s] 83%|████████▎ | 362/437 [00:08<00:01, 43.97it/s] 84%|████████▍ | 367/437 [00:08<00:01, 43.93it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.03it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.10it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.31it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.41it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.35it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.26it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.10it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.10it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.08it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.05it/s] 97%|█████████▋| 422/437 [00:09<00:00, 43.66it/s] 98%|█████████▊| 427/437 [00:09<00:00, 43.98it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.10it/s]100%|██████████| 437/437 [00:09<00:00, 44.12it/s]100%|██████████| 437/437 [00:09<00:00, 44.34it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:36:06,592 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:36:06,592 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:36:06,592 >>   eval_loss               =     1.0159
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:36:06,592 >>   eval_runtime            = 0:00:09.87
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:36:06,592 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:36:06,592 >>   eval_samples_per_second =    353.846
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:36:06,592 >>   eval_steps_per_second   =     44.269
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:36:06,592 >>   perplexity              =     2.7619
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:14,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:14,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:14,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:14,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:14,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:36:14,421 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:36:14,422 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:36:14,693 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:36:15,759 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:36:15,759 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:17,165 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:17,170 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:17,170 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:17,170 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:17,170 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:36:17,506 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:36:17,507 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:36:17,772 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:36:17,930 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:36:17,931 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.74it/s]Extractor Predicting: 5it [00:02,  1.67it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:12,  1.61it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.74it/s]Extractor Predicting: 29it [00:17,  1.71it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:18,  1.64it/s]Extractor Predicting: 32it [00:19,  1.62it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:20,  1.56it/s]Extractor Predicting: 35it [00:21,  1.57it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:22,  1.55it/s]Extractor Predicting: 38it [00:23,  1.54it/s]Extractor Predicting: 39it [00:23,  1.57it/s]Extractor Predicting: 40it [00:24,  1.57it/s]Extractor Predicting: 41it [00:25,  1.56it/s]Extractor Predicting: 42it [00:25,  1.58it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:27,  1.62it/s]Extractor Predicting: 46it [00:28,  1.58it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:29,  1.57it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:30,  1.56it/s]Extractor Predicting: 51it [00:31,  1.58it/s]Extractor Predicting: 52it [00:32,  1.56it/s]Extractor Predicting: 53it [00:32,  1.60it/s]Extractor Predicting: 54it [00:33,  1.58it/s]Extractor Predicting: 55it [00:34,  1.55it/s]Extractor Predicting: 56it [00:34,  1.56it/s]Extractor Predicting: 57it [00:35,  1.53it/s]Extractor Predicting: 58it [00:36,  1.54it/s]Extractor Predicting: 59it [00:36,  1.53it/s]Extractor Predicting: 60it [00:37,  1.45it/s]Extractor Predicting: 61it [00:38,  1.49it/s]Extractor Predicting: 62it [00:38,  1.51it/s]Extractor Predicting: 63it [00:39,  1.55it/s]Extractor Predicting: 64it [00:40,  1.58it/s]Extractor Predicting: 65it [00:40,  1.55it/s]Extractor Predicting: 66it [00:41,  1.60it/s]Extractor Predicting: 67it [00:41,  1.58it/s]Extractor Predicting: 68it [00:42,  1.57it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:43,  1.55it/s]Extractor Predicting: 71it [00:44,  1.56it/s]Extractor Predicting: 72it [00:45,  1.57it/s]Extractor Predicting: 73it [00:45,  1.57it/s]Extractor Predicting: 74it [00:46,  1.60it/s]Extractor Predicting: 75it [00:46,  1.62it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:48,  1.57it/s]Extractor Predicting: 78it [00:48,  1.56it/s]Extractor Predicting: 79it [00:49,  1.57it/s]Extractor Predicting: 80it [00:50,  1.55it/s]Extractor Predicting: 81it [00:50,  1.60it/s]Extractor Predicting: 82it [00:51,  1.58it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:52,  1.44it/s]Extractor Predicting: 85it [00:53,  1.50it/s]Extractor Predicting: 86it [00:54,  1.58it/s]Extractor Predicting: 87it [00:54,  1.63it/s]Extractor Predicting: 88it [00:55,  1.65it/s]Extractor Predicting: 89it [00:55,  1.65it/s]Extractor Predicting: 90it [00:56,  1.68it/s]Extractor Predicting: 91it [00:57,  1.66it/s]Extractor Predicting: 92it [00:57,  1.64it/s]Extractor Predicting: 93it [00:58,  1.67it/s]Extractor Predicting: 94it [00:58,  1.69it/s]Extractor Predicting: 95it [00:59,  1.69it/s]Extractor Predicting: 96it [00:59,  1.70it/s]Extractor Predicting: 97it [01:00,  1.68it/s]Extractor Predicting: 98it [01:01,  1.64it/s]Extractor Predicting: 99it [01:01,  1.60it/s]Extractor Predicting: 100it [01:02,  1.63it/s]Extractor Predicting: 101it [01:03,  1.67it/s]Extractor Predicting: 102it [01:03,  1.66it/s]Extractor Predicting: 103it [01:04,  1.65it/s]Extractor Predicting: 104it [01:04,  1.63it/s]Extractor Predicting: 105it [01:05,  1.67it/s]Extractor Predicting: 106it [01:06,  1.65it/s]Extractor Predicting: 107it [01:06,  1.67it/s]Extractor Predicting: 108it [01:07,  1.67it/s]Extractor Predicting: 109it [01:07,  1.70it/s]Extractor Predicting: 110it [01:08,  1.69it/s]Extractor Predicting: 111it [01:09,  1.67it/s]Extractor Predicting: 112it [01:09,  1.65it/s]Extractor Predicting: 113it [01:10,  1.65it/s]Extractor Predicting: 114it [01:10,  1.60it/s]Extractor Predicting: 115it [01:11,  1.62it/s]Extractor Predicting: 116it [01:12,  1.62it/s]Extractor Predicting: 117it [01:12,  1.60it/s]Extractor Predicting: 118it [01:13,  1.67it/s]Extractor Predicting: 119it [01:13,  1.66it/s]Extractor Predicting: 120it [01:14,  1.68it/s]Extractor Predicting: 121it [01:15,  1.65it/s]Extractor Predicting: 122it [01:15,  1.64it/s]Extractor Predicting: 123it [01:16,  1.62it/s]Extractor Predicting: 124it [01:17,  1.59it/s]Extractor Predicting: 125it [01:17,  1.59it/s]Extractor Predicting: 126it [01:18,  1.60it/s]Extractor Predicting: 127it [01:18,  1.58it/s]Extractor Predicting: 128it [01:19,  1.58it/s]Extractor Predicting: 129it [01:20,  1.54it/s]Extractor Predicting: 130it [01:20,  1.56it/s]Extractor Predicting: 131it [01:21,  1.57it/s]Extractor Predicting: 132it [01:22,  1.61it/s]Extractor Predicting: 133it [01:22,  1.60it/s]Extractor Predicting: 134it [01:23,  1.60it/s]Extractor Predicting: 135it [01:24,  1.60it/s]Extractor Predicting: 136it [01:24,  1.60it/s]Extractor Predicting: 137it [01:25,  1.59it/s]Extractor Predicting: 138it [01:25,  1.59it/s]Extractor Predicting: 139it [01:26,  1.57it/s]Extractor Predicting: 140it [01:27,  1.62it/s]Extractor Predicting: 141it [01:27,  1.60it/s]Extractor Predicting: 142it [01:28,  1.66it/s]Extractor Predicting: 142it [01:28,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:56,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:56,073 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:56,073 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:56,073 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:56,073 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:37:56,502 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:37:56,503 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:37:56,931 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:57,968 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:57,968 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:00,112 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:00,135 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:00,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:00,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:00,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:38:00,594 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:38:00,595 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:38:01,036 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:38:01,188 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:38:01,188 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2283464566929134,
  "recall": 0.07472087031205267,
  "score": 0.11259706643658325,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.67it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.69it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:10,  1.62it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:15,  1.61it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:18,  1.54it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:19,  1.54it/s]Extractor Predicting: 33it [00:20,  1.56it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:24,  1.63it/s]Extractor Predicting: 41it [00:25,  1.63it/s]Extractor Predicting: 42it [00:25,  1.61it/s]Extractor Predicting: 43it [00:26,  1.61it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.61it/s]Extractor Predicting: 47it [00:29,  1.61it/s]Extractor Predicting: 48it [00:29,  1.60it/s]Extractor Predicting: 49it [00:30,  1.60it/s]Extractor Predicting: 50it [00:30,  1.58it/s]Extractor Predicting: 51it [00:31,  1.60it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:32,  1.60it/s]Extractor Predicting: 54it [00:33,  1.61it/s]Extractor Predicting: 55it [00:34,  1.57it/s]Extractor Predicting: 56it [00:34,  1.59it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:35,  1.65it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 60it [00:37,  1.65it/s]Extractor Predicting: 61it [00:37,  1.64it/s]Extractor Predicting: 62it [00:38,  1.64it/s]Extractor Predicting: 63it [00:38,  1.65it/s]Extractor Predicting: 64it [00:39,  1.60it/s]Extractor Predicting: 65it [00:40,  1.60it/s]Extractor Predicting: 66it [00:40,  1.59it/s]Extractor Predicting: 67it [00:41,  1.60it/s]Extractor Predicting: 68it [00:41,  1.66it/s]Extractor Predicting: 69it [00:42,  1.65it/s]Extractor Predicting: 70it [00:43,  1.65it/s]Extractor Predicting: 71it [00:43,  1.62it/s]Extractor Predicting: 72it [00:44,  1.63it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:45,  1.59it/s]Extractor Predicting: 75it [00:46,  1.60it/s]Extractor Predicting: 76it [00:46,  1.61it/s]Extractor Predicting: 77it [00:47,  1.58it/s]Extractor Predicting: 78it [00:48,  1.44it/s]Extractor Predicting: 79it [00:49,  1.48it/s]Extractor Predicting: 80it [00:49,  1.50it/s]Extractor Predicting: 81it [00:50,  1.54it/s]Extractor Predicting: 82it [00:51,  1.52it/s]Extractor Predicting: 83it [00:51,  1.54it/s]Extractor Predicting: 84it [00:52,  1.56it/s]Extractor Predicting: 85it [00:52,  1.59it/s]Extractor Predicting: 86it [00:53,  1.56it/s]Extractor Predicting: 87it [00:54,  1.57it/s]Extractor Predicting: 88it [00:54,  1.53it/s]Extractor Predicting: 89it [00:55,  1.55it/s]Extractor Predicting: 90it [00:56,  1.58it/s]Extractor Predicting: 91it [00:56,  1.55it/s]Extractor Predicting: 92it [00:57,  1.56it/s]Extractor Predicting: 93it [00:58,  1.53it/s]Extractor Predicting: 94it [00:58,  1.51it/s]Extractor Predicting: 95it [00:59,  1.52it/s]Extractor Predicting: 96it [01:00,  1.52it/s]Extractor Predicting: 97it [01:00,  1.53it/s]Extractor Predicting: 98it [01:01,  1.53it/s]Extractor Predicting: 99it [01:02,  1.54it/s]Extractor Predicting: 100it [01:02,  1.54it/s]Extractor Predicting: 101it [01:03,  1.54it/s]Extractor Predicting: 102it [01:03,  1.54it/s]Extractor Predicting: 103it [01:04,  1.55it/s]Extractor Predicting: 104it [01:05,  1.54it/s]Extractor Predicting: 105it [01:05,  1.54it/s]Extractor Predicting: 106it [01:06,  1.55it/s]Extractor Predicting: 107it [01:07,  1.53it/s]Extractor Predicting: 108it [01:07,  1.52it/s]Extractor Predicting: 109it [01:08,  1.53it/s]Extractor Predicting: 110it [01:09,  1.51it/s]Extractor Predicting: 111it [01:09,  1.52it/s]Extractor Predicting: 112it [01:10,  1.55it/s]Extractor Predicting: 113it [01:11,  1.58it/s]Extractor Predicting: 114it [01:11,  1.60it/s]Extractor Predicting: 115it [01:12,  1.60it/s]Extractor Predicting: 116it [01:12,  1.60it/s]Extractor Predicting: 117it [01:13,  1.61it/s]Extractor Predicting: 118it [01:14,  1.62it/s]Extractor Predicting: 119it [01:14,  1.62it/s]Extractor Predicting: 120it [01:15,  1.62it/s]Extractor Predicting: 121it [01:16,  1.62it/s]Extractor Predicting: 122it [01:16,  1.67it/s]Extractor Predicting: 123it [01:17,  1.66it/s]Extractor Predicting: 124it [01:17,  1.66it/s]Extractor Predicting: 125it [01:18,  1.65it/s]Extractor Predicting: 126it [01:19,  1.63it/s]Extractor Predicting: 127it [01:19,  1.63it/s]Extractor Predicting: 128it [01:20,  1.64it/s]Extractor Predicting: 129it [01:20,  1.55it/s]Extractor Predicting: 130it [01:21,  1.58it/s]Extractor Predicting: 131it [01:22,  1.58it/s]Extractor Predicting: 132it [01:22,  1.60it/s]Extractor Predicting: 133it [01:23,  1.60it/s]Extractor Predicting: 134it [01:24,  1.61it/s]Extractor Predicting: 135it [01:24,  1.64it/s]Extractor Predicting: 136it [01:25,  1.64it/s]Extractor Predicting: 137it [01:25,  1.62it/s]Extractor Predicting: 138it [01:26,  1.62it/s]Extractor Predicting: 139it [01:27,  1.67it/s]Extractor Predicting: 140it [01:27,  1.64it/s]Extractor Predicting: 141it [01:28,  1.62it/s]Extractor Predicting: 142it [01:28,  1.66it/s]Extractor Predicting: 143it [01:29,  1.65it/s]Extractor Predicting: 144it [01:30,  1.62it/s]Extractor Predicting: 145it [01:30,  1.63it/s]Extractor Predicting: 146it [01:31,  1.62it/s]Extractor Predicting: 147it [01:32,  1.60it/s]Extractor Predicting: 148it [01:32,  1.60it/s]Extractor Predicting: 149it [01:33,  1.59it/s]Extractor Predicting: 150it [01:33,  1.61it/s]Extractor Predicting: 151it [01:34,  1.64it/s]Extractor Predicting: 152it [01:35,  1.65it/s]Extractor Predicting: 153it [01:35,  1.63it/s]Extractor Predicting: 154it [01:36,  1.61it/s]Extractor Predicting: 155it [01:36,  1.60it/s]Extractor Predicting: 156it [01:37,  1.59it/s]Extractor Predicting: 157it [01:38,  1.56it/s]Extractor Predicting: 158it [01:38,  1.56it/s]Extractor Predicting: 159it [01:39,  1.57it/s]Extractor Predicting: 160it [01:40,  1.42it/s]Extractor Predicting: 161it [01:41,  1.48it/s]Extractor Predicting: 162it [01:41,  1.52it/s]Extractor Predicting: 163it [01:42,  1.54it/s]Extractor Predicting: 164it [01:42,  1.57it/s]Extractor Predicting: 165it [01:43,  1.55it/s]Extractor Predicting: 166it [01:44,  1.56it/s]Extractor Predicting: 167it [01:44,  1.60it/s]Extractor Predicting: 168it [01:45,  1.59it/s]Extractor Predicting: 169it [01:46,  1.57it/s]Extractor Predicting: 170it [01:46,  1.55it/s]Extractor Predicting: 171it [01:47,  1.52it/s]Extractor Predicting: 172it [01:48,  1.53it/s]Extractor Predicting: 173it [01:48,  1.54it/s]Extractor Predicting: 174it [01:49,  1.49it/s]Extractor Predicting: 175it [01:50,  1.46it/s]Extractor Predicting: 176it [01:50,  1.48it/s]Extractor Predicting: 177it [01:51,  1.49it/s]Extractor Predicting: 178it [01:52,  1.53it/s]Extractor Predicting: 179it [01:52,  1.53it/s]Extractor Predicting: 180it [01:53,  1.58it/s]Extractor Predicting: 181it [01:53,  1.57it/s]Extractor Predicting: 182it [01:54,  1.59it/s]Extractor Predicting: 183it [01:55,  1.59it/s]Extractor Predicting: 184it [01:55,  1.63it/s]Extractor Predicting: 185it [01:56,  1.64it/s]Extractor Predicting: 186it [01:56,  1.65it/s]Extractor Predicting: 187it [01:57,  1.65it/s]Extractor Predicting: 188it [01:58,  1.64it/s]Extractor Predicting: 189it [01:58,  1.64it/s]Extractor Predicting: 190it [01:59,  1.61it/s]Extractor Predicting: 191it [02:00,  1.56it/s]Extractor Predicting: 192it [02:00,  1.58it/s]Extractor Predicting: 193it [02:01,  1.63it/s]Extractor Predicting: 194it [02:01,  1.62it/s]Extractor Predicting: 195it [02:02,  1.62it/s]Extractor Predicting: 196it [02:03,  1.64it/s]Extractor Predicting: 197it [02:03,  1.65it/s]Extractor Predicting: 198it [02:04,  1.62it/s]Extractor Predicting: 199it [02:04,  1.63it/s]Extractor Predicting: 200it [02:05,  1.62it/s]Extractor Predicting: 201it [02:06,  1.63it/s]Extractor Predicting: 202it [02:06,  1.66it/s]Extractor Predicting: 203it [02:07,  1.67it/s]Extractor Predicting: 204it [02:08,  1.65it/s]Extractor Predicting: 205it [02:08,  1.60it/s]Extractor Predicting: 206it [02:09,  1.60it/s]Extractor Predicting: 207it [02:09,  1.63it/s]Extractor Predicting: 208it [02:10,  1.64it/s]Extractor Predicting: 209it [02:11,  1.58it/s]Extractor Predicting: 210it [02:11,  1.56it/s]Extractor Predicting: 211it [02:12,  1.57it/s]Extractor Predicting: 212it [02:13,  1.60it/s]Extractor Predicting: 213it [02:13,  1.61it/s]Extractor Predicting: 214it [02:14,  1.55it/s]Extractor Predicting: 215it [02:15,  1.55it/s]Extractor Predicting: 216it [02:15,  1.58it/s]Extractor Predicting: 217it [02:16,  1.60it/s]Extractor Predicting: 218it [02:16,  1.54it/s]Extractor Predicting: 219it [02:17,  1.56it/s]Extractor Predicting: 220it [02:18,  1.57it/s]Extractor Predicting: 221it [02:18,  1.54it/s]Extractor Predicting: 222it [02:19,  1.54it/s]Extractor Predicting: 223it [02:20,  1.55it/s]Extractor Predicting: 224it [02:20,  1.59it/s]Extractor Predicting: 225it [02:21,  1.59it/s]Extractor Predicting: 226it [02:21,  1.58it/s]Extractor Predicting: 227it [02:22,  1.63it/s]Extractor Predicting: 228it [02:23,  1.61it/s]Extractor Predicting: 229it [02:23,  1.62it/s]Extractor Predicting: 230it [02:24,  1.59it/s]Extractor Predicting: 231it [02:25,  1.59it/s]Extractor Predicting: 232it [02:25,  1.59it/s]Extractor Predicting: 233it [02:26,  1.63it/s]Extractor Predicting: 234it [02:26,  1.61it/s]Extractor Predicting: 235it [02:27,  1.62it/s]Extractor Predicting: 236it [02:28,  1.59it/s]Extractor Predicting: 237it [02:28,  1.59it/s]Extractor Predicting: 238it [02:29,  1.62it/s]Extractor Predicting: 239it [02:30,  1.63it/s]Extractor Predicting: 240it [02:30,  1.61it/s]Extractor Predicting: 241it [02:31,  1.61it/s]Extractor Predicting: 242it [02:31,  1.59it/s]Extractor Predicting: 243it [02:32,  1.54it/s]Extractor Predicting: 244it [02:33,  1.56it/s]Extractor Predicting: 245it [02:33,  1.62it/s]Extractor Predicting: 246it [02:34,  1.60it/s]Extractor Predicting: 247it [02:35,  1.62it/s]Extractor Predicting: 248it [02:35,  1.62it/s]Extractor Predicting: 249it [02:36,  1.62it/s]Extractor Predicting: 250it [02:36,  1.61it/s]Extractor Predicting: 251it [02:37,  1.57it/s]Extractor Predicting: 252it [02:38,  1.57it/s]Extractor Predicting: 253it [02:38,  1.58it/s]Extractor Predicting: 254it [02:39,  1.60it/s]Extractor Predicting: 255it [02:40,  1.60it/s]Extractor Predicting: 256it [02:40,  1.59it/s]Extractor Predicting: 257it [02:41,  1.61it/s]Extractor Predicting: 258it [02:41,  1.60it/s]Extractor Predicting: 259it [02:42,  1.55it/s]Extractor Predicting: 260it [02:43,  1.57it/s]Extractor Predicting: 261it [02:43,  1.59it/s]Extractor Predicting: 262it [02:44,  1.57it/s]Extractor Predicting: 263it [02:45,  1.56it/s]Extractor Predicting: 264it [02:45,  1.56it/s]Extractor Predicting: 265it [02:46,  1.55it/s]Extractor Predicting: 266it [02:47,  1.53it/s]Extractor Predicting: 267it [02:47,  1.51it/s]Extractor Predicting: 268it [02:48,  1.53it/s]Extractor Predicting: 269it [02:49,  1.53it/s]Extractor Predicting: 270it [02:50,  1.37it/s]Extractor Predicting: 271it [02:50,  1.42it/s]Extractor Predicting: 272it [02:51,  1.45it/s]Extractor Predicting: 273it [02:52,  1.46it/s]Extractor Predicting: 274it [02:52,  1.46it/s]Extractor Predicting: 275it [02:53,  1.52it/s]Extractor Predicting: 276it [02:53,  1.53it/s]Extractor Predicting: 277it [02:54,  1.50it/s]Extractor Predicting: 278it [02:55,  1.51it/s]Extractor Predicting: 279it [02:55,  1.52it/s]Extractor Predicting: 280it [02:56,  1.51it/s]Extractor Predicting: 281it [02:57,  1.51it/s]Extractor Predicting: 282it [02:57,  1.53it/s]Extractor Predicting: 283it [02:58,  1.48it/s]Extractor Predicting: 284it [02:59,  1.47it/s]Extractor Predicting: 285it [02:59,  1.47it/s]Extractor Predicting: 286it [03:00,  1.47it/s]Extractor Predicting: 287it [03:01,  1.49it/s]Extractor Predicting: 288it [03:01,  1.98it/s]Extractor Predicting: 288it [03:01,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:09,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:09,565 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:09,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:09,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:09,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:41:10,439 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:41:10,454 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:41:10,714 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:41:11,867 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:41:11,867 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:13,267 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:13,270 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:13,270 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:13,270 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:13,270 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:41:13,656 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:41:13,657 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:41:13,921 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:41:14,069 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:41:14,069 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.48847736625514404,
  "recall": 0.17230367252141093,
  "score": 0.25474836355832176,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:01,  1.86it/s]Extractor Predicting: 3it [00:01,  1.71it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:41:16,316 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:41:16,317 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:41:16,325 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:41:16,326 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:41:16,329 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:41:22,995 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:41:23,085 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:41:23,155 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:41:23,156 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:41:23,163 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:23,170 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:23,170 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:23,170 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:23,170 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:23,170 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:23,170 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6428571428571429,
  "recall": 0.08108108108108109,
  "score": 0.14400000000000002,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:41:23,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:23,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:24,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:25,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:25,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:26,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:26,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:27,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:27,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:28,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:28,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:29,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:30,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:30,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:31,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:31,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:32,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:32,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:33,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:33,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:10<02:33, 11.00s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:34,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:34,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:35,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:36,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:36,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:37,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:37,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:38,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:38,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:39,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:39,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:40,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:40,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:41,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:41,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:42,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:43,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:43,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:44,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:44,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:21<02:21, 10.92s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:45,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:45,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:46,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:46,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:47,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:48,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:48,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:49,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:49,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:50,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:51,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:51,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:52,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:52,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:53,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:54,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:54,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:55,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:55,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:56,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:56,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:34<02:18, 11.53s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:57,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:58,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:58,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:00,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:00,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:01,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:01,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:02,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:03,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:03,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:05,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:05,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:06,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:06,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:07,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:07,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:08,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:45<02:07, 11.55s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:09,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:09,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:10,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:10,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:11,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:11,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:12,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:12,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:13,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:13,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:15,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:15,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:16,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:16,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:17,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:17,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:18,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:18,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [00:56<01:52, 11.27s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:19,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:20,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:21,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:23,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:24,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:25,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:25,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:26,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:29,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:30,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:30,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:32,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:34,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:11<01:53, 12.59s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:35,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:35,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:36,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:36,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:38,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:39,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:39,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:40,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:40,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:41,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:41,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:42,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:42,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:43,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:44,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:44,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:45,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:45,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:46,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:47,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:24<01:40, 12.60s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:47,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:48,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:48,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:49,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:50,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:50,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:51,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:51,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:52,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:53,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:53,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:54,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:54,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:55,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:56,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:57,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:57,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:58,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:58,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:59,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:59,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:00,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:00,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:01,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:38<01:32, 13.24s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:02,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:02,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:03,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:03,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:04,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:04,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:05,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:05,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:06,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:07,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:07,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:08,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:08,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:09,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:09,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:10,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:10,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:11,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:11,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:12,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:13,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:50<01:15, 12.63s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:13,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:14,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:14,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:15,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:15,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:16,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:17,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:17,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:18,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:18,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:19,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:19,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:20,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:21,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:21,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:22,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:22,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:23,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:24,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:24,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:01<01:01, 12.33s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:25,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:25,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:26,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:26,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:27,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:27,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:28,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:28,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:29,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:29,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:30,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:30,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:31,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:32,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:32,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:33,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:33,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:34,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:34,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:35,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:35,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:12<00:47, 11.96s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:36,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:36,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:37,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:37,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:38,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:39,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:39,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:40,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:40,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:41,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:41,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:42,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:42,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:43,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:44,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:44,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:45,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:45,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:46,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:46,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:47,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:47,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:25<00:36, 12.01s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:48,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:49,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:49,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:50,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:50,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:51,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:51,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:52,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:52,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:53,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:54,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:54,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:55,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:55,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:56,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:57,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:57,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:58,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:58,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:59,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:59,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:00,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:37<00:24, 12.22s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:01,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:01,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:02,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:02,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:03,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:04,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:04,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:05,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:05,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:06,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:06,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:07,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:07,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:08,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:09,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:09,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:10,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:10,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:11,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:11,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:48<00:11, 11.88s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:12,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:12,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:13,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:13,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:14,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:14,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:15,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:15,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:16,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:16,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:17,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:17,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:18,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:18,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:19,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:20,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:20,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:21,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:21,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:22,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:22,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:23,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:23,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:24,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:24,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:01<00:00, 12.22s/it]Generating: 100%|██████████| 15/15 [03:01<00:00, 12.12s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:33,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:33,382 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:33,382 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:33,382 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:33,382 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:44:34,051 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:44:34,052 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:44:34,853 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:44:35,957 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:44:35,957 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:40,206 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:40,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:40,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:40,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:40,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:44:40,952 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:44:40,953 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:44:41,789 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:44:41,980 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:44:41,980 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : place served by transport hub . Context : The city 's most important railway station , the station is at Düsseldorf station , between the two stations . Head Entity : Düsseldorf , Tail Entity : Ruhr .\n"]
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : architect .', 'success_rate': 0.9330357142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8849431818181818, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8059895833333334, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.9241071428571429, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9330357142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('River Câzango', 'located in or next to body of water', '', 'The town has tributaries : the River Câzango .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8664772727272727, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 536, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 584, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : position held .', 'success_rate': 0.755, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 8607
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8707, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.62it/s]Extractor Estimating: 2it [00:01,  1.69it/s]Extractor Estimating: 3it [00:01,  1.66it/s]Extractor Estimating: 4it [00:02,  1.70it/s]Extractor Estimating: 5it [00:02,  1.73it/s]Extractor Estimating: 6it [00:03,  1.70it/s]Extractor Estimating: 7it [00:04,  1.73it/s]Extractor Estimating: 8it [00:04,  1.68it/s]Extractor Estimating: 9it [00:05,  1.71it/s]Extractor Estimating: 10it [00:05,  1.65it/s]Extractor Estimating: 11it [00:06,  1.71it/s]Extractor Estimating: 12it [00:07,  1.72it/s]Extractor Estimating: 13it [00:07,  1.70it/s]Extractor Estimating: 14it [00:08,  1.74it/s]Extractor Estimating: 15it [00:08,  1.67it/s]Extractor Estimating: 16it [00:09,  1.69it/s]Extractor Estimating: 17it [00:09,  1.75it/s]Extractor Estimating: 18it [00:10,  1.78it/s]Extractor Estimating: 19it [00:10,  1.86it/s]Extractor Estimating: 20it [00:11,  1.83it/s]Extractor Estimating: 21it [00:12,  1.86it/s]Extractor Estimating: 22it [00:12,  1.76it/s]Extractor Estimating: 23it [00:13,  1.70it/s]Extractor Estimating: 24it [00:13,  1.74it/s]Extractor Estimating: 25it [00:14,  1.75it/s]Extractor Estimating: 26it [00:15,  1.74it/s]Extractor Estimating: 27it [00:15,  1.80it/s]Extractor Estimating: 28it [00:16,  1.86it/s]Extractor Estimating: 29it [00:16,  1.70it/s]Extractor Estimating: 30it [00:17,  1.74it/s]Extractor Estimating: 31it [00:17,  1.82it/s]Extractor Estimating: 32it [00:18,  1.86it/s]Extractor Estimating: 33it [00:18,  1.86it/s]Extractor Estimating: 34it [00:19,  1.86it/s]Extractor Estimating: 35it [00:19,  1.89it/s]Extractor Estimating: 36it [00:20,  1.90it/s]Extractor Estimating: 37it [00:20,  1.92it/s]Extractor Estimating: 38it [00:21,  1.98it/s]Extractor Estimating: 39it [00:21,  1.94it/s]Extractor Estimating: 40it [00:22,  1.95it/s]Extractor Estimating: 41it [00:22,  1.97it/s]Extractor Estimating: 42it [00:23,  1.96it/s]Extractor Estimating: 43it [00:23,  2.02it/s]Extractor Estimating: 44it [00:24,  2.01it/s]Extractor Estimating: 45it [00:24,  1.97it/s]Extractor Estimating: 46it [00:25,  1.93it/s]Extractor Estimating: 47it [00:26,  1.85it/s]Extractor Estimating: 48it [00:26,  1.91it/s]Extractor Estimating: 49it [00:27,  1.91it/s]Extractor Estimating: 50it [00:27,  1.88it/s]Extractor Estimating: 51it [00:28,  1.78it/s]Extractor Estimating: 52it [00:28,  1.74it/s]Extractor Estimating: 53it [00:29,  1.69it/s]Extractor Estimating: 54it [00:30,  1.64it/s]Extractor Estimating: 55it [00:30,  1.65it/s]Extractor Estimating: 56it [00:31,  1.62it/s]Extractor Estimating: 57it [00:31,  1.63it/s]Extractor Estimating: 58it [00:32,  1.65it/s]Extractor Estimating: 59it [00:33,  1.67it/s]Extractor Estimating: 60it [00:33,  1.62it/s]Extractor Estimating: 61it [00:34,  1.58it/s]Extractor Estimating: 62it [00:35,  1.57it/s]Extractor Estimating: 63it [00:35,  1.53it/s]Extractor Estimating: 64it [00:37,  1.12it/s]Extractor Estimating: 65it [00:37,  1.23it/s]Extractor Estimating: 66it [00:38,  1.34it/s]Extractor Estimating: 67it [00:39,  1.41it/s]Extractor Estimating: 68it [00:39,  1.47it/s]Extractor Estimating: 69it [00:40,  1.49it/s]Extractor Estimating: 70it [00:40,  1.52it/s]Extractor Estimating: 71it [00:41,  1.57it/s]Extractor Estimating: 72it [00:42,  1.56it/s]Extractor Estimating: 73it [00:42,  1.53it/s]Extractor Estimating: 74it [00:43,  1.54it/s]Extractor Estimating: 75it [00:44,  1.54it/s]Extractor Estimating: 76it [00:44,  1.65it/s]Extractor Estimating: 77it [00:45,  1.70it/s]Extractor Estimating: 78it [00:45,  1.66it/s]Extractor Estimating: 79it [00:46,  1.72it/s]Extractor Estimating: 80it [00:46,  1.79it/s]Extractor Estimating: 81it [00:47,  1.84it/s]Extractor Estimating: 82it [00:47,  1.86it/s]Extractor Estimating: 83it [00:48,  1.92it/s]Extractor Estimating: 84it [00:48,  1.90it/s]Extractor Estimating: 85it [00:49,  1.87it/s]Extractor Estimating: 86it [00:50,  1.88it/s]Extractor Estimating: 87it [00:50,  1.88it/s]Extractor Estimating: 88it [00:51,  1.94it/s]Extractor Estimating: 89it [00:51,  1.97it/s]Extractor Estimating: 90it [00:52,  1.96it/s]Extractor Estimating: 91it [00:52,  1.99it/s]Extractor Estimating: 92it [00:53,  2.00it/s]Extractor Estimating: 93it [00:53,  1.96it/s]Extractor Estimating: 94it [00:54,  2.02it/s]Extractor Estimating: 95it [00:54,  2.08it/s]Extractor Estimating: 96it [00:54,  2.08it/s]Extractor Estimating: 97it [00:55,  2.11it/s]Extractor Estimating: 98it [00:55,  2.09it/s]Extractor Estimating: 99it [00:56,  1.98it/s]Extractor Estimating: 100it [00:56,  1.98it/s]Extractor Estimating: 101it [00:57,  2.00it/s]Extractor Estimating: 102it [00:57,  2.01it/s]Extractor Estimating: 103it [00:58,  1.97it/s]Extractor Estimating: 104it [00:59,  1.94it/s]Extractor Estimating: 105it [00:59,  1.95it/s]Extractor Estimating: 106it [01:00,  1.89it/s]Extractor Estimating: 107it [01:00,  1.95it/s]Extractor Estimating: 108it [01:01,  1.98it/s]Extractor Estimating: 109it [01:01,  2.04it/s]Extractor Estimating: 110it [01:02,  2.04it/s]Extractor Estimating: 111it [01:02,  2.07it/s]Extractor Estimating: 112it [01:02,  2.08it/s]Extractor Estimating: 113it [01:03,  2.09it/s]Extractor Estimating: 114it [01:03,  2.04it/s]Extractor Estimating: 115it [01:04,  2.03it/s]Extractor Estimating: 116it [01:05,  1.90it/s]Extractor Estimating: 117it [01:05,  1.89it/s]Extractor Estimating: 118it [01:06,  1.89it/s]Extractor Estimating: 119it [01:06,  1.90it/s]Extractor Estimating: 120it [01:07,  1.89it/s]Extractor Estimating: 121it [01:07,  1.88it/s]Extractor Estimating: 122it [01:08,  1.90it/s]Extractor Estimating: 123it [01:08,  1.90it/s]Extractor Estimating: 124it [01:09,  1.95it/s]Extractor Estimating: 125it [01:09,  1.97it/s]Extractor Estimating: 126it [01:10,  1.91it/s]Extractor Estimating: 127it [01:10,  1.94it/s]Extractor Estimating: 128it [01:11,  1.90it/s]Extractor Estimating: 129it [01:11,  1.90it/s]Extractor Estimating: 130it [01:12,  1.87it/s]Extractor Estimating: 131it [01:12,  1.92it/s]Extractor Estimating: 132it [01:13,  1.78it/s]Extractor Estimating: 133it [01:14,  1.81it/s]Extractor Estimating: 134it [01:14,  1.81it/s]Extractor Estimating: 135it [01:15,  1.81it/s]Extractor Estimating: 136it [01:15,  1.85it/s]Extractor Estimating: 137it [01:16,  1.82it/s]Extractor Estimating: 138it [01:16,  1.88it/s]Extractor Estimating: 139it [01:17,  1.87it/s]Extractor Estimating: 140it [01:17,  1.85it/s]Extractor Estimating: 141it [01:18,  1.89it/s]Extractor Estimating: 142it [01:18,  1.88it/s]Extractor Estimating: 143it [01:19,  1.95it/s]Extractor Estimating: 144it [01:19,  1.92it/s]Extractor Estimating: 145it [01:20,  1.91it/s]Extractor Estimating: 146it [01:21,  1.84it/s]Extractor Estimating: 147it [01:21,  1.86it/s]Extractor Estimating: 148it [01:22,  1.81it/s]Extractor Estimating: 149it [01:22,  1.80it/s]Extractor Estimating: 150it [01:23,  1.79it/s]Extractor Estimating: 151it [01:23,  1.83it/s]Extractor Estimating: 152it [01:24,  1.82it/s]Extractor Estimating: 153it [01:24,  1.87it/s]Extractor Estimating: 154it [01:25,  1.84it/s]Extractor Estimating: 155it [01:25,  1.86it/s]Extractor Estimating: 156it [01:26,  1.88it/s]Extractor Estimating: 157it [01:26,  1.94it/s]Extractor Estimating: 158it [01:27,  1.94it/s]Extractor Estimating: 159it [01:28,  1.90it/s]Extractor Estimating: 160it [01:28,  1.87it/s]Extractor Estimating: 161it [01:29,  1.88it/s]Extractor Estimating: 162it [01:29,  1.88it/s]Extractor Estimating: 163it [01:30,  1.93it/s]Extractor Estimating: 164it [01:30,  1.94it/s]Extractor Estimating: 165it [01:31,  1.87it/s]Extractor Estimating: 166it [01:31,  1.90it/s]Extractor Estimating: 167it [01:32,  1.91it/s]Extractor Estimating: 168it [01:32,  1.92it/s]Extractor Estimating: 169it [01:33,  1.90it/s]Extractor Estimating: 170it [01:33,  1.74it/s]Extractor Estimating: 171it [01:34,  1.79it/s]Extractor Estimating: 172it [01:35,  1.74it/s]Extractor Estimating: 173it [01:35,  1.72it/s]Extractor Estimating: 174it [01:36,  1.80it/s]Extractor Estimating: 175it [01:36,  1.81it/s]Extractor Estimating: 176it [01:37,  1.85it/s]Extractor Estimating: 177it [01:37,  1.82it/s]Extractor Estimating: 178it [01:38,  1.84it/s]Extractor Estimating: 179it [01:38,  1.80it/s]Extractor Estimating: 180it [01:39,  1.81it/s]Extractor Estimating: 181it [01:39,  1.84it/s]Extractor Estimating: 182it [01:40,  1.79it/s]Extractor Estimating: 183it [01:41,  1.77it/s]Extractor Estimating: 184it [01:41,  1.85it/s]Extractor Estimating: 185it [01:42,  1.90it/s]Extractor Estimating: 186it [01:42,  1.78it/s]Extractor Estimating: 187it [01:43,  1.78it/s]Extractor Estimating: 188it [01:43,  1.78it/s]Extractor Estimating: 189it [01:44,  1.79it/s]Extractor Estimating: 190it [01:45,  1.81it/s]Extractor Estimating: 191it [01:45,  1.80it/s]Extractor Estimating: 192it [01:46,  1.78it/s]Extractor Estimating: 193it [01:46,  1.79it/s]Extractor Estimating: 194it [01:47,  1.80it/s]Extractor Estimating: 195it [01:47,  1.80it/s]Extractor Estimating: 196it [01:48,  1.80it/s]Extractor Estimating: 197it [01:48,  1.79it/s]Extractor Estimating: 198it [01:49,  1.79it/s]Extractor Estimating: 199it [01:50,  1.81it/s]Extractor Estimating: 200it [01:50,  1.82it/s]Extractor Estimating: 201it [01:51,  1.79it/s]Extractor Estimating: 202it [01:51,  1.77it/s]Extractor Estimating: 203it [01:52,  1.67it/s]Extractor Estimating: 204it [01:52,  1.70it/s]Extractor Estimating: 205it [01:53,  1.71it/s]Extractor Estimating: 206it [01:54,  1.68it/s]Extractor Estimating: 207it [01:54,  1.64it/s]Extractor Estimating: 208it [01:55,  1.60it/s]Extractor Estimating: 209it [01:56,  1.56it/s]Extractor Estimating: 210it [01:56,  1.56it/s]Extractor Estimating: 211it [01:57,  1.59it/s]Extractor Estimating: 212it [01:58,  1.59it/s]Extractor Estimating: 213it [01:58,  1.62it/s]Extractor Estimating: 214it [01:59,  1.69it/s]Extractor Estimating: 215it [01:59,  1.71it/s]Extractor Estimating: 216it [02:00,  1.74it/s]Extractor Estimating: 217it [02:00,  1.72it/s]Extractor Estimating: 218it [02:01,  1.69it/s]Extractor Estimating: 219it [02:02,  1.66it/s]Extractor Estimating: 220it [02:02,  1.62it/s]Extractor Estimating: 221it [02:03,  1.59it/s]Extractor Estimating: 222it [02:04,  1.58it/s]Extractor Estimating: 223it [02:04,  1.60it/s]Extractor Estimating: 224it [02:05,  1.63it/s]Extractor Estimating: 225it [02:05,  1.65it/s]Extractor Estimating: 226it [02:06,  1.58it/s]Extractor Estimating: 227it [02:07,  1.58it/s]Extractor Estimating: 228it [02:07,  1.53it/s]Extractor Estimating: 229it [02:08,  1.55it/s]Extractor Estimating: 230it [02:09,  1.56it/s]Extractor Estimating: 231it [02:09,  1.57it/s]Extractor Estimating: 232it [02:10,  1.60it/s]Extractor Estimating: 233it [02:10,  1.58it/s]Extractor Estimating: 234it [02:11,  1.56it/s]Extractor Estimating: 235it [02:12,  1.56it/s]Extractor Estimating: 236it [02:12,  1.53it/s]Extractor Estimating: 237it [02:13,  1.51it/s]Extractor Estimating: 238it [02:14,  1.59it/s]Extractor Estimating: 239it [02:14,  1.63it/s]Extractor Estimating: 240it [02:15,  1.60it/s]Extractor Estimating: 241it [02:16,  1.61it/s]Extractor Estimating: 242it [02:16,  1.66it/s]Extractor Estimating: 243it [02:17,  1.62it/s]Extractor Estimating: 244it [02:17,  1.61it/s]Extractor Estimating: 245it [02:18,  1.57it/s]Extractor Estimating: 246it [02:19,  1.61it/s]Extractor Estimating: 247it [02:19,  1.49it/s]Extractor Estimating: 248it [02:20,  1.54it/s]Extractor Estimating: 249it [02:21,  1.58it/s]Extractor Estimating: 250it [02:21,  1.53it/s]Extractor Estimating: 251it [02:22,  1.74it/s]Extractor Estimating: 252it [02:22,  1.87it/s]Extractor Estimating: 253it [02:23,  1.94it/s]Extractor Estimating: 254it [02:23,  2.08it/s]Extractor Estimating: 255it [02:23,  2.17it/s]Extractor Estimating: 256it [02:24,  2.26it/s]Extractor Estimating: 257it [02:24,  2.26it/s]Extractor Estimating: 258it [02:25,  2.26it/s]Extractor Estimating: 259it [02:25,  2.24it/s]Extractor Estimating: 260it [02:26,  1.96it/s]Extractor Estimating: 261it [02:26,  1.84it/s]Extractor Estimating: 262it [02:27,  1.88it/s]Extractor Estimating: 263it [02:27,  1.93it/s]Extractor Estimating: 264it [02:28,  2.06it/s]Extractor Estimating: 265it [02:28,  1.98it/s]Extractor Estimating: 266it [02:29,  2.04it/s]Extractor Estimating: 267it [02:29,  2.03it/s]Extractor Estimating: 268it [02:30,  2.10it/s]Extractor Estimating: 269it [02:30,  2.18it/s]Extractor Estimating: 270it [02:31,  2.23it/s]Extractor Estimating: 271it [02:31,  2.21it/s]Extractor Estimating: 272it [02:32,  2.25it/s]Extractor Estimating: 273it [02:32,  2.26it/s]Extractor Estimating: 274it [02:32,  2.29it/s]Extractor Estimating: 275it [02:33,  2.28it/s]Extractor Estimating: 276it [02:33,  2.14it/s]Extractor Estimating: 277it [02:34,  1.99it/s]Extractor Estimating: 278it [02:35,  1.92it/s]Extractor Estimating: 279it [02:35,  1.89it/s]Extractor Estimating: 280it [02:36,  1.78it/s]Extractor Estimating: 281it [02:36,  1.75it/s]Extractor Estimating: 282it [02:37,  1.76it/s]Extractor Estimating: 283it [02:37,  1.81it/s]Extractor Estimating: 284it [02:38,  1.82it/s]Extractor Estimating: 285it [02:38,  1.81it/s]Extractor Estimating: 286it [02:39,  1.85it/s]Extractor Estimating: 287it [02:40,  1.78it/s]Extractor Estimating: 288it [02:40,  1.75it/s]Extractor Estimating: 289it [02:41,  1.75it/s]Extractor Estimating: 290it [02:41,  1.73it/s]Extractor Estimating: 291it [02:42,  1.70it/s]Extractor Estimating: 292it [02:43,  1.73it/s]Extractor Estimating: 293it [02:43,  1.71it/s]Extractor Estimating: 294it [02:44,  1.69it/s]Extractor Estimating: 295it [02:44,  1.73it/s]Extractor Estimating: 296it [02:45,  1.77it/s]Extractor Estimating: 297it [02:45,  1.75it/s]Extractor Estimating: 298it [02:46,  1.77it/s]Extractor Estimating: 299it [02:47,  1.75it/s]Extractor Estimating: 300it [02:47,  1.81it/s]Extractor Estimating: 301it [02:48,  1.81it/s]Extractor Estimating: 302it [02:48,  1.73it/s]Extractor Estimating: 303it [02:49,  1.76it/s]Extractor Estimating: 304it [02:49,  1.79it/s]Extractor Estimating: 305it [02:50,  1.80it/s]Extractor Estimating: 306it [02:50,  1.86it/s]Extractor Estimating: 307it [02:51,  1.83it/s]Extractor Estimating: 308it [02:51,  1.87it/s]Extractor Estimating: 309it [02:52,  1.85it/s]Extractor Estimating: 310it [02:53,  1.83it/s]Extractor Estimating: 311it [02:53,  1.85it/s]Extractor Estimating: 312it [02:54,  1.81it/s]Extractor Estimating: 313it [02:54,  1.82it/s]Extractor Estimating: 314it [02:55,  1.82it/s]Extractor Estimating: 315it [02:55,  1.84it/s]Extractor Estimating: 316it [02:56,  1.92it/s]Extractor Estimating: 317it [02:56,  1.94it/s]Extractor Estimating: 318it [02:57,  1.95it/s]Extractor Estimating: 319it [02:57,  1.89it/s]Extractor Estimating: 320it [02:58,  1.83it/s]Extractor Estimating: 321it [02:58,  1.82it/s]Extractor Estimating: 322it [02:59,  1.84it/s]Extractor Estimating: 323it [03:00,  1.79it/s]Extractor Estimating: 324it [03:00,  1.77it/s]Extractor Estimating: 325it [03:01,  1.77it/s]Extractor Estimating: 326it [03:01,  1.71it/s]Extractor Estimating: 327it [03:02,  1.70it/s]Extractor Estimating: 328it [03:02,  1.75it/s]Extractor Estimating: 329it [03:03,  1.66it/s]Extractor Estimating: 330it [03:04,  1.68it/s]Extractor Estimating: 331it [03:04,  1.73it/s]Extractor Estimating: 332it [03:05,  1.77it/s]Extractor Estimating: 333it [03:05,  1.78it/s]Extractor Estimating: 334it [03:06,  1.84it/s]Extractor Estimating: 335it [03:06,  1.83it/s]Extractor Estimating: 336it [03:07,  1.69it/s]Extractor Estimating: 337it [03:08,  1.76it/s]Extractor Estimating: 338it [03:08,  1.70it/s]Extractor Estimating: 339it [03:09,  1.71it/s]Extractor Estimating: 340it [03:09,  1.77it/s]Extractor Estimating: 341it [03:10,  1.74it/s]Extractor Estimating: 342it [03:11,  1.75it/s]Extractor Estimating: 343it [03:11,  1.76it/s]Extractor Estimating: 344it [03:12,  1.82it/s]Extractor Estimating: 345it [03:12,  1.83it/s]Extractor Estimating: 346it [03:13,  1.85it/s]Extractor Estimating: 347it [03:13,  1.87it/s]Extractor Estimating: 348it [03:14,  1.84it/s]Extractor Estimating: 349it [03:14,  1.84it/s]Extractor Estimating: 350it [03:15,  1.83it/s]Extractor Estimating: 351it [03:15,  1.85it/s]Extractor Estimating: 352it [03:16,  1.84it/s]Extractor Estimating: 353it [03:16,  1.85it/s]Extractor Estimating: 354it [03:17,  1.84it/s]Extractor Estimating: 355it [03:18,  1.82it/s]Extractor Estimating: 356it [03:18,  1.79it/s]Extractor Estimating: 357it [03:19,  1.80it/s]Extractor Estimating: 358it [03:19,  1.79it/s]Extractor Estimating: 359it [03:20,  1.87it/s]Extractor Estimating: 360it [03:20,  1.85it/s]Extractor Estimating: 361it [03:21,  1.85it/s]Extractor Estimating: 362it [03:21,  1.86it/s]Extractor Estimating: 363it [03:22,  1.86it/s]Extractor Estimating: 364it [03:23,  1.76it/s]Extractor Estimating: 365it [03:23,  1.85it/s]Extractor Estimating: 366it [03:24,  1.64it/s]Extractor Estimating: 367it [03:24,  1.67it/s]Extractor Estimating: 368it [03:25,  1.71it/s]Extractor Estimating: 369it [03:25,  1.77it/s]Extractor Estimating: 370it [03:26,  1.72it/s]Extractor Estimating: 371it [03:27,  1.80it/s]Extractor Estimating: 372it [03:27,  1.84it/s]Extractor Estimating: 373it [03:28,  1.80it/s]Extractor Estimating: 374it [03:28,  1.79it/s]Extractor Estimating: 375it [03:29,  1.73it/s]Extractor Estimating: 375it [03:29,  1.79it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:22,729 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:22,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:22,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:22,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:22,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:48:23,204 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:48:23,205 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:48:23,466 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:48:24,556 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:48:24,556 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:26,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:26,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:26,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:26,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:26,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:48:26,722 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:48:26,723 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:48:26,998 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:48:27,159 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:48:27,159 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:49:22,181 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:49:22,211 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7500 mean pseudo reward: 0.9398143009776831
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 16125
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16225, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16225, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.929, loss:530.0363
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.932, loss:463.6016
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.935, loss:476.3793
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.941, loss:447.8071
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.920, loss:445.8315
>> valid entity prec:0.5127, rec:0.5411, f1:0.5265
>> valid relation prec:0.1674, rec:0.0621, f1:0.0906
>> valid relation with NER prec:0.1674, rec:0.0621, f1:0.0906
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.136, loss:451.4860
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.925, loss:419.8312
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.923, loss:454.8225
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.936, loss:422.4835
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.931, loss:414.3015
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5439, rec:0.4694, f1:0.5039
>> valid relation prec:0.1152, rec:0.0444, f1:0.0641
>> valid relation with NER prec:0.1152, rec:0.0444, f1:0.0641
g_step 1100, step 161, avg_time 2.129, loss:432.9291
g_step 1200, step 261, avg_time 0.920, loss:440.6244
g_step 1300, step 48, avg_time 0.922, loss:404.8261
g_step 1400, step 148, avg_time 0.914, loss:405.1600
g_step 1500, step 248, avg_time 0.940, loss:413.6808
>> valid entity prec:0.5233, rec:0.4796, f1:0.5005
>> valid relation prec:0.1256, rec:0.0524, f1:0.0739
>> valid relation with NER prec:0.1256, rec:0.0524, f1:0.0739
g_step 1600, step 35, avg_time 2.145, loss:393.4833
g_step 1700, step 135, avg_time 0.923, loss:374.9063
g_step 1800, step 235, avg_time 0.931, loss:387.6941
g_step 1900, step 22, avg_time 0.926, loss:371.4136
g_step 2000, step 122, avg_time 0.906, loss:363.2125
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5104, rec:0.4788, f1:0.4941
>> valid relation prec:0.1218, rec:0.0553, f1:0.0760
>> valid relation with NER prec:0.1218, rec:0.0553, f1:0.0760
g_step 2100, step 222, avg_time 2.140, loss:364.9446
g_step 2200, step 9, avg_time 0.931, loss:360.1831
g_step 2300, step 109, avg_time 0.926, loss:349.9764
g_step 2400, step 209, avg_time 0.919, loss:360.2190
g_step 2500, step 309, avg_time 0.956, loss:347.5935
>> valid entity prec:0.5199, rec:0.4515, f1:0.4833
>> valid relation prec:0.1109, rec:0.0481, f1:0.0671
>> valid relation with NER prec:0.1109, rec:0.0481, f1:0.0671
g_step 2600, step 96, avg_time 2.127, loss:321.0324
g_step 2700, step 196, avg_time 0.929, loss:341.1825
g_step 2800, step 296, avg_time 0.933, loss:331.3454
g_step 2900, step 83, avg_time 0.929, loss:327.4197
g_step 3000, step 183, avg_time 0.935, loss:317.5284
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5199, rec:0.4827, f1:0.5006
>> valid relation prec:0.1366, rec:0.0598, f1:0.0832
>> valid relation with NER prec:0.1366, rec:0.0598, f1:0.0832
g_step 3100, step 283, avg_time 2.126, loss:324.6828
g_step 3200, step 70, avg_time 0.921, loss:296.3334
g_step 3300, step 170, avg_time 0.924, loss:309.8647
g_step 3400, step 270, avg_time 0.942, loss:321.2274
g_step 3500, step 57, avg_time 0.911, loss:309.8395
>> valid entity prec:0.5173, rec:0.4644, f1:0.4895
>> valid relation prec:0.1481, rec:0.0550, f1:0.0802
>> valid relation with NER prec:0.1481, rec:0.0550, f1:0.0802
g_step 3600, step 157, avg_time 2.129, loss:292.4495
g_step 3700, step 257, avg_time 0.933, loss:309.0199
g_step 3800, step 44, avg_time 0.920, loss:284.0177
g_step 3900, step 144, avg_time 0.935, loss:276.6214
g_step 4000, step 244, avg_time 0.928, loss:294.8307
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5038, rec:0.4733, f1:0.4880
>> valid relation prec:0.1279, rec:0.0610, f1:0.0826
>> valid relation with NER prec:0.1279, rec:0.0610, f1:0.0826
g_step 4100, step 31, avg_time 2.130, loss:291.6227
g_step 4200, step 131, avg_time 0.938, loss:270.0398
g_step 4300, step 231, avg_time 0.931, loss:290.6333
g_step 4400, step 18, avg_time 0.925, loss:278.0678
g_step 4500, step 118, avg_time 0.931, loss:260.3735
>> valid entity prec:0.5049, rec:0.4656, f1:0.4845
>> valid relation prec:0.1387, rec:0.0587, f1:0.0825
>> valid relation with NER prec:0.1387, rec:0.0587, f1:0.0825
g_step 4600, step 218, avg_time 2.120, loss:272.5851
g_step 4700, step 5, avg_time 0.931, loss:268.0865
g_step 4800, step 105, avg_time 0.936, loss:257.3432
g_step 4900, step 205, avg_time 0.919, loss:258.3970
g_step 5000, step 305, avg_time 0.934, loss:280.6314
learning rate was adjusted to 0.0008
>> valid entity prec:0.4782, rec:0.4418, f1:0.4593
>> valid relation prec:0.1224, rec:0.0587, f1:0.0793
>> valid relation with NER prec:0.1224, rec:0.0587, f1:0.0793
g_step 5100, step 92, avg_time 2.126, loss:256.6297
g_step 5200, step 192, avg_time 0.930, loss:247.5308
g_step 5300, step 292, avg_time 0.917, loss:255.2496
g_step 5400, step 79, avg_time 0.923, loss:243.1089
g_step 5500, step 179, avg_time 0.935, loss:246.4927
>> valid entity prec:0.4985, rec:0.4982, f1:0.4983
>> valid relation prec:0.1367, rec:0.0676, f1:0.0904
>> valid relation with NER prec:0.1367, rec:0.0676, f1:0.0904
g_step 5600, step 279, avg_time 2.134, loss:247.2863
g_step 5700, step 66, avg_time 0.925, loss:235.9266
g_step 5800, step 166, avg_time 0.939, loss:224.0735
g_step 5900, step 266, avg_time 0.921, loss:227.0385
g_step 6000, step 53, avg_time 0.923, loss:216.9927
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5249, rec:0.4694, f1:0.4956
>> valid relation prec:0.1279, rec:0.0684, f1:0.0892
>> valid relation with NER prec:0.1279, rec:0.0684, f1:0.0892
g_step 6100, step 153, avg_time 2.117, loss:233.1907
g_step 6200, step 253, avg_time 0.917, loss:218.5768
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:49:22 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:49:22 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-49-22_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:49:23 - WARNING - datasets.builder -   Using custom data configuration default-e3531fbf62320874
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e3531fbf62320874/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:49:23,439 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:49:23,440 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:49:23,441 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:49:23,441 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:49:23,458 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:49:23,461 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:49:23,461 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:49:23,462 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:49:23,462 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:49:23,462 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:49:23,462 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:49:23,630 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:49:26,724 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:49:26,724 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e3531fbf62320874/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.28ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.15ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.58ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.79ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.92ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  5.01ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  5.06ba/s]100%|██████████| 8/8 [00:01<00:00,  5.11ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.02ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.25ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.72ba/s]100%|██████████| 4/4 [00:00<00:00,  4.73ba/s]100%|██████████| 4/4 [00:00<00:00,  4.24ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.14ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.82ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 11.14ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 11.35ba/s]100%|██████████| 8/8 [00:00<00:00, 11.90ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.32ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.75ba/s]100%|██████████| 4/4 [00:00<00:00, 12.23ba/s]
[INFO|trainer.py:414] 2023-08-28 23:49:30,638 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:49:30,655 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:49:30,655 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 23:49:30,655 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:49:30,655 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:49:30,655 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:49:30,655 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:49:30,655 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.33it/s]  0%|          | 2/585 [00:00<02:51,  3.39it/s]  1%|          | 3/585 [00:00<02:50,  3.42it/s]  1%|          | 4/585 [00:01<02:49,  3.43it/s]  1%|          | 5/585 [00:01<02:48,  3.44it/s]  1%|          | 6/585 [00:01<02:48,  3.44it/s]  1%|          | 7/585 [00:02<02:47,  3.44it/s]  1%|▏         | 8/585 [00:02<02:47,  3.44it/s]  2%|▏         | 9/585 [00:02<02:47,  3.44it/s]  2%|▏         | 10/585 [00:02<02:47,  3.44it/s]  2%|▏         | 11/585 [00:03<02:46,  3.44it/s]  2%|▏         | 12/585 [00:03<02:46,  3.44it/s]  2%|▏         | 13/585 [00:03<02:45,  3.45it/s]  2%|▏         | 14/585 [00:04<02:46,  3.42it/s]  3%|▎         | 15/585 [00:04<02:46,  3.43it/s]  3%|▎         | 16/585 [00:04<02:45,  3.44it/s]  3%|▎         | 17/585 [00:04<02:45,  3.44it/s]  3%|▎         | 18/585 [00:05<02:44,  3.44it/s]  3%|▎         | 19/585 [00:05<02:44,  3.44it/s]  3%|▎         | 20/585 [00:05<02:44,  3.44it/s]  4%|▎         | 21/585 [00:06<02:43,  3.44it/s]  4%|▍         | 22/585 [00:06<02:43,  3.44it/s]  4%|▍         | 23/585 [00:06<02:43,  3.44it/s]  4%|▍         | 24/585 [00:06<02:42,  3.44it/s]  4%|▍         | 25/585 [00:07<02:42,  3.44it/s]  4%|▍         | 26/585 [00:07<02:42,  3.44it/s]  5%|▍         | 27/585 [00:07<02:41,  3.45it/s]  5%|▍         | 28/585 [00:08<02:41,  3.45it/s]  5%|▍         | 29/585 [00:08<02:41,  3.44it/s]  5%|▌         | 30/585 [00:08<02:41,  3.44it/s]  5%|▌         | 31/585 [00:09<02:42,  3.40it/s]  5%|▌         | 32/585 [00:09<02:41,  3.42it/s]  6%|▌         | 33/585 [00:09<02:41,  3.42it/s]  6%|▌         | 34/585 [00:09<02:40,  3.43it/s]  6%|▌         | 35/585 [00:10<02:40,  3.43it/s]  6%|▌         | 36/585 [00:10<02:39,  3.44it/s]  6%|▋         | 37/585 [00:10<02:39,  3.44it/s]  6%|▋         | 38/585 [00:11<02:39,  3.44it/s]  7%|▋         | 39/585 [00:11<02:38,  3.44it/s]  7%|▋         | 40/585 [00:11<02:38,  3.45it/s]  7%|▋         | 41/585 [00:11<02:38,  3.44it/s]  7%|▋         | 42/585 [00:12<02:37,  3.44it/s]  7%|▋         | 43/585 [00:12<02:37,  3.44it/s]  8%|▊         | 44/585 [00:12<02:37,  3.44it/s]  8%|▊         | 45/585 [00:13<02:36,  3.44it/s]  8%|▊         | 46/585 [00:13<02:36,  3.44it/s]  8%|▊         | 47/585 [00:13<02:36,  3.44it/s]  8%|▊         | 48/585 [00:13<02:36,  3.44it/s]  8%|▊         | 49/585 [00:14<02:36,  3.43it/s]  9%|▊         | 50/585 [00:14<02:35,  3.43it/s]  9%|▊         | 51/585 [00:14<02:35,  3.44it/s]  9%|▉         | 52/585 [00:15<02:34,  3.44it/s]  9%|▉         | 53/585 [00:15<02:34,  3.44it/s]  9%|▉         | 54/585 [00:15<02:34,  3.44it/s]  9%|▉         | 55/585 [00:15<02:33,  3.44it/s] 10%|▉         | 56/585 [00:16<02:33,  3.44it/s] 10%|▉         | 57/585 [00:16<02:33,  3.44it/s] 10%|▉         | 58/585 [00:16<02:33,  3.44it/s] 10%|█         | 59/585 [00:17<02:33,  3.44it/s] 10%|█         | 60/585 [00:17<02:32,  3.44it/s] 10%|█         | 61/585 [00:17<02:32,  3.44it/s] 11%|█         | 62/585 [00:18<02:32,  3.44it/s] 11%|█         | 63/585 [00:18<02:31,  3.44it/s] 11%|█         | 64/585 [00:18<02:31,  3.44it/s] 11%|█         | 65/585 [00:18<02:31,  3.44it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.44it/s] 11%|█▏        | 67/585 [00:19<02:31,  3.43it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 69/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.43it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 72/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.44it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.44it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.44it/s] 14%|█▎        | 79/585 [00:22<02:27,  3.44it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 81/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 82/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.44it/s] 14%|█▍        | 84/585 [00:24<02:26,  3.42it/s] 15%|█▍        | 85/585 [00:24<02:26,  3.42it/s] 15%|█▍        | 86/585 [00:25<02:25,  3.43it/s] 15%|█▍        | 87/585 [00:25<02:25,  3.43it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.43it/s] 15%|█▌        | 89/585 [00:25<02:24,  3.44it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.44it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 92/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 93/585 [00:27<02:23,  3.44it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.44it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.44it/s] 16%|█▋        | 96/585 [00:27<02:22,  3.44it/s] 17%|█▋        | 97/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.43it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.43it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.44it/s] 17%|█▋        | 101/585 [00:29<02:20,  3.43it/s] 17%|█▋        | 102/585 [00:29<02:21,  3.42it/s] 18%|█▊        | 103/585 [00:29<02:20,  3.43it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 106/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.44it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.43it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.43it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.43it/s] 19%|█▉        | 113/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.44it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.44it/s] 20%|█▉        | 116/585 [00:33<02:16,  3.44it/s] 20%|██        | 117/585 [00:34<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 23:50:04,736 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:50:04,737 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:50:04,737 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.13it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.68it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.83it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.06it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.75it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.49it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.36it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.26it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.28it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.38it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.16it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.07it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.98it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.97it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.09it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.98it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.05it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.13it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.17it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.09it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.90it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.97it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.03it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.14it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.91it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.14it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.21it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.26it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.14it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.87it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.98it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.18it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.14it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.05it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.06it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.12it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.21it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.24it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.14it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.09it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.13it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.05it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.03it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.97it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.13it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.08it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.15it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.10it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.13it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.05it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.97it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.04it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.14it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.19it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.09it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.11it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.21it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.16it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.08it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.04it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.06it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.11it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.17it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.00it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.10it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.16it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.04it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.06it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.07it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.13it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.11it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.16it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.03it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.14it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.09it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.81it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.72it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.82it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.96it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.87it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.04it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.05it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.97it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.99it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.02it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.02it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.96it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.96it/s][A 20%|██        | 117/585 [00:44<02:16,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:50:14,677 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 23:50:14,694 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:50:16,872 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:50:16,901 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:50:16,920 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:51<41:21,  5.31s/it] 20%|██        | 119/585 [00:51<29:35,  3.81s/it] 21%|██        | 120/585 [00:51<21:24,  2.76s/it] 21%|██        | 121/585 [00:51<15:38,  2.02s/it] 21%|██        | 122/585 [00:52<11:36,  1.50s/it] 21%|██        | 123/585 [00:52<08:47,  1.14s/it] 21%|██        | 124/585 [00:52<06:49,  1.13it/s] 21%|██▏       | 125/585 [00:53<05:26,  1.41it/s] 22%|██▏       | 126/585 [00:53<04:28,  1.71it/s] 22%|██▏       | 127/585 [00:53<03:48,  2.01it/s] 22%|██▏       | 128/585 [00:54<03:19,  2.29it/s] 22%|██▏       | 129/585 [00:54<02:59,  2.54it/s] 22%|██▏       | 130/585 [00:54<02:45,  2.75it/s] 22%|██▏       | 131/585 [00:54<02:35,  2.92it/s] 23%|██▎       | 132/585 [00:55<02:27,  3.06it/s] 23%|██▎       | 133/585 [00:55<02:22,  3.17it/s] 23%|██▎       | 134/585 [00:55<02:18,  3.25it/s] 23%|██▎       | 135/585 [00:56<02:16,  3.30it/s] 23%|██▎       | 136/585 [00:56<02:14,  3.35it/s] 23%|██▎       | 137/585 [00:56<02:12,  3.37it/s] 24%|██▎       | 138/585 [00:56<02:11,  3.39it/s] 24%|██▍       | 139/585 [00:57<02:11,  3.40it/s] 24%|██▍       | 140/585 [00:57<02:10,  3.41it/s] 24%|██▍       | 141/585 [00:57<02:10,  3.41it/s] 24%|██▍       | 142/585 [00:58<02:09,  3.42it/s] 24%|██▍       | 143/585 [00:58<02:09,  3.42it/s] 25%|██▍       | 144/585 [00:58<02:08,  3.43it/s] 25%|██▍       | 145/585 [00:59<02:08,  3.43it/s] 25%|██▍       | 146/585 [00:59<02:07,  3.44it/s] 25%|██▌       | 147/585 [00:59<02:07,  3.44it/s] 25%|██▌       | 148/585 [00:59<02:07,  3.44it/s] 25%|██▌       | 149/585 [01:00<02:06,  3.44it/s] 26%|██▌       | 150/585 [01:00<02:06,  3.44it/s] 26%|██▌       | 151/585 [01:00<02:06,  3.44it/s] 26%|██▌       | 152/585 [01:01<02:06,  3.43it/s] 26%|██▌       | 153/585 [01:01<02:06,  3.43it/s] 26%|██▋       | 154/585 [01:01<02:05,  3.43it/s] 26%|██▋       | 155/585 [01:01<02:05,  3.44it/s] 27%|██▋       | 156/585 [01:02<02:04,  3.44it/s] 27%|██▋       | 157/585 [01:02<02:04,  3.44it/s] 27%|██▋       | 158/585 [01:02<02:04,  3.44it/s] 27%|██▋       | 159/585 [01:03<02:03,  3.44it/s] 27%|██▋       | 160/585 [01:03<02:03,  3.44it/s] 28%|██▊       | 161/585 [01:03<02:03,  3.44it/s] 28%|██▊       | 162/585 [01:03<02:03,  3.44it/s] 28%|██▊       | 163/585 [01:04<02:03,  3.43it/s] 28%|██▊       | 164/585 [01:04<02:02,  3.43it/s] 28%|██▊       | 165/585 [01:04<02:02,  3.43it/s] 28%|██▊       | 166/585 [01:05<02:02,  3.43it/s] 29%|██▊       | 167/585 [01:05<02:01,  3.43it/s] 29%|██▊       | 168/585 [01:05<02:01,  3.44it/s] 29%|██▉       | 169/585 [01:05<02:01,  3.43it/s] 29%|██▉       | 170/585 [01:06<02:00,  3.44it/s] 29%|██▉       | 171/585 [01:06<02:00,  3.44it/s] 29%|██▉       | 172/585 [01:06<01:59,  3.44it/s] 30%|██▉       | 173/585 [01:07<01:59,  3.44it/s] 30%|██▉       | 174/585 [01:07<02:00,  3.41it/s] 30%|██▉       | 175/585 [01:07<02:00,  3.42it/s] 30%|███       | 176/585 [01:08<01:59,  3.43it/s] 30%|███       | 177/585 [01:08<01:59,  3.43it/s] 30%|███       | 178/585 [01:08<01:58,  3.43it/s] 31%|███       | 179/585 [01:08<01:58,  3.43it/s] 31%|███       | 180/585 [01:09<01:57,  3.44it/s] 31%|███       | 181/585 [01:09<01:57,  3.43it/s] 31%|███       | 182/585 [01:09<01:57,  3.44it/s] 31%|███▏      | 183/585 [01:10<01:57,  3.43it/s] 31%|███▏      | 184/585 [01:10<01:56,  3.44it/s] 32%|███▏      | 185/585 [01:10<01:56,  3.42it/s] 32%|███▏      | 186/585 [01:10<01:56,  3.43it/s] 32%|███▏      | 187/585 [01:11<01:55,  3.43it/s] 32%|███▏      | 188/585 [01:11<01:55,  3.43it/s] 32%|███▏      | 189/585 [01:11<01:55,  3.44it/s] 32%|███▏      | 190/585 [01:12<01:54,  3.44it/s] 33%|███▎      | 191/585 [01:12<01:54,  3.44it/s] 33%|███▎      | 192/585 [01:12<01:54,  3.44it/s] 33%|███▎      | 193/585 [01:12<01:54,  3.44it/s] 33%|███▎      | 194/585 [01:13<01:53,  3.44it/s] 33%|███▎      | 195/585 [01:13<01:53,  3.44it/s] 34%|███▎      | 196/585 [01:13<01:53,  3.44it/s] 34%|███▎      | 197/585 [01:14<01:52,  3.44it/s] 34%|███▍      | 198/585 [01:14<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:14<01:52,  3.44it/s] 34%|███▍      | 200/585 [01:15<01:51,  3.44it/s] 34%|███▍      | 201/585 [01:15<01:51,  3.44it/s] 35%|███▍      | 202/585 [01:15<01:51,  3.43it/s] 35%|███▍      | 203/585 [01:15<01:51,  3.43it/s] 35%|███▍      | 204/585 [01:16<01:50,  3.43it/s] 35%|███▌      | 205/585 [01:16<01:50,  3.44it/s] 35%|███▌      | 206/585 [01:16<01:50,  3.43it/s] 35%|███▌      | 207/585 [01:17<01:49,  3.44it/s] 36%|███▌      | 208/585 [01:17<01:49,  3.44it/s] 36%|███▌      | 209/585 [01:17<01:49,  3.44it/s] 36%|███▌      | 210/585 [01:17<01:49,  3.44it/s] 36%|███▌      | 211/585 [01:18<01:48,  3.44it/s] 36%|███▌      | 212/585 [01:18<01:48,  3.44it/s] 36%|███▋      | 213/585 [01:18<01:48,  3.42it/s] 37%|███▋      | 214/585 [01:19<01:48,  3.43it/s] 37%|███▋      | 215/585 [01:19<01:47,  3.43it/s] 37%|███▋      | 216/585 [01:19<01:47,  3.44it/s] 37%|███▋      | 217/585 [01:19<01:46,  3.44it/s] 37%|███▋      | 218/585 [01:20<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:20<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:20<01:46,  3.44it/s] 38%|███▊      | 221/585 [01:21<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:21<01:45,  3.44it/s] 38%|███▊      | 223/585 [01:21<01:45,  3.44it/s] 38%|███▊      | 224/585 [01:22<01:45,  3.43it/s] 38%|███▊      | 225/585 [01:22<01:45,  3.43it/s] 39%|███▊      | 226/585 [01:22<01:44,  3.43it/s] 39%|███▉      | 227/585 [01:22<01:44,  3.43it/s] 39%|███▉      | 228/585 [01:23<01:43,  3.43it/s] 39%|███▉      | 229/585 [01:23<01:43,  3.44it/s] 39%|███▉      | 230/585 [01:23<01:43,  3.44it/s] 39%|███▉      | 231/585 [01:24<01:42,  3.44it/s] 40%|███▉      | 232/585 [01:24<01:42,  3.44it/s] 40%|███▉      | 233/585 [01:24<01:42,  3.44it/s] 40%|████      | 234/585 [01:24<01:42,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 23:50:55,608 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:50:55,608 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:50:55,608 >>   Batch size = 8
{'eval_loss': 1.0654573440551758, 'eval_runtime': 9.9305, 'eval_samples_per_second': 351.746, 'eval_steps_per_second': 44.006, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.77it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.34it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.84it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.94it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.65it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.42it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.33it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.21it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.29it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.44it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.25it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.10it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.05it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.91it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.02it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.96it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.96it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.14it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.27it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.17it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.11it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.05it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.81it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.04it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.83it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.00it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.18it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.19it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.24it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.06it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.03it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.86it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.03it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.10it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.07it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.08it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.25it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.23it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.10it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.06it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.03it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.86it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 44.00it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.03it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.14it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.32it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.14it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.14it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.98it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.99it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.83it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.99it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.02it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.13it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.30it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.24it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.15it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.98it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.97it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.95it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.07it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.08it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.06it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.23it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.16it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.96it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.92it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.95it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.02it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.10it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.10it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.25it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.21it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.11it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.85it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.83it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.92it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.05it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.16it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.15it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.21it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.17it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.03it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.91it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.89it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.97it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.95it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.95it/s][A 40%|████      | 234/585 [01:34<01:42,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:51:05,558 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 23:51:05,582 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:51:07,699 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:51:07,713 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:51:07,726 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:42<31:11,  5.35s/it] 40%|████      | 236/585 [01:42<22:18,  3.83s/it] 41%|████      | 237/585 [01:42<16:05,  2.77s/it] 41%|████      | 238/585 [01:42<11:44,  2.03s/it] 41%|████      | 239/585 [01:43<08:42,  1.51s/it] 41%|████      | 240/585 [01:43<06:35,  1.15s/it] 41%|████      | 241/585 [01:43<05:06,  1.12it/s] 41%|████▏     | 242/585 [01:44<04:04,  1.41it/s] 42%|████▏     | 243/585 [01:44<03:20,  1.71it/s] 42%|████▏     | 244/585 [01:44<02:50,  2.01it/s] 42%|████▏     | 245/585 [01:45<02:28,  2.28it/s] 42%|████▏     | 246/585 [01:45<02:13,  2.53it/s] 42%|████▏     | 247/585 [01:45<02:03,  2.73it/s] 42%|████▏     | 248/585 [01:45<01:56,  2.90it/s] 43%|████▎     | 249/585 [01:46<01:50,  3.03it/s] 43%|████▎     | 250/585 [01:46<01:47,  3.13it/s] 43%|████▎     | 251/585 [01:46<01:44,  3.20it/s] 43%|████▎     | 252/585 [01:47<01:42,  3.26it/s] 43%|████▎     | 253/585 [01:47<01:40,  3.30it/s] 43%|████▎     | 254/585 [01:47<01:39,  3.32it/s] 44%|████▎     | 255/585 [01:47<01:38,  3.35it/s] 44%|████▍     | 256/585 [01:48<01:37,  3.36it/s] 44%|████▍     | 257/585 [01:48<01:37,  3.37it/s] 44%|████▍     | 258/585 [01:48<01:37,  3.36it/s] 44%|████▍     | 259/585 [01:49<01:36,  3.37it/s] 44%|████▍     | 260/585 [01:49<01:36,  3.38it/s] 45%|████▍     | 261/585 [01:49<01:35,  3.38it/s] 45%|████▍     | 262/585 [01:50<01:35,  3.38it/s] 45%|████▍     | 263/585 [01:50<01:35,  3.38it/s] 45%|████▌     | 264/585 [01:50<01:34,  3.39it/s] 45%|████▌     | 265/585 [01:50<01:34,  3.38it/s] 45%|████▌     | 266/585 [01:51<01:34,  3.39it/s] 46%|████▌     | 267/585 [01:51<01:33,  3.39it/s] 46%|████▌     | 268/585 [01:51<01:36,  3.29it/s] 46%|████▌     | 269/585 [01:52<01:35,  3.31it/s] 46%|████▌     | 270/585 [01:52<01:34,  3.33it/s] 46%|████▋     | 271/585 [01:52<01:33,  3.35it/s] 46%|████▋     | 272/585 [01:53<01:33,  3.36it/s] 47%|████▋     | 273/585 [01:53<01:32,  3.37it/s] 47%|████▋     | 274/585 [01:53<01:32,  3.37it/s] 47%|████▋     | 275/585 [01:53<01:31,  3.38it/s] 47%|████▋     | 276/585 [01:54<01:31,  3.39it/s] 47%|████▋     | 277/585 [01:54<01:30,  3.39it/s] 48%|████▊     | 278/585 [01:54<01:30,  3.39it/s] 48%|████▊     | 279/585 [01:55<01:30,  3.39it/s] 48%|████▊     | 280/585 [01:55<01:30,  3.38it/s] 48%|████▊     | 281/585 [01:55<01:29,  3.39it/s] 48%|████▊     | 282/585 [01:55<01:29,  3.39it/s] 48%|████▊     | 283/585 [01:56<01:29,  3.39it/s] 49%|████▊     | 284/585 [01:56<01:28,  3.39it/s] 49%|████▊     | 285/585 [01:56<01:28,  3.39it/s] 49%|████▉     | 286/585 [01:57<01:28,  3.39it/s] 49%|████▉     | 287/585 [01:57<01:27,  3.39it/s] 49%|████▉     | 288/585 [01:57<01:27,  3.39it/s] 49%|████▉     | 289/585 [01:58<01:27,  3.39it/s] 50%|████▉     | 290/585 [01:58<01:27,  3.39it/s] 50%|████▉     | 291/585 [01:58<01:26,  3.38it/s] 50%|████▉     | 292/585 [01:58<01:26,  3.38it/s] 50%|█████     | 293/585 [01:59<01:26,  3.38it/s] 50%|█████     | 294/585 [01:59<01:26,  3.38it/s] 50%|█████     | 295/585 [01:59<01:25,  3.38it/s] 51%|█████     | 296/585 [02:00<01:25,  3.39it/s] 51%|█████     | 297/585 [02:00<01:24,  3.39it/s] 51%|█████     | 298/585 [02:00<01:24,  3.39it/s] 51%|█████     | 299/585 [02:00<01:24,  3.39it/s] 51%|█████▏    | 300/585 [02:01<01:24,  3.38it/s] 51%|█████▏    | 301/585 [02:01<01:23,  3.39it/s] 52%|█████▏    | 302/585 [02:01<01:23,  3.37it/s] 52%|█████▏    | 303/585 [02:02<01:23,  3.38it/s] 52%|█████▏    | 304/585 [02:02<01:23,  3.38it/s] 52%|█████▏    | 305/585 [02:02<01:22,  3.39it/s] 52%|█████▏    | 306/585 [02:03<01:22,  3.39it/s] 52%|█████▏    | 307/585 [02:03<01:22,  3.39it/s] 53%|█████▎    | 308/585 [02:03<01:21,  3.39it/s] 53%|█████▎    | 309/585 [02:03<01:21,  3.38it/s] 53%|█████▎    | 310/585 [02:04<01:21,  3.39it/s] 53%|█████▎    | 311/585 [02:04<01:20,  3.38it/s] 53%|█████▎    | 312/585 [02:04<01:20,  3.38it/s] 54%|█████▎    | 313/585 [02:05<01:20,  3.38it/s] 54%|█████▎    | 314/585 [02:05<01:20,  3.38it/s] 54%|█████▍    | 315/585 [02:05<01:19,  3.38it/s] 54%|█████▍    | 316/585 [02:06<01:19,  3.38it/s] 54%|█████▍    | 317/585 [02:06<01:19,  3.38it/s] 54%|█████▍    | 318/585 [02:06<01:18,  3.38it/s] 55%|█████▍    | 319/585 [02:06<01:18,  3.37it/s] 55%|█████▍    | 320/585 [02:07<01:18,  3.37it/s] 55%|█████▍    | 321/585 [02:07<01:18,  3.37it/s] 55%|█████▌    | 322/585 [02:07<01:17,  3.38it/s] 55%|█████▌    | 323/585 [02:08<01:17,  3.38it/s] 55%|█████▌    | 324/585 [02:08<01:17,  3.37it/s] 56%|█████▌    | 325/585 [02:08<01:17,  3.38it/s] 56%|█████▌    | 326/585 [02:08<01:16,  3.38it/s] 56%|█████▌    | 327/585 [02:09<01:16,  3.38it/s] 56%|█████▌    | 328/585 [02:09<01:16,  3.38it/s] 56%|█████▌    | 329/585 [02:09<01:15,  3.38it/s] 56%|█████▋    | 330/585 [02:10<01:15,  3.36it/s] 57%|█████▋    | 331/585 [02:10<01:15,  3.36it/s] 57%|█████▋    | 332/585 [02:10<01:15,  3.37it/s] 57%|█████▋    | 333/585 [02:11<01:14,  3.37it/s] 57%|█████▋    | 334/585 [02:11<01:14,  3.36it/s] 57%|█████▋    | 335/585 [02:11<01:14,  3.37it/s] 57%|█████▋    | 336/585 [02:11<01:13,  3.37it/s] 58%|█████▊    | 337/585 [02:12<01:13,  3.37it/s] 58%|█████▊    | 338/585 [02:12<01:13,  3.37it/s] 58%|█████▊    | 339/585 [02:12<01:12,  3.38it/s] 58%|█████▊    | 340/585 [02:13<01:12,  3.38it/s] 58%|█████▊    | 341/585 [02:13<01:12,  3.37it/s] 58%|█████▊    | 342/585 [02:13<01:12,  3.37it/s] 59%|█████▊    | 343/585 [02:14<01:11,  3.37it/s] 59%|█████▉    | 344/585 [02:14<01:11,  3.37it/s] 59%|█████▉    | 345/585 [02:14<01:11,  3.38it/s] 59%|█████▉    | 346/585 [02:14<01:10,  3.38it/s] 59%|█████▉    | 347/585 [02:15<01:10,  3.38it/s] 59%|█████▉    | 348/585 [02:15<01:10,  3.38it/s] 60%|█████▉    | 349/585 [02:15<01:09,  3.39it/s] 60%|█████▉    | 350/585 [02:16<01:09,  3.38it/s] 60%|██████    | 351/585 [02:16<01:09,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 23:51:47,096 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:51:47,097 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:51:47,097 >>   Batch size = 8
{'eval_loss': 1.0783333778381348, 'eval_runtime': 9.9324, 'eval_samples_per_second': 351.679, 'eval_steps_per_second': 43.998, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.92it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.75it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.23it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.09it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.77it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.47it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.36it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.18it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.28it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.44it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.39it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.30it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.09it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.96it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.03it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.06it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.05it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.10it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.24it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.20it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.18it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.18it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.13it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.00it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.00it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.06it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.20it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.15it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.23it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.08it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.11it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.04it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.95it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.03it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.07it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.11it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.16it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.23it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.28it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.21it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.11it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.93it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.03it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.02it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.20it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.23it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.32it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.26it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.19it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.15it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.02it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.87it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.11it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.17it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.23it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.20it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.10it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.20it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.12it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.00it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.87it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.03it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.15it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.25it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.20it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.19it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.18it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.02it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.98it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.00it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.98it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.15it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.23it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.05it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.96it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.79it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.69it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.65it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.71it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.79it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.88it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.01it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.05it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.10it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.11it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.98it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.96it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.96it/s][A 60%|██████    | 351/585 [02:26<01:09,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:51:57,049 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 23:51:57,097 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:51:59,345 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:51:59,371 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:51:59,378 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:33<20:16,  5.22s/it] 60%|██████    | 353/585 [02:33<14:28,  3.74s/it] 61%|██████    | 354/585 [02:33<10:26,  2.71s/it] 61%|██████    | 355/585 [02:33<07:36,  1.99s/it] 61%|██████    | 356/585 [02:34<05:38,  1.48s/it] 61%|██████    | 357/585 [02:34<04:16,  1.12s/it] 61%|██████    | 358/585 [02:34<03:18,  1.14it/s] 61%|██████▏   | 359/585 [02:35<02:38,  1.43it/s] 62%|██████▏   | 360/585 [02:35<02:10,  1.73it/s] 62%|██████▏   | 361/585 [02:35<01:50,  2.02it/s] 62%|██████▏   | 362/585 [02:36<01:36,  2.30it/s] 62%|██████▏   | 363/585 [02:36<01:27,  2.55it/s] 62%|██████▏   | 364/585 [02:36<01:20,  2.75it/s] 62%|██████▏   | 365/585 [02:36<01:15,  2.92it/s] 63%|██████▎   | 366/585 [02:37<01:11,  3.04it/s] 63%|██████▎   | 367/585 [02:37<01:09,  3.14it/s] 63%|██████▎   | 368/585 [02:37<01:07,  3.21it/s] 63%|██████▎   | 369/585 [02:38<01:06,  3.26it/s] 63%|██████▎   | 370/585 [02:38<01:05,  3.30it/s] 63%|██████▎   | 371/585 [02:38<01:04,  3.32it/s] 64%|██████▎   | 372/585 [02:39<01:03,  3.35it/s] 64%|██████▍   | 373/585 [02:39<01:03,  3.36it/s] 64%|██████▍   | 374/585 [02:39<01:02,  3.37it/s] 64%|██████▍   | 375/585 [02:39<01:02,  3.38it/s] 64%|██████▍   | 376/585 [02:40<01:01,  3.38it/s] 64%|██████▍   | 377/585 [02:40<01:01,  3.39it/s] 65%|██████▍   | 378/585 [02:40<01:01,  3.38it/s] 65%|██████▍   | 379/585 [02:41<01:00,  3.38it/s] 65%|██████▍   | 380/585 [02:41<01:00,  3.39it/s] 65%|██████▌   | 381/585 [02:41<01:00,  3.39it/s] 65%|██████▌   | 382/585 [02:41<00:59,  3.39it/s] 65%|██████▌   | 383/585 [02:42<00:59,  3.39it/s] 66%|██████▌   | 384/585 [02:42<00:59,  3.39it/s] 66%|██████▌   | 385/585 [02:42<00:58,  3.39it/s] 66%|██████▌   | 386/585 [02:43<00:58,  3.39it/s] 66%|██████▌   | 387/585 [02:43<00:58,  3.39it/s] 66%|██████▋   | 388/585 [02:43<00:58,  3.39it/s] 66%|██████▋   | 389/585 [02:44<00:57,  3.38it/s] 67%|██████▋   | 390/585 [02:44<00:57,  3.39it/s] 67%|██████▋   | 391/585 [02:44<00:57,  3.38it/s] 67%|██████▋   | 392/585 [02:44<00:56,  3.39it/s] 67%|██████▋   | 393/585 [02:45<00:56,  3.39it/s] 67%|██████▋   | 394/585 [02:45<00:56,  3.39it/s] 68%|██████▊   | 395/585 [02:45<00:56,  3.39it/s] 68%|██████▊   | 396/585 [02:46<00:55,  3.39it/s] 68%|██████▊   | 397/585 [02:46<00:55,  3.39it/s] 68%|██████▊   | 398/585 [02:46<00:55,  3.39it/s] 68%|██████▊   | 399/585 [02:46<00:54,  3.39it/s] 68%|██████▊   | 400/585 [02:47<00:54,  3.38it/s] 69%|██████▊   | 401/585 [02:47<00:54,  3.38it/s] 69%|██████▊   | 402/585 [02:47<00:54,  3.39it/s] 69%|██████▉   | 403/585 [02:48<00:53,  3.39it/s] 69%|██████▉   | 404/585 [02:48<00:53,  3.38it/s] 69%|██████▉   | 405/585 [02:48<00:53,  3.39it/s] 69%|██████▉   | 406/585 [02:49<00:52,  3.39it/s] 70%|██████▉   | 407/585 [02:49<00:52,  3.39it/s] 70%|██████▉   | 408/585 [02:49<00:52,  3.39it/s] 70%|██████▉   | 409/585 [02:49<00:51,  3.39it/s] 70%|███████   | 410/585 [02:50<00:51,  3.39it/s] 70%|███████   | 411/585 [02:50<00:51,  3.37it/s] 70%|███████   | 412/585 [02:50<00:51,  3.38it/s] 71%|███████   | 413/585 [02:51<00:51,  3.37it/s] 71%|███████   | 414/585 [02:51<00:50,  3.38it/s] 71%|███████   | 415/585 [02:51<00:50,  3.38it/s] 71%|███████   | 416/585 [02:52<00:50,  3.34it/s] 71%|███████▏  | 417/585 [02:52<00:50,  3.35it/s] 71%|███████▏  | 418/585 [02:52<00:49,  3.36it/s] 72%|███████▏  | 419/585 [02:52<00:49,  3.37it/s] 72%|███████▏  | 420/585 [02:53<00:48,  3.37it/s] 72%|███████▏  | 421/585 [02:53<00:48,  3.37it/s] 72%|███████▏  | 422/585 [02:53<00:48,  3.37it/s] 72%|███████▏  | 423/585 [02:54<00:47,  3.38it/s] 72%|███████▏  | 424/585 [02:54<00:47,  3.38it/s] 73%|███████▎  | 425/585 [02:54<00:47,  3.38it/s] 73%|███████▎  | 426/585 [02:54<00:47,  3.38it/s] 73%|███████▎  | 427/585 [02:55<00:46,  3.38it/s] 73%|███████▎  | 428/585 [02:55<00:46,  3.39it/s] 73%|███████▎  | 429/585 [02:55<00:46,  3.39it/s] 74%|███████▎  | 430/585 [02:56<00:45,  3.39it/s] 74%|███████▎  | 431/585 [02:56<00:45,  3.39it/s] 74%|███████▍  | 432/585 [02:56<00:45,  3.39it/s] 74%|███████▍  | 433/585 [02:57<00:44,  3.39it/s] 74%|███████▍  | 434/585 [02:57<00:44,  3.37it/s] 74%|███████▍  | 435/585 [02:57<00:44,  3.38it/s] 75%|███████▍  | 436/585 [02:57<00:44,  3.38it/s] 75%|███████▍  | 437/585 [02:58<00:43,  3.38it/s] 75%|███████▍  | 438/585 [02:58<00:43,  3.39it/s] 75%|███████▌  | 439/585 [02:58<00:42,  3.40it/s] 75%|███████▌  | 440/585 [02:59<00:42,  3.42it/s] 75%|███████▌  | 441/585 [02:59<00:42,  3.42it/s] 76%|███████▌  | 442/585 [02:59<00:41,  3.43it/s] 76%|███████▌  | 443/585 [02:59<00:41,  3.43it/s] 76%|███████▌  | 444/585 [03:00<00:41,  3.42it/s] 76%|███████▌  | 445/585 [03:00<00:40,  3.43it/s] 76%|███████▌  | 446/585 [03:00<00:40,  3.43it/s] 76%|███████▋  | 447/585 [03:01<00:40,  3.43it/s] 77%|███████▋  | 448/585 [03:01<00:39,  3.43it/s] 77%|███████▋  | 449/585 [03:01<00:39,  3.43it/s] 77%|███████▋  | 450/585 [03:02<00:39,  3.43it/s] 77%|███████▋  | 451/585 [03:02<00:39,  3.43it/s] 77%|███████▋  | 452/585 [03:02<00:38,  3.43it/s] 77%|███████▋  | 453/585 [03:02<00:38,  3.43it/s] 78%|███████▊  | 454/585 [03:03<00:38,  3.44it/s] 78%|███████▊  | 455/585 [03:03<00:37,  3.43it/s] 78%|███████▊  | 456/585 [03:03<00:37,  3.43it/s] 78%|███████▊  | 457/585 [03:04<00:37,  3.43it/s] 78%|███████▊  | 458/585 [03:04<00:37,  3.43it/s] 78%|███████▊  | 459/585 [03:04<00:36,  3.43it/s] 79%|███████▊  | 460/585 [03:04<00:36,  3.43it/s] 79%|███████▉  | 461/585 [03:05<00:36,  3.43it/s] 79%|███████▉  | 462/585 [03:05<00:35,  3.43it/s] 79%|███████▉  | 463/585 [03:05<00:35,  3.43it/s] 79%|███████▉  | 464/585 [03:06<00:35,  3.43it/s] 79%|███████▉  | 465/585 [03:06<00:34,  3.43it/s] 80%|███████▉  | 466/585 [03:06<00:34,  3.43it/s] 80%|███████▉  | 467/585 [03:06<00:34,  3.43it/s] 80%|████████  | 468/585 [03:07<00:34,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 23:52:37,957 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:52:37,957 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:52:37,957 >>   Batch size = 8
{'eval_loss': 1.094888687133789, 'eval_runtime': 9.9232, 'eval_samples_per_second': 352.005, 'eval_steps_per_second': 44.038, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.32it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.55it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.89it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.02it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.74it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.38it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.31it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.14it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.19it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.32it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.30it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.25it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.07it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.06it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.07it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.04it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.08it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.17it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.20it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.17it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.05it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.02it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.96it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.03it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.98it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.96it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.25it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.28it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.10it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.08it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.98it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.01it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.98it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.08it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.02it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.17it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.14it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.19it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.08it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.05it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.03it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.07it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.09it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.10it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.08it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.19it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.11it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.06it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.04it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.07it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.06it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.01it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.15it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.06it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.20it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.17it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.08it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.01it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.04it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.02it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.01it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.14it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.05it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.09it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.11it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.05it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.06it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.01it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.99it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.19it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.22it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.02it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.13it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.13it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.00it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.87it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.03it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.08it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.30it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.08it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.04it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.10it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.08it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.03it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.12it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.91it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.23it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.23it/s][A 80%|████████  | 468/585 [03:17<00:34,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:52:47,912 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 23:52:47,947 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:52:50,293 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:52:50,308 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:52:50,320 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:24<10:10,  5.26s/it] 80%|████████  | 470/585 [03:24<07:13,  3.77s/it] 81%|████████  | 471/585 [03:24<05:11,  2.73s/it] 81%|████████  | 472/585 [03:25<03:45,  2.00s/it] 81%|████████  | 473/585 [03:25<02:46,  1.49s/it] 81%|████████  | 474/585 [03:25<02:05,  1.13s/it] 81%|████████  | 475/585 [03:25<01:36,  1.14it/s] 81%|████████▏ | 476/585 [03:26<01:16,  1.42it/s] 82%|████████▏ | 477/585 [03:26<01:02,  1.72it/s] 82%|████████▏ | 478/585 [03:26<00:53,  2.02it/s] 82%|████████▏ | 479/585 [03:27<00:46,  2.30it/s] 82%|████████▏ | 480/585 [03:27<00:41,  2.54it/s] 82%|████████▏ | 481/585 [03:27<00:37,  2.74it/s] 82%|████████▏ | 482/585 [03:27<00:35,  2.91it/s] 83%|████████▎ | 483/585 [03:28<00:33,  3.04it/s] 83%|████████▎ | 484/585 [03:28<00:32,  3.14it/s] 83%|████████▎ | 485/585 [03:28<00:31,  3.21it/s] 83%|████████▎ | 486/585 [03:29<00:30,  3.26it/s] 83%|████████▎ | 487/585 [03:29<00:29,  3.30it/s] 83%|████████▎ | 488/585 [03:29<00:29,  3.33it/s] 84%|████████▎ | 489/585 [03:30<00:28,  3.34it/s] 84%|████████▍ | 490/585 [03:30<00:28,  3.36it/s] 84%|████████▍ | 491/585 [03:30<00:27,  3.37it/s] 84%|████████▍ | 492/585 [03:30<00:27,  3.36it/s] 84%|████████▍ | 493/585 [03:31<00:27,  3.37it/s] 84%|████████▍ | 494/585 [03:31<00:26,  3.38it/s] 85%|████████▍ | 495/585 [03:31<00:26,  3.38it/s] 85%|████████▍ | 496/585 [03:32<00:26,  3.39it/s] 85%|████████▍ | 497/585 [03:32<00:25,  3.39it/s] 85%|████████▌ | 498/585 [03:32<00:25,  3.39it/s] 85%|████████▌ | 499/585 [03:32<00:25,  3.39it/s] 85%|████████▌ | 500/585 [03:33<00:25,  3.39it/s]                                                  85%|████████▌ | 500/585 [03:33<00:25,  3.39it/s] 86%|████████▌ | 501/585 [03:33<00:24,  3.39it/s] 86%|████████▌ | 502/585 [03:33<00:24,  3.41it/s] 86%|████████▌ | 503/585 [03:34<00:24,  3.41it/s] 86%|████████▌ | 504/585 [03:34<00:23,  3.42it/s] 86%|████████▋ | 505/585 [03:34<00:23,  3.43it/s] 86%|████████▋ | 506/585 [03:35<00:23,  3.43it/s] 87%|████████▋ | 507/585 [03:35<00:22,  3.43it/s] 87%|████████▋ | 508/585 [03:35<00:22,  3.44it/s] 87%|████████▋ | 509/585 [03:35<00:22,  3.44it/s] 87%|████████▋ | 510/585 [03:36<00:21,  3.44it/s] 87%|████████▋ | 511/585 [03:36<00:21,  3.44it/s] 88%|████████▊ | 512/585 [03:36<00:21,  3.44it/s] 88%|████████▊ | 513/585 [03:37<00:20,  3.44it/s] 88%|████████▊ | 514/585 [03:37<00:20,  3.41it/s] 88%|████████▊ | 515/585 [03:37<00:20,  3.42it/s] 88%|████████▊ | 516/585 [03:37<00:20,  3.43it/s] 88%|████████▊ | 517/585 [03:38<00:19,  3.43it/s] 89%|████████▊ | 518/585 [03:38<00:19,  3.43it/s] 89%|████████▊ | 519/585 [03:38<00:19,  3.44it/s] 89%|████████▉ | 520/585 [03:39<00:18,  3.44it/s] 89%|████████▉ | 521/585 [03:39<00:18,  3.44it/s] 89%|████████▉ | 522/585 [03:39<00:18,  3.44it/s] 89%|████████▉ | 523/585 [03:39<00:18,  3.44it/s] 90%|████████▉ | 524/585 [03:40<00:17,  3.44it/s] 90%|████████▉ | 525/585 [03:40<00:17,  3.42it/s] 90%|████████▉ | 526/585 [03:40<00:17,  3.43it/s] 90%|█████████ | 527/585 [03:41<00:16,  3.43it/s] 90%|█████████ | 528/585 [03:41<00:16,  3.44it/s] 90%|█████████ | 529/585 [03:41<00:16,  3.44it/s] 91%|█████████ | 530/585 [03:42<00:15,  3.44it/s] 91%|█████████ | 531/585 [03:42<00:15,  3.44it/s] 91%|█████████ | 532/585 [03:42<00:15,  3.44it/s] 91%|█████████ | 533/585 [03:42<00:15,  3.44it/s] 91%|█████████▏| 534/585 [03:43<00:14,  3.44it/s] 91%|█████████▏| 535/585 [03:43<00:14,  3.44it/s] 92%|█████████▏| 536/585 [03:43<00:14,  3.40it/s] 92%|█████████▏| 537/585 [03:44<00:14,  3.41it/s] 92%|█████████▏| 538/585 [03:44<00:13,  3.42it/s] 92%|█████████▏| 539/585 [03:44<00:13,  3.43it/s] 92%|█████████▏| 540/585 [03:44<00:13,  3.43it/s] 92%|█████████▏| 541/585 [03:45<00:12,  3.44it/s] 93%|█████████▎| 542/585 [03:45<00:12,  3.44it/s] 93%|█████████▎| 543/585 [03:45<00:12,  3.44it/s] 93%|█████████▎| 544/585 [03:46<00:11,  3.44it/s] 93%|█████████▎| 545/585 [03:46<00:11,  3.44it/s] 93%|█████████▎| 546/585 [03:46<00:11,  3.43it/s] 94%|█████████▎| 547/585 [03:46<00:11,  3.44it/s] 94%|█████████▎| 548/585 [03:47<00:10,  3.44it/s] 94%|█████████▍| 549/585 [03:47<00:10,  3.43it/s] 94%|█████████▍| 550/585 [03:47<00:10,  3.43it/s] 94%|█████████▍| 551/585 [03:48<00:09,  3.44it/s] 94%|█████████▍| 552/585 [03:48<00:09,  3.43it/s] 95%|█████████▍| 553/585 [03:48<00:09,  3.44it/s] 95%|█████████▍| 554/585 [03:49<00:09,  3.44it/s] 95%|█████████▍| 555/585 [03:49<00:08,  3.43it/s] 95%|█████████▌| 556/585 [03:49<00:08,  3.43it/s] 95%|█████████▌| 557/585 [03:49<00:08,  3.43it/s] 95%|█████████▌| 558/585 [03:50<00:07,  3.44it/s] 96%|█████████▌| 559/585 [03:50<00:07,  3.43it/s] 96%|█████████▌| 560/585 [03:50<00:07,  3.43it/s] 96%|█████████▌| 561/585 [03:51<00:06,  3.44it/s] 96%|█████████▌| 562/585 [03:51<00:06,  3.41it/s] 96%|█████████▌| 563/585 [03:51<00:06,  3.42it/s] 96%|█████████▋| 564/585 [03:51<00:06,  3.39it/s] 97%|█████████▋| 565/585 [03:52<00:05,  3.36it/s] 97%|█████████▋| 566/585 [03:52<00:05,  3.38it/s] 97%|█████████▋| 567/585 [03:52<00:05,  3.39it/s] 97%|█████████▋| 568/585 [03:53<00:04,  3.41it/s] 97%|█████████▋| 569/585 [03:53<00:04,  3.41it/s] 97%|█████████▋| 570/585 [03:53<00:04,  3.42it/s] 98%|█████████▊| 571/585 [03:53<00:04,  3.42it/s] 98%|█████████▊| 572/585 [03:54<00:03,  3.42it/s] 98%|█████████▊| 573/585 [03:54<00:03,  3.43it/s] 98%|█████████▊| 574/585 [03:54<00:03,  3.43it/s] 98%|█████████▊| 575/585 [03:55<00:02,  3.43it/s] 98%|█████████▊| 576/585 [03:55<00:02,  3.43it/s] 99%|█████████▊| 577/585 [03:55<00:02,  3.42it/s] 99%|█████████▉| 578/585 [03:56<00:02,  3.43it/s] 99%|█████████▉| 579/585 [03:56<00:01,  3.43it/s] 99%|█████████▉| 580/585 [03:56<00:01,  3.43it/s] 99%|█████████▉| 581/585 [03:56<00:01,  3.43it/s] 99%|█████████▉| 582/585 [03:57<00:00,  3.43it/s]100%|█████████▉| 583/585 [03:57<00:00,  3.43it/s]100%|█████████▉| 584/585 [03:57<00:00,  3.43it/s]100%|██████████| 585/585 [03:58<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 23:53:28,723 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:53:28,723 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:53:28,723 >>   Batch size = 8
{'eval_loss': 1.1059702634811401, 'eval_runtime': 9.9275, 'eval_samples_per_second': 351.851, 'eval_steps_per_second': 44.019, 'epoch': 4.0}
{'loss': 0.4028, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.15it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.76it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.01it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.27it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.90it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.63it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.44it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.34it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.45it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.36it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.12it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.08it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.10it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.08it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.04it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.00it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.18it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.26it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.26it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.08it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.95it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.06it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.89it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.93it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.04it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.13it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.30it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.16it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.10it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.07it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.98it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.92it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.00it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.10it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.12it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.29it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.21it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.01it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.98it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.88it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.91it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.08it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.15it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.12it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.19it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.11it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.09it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.04it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.05it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.99it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.09it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.16it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.19it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.17it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.24it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.06it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.03it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.99it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.07it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.10it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.07it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.17it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.14it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.15it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.10it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.98it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.07it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.09it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.09it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.11it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.13it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.12it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.06it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.01it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.99it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.08it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.11it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.09it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.13it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.15it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.20it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.13it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.99it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.92it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.08it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.15it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.01it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.01it/s][A100%|██████████| 585/585 [04:07<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:53:38,645 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 23:53:38,666 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:53:40,532 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:53:40,554 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:53:40,568 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 23:53:46,047 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 23:53:46,052 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117 (score: 1.0654573440551758).
                                                 100%|██████████| 585/585 [04:17<00:00,  3.43it/s]100%|██████████| 585/585 [04:17<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-28 23:53:48,199 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 23:53:48,219 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:53:50,029 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:53:50,058 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:53:50,075 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:53:50,275 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:53:50,275 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:53:50,275 >>   train_loss               =        0.4
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:53:50,275 >>   train_runtime            = 0:04:17.53
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:53:50,276 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:53:50,276 >>   train_samples_per_second =    145.614
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:53:50,276 >>   train_steps_per_second   =      2.272
{'eval_loss': 1.112900972366333, 'eval_runtime': 9.9067, 'eval_samples_per_second': 352.591, 'eval_steps_per_second': 44.112, 'epoch': 5.0}
{'train_runtime': 257.5305, 'train_samples_per_second': 145.614, 'train_steps_per_second': 2.272, 'train_loss': 0.40001816056732437, 'epoch': 5.0}
08/28/2023 23:53:50 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 23:53:50,342 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:53:50,343 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:53:50,343 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 56.47it/s]  3%|▎         | 12/437 [00:00<00:08, 48.73it/s]  4%|▍         | 17/437 [00:00<00:08, 47.17it/s]  5%|▌         | 22/437 [00:00<00:08, 46.51it/s]  6%|▌         | 27/437 [00:00<00:08, 45.96it/s]  7%|▋         | 32/437 [00:00<00:08, 45.65it/s]  8%|▊         | 37/437 [00:00<00:08, 45.32it/s] 10%|▉         | 42/437 [00:00<00:08, 44.88it/s] 11%|█         | 47/437 [00:01<00:08, 44.30it/s] 12%|█▏        | 52/437 [00:01<00:08, 43.99it/s] 13%|█▎        | 57/437 [00:01<00:08, 43.93it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.22it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.28it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.61it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.71it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.65it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.37it/s] 21%|██        | 92/437 [00:02<00:07, 44.12it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.84it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.94it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.09it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.36it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.46it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.56it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.65it/s] 30%|███       | 132/437 [00:02<00:06, 44.42it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.06it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.94it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.00it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.07it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.29it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.44it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.50it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.61it/s] 41%|████      | 177/437 [00:03<00:05, 44.38it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.02it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.85it/s] 44%|████▍     | 192/437 [00:04<00:05, 43.99it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.12it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.39it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.45it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.58it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.54it/s] 51%|█████     | 222/437 [00:04<00:04, 44.24it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.00it/s] 53%|█████▎    | 232/437 [00:05<00:04, 43.82it/s] 54%|█████▍    | 237/437 [00:05<00:04, 43.90it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.22it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.36it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.46it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.57it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.53it/s] 61%|██████    | 267/437 [00:05<00:03, 44.23it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.04it/s] 63%|██████▎   | 277/437 [00:06<00:03, 43.82it/s] 65%|██████▍   | 282/437 [00:06<00:03, 43.87it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.04it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.37it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.46it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.62it/s] 70%|███████   | 307/437 [00:06<00:02, 44.40it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.31it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.04it/s] 74%|███████▎  | 322/437 [00:07<00:02, 43.93it/s] 75%|███████▍  | 327/437 [00:07<00:02, 43.88it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.08it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.33it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.47it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.54it/s] 81%|████████  | 352/437 [00:07<00:01, 44.50it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.29it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.04it/s] 84%|████████▍ | 367/437 [00:08<00:01, 43.98it/s] 85%|████████▌ | 372/437 [00:08<00:01, 43.97it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.10it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.25it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.32it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.55it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.53it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.25it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.07it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.01it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.06it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.14it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.24it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.44it/s]100%|██████████| 437/437 [00:09<00:00, 44.61it/s]100%|██████████| 437/437 [00:09<00:00, 44.39it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:54:00,206 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:54:00,206 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:54:00,206 >>   eval_loss               =     1.0655
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:54:00,206 >>   eval_runtime            = 0:00:09.86
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:54:00,206 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:54:00,206 >>   eval_samples_per_second =    354.153
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:54:00,206 >>   eval_steps_per_second   =     44.307
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:54:00,206 >>   perplexity              =     2.9022
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:54:06,775 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:54:06,780 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:54:06,781 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:54:06,781 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:54:06,781 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:54:07,397 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:54:07,397 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:54:07,969 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:54:08,997 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:54:08,997 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:54:11,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:54:11,822 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:54:11,822 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:54:11,822 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:54:11,822 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:54:12,484 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:54:12,485 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:54:13,049 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:54:13,233 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:54:13,233 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.69it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.62it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:13,  1.62it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.74it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:19,  1.62it/s]Extractor Predicting: 33it [00:19,  1.58it/s]Extractor Predicting: 34it [00:20,  1.56it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.46it/s]Extractor Predicting: 37it [00:22,  1.48it/s]Extractor Predicting: 38it [00:23,  1.49it/s]Extractor Predicting: 39it [00:23,  1.53it/s]Extractor Predicting: 40it [00:24,  1.54it/s]Extractor Predicting: 41it [00:25,  1.53it/s]Extractor Predicting: 42it [00:25,  1.55it/s]Extractor Predicting: 43it [00:26,  1.56it/s]Extractor Predicting: 44it [00:27,  1.59it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.56it/s]Extractor Predicting: 47it [00:29,  1.54it/s]Extractor Predicting: 48it [00:29,  1.55it/s]Extractor Predicting: 49it [00:30,  1.54it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:31,  1.56it/s]Extractor Predicting: 52it [00:32,  1.54it/s]Extractor Predicting: 53it [00:32,  1.58it/s]Extractor Predicting: 54it [00:33,  1.56it/s]Extractor Predicting: 55it [00:34,  1.54it/s]Extractor Predicting: 56it [00:34,  1.54it/s]Extractor Predicting: 57it [00:35,  1.52it/s]Extractor Predicting: 58it [00:36,  1.52it/s]Extractor Predicting: 59it [00:36,  1.52it/s]Extractor Predicting: 60it [00:37,  1.51it/s]Extractor Predicting: 61it [00:38,  1.53it/s]Extractor Predicting: 62it [00:38,  1.54it/s]Extractor Predicting: 63it [00:39,  1.57it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:40,  1.58it/s]Extractor Predicting: 66it [00:41,  1.61it/s]Extractor Predicting: 67it [00:41,  1.59it/s]Extractor Predicting: 68it [00:42,  1.58it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:43,  1.55it/s]Extractor Predicting: 71it [00:44,  1.56it/s]Extractor Predicting: 72it [00:45,  1.58it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:46,  1.60it/s]Extractor Predicting: 75it [00:46,  1.61it/s]Extractor Predicting: 76it [00:47,  1.58it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:48,  1.56it/s]Extractor Predicting: 79it [00:49,  1.57it/s]Extractor Predicting: 80it [00:50,  1.55it/s]Extractor Predicting: 81it [00:50,  1.60it/s]Extractor Predicting: 82it [00:51,  1.59it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:53,  1.60it/s]Extractor Predicting: 86it [00:53,  1.66it/s]Extractor Predicting: 87it [00:54,  1.70it/s]Extractor Predicting: 88it [00:55,  1.70it/s]Extractor Predicting: 89it [00:55,  1.70it/s]Extractor Predicting: 90it [00:56,  1.71it/s]Extractor Predicting: 91it [00:56,  1.69it/s]Extractor Predicting: 92it [00:57,  1.66it/s]Extractor Predicting: 93it [00:58,  1.70it/s]Extractor Predicting: 94it [00:58,  1.72it/s]Extractor Predicting: 95it [00:59,  1.70it/s]Extractor Predicting: 96it [00:59,  1.70it/s]Extractor Predicting: 97it [01:00,  1.67it/s]Extractor Predicting: 98it [01:01,  1.64it/s]Extractor Predicting: 99it [01:01,  1.61it/s]Extractor Predicting: 100it [01:02,  1.64it/s]Extractor Predicting: 101it [01:02,  1.68it/s]Extractor Predicting: 102it [01:03,  1.67it/s]Extractor Predicting: 103it [01:04,  1.67it/s]Extractor Predicting: 104it [01:04,  1.67it/s]Extractor Predicting: 105it [01:05,  1.70it/s]Extractor Predicting: 106it [01:05,  1.67it/s]Extractor Predicting: 107it [01:06,  1.69it/s]Extractor Predicting: 108it [01:06,  1.69it/s]Extractor Predicting: 109it [01:07,  1.72it/s]Extractor Predicting: 110it [01:08,  1.70it/s]Extractor Predicting: 111it [01:08,  1.69it/s]Extractor Predicting: 112it [01:09,  1.52it/s]Extractor Predicting: 113it [01:10,  1.56it/s]Extractor Predicting: 114it [01:10,  1.54it/s]Extractor Predicting: 115it [01:11,  1.58it/s]Extractor Predicting: 116it [01:12,  1.57it/s]Extractor Predicting: 117it [01:12,  1.57it/s]Extractor Predicting: 118it [01:13,  1.64it/s]Extractor Predicting: 119it [01:13,  1.63it/s]Extractor Predicting: 120it [01:14,  1.64it/s]Extractor Predicting: 121it [01:15,  1.62it/s]Extractor Predicting: 122it [01:15,  1.61it/s]Extractor Predicting: 123it [01:16,  1.61it/s]Extractor Predicting: 124it [01:17,  1.57it/s]Extractor Predicting: 125it [01:17,  1.57it/s]Extractor Predicting: 126it [01:18,  1.58it/s]Extractor Predicting: 127it [01:18,  1.57it/s]Extractor Predicting: 128it [01:19,  1.56it/s]Extractor Predicting: 129it [01:20,  1.53it/s]Extractor Predicting: 130it [01:20,  1.55it/s]Extractor Predicting: 131it [01:21,  1.56it/s]Extractor Predicting: 132it [01:22,  1.60it/s]Extractor Predicting: 133it [01:22,  1.60it/s]Extractor Predicting: 134it [01:23,  1.60it/s]Extractor Predicting: 135it [01:23,  1.60it/s]Extractor Predicting: 136it [01:24,  1.59it/s]Extractor Predicting: 137it [01:25,  1.58it/s]Extractor Predicting: 138it [01:25,  1.58it/s]Extractor Predicting: 139it [01:26,  1.57it/s]Extractor Predicting: 140it [01:27,  1.62it/s]Extractor Predicting: 141it [01:27,  1.59it/s]Extractor Predicting: 142it [01:28,  1.65it/s]Extractor Predicting: 142it [01:28,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:55:49,900 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:55:49,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:55:49,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:55:49,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:55:49,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:55:50,524 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:55:50,524 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:55:51,079 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:55:52,107 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:55:52,107 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:55:54,946 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:55:54,952 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:55:54,952 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:55:54,952 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:55:54,952 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:55:55,612 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:55:55,613 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:55:56,201 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:55:56,371 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:55:56,371 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.19485903814262023,
  "recall": 0.06727741196679073,
  "score": 0.10002128112364334,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.69it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:15,  1.62it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:19,  1.55it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:24,  1.63it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.62it/s]Extractor Predicting: 44it [00:27,  1.61it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.62it/s]Extractor Predicting: 47it [00:29,  1.62it/s]Extractor Predicting: 48it [00:29,  1.62it/s]Extractor Predicting: 49it [00:30,  1.62it/s]Extractor Predicting: 50it [00:30,  1.59it/s]Extractor Predicting: 51it [00:31,  1.61it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:32,  1.61it/s]Extractor Predicting: 54it [00:33,  1.61it/s]Extractor Predicting: 55it [00:34,  1.57it/s]Extractor Predicting: 56it [00:34,  1.59it/s]Extractor Predicting: 57it [00:35,  1.63it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 60it [00:37,  1.65it/s]Extractor Predicting: 61it [00:37,  1.64it/s]Extractor Predicting: 62it [00:38,  1.64it/s]Extractor Predicting: 63it [00:38,  1.65it/s]Extractor Predicting: 64it [00:39,  1.60it/s]Extractor Predicting: 65it [00:40,  1.60it/s]Extractor Predicting: 66it [00:40,  1.59it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:41,  1.66it/s]Extractor Predicting: 69it [00:42,  1.64it/s]Extractor Predicting: 70it [00:43,  1.64it/s]Extractor Predicting: 71it [00:43,  1.62it/s]Extractor Predicting: 72it [00:44,  1.63it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:45,  1.59it/s]Extractor Predicting: 75it [00:46,  1.60it/s]Extractor Predicting: 76it [00:46,  1.61it/s]Extractor Predicting: 77it [00:47,  1.42it/s]Extractor Predicting: 78it [00:48,  1.47it/s]Extractor Predicting: 79it [00:49,  1.49it/s]Extractor Predicting: 80it [00:49,  1.52it/s]Extractor Predicting: 81it [00:50,  1.55it/s]Extractor Predicting: 82it [00:51,  1.53it/s]Extractor Predicting: 83it [00:51,  1.55it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:52,  1.59it/s]Extractor Predicting: 86it [00:53,  1.56it/s]Extractor Predicting: 87it [00:54,  1.56it/s]Extractor Predicting: 88it [00:54,  1.53it/s]Extractor Predicting: 89it [00:55,  1.55it/s]Extractor Predicting: 90it [00:56,  1.57it/s]Extractor Predicting: 91it [00:56,  1.55it/s]Extractor Predicting: 92it [00:57,  1.55it/s]Extractor Predicting: 93it [00:58,  1.53it/s]Extractor Predicting: 94it [00:58,  1.51it/s]Extractor Predicting: 95it [00:59,  1.52it/s]Extractor Predicting: 96it [01:00,  1.53it/s]Extractor Predicting: 97it [01:00,  1.54it/s]Extractor Predicting: 98it [01:01,  1.53it/s]Extractor Predicting: 99it [01:02,  1.55it/s]Extractor Predicting: 100it [01:02,  1.56it/s]Extractor Predicting: 101it [01:03,  1.55it/s]Extractor Predicting: 102it [01:04,  1.54it/s]Extractor Predicting: 103it [01:04,  1.55it/s]Extractor Predicting: 104it [01:05,  1.55it/s]Extractor Predicting: 105it [01:05,  1.54it/s]Extractor Predicting: 106it [01:06,  1.56it/s]Extractor Predicting: 107it [01:07,  1.53it/s]Extractor Predicting: 108it [01:07,  1.53it/s]Extractor Predicting: 109it [01:08,  1.54it/s]Extractor Predicting: 110it [01:09,  1.52it/s]Extractor Predicting: 111it [01:09,  1.52it/s]Extractor Predicting: 112it [01:10,  1.56it/s]Extractor Predicting: 113it [01:11,  1.59it/s]Extractor Predicting: 114it [01:11,  1.60it/s]Extractor Predicting: 115it [01:12,  1.59it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:13,  1.61it/s]Extractor Predicting: 118it [01:14,  1.62it/s]Extractor Predicting: 119it [01:14,  1.62it/s]Extractor Predicting: 120it [01:15,  1.63it/s]Extractor Predicting: 121it [01:16,  1.62it/s]Extractor Predicting: 122it [01:16,  1.67it/s]Extractor Predicting: 123it [01:17,  1.66it/s]Extractor Predicting: 124it [01:17,  1.66it/s]Extractor Predicting: 125it [01:18,  1.65it/s]Extractor Predicting: 126it [01:19,  1.62it/s]Extractor Predicting: 127it [01:19,  1.63it/s]Extractor Predicting: 128it [01:20,  1.64it/s]Extractor Predicting: 129it [01:20,  1.61it/s]Extractor Predicting: 130it [01:21,  1.62it/s]Extractor Predicting: 131it [01:22,  1.60it/s]Extractor Predicting: 132it [01:22,  1.62it/s]Extractor Predicting: 133it [01:23,  1.61it/s]Extractor Predicting: 134it [01:23,  1.62it/s]Extractor Predicting: 135it [01:24,  1.65it/s]Extractor Predicting: 136it [01:25,  1.65it/s]Extractor Predicting: 137it [01:25,  1.63it/s]Extractor Predicting: 138it [01:26,  1.63it/s]Extractor Predicting: 139it [01:26,  1.67it/s]Extractor Predicting: 140it [01:27,  1.64it/s]Extractor Predicting: 141it [01:28,  1.62it/s]Extractor Predicting: 142it [01:28,  1.67it/s]Extractor Predicting: 143it [01:29,  1.65it/s]Extractor Predicting: 144it [01:30,  1.63it/s]Extractor Predicting: 145it [01:30,  1.64it/s]Extractor Predicting: 146it [01:31,  1.63it/s]Extractor Predicting: 147it [01:31,  1.60it/s]Extractor Predicting: 148it [01:32,  1.60it/s]Extractor Predicting: 149it [01:33,  1.59it/s]Extractor Predicting: 150it [01:33,  1.61it/s]Extractor Predicting: 151it [01:34,  1.63it/s]Extractor Predicting: 152it [01:34,  1.65it/s]Extractor Predicting: 153it [01:35,  1.64it/s]Extractor Predicting: 154it [01:36,  1.62it/s]Extractor Predicting: 155it [01:36,  1.60it/s]Extractor Predicting: 156it [01:37,  1.59it/s]Extractor Predicting: 157it [01:38,  1.57it/s]Extractor Predicting: 158it [01:38,  1.57it/s]Extractor Predicting: 159it [01:39,  1.57it/s]Extractor Predicting: 160it [01:40,  1.58it/s]Extractor Predicting: 161it [01:40,  1.61it/s]Extractor Predicting: 162it [01:41,  1.62it/s]Extractor Predicting: 163it [01:41,  1.61it/s]Extractor Predicting: 164it [01:42,  1.62it/s]Extractor Predicting: 165it [01:43,  1.59it/s]Extractor Predicting: 166it [01:43,  1.61it/s]Extractor Predicting: 167it [01:44,  1.63it/s]Extractor Predicting: 168it [01:44,  1.62it/s]Extractor Predicting: 169it [01:45,  1.60it/s]Extractor Predicting: 170it [01:46,  1.58it/s]Extractor Predicting: 171it [01:46,  1.55it/s]Extractor Predicting: 172it [01:47,  1.56it/s]Extractor Predicting: 173it [01:48,  1.56it/s]Extractor Predicting: 174it [01:48,  1.51it/s]Extractor Predicting: 175it [01:49,  1.33it/s]Extractor Predicting: 176it [01:50,  1.39it/s]Extractor Predicting: 177it [01:51,  1.42it/s]Extractor Predicting: 178it [01:51,  1.49it/s]Extractor Predicting: 179it [01:52,  1.50it/s]Extractor Predicting: 180it [01:53,  1.56it/s]Extractor Predicting: 181it [01:53,  1.54it/s]Extractor Predicting: 182it [01:54,  1.58it/s]Extractor Predicting: 183it [01:54,  1.58it/s]Extractor Predicting: 184it [01:55,  1.62it/s]Extractor Predicting: 185it [01:56,  1.64it/s]Extractor Predicting: 186it [01:56,  1.64it/s]Extractor Predicting: 187it [01:57,  1.65it/s]Extractor Predicting: 188it [01:57,  1.63it/s]Extractor Predicting: 189it [01:58,  1.63it/s]Extractor Predicting: 190it [01:59,  1.61it/s]Extractor Predicting: 191it [01:59,  1.56it/s]Extractor Predicting: 192it [02:00,  1.59it/s]Extractor Predicting: 193it [02:01,  1.63it/s]Extractor Predicting: 194it [02:01,  1.62it/s]Extractor Predicting: 195it [02:02,  1.62it/s]Extractor Predicting: 196it [02:02,  1.64it/s]Extractor Predicting: 197it [02:03,  1.65it/s]Extractor Predicting: 198it [02:04,  1.61it/s]Extractor Predicting: 199it [02:04,  1.61it/s]Extractor Predicting: 200it [02:05,  1.61it/s]Extractor Predicting: 201it [02:06,  1.61it/s]Extractor Predicting: 202it [02:06,  1.65it/s]Extractor Predicting: 203it [02:07,  1.66it/s]Extractor Predicting: 204it [02:07,  1.64it/s]Extractor Predicting: 205it [02:08,  1.59it/s]Extractor Predicting: 206it [02:09,  1.59it/s]Extractor Predicting: 207it [02:09,  1.62it/s]Extractor Predicting: 208it [02:10,  1.63it/s]Extractor Predicting: 209it [02:11,  1.57it/s]Extractor Predicting: 210it [02:11,  1.55it/s]Extractor Predicting: 211it [02:12,  1.56it/s]Extractor Predicting: 212it [02:12,  1.59it/s]Extractor Predicting: 213it [02:13,  1.60it/s]Extractor Predicting: 214it [02:14,  1.55it/s]Extractor Predicting: 215it [02:14,  1.56it/s]Extractor Predicting: 216it [02:15,  1.60it/s]Extractor Predicting: 217it [02:16,  1.61it/s]Extractor Predicting: 218it [02:16,  1.55it/s]Extractor Predicting: 219it [02:17,  1.57it/s]Extractor Predicting: 220it [02:18,  1.57it/s]Extractor Predicting: 221it [02:18,  1.54it/s]Extractor Predicting: 222it [02:19,  1.54it/s]Extractor Predicting: 223it [02:19,  1.54it/s]Extractor Predicting: 224it [02:20,  1.59it/s]Extractor Predicting: 225it [02:21,  1.57it/s]Extractor Predicting: 226it [02:21,  1.63it/s]Extractor Predicting: 227it [02:22,  1.65it/s]Extractor Predicting: 228it [02:23,  1.62it/s]Extractor Predicting: 229it [02:23,  1.63it/s]Extractor Predicting: 230it [02:24,  1.59it/s]Extractor Predicting: 231it [02:24,  1.59it/s]Extractor Predicting: 232it [02:25,  1.59it/s]Extractor Predicting: 233it [02:26,  1.63it/s]Extractor Predicting: 234it [02:26,  1.61it/s]Extractor Predicting: 235it [02:27,  1.62it/s]Extractor Predicting: 236it [02:28,  1.59it/s]Extractor Predicting: 237it [02:28,  1.58it/s]Extractor Predicting: 238it [02:29,  1.61it/s]Extractor Predicting: 239it [02:29,  1.62it/s]Extractor Predicting: 240it [02:30,  1.61it/s]Extractor Predicting: 241it [02:31,  1.60it/s]Extractor Predicting: 242it [02:31,  1.59it/s]Extractor Predicting: 243it [02:32,  1.54it/s]Extractor Predicting: 244it [02:33,  1.56it/s]Extractor Predicting: 245it [02:33,  1.62it/s]Extractor Predicting: 246it [02:34,  1.60it/s]Extractor Predicting: 247it [02:34,  1.62it/s]Extractor Predicting: 248it [02:35,  1.62it/s]Extractor Predicting: 249it [02:36,  1.61it/s]Extractor Predicting: 250it [02:36,  1.61it/s]Extractor Predicting: 251it [02:37,  1.57it/s]Extractor Predicting: 252it [02:38,  1.57it/s]Extractor Predicting: 253it [02:38,  1.58it/s]Extractor Predicting: 254it [02:39,  1.61it/s]Extractor Predicting: 255it [02:39,  1.60it/s]Extractor Predicting: 256it [02:40,  1.59it/s]Extractor Predicting: 257it [02:41,  1.61it/s]Extractor Predicting: 258it [02:41,  1.60it/s]Extractor Predicting: 259it [02:42,  1.56it/s]Extractor Predicting: 260it [02:43,  1.58it/s]Extractor Predicting: 261it [02:43,  1.60it/s]Extractor Predicting: 262it [02:44,  1.58it/s]Extractor Predicting: 263it [02:45,  1.56it/s]Extractor Predicting: 264it [02:45,  1.56it/s]Extractor Predicting: 265it [02:46,  1.55it/s]Extractor Predicting: 266it [02:46,  1.53it/s]Extractor Predicting: 267it [02:47,  1.51it/s]Extractor Predicting: 268it [02:48,  1.53it/s]Extractor Predicting: 269it [02:48,  1.53it/s]Extractor Predicting: 270it [02:49,  1.52it/s]Extractor Predicting: 271it [02:50,  1.53it/s]Extractor Predicting: 272it [02:50,  1.54it/s]Extractor Predicting: 273it [02:51,  1.53it/s]Extractor Predicting: 274it [02:52,  1.53it/s]Extractor Predicting: 275it [02:52,  1.58it/s]Extractor Predicting: 276it [02:53,  1.56it/s]Extractor Predicting: 277it [02:54,  1.54it/s]Extractor Predicting: 278it [02:54,  1.54it/s]Extractor Predicting: 279it [02:55,  1.55it/s]Extractor Predicting: 280it [02:56,  1.38it/s]Extractor Predicting: 281it [02:56,  1.41it/s]Extractor Predicting: 282it [02:57,  1.45it/s]Extractor Predicting: 283it [02:58,  1.44it/s]Extractor Predicting: 284it [02:59,  1.44it/s]Extractor Predicting: 285it [02:59,  1.45it/s]Extractor Predicting: 286it [03:00,  1.45it/s]Extractor Predicting: 287it [03:01,  1.48it/s]Extractor Predicting: 288it [03:01,  1.96it/s]Extractor Predicting: 288it [03:01,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:05,316 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:05,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:05,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:05,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:05,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:59:06,052 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:59:06,053 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:59:06,617 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:59:07,625 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:59:07,625 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:10,444 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:10,450 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:10,450 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:10,450 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:10,450 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:59:11,080 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:59:11,081 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:59:11,645 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:59:11,814 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:59:11,814 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5451145395044413,
  "recall": 0.16925533459137757,
  "score": 0.2583074878156846,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:01,  1.88it/s]Extractor Predicting: 3it [00:01,  1.72it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6428571428571429,
  "recall": 0.08108108108108109,
  "score": 0.14400000000000002,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_4', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:14, 14.17s/it]Extractor Predicting: 2it [00:15,  6.46s/it]Extractor Predicting: 3it [00:15,  3.77s/it]Extractor Predicting: 4it [00:16,  2.49s/it]Extractor Predicting: 5it [00:16,  1.80s/it]Extractor Predicting: 6it [00:17,  1.38s/it]Extractor Predicting: 7it [00:18,  1.11s/it]Extractor Predicting: 8it [00:18,  1.07it/s]Extractor Predicting: 9it [00:19,  1.23it/s]Extractor Predicting: 10it [00:19,  1.33it/s]Extractor Predicting: 11it [00:20,  1.42it/s]Extractor Predicting: 12it [00:20,  1.54it/s]Extractor Predicting: 13it [00:21,  1.62it/s]Extractor Predicting: 14it [00:21,  1.65it/s]Extractor Predicting: 15it [00:22,  1.68it/s]Extractor Predicting: 16it [00:23,  1.73it/s]Extractor Predicting: 17it [00:23,  1.78it/s]Extractor Predicting: 18it [00:24,  1.80it/s]Extractor Predicting: 19it [00:24,  1.81it/s]Extractor Predicting: 20it [00:25,  1.78it/s]Extractor Predicting: 21it [00:25,  1.77it/s]Extractor Predicting: 22it [00:26,  1.74it/s]Extractor Predicting: 23it [00:27,  1.75it/s]Extractor Predicting: 24it [00:27,  1.80it/s]Extractor Predicting: 25it [00:28,  1.77it/s]Extractor Predicting: 26it [00:28,  1.87it/s]Extractor Predicting: 27it [00:29,  1.86it/s]Extractor Predicting: 28it [00:29,  1.91it/s]Extractor Predicting: 29it [00:30,  1.88it/s]Extractor Predicting: 30it [00:30,  1.81it/s]Extractor Predicting: 31it [00:31,  1.81it/s]Extractor Predicting: 32it [00:31,  1.77it/s]Extractor Predicting: 33it [00:32,  1.73it/s]Extractor Predicting: 34it [00:33,  1.69it/s]Extractor Predicting: 35it [00:33,  1.70it/s]Extractor Predicting: 36it [00:34,  1.68it/s]Extractor Predicting: 37it [00:34,  1.68it/s]Extractor Predicting: 38it [00:35,  1.67it/s]Extractor Predicting: 39it [00:36,  1.70it/s]Extractor Predicting: 40it [00:36,  1.69it/s]Extractor Predicting: 41it [00:37,  1.69it/s]Extractor Predicting: 42it [00:37,  1.72it/s]Extractor Predicting: 43it [00:38,  1.72it/s]Extractor Predicting: 44it [00:38,  1.76it/s]Extractor Predicting: 45it [00:39,  1.77it/s]Extractor Predicting: 46it [00:40,  1.72it/s]Extractor Predicting: 47it [00:40,  1.70it/s]Extractor Predicting: 48it [00:41,  1.70it/s]Extractor Predicting: 49it [00:41,  1.67it/s]Extractor Predicting: 50it [00:42,  1.68it/s]Extractor Predicting: 51it [00:43,  1.71it/s]Extractor Predicting: 52it [00:43,  1.57it/s]Extractor Predicting: 53it [00:44,  1.64it/s]Extractor Predicting: 54it [00:45,  1.65it/s]Extractor Predicting: 55it [00:45,  1.64it/s]Extractor Predicting: 56it [00:46,  1.66it/s]Extractor Predicting: 57it [00:46,  1.65it/s]Extractor Predicting: 58it [00:47,  1.66it/s]Extractor Predicting: 59it [00:48,  1.66it/s]Extractor Predicting: 60it [00:48,  1.65it/s]Extractor Predicting: 61it [00:49,  1.67it/s]Extractor Predicting: 62it [00:49,  1.69it/s]Extractor Predicting: 63it [00:50,  1.72it/s]Extractor Predicting: 64it [00:50,  1.76it/s]Extractor Predicting: 65it [00:51,  1.73it/s]Extractor Predicting: 66it [00:52,  1.77it/s]Extractor Predicting: 67it [00:52,  1.75it/s]Extractor Predicting: 68it [00:53,  1.75it/s]Extractor Predicting: 69it [00:53,  1.73it/s]Extractor Predicting: 70it [00:54,  1.70it/s]Extractor Predicting: 71it [00:54,  1.70it/s]Extractor Predicting: 72it [00:55,  1.73it/s]Extractor Predicting: 73it [00:56,  1.72it/s]Extractor Predicting: 74it [00:56,  1.75it/s]Extractor Predicting: 75it [00:57,  1.76it/s]Extractor Predicting: 76it [00:57,  1.72it/s]Extractor Predicting: 77it [00:58,  1.72it/s]Extractor Predicting: 78it [00:59,  1.70it/s]Extractor Predicting: 79it [00:59,  1.70it/s]Extractor Predicting: 80it [01:00,  1.68it/s]Extractor Predicting: 81it [01:00,  1.75it/s]Extractor Predicting: 82it [01:01,  1.73it/s]Extractor Predicting: 83it [01:01,  1.71it/s]Extractor Predicting: 84it [01:02,  1.72it/s]Extractor Predicting: 85it [01:03,  1.76it/s]Extractor Predicting: 86it [01:03,  1.82it/s]Extractor Predicting: 87it [01:04,  1.85it/s]Extractor Predicting: 88it [01:04,  1.86it/s]Extractor Predicting: 89it [01:05,  1.86it/s]Extractor Predicting: 90it [01:05,  1.87it/s]Extractor Predicting: 91it [01:06,  1.82it/s]Extractor Predicting: 92it [01:06,  1.79it/s]Extractor Predicting: 93it [01:07,  1.85it/s]Extractor Predicting: 94it [01:07,  1.88it/s]Extractor Predicting: 95it [01:08,  1.87it/s]Extractor Predicting: 96it [01:08,  1.88it/s]Extractor Predicting: 97it [01:09,  1.84it/s]Extractor Predicting: 98it [01:10,  1.80it/s]Extractor Predicting: 99it [01:10,  1.78it/s]Extractor Predicting: 100it [01:11,  1.81it/s]Extractor Predicting: 101it [01:11,  1.86it/s]Extractor Predicting: 102it [01:12,  1.83it/s]Extractor Predicting: 103it [01:12,  1.84it/s]Extractor Predicting: 104it [01:13,  1.83it/s]Extractor Predicting: 105it [01:13,  1.88it/s]Extractor Predicting: 106it [01:14,  1.84it/s]Extractor Predicting: 107it [01:14,  1.85it/s]Extractor Predicting: 108it [01:15,  1.85it/s]Extractor Predicting: 109it [01:16,  1.86it/s]Extractor Predicting: 110it [01:16,  1.80it/s]Extractor Predicting: 111it [01:17,  1.80it/s]Extractor Predicting: 112it [01:17,  1.80it/s]Extractor Predicting: 113it [01:18,  1.81it/s]Extractor Predicting: 114it [01:18,  1.75it/s]Extractor Predicting: 115it [01:19,  1.77it/s]Extractor Predicting: 116it [01:20,  1.74it/s]Extractor Predicting: 117it [01:20,  1.73it/s]Extractor Predicting: 118it [01:21,  1.82it/s]Extractor Predicting: 119it [01:21,  1.79it/s]Extractor Predicting: 120it [01:22,  1.79it/s]Extractor Predicting: 121it [01:22,  1.77it/s]Extractor Predicting: 122it [01:23,  1.75it/s]Extractor Predicting: 123it [01:24,  1.60it/s]Extractor Predicting: 124it [01:24,  1.55it/s]Extractor Predicting: 125it [01:25,  1.60it/s]Extractor Predicting: 126it [01:25,  1.65it/s]Extractor Predicting: 127it [01:26,  1.66it/s]Extractor Predicting: 128it [01:27,  1.67it/s]Extractor Predicting: 129it [01:27,  1.64it/s]Extractor Predicting: 130it [01:28,  1.67it/s]Extractor Predicting: 131it [01:28,  1.70it/s]Extractor Predicting: 132it [01:29,  1.74it/s]Extractor Predicting: 133it [01:30,  1.75it/s]Extractor Predicting: 134it [01:30,  1.77it/s]Extractor Predicting: 135it [01:31,  1.76it/s]Extractor Predicting: 136it [01:31,  1.76it/s]Extractor Predicting: 137it [01:32,  1.75it/s]Extractor Predicting: 138it [01:32,  1.75it/s]Extractor Predicting: 139it [01:33,  1.73it/s]Extractor Predicting: 140it [01:33,  1.78it/s]Extractor Predicting: 141it [01:34,  1.74it/s]Extractor Predicting: 142it [01:35,  1.83it/s]Extractor Predicting: 142it [01:35,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.74it/s]Extractor Predicting: 3it [00:01,  1.76it/s]Extractor Predicting: 4it [00:02,  1.86it/s]Extractor Predicting: 5it [00:02,  1.83it/s]Extractor Predicting: 6it [00:03,  1.79it/s]Extractor Predicting: 7it [00:03,  1.81it/s]Extractor Predicting: 8it [00:04,  1.87it/s]Extractor Predicting: 9it [00:04,  1.87it/s]Extractor Predicting: 10it [00:05,  1.91it/s]Extractor Predicting: 11it [00:05,  1.86it/s]Extractor Predicting: 12it [00:06,  1.87it/s]Extractor Predicting: 13it [00:07,  1.83it/s]Extractor Predicting: 14it [00:07,  1.86it/s]Extractor Predicting: 15it [00:08,  1.84it/s]Extractor Predicting: 16it [00:08,  1.68it/s]Extractor Predicting: 17it [00:09,  1.71it/s]Extractor Predicting: 18it [00:10,  1.73it/s]Extractor Predicting: 19it [00:10,  1.78it/s]Extractor Predicting: 20it [00:11,  1.76it/s]Extractor Predicting: 21it [00:11,  1.79it/s]Extractor Predicting: 22it [00:12,  1.82it/s]Extractor Predicting: 23it [00:12,  1.78it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:13,  1.79it/s]Extractor Predicting: 26it [00:14,  1.80it/s]Extractor Predicting: 27it [00:15,  1.78it/s]Extractor Predicting: 28it [00:15,  1.81it/s]Extractor Predicting: 29it [00:16,  1.76it/s]Extractor Predicting: 30it [00:16,  1.71it/s]Extractor Predicting: 31it [00:17,  1.70it/s]Extractor Predicting: 32it [00:17,  1.71it/s]Extractor Predicting: 33it [00:18,  1.73it/s]Extractor Predicting: 34it [00:19,  1.73it/s]Extractor Predicting: 35it [00:19,  1.74it/s]Extractor Predicting: 36it [00:20,  1.79it/s]Extractor Predicting: 37it [00:20,  1.80it/s]Extractor Predicting: 38it [00:21,  1.82it/s]Extractor Predicting: 39it [00:21,  1.80it/s]Extractor Predicting: 40it [00:22,  1.80it/s]Extractor Predicting: 41it [00:22,  1.79it/s]Extractor Predicting: 42it [00:23,  1.78it/s]Extractor Predicting: 43it [00:24,  1.78it/s]Extractor Predicting: 44it [00:24,  1.76it/s]Extractor Predicting: 45it [00:25,  1.76it/s]Extractor Predicting: 46it [00:25,  1.77it/s]Extractor Predicting: 47it [00:26,  1.78it/s]Extractor Predicting: 48it [00:26,  1.78it/s]Extractor Predicting: 49it [00:27,  1.77it/s]Extractor Predicting: 50it [00:28,  1.74it/s]Extractor Predicting: 51it [00:28,  1.78it/s]Extractor Predicting: 52it [00:29,  1.79it/s]Extractor Predicting: 53it [00:29,  1.78it/s]Extractor Predicting: 54it [00:30,  1.77it/s]Extractor Predicting: 55it [00:30,  1.72it/s]Extractor Predicting: 56it [00:31,  1.75it/s]Extractor Predicting: 57it [00:31,  1.79it/s]Extractor Predicting: 58it [00:32,  1.82it/s]Extractor Predicting: 59it [00:33,  1.82it/s]Extractor Predicting: 60it [00:33,  1.82it/s]Extractor Predicting: 61it [00:34,  1.82it/s]Extractor Predicting: 62it [00:34,  1.82it/s]Extractor Predicting: 63it [00:35,  1.82it/s]Extractor Predicting: 64it [00:35,  1.76it/s]Extractor Predicting: 65it [00:36,  1.76it/s]Extractor Predicting: 66it [00:37,  1.75it/s]Extractor Predicting: 67it [00:37,  1.77it/s]Extractor Predicting: 68it [00:38,  1.83it/s]Extractor Predicting: 69it [00:38,  1.81it/s]Extractor Predicting: 70it [00:39,  1.81it/s]Extractor Predicting: 71it [00:39,  1.78it/s]Extractor Predicting: 72it [00:40,  1.80it/s]Extractor Predicting: 73it [00:40,  1.73it/s]Extractor Predicting: 74it [00:41,  1.74it/s]Extractor Predicting: 75it [00:42,  1.76it/s]Extractor Predicting: 76it [00:42,  1.75it/s]Extractor Predicting: 77it [00:43,  1.73it/s]Extractor Predicting: 78it [00:43,  1.74it/s]Extractor Predicting: 79it [00:44,  1.74it/s]Extractor Predicting: 80it [00:44,  1.75it/s]Extractor Predicting: 81it [00:45,  1.76it/s]Extractor Predicting: 82it [00:46,  1.71it/s]Extractor Predicting: 83it [00:46,  1.72it/s]Extractor Predicting: 84it [00:47,  1.74it/s]Extractor Predicting: 85it [00:47,  1.77it/s]Extractor Predicting: 86it [00:48,  1.73it/s]Extractor Predicting: 87it [00:48,  1.74it/s]Extractor Predicting: 88it [00:49,  1.69it/s]Extractor Predicting: 89it [00:50,  1.71it/s]Extractor Predicting: 90it [00:50,  1.74it/s]Extractor Predicting: 91it [00:51,  1.71it/s]Extractor Predicting: 92it [00:51,  1.72it/s]Extractor Predicting: 93it [00:52,  1.68it/s]Extractor Predicting: 94it [00:53,  1.66it/s]Extractor Predicting: 95it [00:53,  1.68it/s]Extractor Predicting: 96it [00:54,  1.68it/s]Extractor Predicting: 97it [00:54,  1.69it/s]Extractor Predicting: 98it [00:55,  1.69it/s]Extractor Predicting: 99it [00:56,  1.70it/s]Extractor Predicting: 100it [00:56,  1.71it/s]Extractor Predicting: 101it [00:57,  1.70it/s]Extractor Predicting: 102it [00:57,  1.70it/s]Extractor Predicting: 103it [00:58,  1.71it/s]Extractor Predicting: 104it [00:59,  1.70it/s]Extractor Predicting: 105it [00:59,  1.70it/s]Extractor Predicting: 106it [01:00,  1.71it/s]Extractor Predicting: 107it [01:00,  1.67it/s]Extractor Predicting: 108it [01:01,  1.67it/s]Extractor Predicting: 109it [01:01,  1.68it/s]Extractor Predicting: 110it [01:02,  1.66it/s]Extractor Predicting: 111it [01:03,  1.65it/s]Extractor Predicting: 112it [01:03,  1.71it/s]Extractor Predicting: 113it [01:04,  1.57it/s]Extractor Predicting: 114it [01:05,  1.64it/s]Extractor Predicting: 115it [01:05,  1.67it/s]Extractor Predicting: 116it [01:06,  1.69it/s]Extractor Predicting: 117it [01:06,  1.72it/s]Extractor Predicting: 118it [01:07,  1.75it/s]Extractor Predicting: 119it [01:07,  1.74it/s]Extractor Predicting: 120it [01:08,  1.77it/s]Extractor Predicting: 121it [01:09,  1.77it/s]Extractor Predicting: 122it [01:09,  1.84it/s]Extractor Predicting: 123it [01:10,  1.82it/s]Extractor Predicting: 124it [01:10,  1.81it/s]Extractor Predicting: 125it [01:11,  1.81it/s]Extractor Predicting: 126it [01:11,  1.78it/s]Extractor Predicting: 127it [01:12,  1.79it/s]Extractor Predicting: 128it [01:12,  1.82it/s]Extractor Predicting: 129it [01:13,  1.77it/s]Extractor Predicting: 130it [01:13,  1.79it/s]Extractor Predicting: 131it [01:14,  1.76it/s]Extractor Predicting: 132it [01:15,  1.77it/s]Extractor Predicting: 133it [01:15,  1.75it/s]Extractor Predicting: 134it [01:16,  1.77it/s]Extractor Predicting: 135it [01:16,  1.81it/s]Extractor Predicting: 136it [01:17,  1.81it/s]Extractor Predicting: 137it [01:17,  1.80it/s]Extractor Predicting: 138it [01:18,  1.79it/s]Extractor Predicting: 139it [01:19,  1.83it/s]Extractor Predicting: 140it [01:19,  1.80it/s]Extractor Predicting: 141it [01:20,  1.78it/s]Extractor Predicting: 142it [01:20,  1.84it/s]Extractor Predicting: 143it [01:21,  1.82it/s]Extractor Predicting: 144it [01:21,  1.78it/s]Extractor Predicting: 145it [01:22,  1.80it/s]Extractor Predicting: 146it [01:22,  1.79it/s]Extractor Predicting: 147it [01:23,  1.76it/s]Extractor Predicting: 148it [01:24,  1.76it/s]Extractor Predicting: 149it [01:24,  1.74it/s]Extractor Predicting: 150it [01:25,  1.77it/s]Extractor Predicting: 151it [01:25,  1.80it/s]Extractor Predicting: 152it [01:26,  1.82it/s]Extractor Predicting: 153it [01:26,  1.81it/s]Extractor Predicting: 154it [01:27,  1.78it/s]Extractor Predicting: 155it [01:27,  1.77it/s]Extractor Predicting: 156it [01:28,  1.75it/s]Extractor Predicting: 157it [01:29,  1.72it/s]Extractor Predicting: 158it [01:29,  1.73it/s]Extractor Predicting: 159it [01:30,  1.72it/s]Extractor Predicting: 160it [01:30,  1.72it/s]Extractor Predicting: 161it [01:31,  1.76it/s]Extractor Predicting: 162it [01:32,  1.78it/s]Extractor Predicting: 163it [01:32,  1.77it/s]Extractor Predicting: 164it [01:33,  1.78it/s]Extractor Predicting: 165it [01:33,  1.74it/s]Extractor Predicting: 166it [01:34,  1.76it/s]Extractor Predicting: 167it [01:34,  1.79it/s]Extractor Predicting: 168it [01:35,  1.77it/s]Extractor Predicting: 169it [01:35,  1.76it/s]Extractor Predicting: 170it [01:36,  1.73it/s]Extractor Predicting: 171it [01:37,  1.69it/s]Extractor Predicting: 172it [01:37,  1.70it/s]Extractor Predicting: 173it [01:38,  1.71it/s]Extractor Predicting: 174it [01:39,  1.65it/s]Extractor Predicting: 175it [01:39,  1.60it/s]Extractor Predicting: 176it [01:40,  1.64it/s]Extractor Predicting: 177it [01:40,  1.63it/s]Extractor Predicting: 178it [01:41,  1.69it/s]Extractor Predicting: 179it [01:42,  1.69it/s]Extractor Predicting: 180it [01:42,  1.76it/s]Extractor Predicting: 181it [01:43,  1.73it/s]Extractor Predicting: 182it [01:43,  1.75it/s]Extractor Predicting: 183it [01:44,  1.75it/s]Extractor Predicting: 184it [01:44,  1.79it/s]Extractor Predicting: 185it [01:45,  1.80it/s]Extractor Predicting: 186it [01:45,  1.82it/s]Extractor Predicting: 187it [01:46,  1.82it/s]Extractor Predicting: 188it [01:46,  1.81it/s]Extractor Predicting: 189it [01:47,  1.80it/s]Extractor Predicting: 190it [01:48,  1.78it/s]Extractor Predicting: 191it [01:48,  1.72it/s]Extractor Predicting: 192it [01:49,  1.74it/s]Extractor Predicting: 193it [01:49,  1.78it/s]Extractor Predicting: 194it [01:50,  1.75it/s]Extractor Predicting: 195it [01:50,  1.76it/s]Extractor Predicting: 196it [01:51,  1.75it/s]Extractor Predicting: 197it [01:52,  1.77it/s]Extractor Predicting: 198it [01:52,  1.58it/s]Extractor Predicting: 199it [01:53,  1.64it/s]Extractor Predicting: 200it [01:54,  1.68it/s]Extractor Predicting: 201it [01:54,  1.71it/s]Extractor Predicting: 202it [01:55,  1.78it/s]Extractor Predicting: 203it [01:55,  1.80it/s]Extractor Predicting: 204it [01:56,  1.80it/s]Extractor Predicting: 205it [01:56,  1.75it/s]Extractor Predicting: 206it [01:57,  1.76it/s]Extractor Predicting: 207it [01:57,  1.79it/s]Extractor Predicting: 208it [01:58,  1.80it/s]Extractor Predicting: 209it [01:59,  1.73it/s]Extractor Predicting: 210it [01:59,  1.72it/s]Extractor Predicting: 211it [02:00,  1.72it/s]Extractor Predicting: 212it [02:00,  1.75it/s]Extractor Predicting: 213it [02:01,  1.77it/s]Extractor Predicting: 214it [02:01,  1.72it/s]Extractor Predicting: 215it [02:02,  1.73it/s]Extractor Predicting: 216it [02:03,  1.76it/s]Extractor Predicting: 217it [02:03,  1.77it/s]Extractor Predicting: 218it [02:04,  1.69it/s]Extractor Predicting: 219it [02:04,  1.71it/s]Extractor Predicting: 220it [02:05,  1.72it/s]Extractor Predicting: 221it [02:06,  1.68it/s]Extractor Predicting: 222it [02:06,  1.69it/s]Extractor Predicting: 223it [02:07,  1.70it/s]Extractor Predicting: 224it [02:07,  1.74it/s]Extractor Predicting: 225it [02:08,  1.74it/s]Extractor Predicting: 226it [02:08,  1.80it/s]Extractor Predicting: 227it [02:09,  1.82it/s]Extractor Predicting: 228it [02:09,  1.79it/s]Extractor Predicting: 229it [02:10,  1.79it/s]Extractor Predicting: 230it [02:11,  1.75it/s]Extractor Predicting: 231it [02:11,  1.74it/s]Extractor Predicting: 232it [02:12,  1.74it/s]Extractor Predicting: 233it [02:12,  1.79it/s]Extractor Predicting: 234it [02:13,  1.77it/s]Extractor Predicting: 235it [02:13,  1.78it/s]Extractor Predicting: 236it [02:14,  1.74it/s]Extractor Predicting: 237it [02:15,  1.74it/s]Extractor Predicting: 238it [02:15,  1.76it/s]Extractor Predicting: 239it [02:16,  1.78it/s]Extractor Predicting: 240it [02:16,  1.77it/s]Extractor Predicting: 241it [02:17,  1.76it/s]Extractor Predicting: 242it [02:17,  1.74it/s]Extractor Predicting: 243it [02:18,  1.70it/s]Extractor Predicting: 244it [02:19,  1.72it/s]Extractor Predicting: 245it [02:19,  1.78it/s]Extractor Predicting: 246it [02:20,  1.75it/s]Extractor Predicting: 247it [02:20,  1.78it/s]Extractor Predicting: 248it [02:21,  1.78it/s]Extractor Predicting: 249it [02:21,  1.78it/s]Extractor Predicting: 250it [02:22,  1.76it/s]Extractor Predicting: 251it [02:23,  1.72it/s]Extractor Predicting: 252it [02:23,  1.73it/s]Extractor Predicting: 253it [02:24,  1.75it/s]Extractor Predicting: 254it [02:24,  1.77it/s]Extractor Predicting: 255it [02:25,  1.76it/s]Extractor Predicting: 256it [02:25,  1.74it/s]Extractor Predicting: 257it [02:26,  1.77it/s]Extractor Predicting: 258it [02:27,  1.76it/s]Extractor Predicting: 259it [02:27,  1.71it/s]Extractor Predicting: 260it [02:28,  1.73it/s]Extractor Predicting: 261it [02:28,  1.75it/s]Extractor Predicting: 262it [02:29,  1.73it/s]Extractor Predicting: 263it [02:30,  1.71it/s]Extractor Predicting: 264it [02:30,  1.70it/s]Extractor Predicting: 265it [02:31,  1.69it/s]Extractor Predicting: 266it [02:31,  1.66it/s]Extractor Predicting: 267it [02:32,  1.64it/s]Extractor Predicting: 268it [02:33,  1.66it/s]Extractor Predicting: 269it [02:33,  1.67it/s]Extractor Predicting: 270it [02:34,  1.66it/s]Extractor Predicting: 271it [02:34,  1.66it/s]Extractor Predicting: 272it [02:35,  1.68it/s]Extractor Predicting: 273it [02:36,  1.67it/s]Extractor Predicting: 274it [02:36,  1.67it/s]Extractor Predicting: 275it [02:37,  1.71it/s]Extractor Predicting: 276it [02:37,  1.71it/s]Extractor Predicting: 277it [02:38,  1.68it/s]Extractor Predicting: 278it [02:38,  1.68it/s]Extractor Predicting: 279it [02:39,  1.68it/s]Extractor Predicting: 280it [02:40,  1.68it/s]Extractor Predicting: 281it [02:40,  1.66it/s]Extractor Predicting: 282it [02:41,  1.69it/s]Extractor Predicting: 283it [02:42,  1.64it/s]Extractor Predicting: 284it [02:42,  1.62it/s]Extractor Predicting: 285it [02:43,  1.62it/s]Extractor Predicting: 286it [02:43,  1.61it/s]Extractor Predicting: 287it [02:44,  1.64it/s]Extractor Predicting: 288it [02:45,  1.43it/s]Extractor Predicting: 288it [02:45,  1.74it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  2.10it/s]Extractor Predicting: 3it [00:01,  1.88it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:34, 15.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:10, 14.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:47<03:14, 16.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:01<02:49, 15.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:17<02:33, 15.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:32<02:18, 15.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:47<02:02, 15.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:05<01:51, 15.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:18<01:31, 15.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:33<01:15, 15.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:47<00:59, 14.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:02<00:44, 14.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:16<00:29, 14.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:31<00:14, 14.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:50<00:00, 16.07s/it]Generating: 100%|██████████| 15/15 [03:50<00:00, 15.38s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.859375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 100, 'raw': 160}
{'target': 600, 'success': 124, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 180, 'raw': 288}
{'target': 600, 'success': 200, 'raw': 320}
{'target': 600, 'success': 218, 'raw': 352}
{'target': 600, 'success': 240, 'raw': 384}
{'target': 600, 'success': 258, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 307, 'raw': 480}
{'target': 600, 'success': 328, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 374, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 418, 'raw': 640}
{'target': 600, 'success': 440, 'raw': 672}
{'target': 600, 'success': 460, 'raw': 704}
{'target': 600, 'success': 487, 'raw': 736}
{'target': 600, 'success': 506, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 569, 'raw': 864}
{'target': 600, 'success': 595, 'raw': 896}
{'target': 600, 'success': 617, 'raw': 928}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.6648706896551724, 'errors': {'', '(\'2012 MTV Video Music Video Awards\', \'nominated for\', \'\', \'He was in a songwriting competition on " The Simpsons " at the 2012 MTV Video Music Video Awards , winning in one of his four categories .\')', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 402, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 579, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.7271634615384616, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : architect .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 496, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 551, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 594, 'raw': 800}
{'target': 600, 'success': 616, 'raw': 832}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7403846153846154, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a new development project called " Bifurcio " , which was developed by the Swiss developers , EMC , for Bifurcio . Head Entity : Bifurcio , Tail Entity : Ericsson .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.80078125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8152173913043478, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.77375, 'errors': {'', '(\'Pristiniki\', \'member of political party\', \'\', \'In 2005 , she became the first female politician to serve in the parliament of Bulgaria , as first and leader of " Pristiniki " in the new parliament .\')', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : operator . Context : Later in the year , the company built a high - speed commuter railway crossing the Bordeaux via Bordeaux Station to Marseille , France . Head Entity : Marseille , Tail Entity : Ralf Rummel .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 76, 'raw': 128}
{'target': 600, 'success': 92, 'raw': 160}
{'target': 600, 'success': 111, 'raw': 192}
{'target': 600, 'success': 130, 'raw': 224}
{'target': 600, 'success': 144, 'raw': 256}
{'target': 600, 'success': 158, 'raw': 288}
{'target': 600, 'success': 176, 'raw': 320}
{'target': 600, 'success': 191, 'raw': 352}
{'target': 600, 'success': 208, 'raw': 384}
{'target': 600, 'success': 226, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 267, 'raw': 480}
{'target': 600, 'success': 284, 'raw': 512}
{'target': 600, 'success': 298, 'raw': 544}
{'target': 600, 'success': 315, 'raw': 576}
{'target': 600, 'success': 332, 'raw': 608}
{'target': 600, 'success': 349, 'raw': 640}
{'target': 600, 'success': 372, 'raw': 672}
{'target': 600, 'success': 388, 'raw': 704}
{'target': 600, 'success': 406, 'raw': 736}
{'target': 600, 'success': 424, 'raw': 768}
{'target': 600, 'success': 439, 'raw': 800}
{'target': 600, 'success': 454, 'raw': 832}
{'target': 600, 'success': 473, 'raw': 864}
{'target': 600, 'success': 490, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 530, 'raw': 960}
{'target': 600, 'success': 551, 'raw': 992}
{'target': 600, 'success': 567, 'raw': 1024}
{'target': 600, 'success': 586, 'raw': 1056}
{'target': 600, 'success': 608, 'raw': 1088}
{'prompt': 'Relation : position held .', 'success_rate': 0.5588235294117647, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 14871
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14971, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:03,  3.37s/it]Extractor Estimating: 2it [00:04,  1.77s/it]Extractor Estimating: 3it [00:05,  1.54s/it]Extractor Estimating: 4it [00:05,  1.19s/it]Extractor Estimating: 5it [00:06,  1.03it/s]Extractor Estimating: 6it [00:07,  1.16it/s]Extractor Estimating: 7it [00:07,  1.26it/s]Extractor Estimating: 8it [00:08,  1.37it/s]Extractor Estimating: 9it [00:09,  1.39it/s]Extractor Estimating: 10it [00:09,  1.48it/s]Extractor Estimating: 11it [00:12,  1.19s/it]Extractor Estimating: 12it [00:12,  1.01s/it]Extractor Estimating: 13it [00:13,  1.08it/s]Extractor Estimating: 14it [00:14,  1.18it/s]Extractor Estimating: 15it [00:14,  1.26it/s]Extractor Estimating: 16it [00:15,  1.31it/s]Extractor Estimating: 17it [00:16,  1.40it/s]Extractor Estimating: 18it [00:16,  1.40it/s]Extractor Estimating: 19it [00:17,  1.50it/s]Extractor Estimating: 20it [00:17,  1.50it/s]Extractor Estimating: 21it [00:18,  1.53it/s]Extractor Estimating: 22it [00:19,  1.59it/s]Extractor Estimating: 23it [00:19,  1.57it/s]Extractor Estimating: 24it [00:20,  1.59it/s]Extractor Estimating: 25it [00:20,  1.62it/s]Extractor Estimating: 26it [00:21,  1.56it/s]Extractor Estimating: 27it [00:22,  1.56it/s]Extractor Estimating: 28it [00:22,  1.60it/s]Extractor Estimating: 29it [00:23,  1.62it/s]Extractor Estimating: 30it [00:24,  1.62it/s]Extractor Estimating: 31it [00:24,  1.65it/s]Extractor Estimating: 32it [00:25,  1.66it/s]Extractor Estimating: 33it [00:25,  1.60it/s]Extractor Estimating: 34it [00:26,  1.52it/s]Extractor Estimating: 35it [00:27,  1.58it/s]Extractor Estimating: 36it [00:27,  1.60it/s]Extractor Estimating: 37it [00:28,  1.49it/s]Extractor Estimating: 38it [00:29,  1.44it/s]Extractor Estimating: 39it [00:30,  1.48it/s]Extractor Estimating: 40it [00:30,  1.49it/s]Extractor Estimating: 41it [00:31,  1.48it/s]Extractor Estimating: 42it [00:32,  1.47it/s]Extractor Estimating: 43it [00:32,  1.49it/s]Extractor Estimating: 44it [00:33,  1.45it/s]Extractor Estimating: 45it [00:34,  1.47it/s]Extractor Estimating: 46it [00:34,  1.52it/s]Extractor Estimating: 47it [00:35,  1.53it/s]Extractor Estimating: 48it [00:36,  1.51it/s]Extractor Estimating: 49it [00:36,  1.56it/s]Extractor Estimating: 50it [00:37,  1.54it/s]Extractor Estimating: 51it [00:38,  1.52it/s]Extractor Estimating: 52it [00:38,  1.51it/s]Extractor Estimating: 53it [00:39,  1.51it/s]Extractor Estimating: 54it [00:40,  1.47it/s]Extractor Estimating: 55it [00:40,  1.44it/s]Extractor Estimating: 56it [00:41,  1.46it/s]Extractor Estimating: 57it [00:42,  1.44it/s]Extractor Estimating: 58it [00:42,  1.46it/s]Extractor Estimating: 59it [00:43,  1.43it/s]Extractor Estimating: 60it [00:44,  1.46it/s]Extractor Estimating: 61it [00:44,  1.43it/s]Extractor Estimating: 62it [00:45,  1.46it/s]Extractor Estimating: 63it [00:46,  1.44it/s]Extractor Estimating: 64it [00:46,  1.48it/s]Extractor Estimating: 65it [00:47,  1.44it/s]Extractor Estimating: 66it [00:48,  1.44it/s]Extractor Estimating: 67it [00:49,  1.44it/s]Extractor Estimating: 68it [00:49,  1.50it/s]Extractor Estimating: 69it [00:50,  1.52it/s]Extractor Estimating: 70it [00:50,  1.52it/s]Extractor Estimating: 71it [00:51,  1.49it/s]Extractor Estimating: 72it [00:52,  1.50it/s]Extractor Estimating: 73it [00:53,  1.49it/s]Extractor Estimating: 74it [00:53,  1.53it/s]Extractor Estimating: 75it [00:54,  1.51it/s]Extractor Estimating: 76it [00:54,  1.56it/s]Extractor Estimating: 77it [00:55,  1.59it/s]Extractor Estimating: 78it [00:56,  1.66it/s]Extractor Estimating: 79it [00:56,  1.67it/s]Extractor Estimating: 80it [00:57,  1.72it/s]Extractor Estimating: 81it [00:57,  1.72it/s]Extractor Estimating: 82it [00:58,  1.69it/s]Extractor Estimating: 83it [00:58,  1.68it/s]Extractor Estimating: 84it [00:59,  1.68it/s]Extractor Estimating: 85it [01:00,  1.64it/s]Extractor Estimating: 86it [01:00,  1.65it/s]Extractor Estimating: 87it [01:01,  1.64it/s]Extractor Estimating: 88it [01:01,  1.70it/s]Extractor Estimating: 89it [01:02,  1.71it/s]Extractor Estimating: 90it [01:03,  1.73it/s]Extractor Estimating: 91it [01:03,  1.68it/s]Extractor Estimating: 92it [01:04,  1.71it/s]Extractor Estimating: 93it [01:04,  1.71it/s]Extractor Estimating: 94it [01:05,  1.60it/s]Extractor Estimating: 95it [01:06,  1.53it/s]Extractor Estimating: 96it [01:06,  1.56it/s]Extractor Estimating: 97it [01:07,  1.59it/s]Extractor Estimating: 98it [01:08,  1.64it/s]Extractor Estimating: 99it [01:08,  1.65it/s]Extractor Estimating: 100it [01:09,  1.70it/s]Extractor Estimating: 101it [01:09,  1.71it/s]Extractor Estimating: 102it [01:10,  1.70it/s]Extractor Estimating: 103it [01:11,  1.71it/s]Extractor Estimating: 104it [01:11,  1.75it/s]Extractor Estimating: 105it [01:12,  1.73it/s]Extractor Estimating: 106it [01:12,  1.70it/s]Extractor Estimating: 107it [01:13,  1.75it/s]Extractor Estimating: 108it [01:13,  1.81it/s]Extractor Estimating: 109it [01:14,  1.78it/s]Extractor Estimating: 110it [01:14,  1.80it/s]Extractor Estimating: 111it [01:15,  1.63it/s]Extractor Estimating: 112it [01:16,  1.68it/s]Extractor Estimating: 113it [01:16,  1.67it/s]Extractor Estimating: 114it [01:17,  1.71it/s]Extractor Estimating: 115it [01:17,  1.72it/s]Extractor Estimating: 116it [01:18,  1.73it/s]Extractor Estimating: 117it [01:19,  1.68it/s]Extractor Estimating: 118it [01:19,  1.69it/s]Extractor Estimating: 119it [01:20,  1.63it/s]Extractor Estimating: 120it [01:20,  1.71it/s]Extractor Estimating: 121it [01:21,  1.67it/s]Extractor Estimating: 122it [01:22,  1.59it/s]Extractor Estimating: 123it [01:22,  1.65it/s]Extractor Estimating: 124it [01:23,  1.69it/s]Extractor Estimating: 125it [01:23,  1.68it/s]Extractor Estimating: 126it [01:24,  1.66it/s]Extractor Estimating: 127it [01:25,  1.63it/s]Extractor Estimating: 128it [01:25,  1.67it/s]Extractor Estimating: 129it [01:26,  1.69it/s]Extractor Estimating: 130it [01:26,  1.70it/s]Extractor Estimating: 131it [01:27,  1.68it/s]Extractor Estimating: 132it [01:28,  1.68it/s]Extractor Estimating: 133it [01:28,  1.64it/s]Extractor Estimating: 134it [01:29,  1.68it/s]Extractor Estimating: 135it [01:30,  1.63it/s]Extractor Estimating: 136it [01:30,  1.61it/s]Extractor Estimating: 137it [01:31,  1.60it/s]Extractor Estimating: 138it [01:31,  1.63it/s]Extractor Estimating: 139it [01:32,  1.63it/s]Extractor Estimating: 140it [01:33,  1.63it/s]Extractor Estimating: 141it [01:33,  1.60it/s]Extractor Estimating: 142it [01:34,  1.62it/s]Extractor Estimating: 143it [01:34,  1.67it/s]Extractor Estimating: 144it [01:35,  1.67it/s]Extractor Estimating: 145it [01:36,  1.68it/s]Extractor Estimating: 146it [01:36,  1.69it/s]Extractor Estimating: 147it [01:37,  1.65it/s]Extractor Estimating: 148it [01:38,  1.59it/s]Extractor Estimating: 149it [01:38,  1.58it/s]Extractor Estimating: 150it [01:39,  1.59it/s]Extractor Estimating: 151it [01:39,  1.55it/s]Extractor Estimating: 152it [01:40,  1.53it/s]Extractor Estimating: 153it [01:41,  1.54it/s]Extractor Estimating: 154it [01:41,  1.58it/s]Extractor Estimating: 155it [01:42,  1.61it/s]Extractor Estimating: 156it [01:43,  1.65it/s]Extractor Estimating: 157it [01:43,  1.69it/s]Extractor Estimating: 158it [01:44,  1.62it/s]Extractor Estimating: 159it [01:44,  1.69it/s]Extractor Estimating: 160it [01:45,  1.66it/s]Extractor Estimating: 161it [01:46,  1.63it/s]Extractor Estimating: 162it [01:46,  1.62it/s]Extractor Estimating: 163it [01:47,  1.55it/s]Extractor Estimating: 164it [01:48,  1.57it/s]Extractor Estimating: 165it [01:48,  1.51it/s]Extractor Estimating: 166it [01:49,  1.51it/s]Extractor Estimating: 167it [01:49,  1.56it/s]Extractor Estimating: 168it [01:50,  1.51it/s]Extractor Estimating: 169it [01:51,  1.52it/s]Extractor Estimating: 170it [01:51,  1.59it/s]Extractor Estimating: 171it [01:52,  1.53it/s]Extractor Estimating: 172it [01:53,  1.54it/s]Extractor Estimating: 173it [01:53,  1.54it/s]Extractor Estimating: 174it [01:54,  1.49it/s]Extractor Estimating: 175it [01:55,  1.53it/s]Extractor Estimating: 176it [01:55,  1.47it/s]Extractor Estimating: 177it [01:56,  1.42it/s]Extractor Estimating: 178it [01:57,  1.45it/s]Extractor Estimating: 179it [01:57,  1.53it/s]Extractor Estimating: 180it [01:58,  1.60it/s]Extractor Estimating: 181it [01:59,  1.58it/s]Extractor Estimating: 182it [01:59,  1.59it/s]Extractor Estimating: 183it [02:00,  1.54it/s]Extractor Estimating: 184it [02:01,  1.48it/s]Extractor Estimating: 185it [02:01,  1.51it/s]Extractor Estimating: 186it [02:02,  1.57it/s]Extractor Estimating: 187it [02:03,  1.59it/s]Extractor Estimating: 188it [02:03,  1.56it/s]Extractor Estimating: 189it [02:04,  1.63it/s]Extractor Estimating: 190it [02:04,  1.65it/s]Extractor Estimating: 191it [02:05,  1.59it/s]Extractor Estimating: 192it [02:06,  1.62it/s]Extractor Estimating: 193it [02:06,  1.58it/s]Extractor Estimating: 194it [02:07,  1.56it/s]Extractor Estimating: 195it [02:08,  1.57it/s]Extractor Estimating: 196it [02:08,  1.59it/s]Extractor Estimating: 197it [02:09,  1.59it/s]Extractor Estimating: 198it [02:09,  1.57it/s]Extractor Estimating: 199it [02:10,  1.52it/s]Extractor Estimating: 200it [02:11,  1.59it/s]Extractor Estimating: 201it [02:11,  1.59it/s]Extractor Estimating: 202it [02:12,  1.56it/s]Extractor Estimating: 203it [02:13,  1.58it/s]Extractor Estimating: 204it [02:13,  1.50it/s]Extractor Estimating: 205it [02:14,  1.51it/s]Extractor Estimating: 206it [02:15,  1.55it/s]Extractor Estimating: 207it [02:15,  1.47it/s]Extractor Estimating: 208it [02:16,  1.48it/s]Extractor Estimating: 209it [02:17,  1.47it/s]Extractor Estimating: 210it [02:17,  1.50it/s]Extractor Estimating: 211it [02:18,  1.55it/s]Extractor Estimating: 212it [02:19,  1.52it/s]Extractor Estimating: 213it [02:19,  1.53it/s]Extractor Estimating: 214it [02:20,  1.50it/s]Extractor Estimating: 215it [02:21,  1.56it/s]Extractor Estimating: 216it [02:21,  1.57it/s]Extractor Estimating: 217it [02:22,  1.54it/s]Extractor Estimating: 218it [02:23,  1.57it/s]Extractor Estimating: 219it [02:23,  1.55it/s]Extractor Estimating: 220it [02:24,  1.56it/s]Extractor Estimating: 221it [02:24,  1.53it/s]Extractor Estimating: 222it [02:25,  1.58it/s]Extractor Estimating: 223it [02:26,  1.62it/s]Extractor Estimating: 224it [02:26,  1.63it/s]Extractor Estimating: 225it [02:27,  1.59it/s]Extractor Estimating: 226it [02:28,  1.55it/s]Extractor Estimating: 227it [02:28,  1.54it/s]Extractor Estimating: 228it [02:29,  1.54it/s]Extractor Estimating: 229it [02:30,  1.50it/s]Extractor Estimating: 230it [02:30,  1.47it/s]Extractor Estimating: 231it [02:31,  1.46it/s]Extractor Estimating: 232it [02:32,  1.48it/s]Extractor Estimating: 233it [02:32,  1.48it/s]Extractor Estimating: 234it [02:33,  1.51it/s]Extractor Estimating: 235it [02:34,  1.48it/s]Extractor Estimating: 236it [02:34,  1.47it/s]Extractor Estimating: 237it [02:35,  1.49it/s]Extractor Estimating: 238it [02:36,  1.45it/s]Extractor Estimating: 239it [02:37,  1.42it/s]Extractor Estimating: 240it [02:37,  1.45it/s]Extractor Estimating: 241it [02:38,  1.47it/s]Extractor Estimating: 242it [02:38,  1.48it/s]Extractor Estimating: 243it [02:39,  1.49it/s]Extractor Estimating: 244it [02:40,  1.46it/s]Extractor Estimating: 245it [02:41,  1.41it/s]Extractor Estimating: 246it [02:41,  1.47it/s]Extractor Estimating: 247it [02:42,  1.47it/s]Extractor Estimating: 248it [02:43,  1.42it/s]Extractor Estimating: 249it [02:43,  1.45it/s]Extractor Estimating: 250it [02:44,  1.48it/s]Extractor Estimating: 251it [02:45,  1.44it/s]Extractor Estimating: 252it [02:45,  1.55it/s]Extractor Estimating: 253it [02:46,  1.51it/s]Extractor Estimating: 254it [02:47,  1.59it/s]Extractor Estimating: 255it [02:47,  1.65it/s]Extractor Estimating: 256it [02:48,  1.67it/s]Extractor Estimating: 257it [02:48,  1.70it/s]Extractor Estimating: 258it [02:49,  1.68it/s]Extractor Estimating: 259it [02:49,  1.66it/s]Extractor Estimating: 260it [02:50,  1.59it/s]Extractor Estimating: 261it [02:51,  1.61it/s]Extractor Estimating: 262it [02:51,  1.67it/s]Extractor Estimating: 263it [02:52,  1.66it/s]Extractor Estimating: 264it [02:52,  1.71it/s]Extractor Estimating: 265it [02:53,  1.70it/s]Extractor Estimating: 266it [02:54,  1.77it/s]Extractor Estimating: 267it [02:54,  1.76it/s]Extractor Estimating: 268it [02:55,  1.76it/s]Extractor Estimating: 269it [02:55,  1.76it/s]Extractor Estimating: 270it [02:56,  1.72it/s]Extractor Estimating: 271it [02:56,  1.72it/s]Extractor Estimating: 272it [02:57,  1.71it/s]Extractor Estimating: 273it [02:58,  1.75it/s]Extractor Estimating: 274it [02:58,  1.72it/s]Extractor Estimating: 275it [02:59,  1.74it/s]Extractor Estimating: 276it [02:59,  1.69it/s]Extractor Estimating: 277it [03:00,  1.62it/s]Extractor Estimating: 278it [03:01,  1.60it/s]Extractor Estimating: 279it [03:01,  1.57it/s]Extractor Estimating: 280it [03:02,  1.60it/s]Extractor Estimating: 281it [03:03,  1.61it/s]Extractor Estimating: 282it [03:03,  1.54it/s]Extractor Estimating: 283it [03:04,  1.56it/s]Extractor Estimating: 284it [03:05,  1.52it/s]Extractor Estimating: 285it [03:05,  1.53it/s]Extractor Estimating: 286it [03:06,  1.57it/s]Extractor Estimating: 287it [03:06,  1.58it/s]Extractor Estimating: 288it [03:07,  1.64it/s]Extractor Estimating: 289it [03:08,  1.61it/s]Extractor Estimating: 290it [03:08,  1.64it/s]Extractor Estimating: 291it [03:09,  1.64it/s]Extractor Estimating: 292it [03:09,  1.66it/s]Extractor Estimating: 293it [03:10,  1.63it/s]Extractor Estimating: 294it [03:11,  1.60it/s]Extractor Estimating: 295it [03:11,  1.59it/s]Extractor Estimating: 296it [03:12,  1.59it/s]Extractor Estimating: 297it [03:13,  1.59it/s]Extractor Estimating: 298it [03:13,  1.60it/s]Extractor Estimating: 299it [03:14,  1.60it/s]Extractor Estimating: 300it [03:15,  1.60it/s]Extractor Estimating: 301it [03:15,  1.59it/s]Extractor Estimating: 302it [03:16,  1.55it/s]Extractor Estimating: 303it [03:16,  1.54it/s]Extractor Estimating: 304it [03:17,  1.55it/s]Extractor Estimating: 305it [03:18,  1.54it/s]Extractor Estimating: 306it [03:18,  1.59it/s]Extractor Estimating: 307it [03:19,  1.59it/s]Extractor Estimating: 308it [03:20,  1.61it/s]Extractor Estimating: 309it [03:20,  1.60it/s]Extractor Estimating: 310it [03:21,  1.58it/s]Extractor Estimating: 311it [03:22,  1.53it/s]Extractor Estimating: 312it [03:22,  1.46it/s]Extractor Estimating: 313it [03:23,  1.43it/s]Extractor Estimating: 314it [03:24,  1.49it/s]Extractor Estimating: 315it [03:24,  1.49it/s]Extractor Estimating: 316it [03:25,  1.52it/s]Extractor Estimating: 317it [03:26,  1.51it/s]Extractor Estimating: 318it [03:26,  1.53it/s]Extractor Estimating: 319it [03:27,  1.58it/s]Extractor Estimating: 320it [03:27,  1.62it/s]Extractor Estimating: 321it [03:28,  1.59it/s]Extractor Estimating: 322it [03:29,  1.55it/s]Extractor Estimating: 323it [03:29,  1.55it/s]Extractor Estimating: 324it [03:30,  1.55it/s]Extractor Estimating: 325it [03:31,  1.52it/s]Extractor Estimating: 326it [03:31,  1.55it/s]Extractor Estimating: 327it [03:32,  1.50it/s]Extractor Estimating: 328it [03:33,  1.49it/s]Extractor Estimating: 329it [03:33,  1.47it/s]Extractor Estimating: 330it [03:34,  1.47it/s]Extractor Estimating: 331it [03:35,  1.47it/s]Extractor Estimating: 332it [03:35,  1.55it/s]Extractor Estimating: 333it [03:36,  1.55it/s]Extractor Estimating: 334it [03:37,  1.58it/s]Extractor Estimating: 335it [03:38,  1.41it/s]Extractor Estimating: 336it [03:38,  1.45it/s]Extractor Estimating: 337it [03:39,  1.43it/s]Extractor Estimating: 338it [03:40,  1.46it/s]Extractor Estimating: 339it [03:40,  1.49it/s]Extractor Estimating: 340it [03:41,  1.55it/s]Extractor Estimating: 341it [03:41,  1.57it/s]Extractor Estimating: 342it [03:42,  1.57it/s]Extractor Estimating: 343it [03:43,  1.58it/s]Extractor Estimating: 344it [03:43,  1.54it/s]Extractor Estimating: 345it [03:44,  1.53it/s]Extractor Estimating: 346it [03:45,  1.43it/s]Extractor Estimating: 347it [03:45,  1.46it/s]Extractor Estimating: 348it [03:46,  1.53it/s]Extractor Estimating: 349it [03:47,  1.54it/s]Extractor Estimating: 350it [03:47,  1.51it/s]Extractor Estimating: 351it [03:48,  1.54it/s]Extractor Estimating: 352it [03:49,  1.52it/s]Extractor Estimating: 353it [03:49,  1.57it/s]Extractor Estimating: 354it [03:50,  1.58it/s]Extractor Estimating: 355it [03:51,  1.58it/s]Extractor Estimating: 356it [03:51,  1.55it/s]Extractor Estimating: 357it [03:52,  1.59it/s]Extractor Estimating: 358it [03:52,  1.60it/s]Extractor Estimating: 359it [03:53,  1.62it/s]Extractor Estimating: 360it [03:54,  1.58it/s]Extractor Estimating: 361it [03:54,  1.59it/s]Extractor Estimating: 362it [03:55,  1.63it/s]Extractor Estimating: 363it [03:56,  1.59it/s]Extractor Estimating: 364it [03:56,  1.59it/s]Extractor Estimating: 365it [03:57,  1.59it/s]Extractor Estimating: 366it [03:57,  1.56it/s]Extractor Estimating: 367it [03:58,  1.55it/s]Extractor Estimating: 368it [03:59,  1.59it/s]Extractor Estimating: 369it [03:59,  1.62it/s]Extractor Estimating: 370it [04:00,  1.60it/s]Extractor Estimating: 371it [04:01,  1.59it/s]Extractor Estimating: 372it [04:01,  1.67it/s]Extractor Estimating: 373it [04:02,  1.69it/s]Extractor Estimating: 374it [04:02,  1.70it/s]Extractor Estimating: 375it [04:03,  1.62it/s]Extractor Estimating: 375it [04:03,  1.54it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7874 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 26039
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26139, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26139, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.348, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.037, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.007, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 71, avg_time 1.016, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 171, avg_time 1.016, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 271, avg_time 2.043, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 42, avg_time 0.994, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 142, avg_time 1.029, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 242, avg_time 1.020, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 13, avg_time 1.005, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 113, avg_time 2.043, loss:nan
g_step 1200, step 213, avg_time 1.014, loss:nan
g_step 1300, step 313, avg_time 1.016, loss:nan
g_step 1400, step 84, avg_time 1.006, loss:nan
g_step 1500, step 184, avg_time 1.021, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 284, avg_time 2.037, loss:nan
g_step 1700, step 55, avg_time 0.998, loss:nan
g_step 1800, step 155, avg_time 1.001, loss:nan
g_step 1900, step 255, avg_time 1.041, loss:nan
g_step 2000, step 26, avg_time 1.021, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 126, avg_time 2.042, loss:nan
g_step 2200, step 226, avg_time 1.009, loss:nan
g_step 2300, step 326, avg_time 1.017, loss:nan
g_step 2400, step 97, avg_time 1.008, loss:nan
g_step 2500, step 197, avg_time 1.037, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 297, avg_time 2.034, loss:nan
g_step 2700, step 68, avg_time 0.999, loss:nan
g_step 2800, step 168, avg_time 1.012, loss:nan
g_step 2900, step 268, avg_time 1.023, loss:nan
g_step 3000, step 39, avg_time 1.029, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 139, avg_time 2.044, loss:nan
g_step 3200, step 239, avg_time 1.034, loss:nan
g_step 3300, step 10, avg_time 0.996, loss:nan
g_step 3400, step 110, avg_time 1.031, loss:nan
g_step 3500, step 210, avg_time 1.014, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 310, avg_time 2.043, loss:nan
g_step 3700, step 81, avg_time 1.015, loss:nan
g_step 3800, step 181, avg_time 1.017, loss:nan
g_step 3900, step 281, avg_time 1.016, loss:nan
g_step 4000, step 52, avg_time 0.995, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 152, avg_time 2.047, loss:nan
g_step 4200, step 252, avg_time 1.029, loss:nan
g_step 4300, step 23, avg_time 0.991, loss:nan
g_step 4400, step 123, avg_time 1.014, loss:nan
g_step 4500, step 223, avg_time 1.018, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 323, avg_time 2.043, loss:nan
g_step 4700, step 94, avg_time 1.007, loss:nan
g_step 4800, step 194, avg_time 1.016, loss:nan
g_step 4900, step 294, avg_time 0.998, loss:nan
g_step 5000, step 65, avg_time 1.008, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 165, avg_time 2.034, loss:nan
g_step 5200, step 265, avg_time 1.019, loss:nan
g_step 5300, step 36, avg_time 1.018, loss:nan
g_step 5400, step 136, avg_time 1.018, loss:nan
g_step 5500, step 236, avg_time 1.016, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 7, avg_time 2.041, loss:nan
g_step 5700, step 107, avg_time 1.019, loss:nan
g_step 5800, step 207, avg_time 1.027, loss:nan
g_step 5900, step 307, avg_time 1.011, loss:nan
g_step 6000, step 78, avg_time 1.005, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 178, avg_time 2.050, loss:nan
g_step 6200, step 278, avg_time 1.022, loss:nan
g_step 6300, step 49, avg_time 1.018, loss:nan
g_step 6400, step 149, avg_time 1.023, loss:nan
g_step 6500, step 249, avg_time 1.020, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 02:27:40 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 02:27:40 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_02-27-40_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 02:27:41 - WARNING - datasets.builder -   Using custom data configuration default-7288c74d5857ef4f
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7288c74d5857ef4f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 02:27:42,006 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:27:42,007 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:27:42,007 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:27:42,008 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:27:42,021 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:27:42,027 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:27:42,027 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:27:42,027 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:27:42,027 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:27:42,027 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:27:42,027 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 02:27:42,175 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:27:45,208 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 02:27:45,212 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7288c74d5857ef4f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 02:27:45 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1455b20b0170> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.95ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.82ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.16ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.32ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.43ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.48ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.52ba/s]100%|██████████| 8/8 [00:01<00:00,  4.70ba/s]100%|██████████| 8/8 [00:01<00:00,  4.38ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.08ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.32ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.40ba/s]100%|██████████| 4/4 [00:00<00:00,  4.33ba/s]100%|██████████| 4/4 [00:00<00:00,  4.32ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.31ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.93ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 11.11ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 11.29ba/s]100%|██████████| 8/8 [00:00<00:00, 11.32ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.15ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.52ba/s]100%|██████████| 4/4 [00:00<00:00, 11.87ba/s]
[INFO|trainer.py:414] 2023-08-29 02:27:49,388 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 02:27:49,399 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 02:27:49,399 >>   Num examples = 7900
[INFO|trainer.py:1149] 2023-08-29 02:27:49,400 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 02:27:49,400 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 02:27:49,400 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 02:27:49,400 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 02:27:49,400 >>   Total optimization steps = 615
  0%|          | 0/615 [00:00<?, ?it/s]  0%|          | 1/615 [00:00<03:00,  3.41it/s]  0%|          | 2/615 [00:00<02:53,  3.54it/s]  0%|          | 3/615 [00:00<02:50,  3.58it/s]  1%|          | 4/615 [00:01<02:49,  3.60it/s]  1%|          | 5/615 [00:01<02:48,  3.62it/s]  1%|          | 6/615 [00:01<02:48,  3.62it/s]  1%|          | 7/615 [00:01<02:47,  3.63it/s]  1%|▏         | 8/615 [00:02<02:47,  3.63it/s]  1%|▏         | 9/615 [00:02<02:47,  3.62it/s]  2%|▏         | 10/615 [00:02<02:46,  3.62it/s]  2%|▏         | 11/615 [00:03<02:46,  3.63it/s]  2%|▏         | 12/615 [00:03<02:46,  3.63it/s]  2%|▏         | 13/615 [00:03<02:45,  3.63it/s]  2%|▏         | 14/615 [00:03<02:45,  3.63it/s]  2%|▏         | 15/615 [00:04<02:45,  3.63it/s]  3%|▎         | 16/615 [00:04<02:44,  3.63it/s]  3%|▎         | 17/615 [00:04<02:44,  3.63it/s]  3%|▎         | 18/615 [00:04<02:44,  3.63it/s]  3%|▎         | 19/615 [00:05<02:43,  3.63it/s]  3%|▎         | 20/615 [00:05<02:44,  3.61it/s]  3%|▎         | 21/615 [00:05<02:44,  3.62it/s]  4%|▎         | 22/615 [00:06<02:43,  3.62it/s]  4%|▎         | 23/615 [00:06<02:43,  3.62it/s]  4%|▍         | 24/615 [00:06<02:43,  3.63it/s]  4%|▍         | 25/615 [00:06<02:42,  3.63it/s]  4%|▍         | 26/615 [00:07<02:42,  3.63it/s]  4%|▍         | 27/615 [00:07<02:42,  3.63it/s]  5%|▍         | 28/615 [00:07<02:41,  3.63it/s]  5%|▍         | 29/615 [00:08<02:41,  3.63it/s]  5%|▍         | 30/615 [00:08<02:41,  3.63it/s]  5%|▌         | 31/615 [00:08<02:41,  3.61it/s]  5%|▌         | 32/615 [00:08<02:41,  3.61it/s]  5%|▌         | 33/615 [00:09<02:40,  3.62it/s]  6%|▌         | 34/615 [00:09<02:40,  3.62it/s]  6%|▌         | 35/615 [00:09<02:40,  3.62it/s]  6%|▌         | 36/615 [00:09<02:39,  3.62it/s]  6%|▌         | 37/615 [00:10<02:39,  3.62it/s]  6%|▌         | 38/615 [00:10<02:39,  3.63it/s]  6%|▋         | 39/615 [00:10<02:38,  3.63it/s]  7%|▋         | 40/615 [00:11<02:38,  3.63it/s]  7%|▋         | 41/615 [00:11<02:38,  3.62it/s]  7%|▋         | 42/615 [00:11<02:39,  3.60it/s]  7%|▋         | 43/615 [00:11<02:38,  3.61it/s]  7%|▋         | 44/615 [00:12<02:38,  3.61it/s]  7%|▋         | 45/615 [00:12<02:37,  3.61it/s]  7%|▋         | 46/615 [00:12<02:37,  3.62it/s]  8%|▊         | 47/615 [00:12<02:36,  3.62it/s]  8%|▊         | 48/615 [00:13<02:36,  3.62it/s]  8%|▊         | 49/615 [00:13<02:36,  3.62it/s]  8%|▊         | 50/615 [00:13<02:35,  3.62it/s]  8%|▊         | 51/615 [00:14<02:35,  3.62it/s]  8%|▊         | 52/615 [00:14<02:35,  3.62it/s]  9%|▊         | 53/615 [00:14<02:35,  3.60it/s]  9%|▉         | 54/615 [00:14<02:35,  3.61it/s]  9%|▉         | 55/615 [00:15<02:35,  3.61it/s]  9%|▉         | 56/615 [00:15<02:34,  3.62it/s]  9%|▉         | 57/615 [00:15<02:34,  3.62it/s]  9%|▉         | 58/615 [00:16<02:33,  3.62it/s] 10%|▉         | 59/615 [00:16<02:33,  3.62it/s] 10%|▉         | 60/615 [00:16<02:33,  3.62it/s] 10%|▉         | 61/615 [00:16<02:32,  3.63it/s] 10%|█         | 62/615 [00:17<02:32,  3.63it/s] 10%|█         | 63/615 [00:17<02:32,  3.62it/s] 10%|█         | 64/615 [00:17<02:32,  3.61it/s] 11%|█         | 65/615 [00:17<02:32,  3.61it/s] 11%|█         | 66/615 [00:18<02:31,  3.61it/s] 11%|█         | 67/615 [00:18<02:31,  3.62it/s] 11%|█         | 68/615 [00:18<02:31,  3.62it/s] 11%|█         | 69/615 [00:19<02:30,  3.62it/s] 11%|█▏        | 70/615 [00:19<02:30,  3.62it/s] 12%|█▏        | 71/615 [00:19<02:30,  3.62it/s] 12%|█▏        | 72/615 [00:19<02:29,  3.62it/s] 12%|█▏        | 73/615 [00:20<02:29,  3.62it/s] 12%|█▏        | 74/615 [00:20<02:29,  3.62it/s] 12%|█▏        | 75/615 [00:20<02:29,  3.61it/s] 12%|█▏        | 76/615 [00:21<02:29,  3.61it/s] 13%|█▎        | 77/615 [00:21<02:28,  3.62it/s] 13%|█▎        | 78/615 [00:21<02:28,  3.62it/s] 13%|█▎        | 79/615 [00:21<02:28,  3.62it/s] 13%|█▎        | 80/615 [00:22<02:27,  3.62it/s] 13%|█▎        | 81/615 [00:22<02:27,  3.62it/s] 13%|█▎        | 82/615 [00:22<02:27,  3.62it/s] 13%|█▎        | 83/615 [00:22<02:26,  3.62it/s] 14%|█▎        | 84/615 [00:23<02:26,  3.63it/s] 14%|█▍        | 85/615 [00:23<02:26,  3.63it/s] 14%|█▍        | 86/615 [00:23<02:25,  3.62it/s] 14%|█▍        | 87/615 [00:24<02:25,  3.62it/s] 14%|█▍        | 88/615 [00:24<02:25,  3.62it/s] 14%|█▍        | 89/615 [00:24<02:25,  3.62it/s] 15%|█▍        | 90/615 [00:24<02:24,  3.62it/s] 15%|█▍        | 91/615 [00:25<02:24,  3.62it/s] 15%|█▍        | 92/615 [00:25<02:24,  3.61it/s] 15%|█▌        | 93/615 [00:25<02:24,  3.61it/s] 15%|█▌        | 94/615 [00:25<02:24,  3.61it/s] 15%|█▌        | 95/615 [00:26<02:23,  3.61it/s] 16%|█▌        | 96/615 [00:26<02:23,  3.62it/s] 16%|█▌        | 97/615 [00:26<02:23,  3.62it/s] 16%|█▌        | 98/615 [00:27<02:22,  3.62it/s] 16%|█▌        | 99/615 [00:27<02:22,  3.62it/s] 16%|█▋        | 100/615 [00:27<02:22,  3.62it/s] 16%|█▋        | 101/615 [00:27<02:21,  3.62it/s] 17%|█▋        | 102/615 [00:28<02:21,  3.62it/s] 17%|█▋        | 103/615 [00:28<02:21,  3.62it/s] 17%|█▋        | 104/615 [00:28<02:21,  3.62it/s] 17%|█▋        | 105/615 [00:29<02:20,  3.62it/s] 17%|█▋        | 106/615 [00:29<02:20,  3.62it/s] 17%|█▋        | 107/615 [00:29<02:20,  3.62it/s] 18%|█▊        | 108/615 [00:29<02:19,  3.62it/s] 18%|█▊        | 109/615 [00:30<02:19,  3.62it/s] 18%|█▊        | 110/615 [00:30<02:20,  3.61it/s] 18%|█▊        | 111/615 [00:30<02:19,  3.61it/s] 18%|█▊        | 112/615 [00:30<02:19,  3.61it/s] 18%|█▊        | 113/615 [00:31<02:18,  3.62it/s] 19%|█▊        | 114/615 [00:31<02:18,  3.62it/s] 19%|█▊        | 115/615 [00:31<02:18,  3.62it/s] 19%|█▉        | 116/615 [00:32<02:17,  3.62it/s] 19%|█▉        | 117/615 [00:32<02:17,  3.62it/s] 19%|█▉        | 118/615 [00:32<02:17,  3.62it/s] 19%|█▉        | 119/615 [00:32<02:16,  3.62it/s] 20%|█▉        | 120/615 [00:33<02:16,  3.62it/s] 20%|█▉        | 121/615 [00:33<02:16,  3.62it/s] 20%|█▉        | 122/615 [00:33<02:16,  3.62it/s] 20%|██        | 123/615 [00:33<02:16,  3.62it/s][INFO|trainer.py:2140] 2023-08-29 02:28:23,511 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:28:23,511 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 02:28:23,511 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.93it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.46it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.28it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.36it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.71it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.31it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.06it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.97it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.02it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.11it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.29it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.20it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.03it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.92it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.86it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.79it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.89it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.97it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.06it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.12it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.17it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.05it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.88it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.83it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.82it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.87it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.05it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.19it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.11it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.09it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.07it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.95it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.81it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.86it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.85it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.04it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.12it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 42.87it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.35it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.52it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.55it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.56it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.67it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.74it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.98it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.94it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.02it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.04it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.03it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.85it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.86it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.84it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.80it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.76it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.99it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.02it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.05it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.04it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.87it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.78it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.82it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.94it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.92it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.10it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.10it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.05it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.01it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.87it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.72it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.74it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.92it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.87it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.08it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.11it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.10it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.01it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.85it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.66it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.88it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.96it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.03it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.12it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.19it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.20it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.95it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.89it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.90it/s][A                                                 
                                                 [A 20%|██        | 123/615 [00:44<02:16,  3.62it/s]
100%|██████████| 437/437 [00:09<00:00, 43.90it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:28:33,468 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-123
[INFO|configuration_utils.py:351] 2023-08-29 02:28:33,488 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-123/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:28:35,343 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-123/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:28:35,365 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-123/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:28:35,376 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-123/special_tokens_map.json
 20%|██        | 124/615 [00:46<32:25,  3.96s/it] 20%|██        | 125/615 [00:46<23:20,  2.86s/it] 20%|██        | 126/615 [00:47<16:59,  2.09s/it] 21%|██        | 127/615 [00:47<12:33,  1.54s/it] 21%|██        | 128/615 [00:47<09:27,  1.16s/it] 21%|██        | 129/615 [00:47<07:17,  1.11it/s] 21%|██        | 130/615 [00:48<05:46,  1.40it/s] 21%|██▏       | 131/615 [00:48<04:42,  1.71it/s] 21%|██▏       | 132/615 [00:48<03:58,  2.03it/s] 22%|██▏       | 133/615 [00:49<03:26,  2.33it/s] 22%|██▏       | 134/615 [00:49<03:05,  2.60it/s] 22%|██▏       | 135/615 [00:49<02:49,  2.84it/s] 22%|██▏       | 136/615 [00:49<02:37,  3.03it/s] 22%|██▏       | 137/615 [00:50<02:29,  3.19it/s] 22%|██▏       | 138/615 [00:50<02:24,  3.31it/s] 23%|██▎       | 139/615 [00:50<02:20,  3.40it/s] 23%|██▎       | 140/615 [00:51<02:17,  3.46it/s] 23%|██▎       | 141/615 [00:51<02:15,  3.51it/s] 23%|██▎       | 142/615 [00:51<02:13,  3.54it/s] 23%|██▎       | 143/615 [00:51<02:12,  3.56it/s] 23%|██▎       | 144/615 [00:52<02:11,  3.58it/s] 24%|██▎       | 145/615 [00:52<02:11,  3.58it/s] 24%|██▎       | 146/615 [00:52<02:10,  3.59it/s] 24%|██▍       | 147/615 [00:52<02:10,  3.60it/s] 24%|██▍       | 148/615 [00:53<02:09,  3.61it/s] 24%|██▍       | 149/615 [00:53<02:09,  3.61it/s] 24%|██▍       | 150/615 [00:53<02:08,  3.61it/s] 25%|██▍       | 151/615 [00:54<02:08,  3.61it/s] 25%|██▍       | 152/615 [00:54<02:08,  3.61it/s] 25%|██▍       | 153/615 [00:54<02:08,  3.60it/s] 25%|██▌       | 154/615 [00:54<02:08,  3.60it/s] 25%|██▌       | 155/615 [00:55<02:07,  3.60it/s] 25%|██▌       | 156/615 [00:55<02:07,  3.59it/s] 26%|██▌       | 157/615 [00:55<02:07,  3.59it/s] 26%|██▌       | 158/615 [00:56<02:07,  3.60it/s] 26%|██▌       | 159/615 [00:56<02:06,  3.59it/s] 26%|██▌       | 160/615 [00:56<02:06,  3.59it/s] 26%|██▌       | 161/615 [00:56<02:06,  3.59it/s] 26%|██▋       | 162/615 [00:57<02:06,  3.60it/s] 27%|██▋       | 163/615 [00:57<02:05,  3.60it/s] 27%|██▋       | 164/615 [00:57<02:05,  3.60it/s] 27%|██▋       | 165/615 [00:57<02:05,  3.60it/s] 27%|██▋       | 166/615 [00:58<02:04,  3.60it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/615 [00:58<02:06,  3.53it/s] 27%|██▋       | 168/615 [00:58<02:06,  3.54it/s] 27%|██▋       | 169/615 [00:59<02:05,  3.56it/s] 28%|██▊       | 170/615 [00:59<02:04,  3.58it/s] 28%|██▊       | 171/615 [00:59<02:03,  3.59it/s] 28%|██▊       | 172/615 [00:59<02:03,  3.60it/s] 28%|██▊       | 173/615 [01:00<02:02,  3.60it/s] 28%|██▊       | 174/615 [01:00<02:02,  3.61it/s] 28%|██▊       | 175/615 [01:00<02:01,  3.61it/s] 29%|██▊       | 176/615 [01:01<02:01,  3.61it/s] 29%|██▉       | 177/615 [01:01<02:01,  3.61it/s] 29%|██▉       | 178/615 [01:01<02:01,  3.61it/s] 29%|██▉       | 179/615 [01:01<02:00,  3.61it/s] 29%|██▉       | 180/615 [01:02<02:00,  3.61it/s] 29%|██▉       | 181/615 [01:02<02:00,  3.61it/s] 30%|██▉       | 182/615 [01:02<01:59,  3.61it/s] 30%|██▉       | 183/615 [01:02<01:59,  3.62it/s] 30%|██▉       | 184/615 [01:03<01:59,  3.62it/s] 30%|███       | 185/615 [01:03<01:58,  3.62it/s] 30%|███       | 186/615 [01:03<01:58,  3.62it/s] 30%|███       | 187/615 [01:04<01:58,  3.62it/s] 31%|███       | 188/615 [01:04<01:58,  3.62it/s] 31%|███       | 189/615 [01:04<01:58,  3.59it/s] 31%|███       | 190/615 [01:04<01:58,  3.60it/s] 31%|███       | 191/615 [01:05<01:57,  3.61it/s] 31%|███       | 192/615 [01:05<01:57,  3.61it/s] 31%|███▏      | 193/615 [01:05<01:56,  3.61it/s] 32%|███▏      | 194/615 [01:06<01:56,  3.61it/s] 32%|███▏      | 195/615 [01:06<01:56,  3.61it/s] 32%|███▏      | 196/615 [01:06<01:56,  3.61it/s] 32%|███▏      | 197/615 [01:06<01:55,  3.62it/s] 32%|███▏      | 198/615 [01:07<01:55,  3.62it/s] 32%|███▏      | 199/615 [01:07<01:54,  3.62it/s] 33%|███▎      | 200/615 [01:07<01:55,  3.60it/s] 33%|███▎      | 201/615 [01:07<01:54,  3.60it/s] 33%|███▎      | 202/615 [01:08<01:54,  3.61it/s] 33%|███▎      | 203/615 [01:08<01:54,  3.61it/s] 33%|███▎      | 204/615 [01:08<01:53,  3.61it/s] 33%|███▎      | 205/615 [01:09<01:53,  3.61it/s] 33%|███▎      | 206/615 [01:09<01:53,  3.61it/s] 34%|███▎      | 207/615 [01:09<01:52,  3.61it/s] 34%|███▍      | 208/615 [01:09<01:52,  3.61it/s] 34%|███▍      | 209/615 [01:10<01:52,  3.62it/s] 34%|███▍      | 210/615 [01:10<01:51,  3.62it/s] 34%|███▍      | 211/615 [01:10<01:51,  3.61it/s] 34%|███▍      | 212/615 [01:10<01:51,  3.61it/s] 35%|███▍      | 213/615 [01:11<01:51,  3.61it/s] 35%|███▍      | 214/615 [01:11<01:50,  3.62it/s] 35%|███▍      | 215/615 [01:11<01:50,  3.61it/s] 35%|███▌      | 216/615 [01:12<01:50,  3.61it/s] 35%|███▌      | 217/615 [01:12<01:50,  3.61it/s] 35%|███▌      | 218/615 [01:12<01:49,  3.61it/s] 36%|███▌      | 219/615 [01:12<01:49,  3.62it/s] 36%|███▌      | 220/615 [01:13<01:49,  3.61it/s] 36%|███▌      | 221/615 [01:13<01:49,  3.61it/s] 36%|███▌      | 222/615 [01:13<01:48,  3.61it/s] 36%|███▋      | 223/615 [01:14<01:48,  3.62it/s] 36%|███▋      | 224/615 [01:14<01:48,  3.62it/s] 37%|███▋      | 225/615 [01:14<01:47,  3.62it/s] 37%|███▋      | 226/615 [01:14<01:47,  3.62it/s] 37%|███▋      | 227/615 [01:15<01:47,  3.62it/s] 37%|███▋      | 228/615 [01:15<01:46,  3.62it/s] 37%|███▋      | 229/615 [01:15<01:46,  3.62it/s] 37%|███▋      | 230/615 [01:15<01:46,  3.62it/s] 38%|███▊      | 231/615 [01:16<01:46,  3.62it/s] 38%|███▊      | 232/615 [01:16<01:46,  3.60it/s] 38%|███▊      | 233/615 [01:16<01:46,  3.60it/s] 38%|███▊      | 234/615 [01:17<01:45,  3.60it/s] 38%|███▊      | 235/615 [01:17<01:45,  3.61it/s] 38%|███▊      | 236/615 [01:17<01:44,  3.61it/s] 39%|███▊      | 237/615 [01:17<01:44,  3.62it/s] 39%|███▊      | 238/615 [01:18<01:44,  3.62it/s] 39%|███▉      | 239/615 [01:18<01:44,  3.61it/s] 39%|███▉      | 240/615 [01:18<01:43,  3.61it/s] 39%|███▉      | 241/615 [01:19<01:43,  3.61it/s] 39%|███▉      | 242/615 [01:19<01:43,  3.61it/s] 40%|███▉      | 243/615 [01:19<01:43,  3.61it/s] 40%|███▉      | 244/615 [01:19<01:42,  3.61it/s] 40%|███▉      | 245/615 [01:20<01:42,  3.62it/s] 40%|████      | 246/615 [01:20<01:42,  3.61it/s][INFO|trainer.py:2140] 2023-08-29 02:29:09,925 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:29:09,925 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 02:29:09,925 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9424, 'eval_samples_per_second': 351.323, 'eval_steps_per_second': 43.953, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.48it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.27it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.52it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.42it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.70it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.24it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.01it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.82it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.01it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.12it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.25it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.32it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.14it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.90it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.69it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.63it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.70it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.86it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.03it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.20it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.10it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.04it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.81it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.68it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.62it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.73it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.95it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.03it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.21it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.15it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.90it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.67it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.68it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.64it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.80it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.98it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.10it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.13it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.13it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.87it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.66it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.57it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.75it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.85it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.97it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.10it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.13it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.08it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.94it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.77it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.59it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 43.72it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.88it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.01it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.06it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.15it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.94it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.85it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.79it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.80it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.80it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.95it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.93it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.08it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.14it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.07it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.94it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.72it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.85it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.80it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.90it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.03it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.99it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.97it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.95it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.88it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.80it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.80it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.93it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.98it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.08it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.06it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.01it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.95it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.90it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.83it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.74it/s][A                                                 
                                                 [A 40%|████      | 246/615 [01:30<01:42,  3.61it/s]
100%|██████████| 437/437 [00:09<00:00, 43.74it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:29:19,894 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-29 02:29:19,919 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:29:21,705 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:29:21,721 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:29:21,737 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-246/special_tokens_map.json
 40%|████      | 247/615 [01:32<24:19,  3.96s/it] 40%|████      | 248/615 [01:33<17:29,  2.86s/it] 40%|████      | 249/615 [01:33<12:43,  2.09s/it] 41%|████      | 250/615 [01:33<09:23,  1.54s/it] 41%|████      | 251/615 [01:34<07:04,  1.16s/it] 41%|████      | 252/615 [01:34<05:26,  1.11it/s] 41%|████      | 253/615 [01:34<04:18,  1.40it/s] 41%|████▏     | 254/615 [01:34<03:30,  1.71it/s] 41%|████▏     | 255/615 [01:35<02:57,  2.03it/s] 42%|████▏     | 256/615 [01:35<02:34,  2.33it/s] 42%|████▏     | 257/615 [01:35<02:18,  2.59it/s] 42%|████▏     | 258/615 [01:36<02:06,  2.82it/s] 42%|████▏     | 259/615 [01:36<01:58,  3.01it/s] 42%|████▏     | 260/615 [01:36<01:52,  3.16it/s] 42%|████▏     | 261/615 [01:36<01:48,  3.27it/s] 43%|████▎     | 262/615 [01:37<01:45,  3.35it/s] 43%|████▎     | 263/615 [01:37<01:43,  3.42it/s] 43%|████▎     | 264/615 [01:37<01:41,  3.46it/s] 43%|████▎     | 265/615 [01:38<01:40,  3.49it/s] 43%|████▎     | 266/615 [01:38<01:39,  3.51it/s] 43%|████▎     | 267/615 [01:38<01:38,  3.53it/s] 44%|████▎     | 268/615 [01:38<01:38,  3.53it/s] 44%|████▎     | 269/615 [01:39<01:37,  3.53it/s] 44%|████▍     | 270/615 [01:39<01:37,  3.54it/s] 44%|████▍     | 271/615 [01:39<01:36,  3.55it/s] 44%|████▍     | 272/615 [01:39<01:36,  3.55it/s] 44%|████▍     | 273/615 [01:40<01:36,  3.55it/s] 45%|████▍     | 274/615 [01:40<01:35,  3.57it/s] 45%|████▍     | 275/615 [01:40<01:34,  3.58it/s] 45%|████▍     | 276/615 [01:41<01:34,  3.59it/s] 45%|████▌     | 277/615 [01:41<01:33,  3.60it/s] 45%|████▌     | 278/615 [01:41<01:33,  3.61it/s] 45%|████▌     | 279/615 [01:41<01:33,  3.60it/s] 46%|████▌     | 280/615 [01:42<01:33,  3.60it/s] 46%|████▌     | 281/615 [01:42<01:32,  3.60it/s] 46%|████▌     | 282/615 [01:42<01:32,  3.60it/s] 46%|████▌     | 283/615 [01:43<01:31,  3.61it/s] 46%|████▌     | 284/615 [01:43<01:31,  3.61it/s] 46%|████▋     | 285/615 [01:43<01:31,  3.62it/s] 47%|████▋     | 286/615 [01:43<01:30,  3.62it/s] 47%|████▋     | 287/615 [01:44<01:30,  3.62it/s] 47%|████▋     | 288/615 [01:44<01:30,  3.62it/s] 47%|████▋     | 289/615 [01:44<01:30,  3.62it/s] 47%|████▋     | 290/615 [01:44<01:29,  3.62it/s] 47%|████▋     | 291/615 [01:45<01:29,  3.62it/s] 47%|████▋     | 292/615 [01:45<01:29,  3.62it/s] 48%|████▊     | 293/615 [01:45<01:29,  3.62it/s] 48%|████▊     | 294/615 [01:46<01:28,  3.62it/s] 48%|████▊     | 295/615 [01:46<01:28,  3.62it/s] 48%|████▊     | 296/615 [01:46<01:28,  3.62it/s] 48%|████▊     | 297/615 [01:46<01:27,  3.62it/s] 48%|████▊     | 298/615 [01:47<01:27,  3.60it/s] 49%|████▊     | 299/615 [01:47<01:27,  3.61it/s] 49%|████▉     | 300/615 [01:47<01:27,  3.61it/s] 49%|████▉     | 301/615 [01:48<01:26,  3.62it/s] 49%|████▉     | 302/615 [01:48<01:26,  3.62it/s] 49%|████▉     | 303/615 [01:48<01:26,  3.62it/s] 49%|████▉     | 304/615 [01:48<01:26,  3.62it/s] 50%|████▉     | 305/615 [01:49<01:25,  3.61it/s] 50%|████▉     | 306/615 [01:49<01:25,  3.62it/s] 50%|████▉     | 307/615 [01:49<01:25,  3.62it/s] 50%|█████     | 308/615 [01:49<01:24,  3.62it/s] 50%|█████     | 309/615 [01:50<01:24,  3.61it/s] 50%|█████     | 310/615 [01:50<01:24,  3.61it/s] 51%|█████     | 311/615 [01:50<01:24,  3.61it/s] 51%|█████     | 312/615 [01:51<01:24,  3.61it/s] 51%|█████     | 313/615 [01:51<01:23,  3.61it/s] 51%|█████     | 314/615 [01:51<01:23,  3.62it/s] 51%|█████     | 315/615 [01:51<01:22,  3.62it/s] 51%|█████▏    | 316/615 [01:52<01:22,  3.62it/s] 52%|█████▏    | 317/615 [01:52<01:22,  3.62it/s] 52%|█████▏    | 318/615 [01:52<01:22,  3.62it/s] 52%|█████▏    | 319/615 [01:52<01:21,  3.62it/s] 52%|█████▏    | 320/615 [01:53<01:21,  3.61it/s] 52%|█████▏    | 321/615 [01:53<01:21,  3.61it/s] 52%|█████▏    | 322/615 [01:53<01:21,  3.61it/s] 53%|█████▎    | 323/615 [01:54<01:20,  3.61it/s] 53%|█████▎    | 324/615 [01:54<01:20,  3.61it/s] 53%|█████▎    | 325/615 [01:54<01:20,  3.62it/s] 53%|█████▎    | 326/615 [01:54<01:19,  3.62it/s] 53%|█████▎    | 327/615 [01:55<01:19,  3.62it/s] 53%|█████▎    | 328/615 [01:55<01:19,  3.62it/s] 53%|█████▎    | 329/615 [01:55<01:19,  3.62it/s] 54%|█████▎    | 330/615 [01:56<01:18,  3.62it/s] 54%|█████▍    | 331/615 [01:56<01:18,  3.61it/s] 54%|█████▍    | 332/615 [01:56<01:18,  3.61it/s] 54%|█████▍    | 333/615 [01:56<01:18,  3.61it/s] 54%|█████▍    | 334/615 [01:57<01:17,  3.60it/s] 54%|█████▍    | 335/615 [01:57<01:17,  3.60it/s] 55%|█████▍    | 336/615 [01:57<01:17,  3.60it/s] 55%|█████▍    | 337/615 [01:57<01:17,  3.60it/s] 55%|█████▍    | 338/615 [01:58<01:16,  3.60it/s] 55%|█████▌    | 339/615 [01:58<01:16,  3.60it/s] 55%|█████▌    | 340/615 [01:58<01:16,  3.60it/s] 55%|█████▌    | 341/615 [01:59<01:16,  3.60it/s] 56%|█████▌    | 342/615 [01:59<01:16,  3.59it/s] 56%|█████▌    | 343/615 [01:59<01:15,  3.59it/s] 56%|█████▌    | 344/615 [01:59<01:15,  3.59it/s] 56%|█████▌    | 345/615 [02:00<01:15,  3.60it/s] 56%|█████▋    | 346/615 [02:00<01:14,  3.59it/s] 56%|█████▋    | 347/615 [02:00<01:14,  3.59it/s] 57%|█████▋    | 348/615 [02:01<01:14,  3.59it/s] 57%|█████▋    | 349/615 [02:01<01:13,  3.60it/s] 57%|█████▋    | 350/615 [02:01<01:13,  3.60it/s] 57%|█████▋    | 351/615 [02:01<01:13,  3.61it/s] 57%|█████▋    | 352/615 [02:02<01:12,  3.61it/s] 57%|█████▋    | 353/615 [02:02<01:12,  3.59it/s] 58%|█████▊    | 354/615 [02:02<01:12,  3.60it/s] 58%|█████▊    | 355/615 [02:02<01:12,  3.60it/s] 58%|█████▊    | 356/615 [02:03<01:11,  3.61it/s] 58%|█████▊    | 357/615 [02:03<01:11,  3.60it/s] 58%|█████▊    | 358/615 [02:03<01:11,  3.61it/s] 58%|█████▊    | 359/615 [02:04<01:10,  3.61it/s] 59%|█████▊    | 360/615 [02:04<01:10,  3.62it/s] 59%|█████▊    | 361/615 [02:04<01:10,  3.62it/s] 59%|█████▉    | 362/615 [02:04<01:09,  3.61it/s] 59%|█████▉    | 363/615 [02:05<01:09,  3.62it/s] 59%|█████▉    | 364/615 [02:05<01:09,  3.60it/s] 59%|█████▉    | 365/615 [02:05<01:09,  3.60it/s] 60%|█████▉    | 366/615 [02:06<01:09,  3.61it/s] 60%|█████▉    | 367/615 [02:06<01:08,  3.61it/s] 60%|█████▉    | 368/615 [02:06<01:08,  3.61it/s] 60%|██████    | 369/615 [02:06<01:08,  3.61it/s][INFO|trainer.py:2140] 2023-08-29 02:29:56,391 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:29:56,391 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 02:29:56,391 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.945, 'eval_samples_per_second': 351.232, 'eval_steps_per_second': 43.942, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.39it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.43it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.54it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.45it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.74it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.42it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.05it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.83it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.93it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.13it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.18it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.23it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.06it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.96it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.84it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.71it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.64it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.85it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.03it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.20it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.12it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.08it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.00it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.91it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.81it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.81it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.83it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.98it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.08it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.13it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.02it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.00it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.88it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.85it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.82it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.88it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.71it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.95it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.98it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.91it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.82it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.89it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.85it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.81it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.75it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.89it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.98it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.05it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.93it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.81it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.79it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.92it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.87it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.76it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.88it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.06it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.00it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.97it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.83it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.82it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.64it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.83it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.79it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.91it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.07it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.07it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.85it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.86it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.90it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.94it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.87it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.87it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.95it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.00it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.06it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.94it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.80it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.90it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.94it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.83it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.77it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.90it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.00it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.01it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.97it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.90it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.89it/s][A                                                 
                                                 [A 60%|██████    | 369/615 [02:16<01:08,  3.61it/s]
100%|██████████| 437/437 [00:09<00:00, 43.89it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:30:06,349 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-369
[INFO|configuration_utils.py:351] 2023-08-29 02:30:06,372 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-369/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:30:08,023 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-369/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:30:08,042 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-369/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:30:08,062 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-369/special_tokens_map.json
 60%|██████    | 370/615 [02:19<15:55,  3.90s/it] 60%|██████    | 371/615 [02:19<11:26,  2.81s/it] 60%|██████    | 372/615 [02:19<08:19,  2.05s/it] 61%|██████    | 373/615 [02:20<06:08,  1.52s/it] 61%|██████    | 374/615 [02:20<04:37,  1.15s/it] 61%|██████    | 375/615 [02:20<03:33,  1.12it/s] 61%|██████    | 376/615 [02:20<02:48,  1.41it/s] 61%|██████▏   | 377/615 [02:21<02:17,  1.73it/s] 61%|██████▏   | 378/615 [02:21<01:55,  2.05it/s] 62%|██████▏   | 379/615 [02:21<01:40,  2.36it/s] 62%|██████▏   | 380/615 [02:22<01:29,  2.62it/s] 62%|██████▏   | 381/615 [02:22<01:21,  2.86it/s] 62%|██████▏   | 382/615 [02:22<01:16,  3.05it/s] 62%|██████▏   | 383/615 [02:22<01:12,  3.20it/s] 62%|██████▏   | 384/615 [02:23<01:09,  3.32it/s] 63%|██████▎   | 385/615 [02:23<01:07,  3.41it/s] 63%|██████▎   | 386/615 [02:23<01:06,  3.46it/s] 63%|██████▎   | 387/615 [02:23<01:05,  3.50it/s] 63%|██████▎   | 388/615 [02:24<01:04,  3.53it/s] 63%|██████▎   | 389/615 [02:24<01:03,  3.55it/s] 63%|██████▎   | 390/615 [02:24<01:02,  3.58it/s] 64%|██████▎   | 391/615 [02:25<01:02,  3.56it/s] 64%|██████▎   | 392/615 [02:25<01:02,  3.58it/s] 64%|██████▍   | 393/615 [02:25<01:01,  3.59it/s] 64%|██████▍   | 394/615 [02:25<01:01,  3.60it/s] 64%|██████▍   | 395/615 [02:26<01:01,  3.60it/s] 64%|██████▍   | 396/615 [02:26<01:00,  3.61it/s] 65%|██████▍   | 397/615 [02:26<01:00,  3.61it/s] 65%|██████▍   | 398/615 [02:27<00:59,  3.62it/s] 65%|██████▍   | 399/615 [02:27<00:59,  3.62it/s] 65%|██████▌   | 400/615 [02:27<00:59,  3.62it/s] 65%|██████▌   | 401/615 [02:27<00:59,  3.62it/s] 65%|██████▌   | 402/615 [02:28<00:59,  3.61it/s] 66%|██████▌   | 403/615 [02:28<00:58,  3.61it/s] 66%|██████▌   | 404/615 [02:28<00:58,  3.62it/s] 66%|██████▌   | 405/615 [02:28<00:58,  3.62it/s] 66%|██████▌   | 406/615 [02:29<00:57,  3.62it/s] 66%|██████▌   | 407/615 [02:29<00:57,  3.62it/s] 66%|██████▋   | 408/615 [02:29<00:57,  3.62it/s] 67%|██████▋   | 409/615 [02:30<00:56,  3.62it/s] 67%|██████▋   | 410/615 [02:30<00:56,  3.62it/s] 67%|██████▋   | 411/615 [02:30<00:56,  3.62it/s] 67%|██████▋   | 412/615 [02:30<00:56,  3.62it/s] 67%|██████▋   | 413/615 [02:31<00:55,  3.61it/s] 67%|██████▋   | 414/615 [02:31<00:55,  3.61it/s] 67%|██████▋   | 415/615 [02:31<00:55,  3.62it/s] 68%|██████▊   | 416/615 [02:31<00:54,  3.63it/s] 68%|██████▊   | 417/615 [02:32<00:54,  3.63it/s] 68%|██████▊   | 418/615 [02:32<00:54,  3.63it/s] 68%|██████▊   | 419/615 [02:32<00:53,  3.63it/s] 68%|██████▊   | 420/615 [02:33<00:53,  3.63it/s] 68%|██████▊   | 421/615 [02:33<00:53,  3.62it/s] 69%|██████▊   | 422/615 [02:33<00:53,  3.62it/s] 69%|██████▉   | 423/615 [02:33<00:52,  3.63it/s] 69%|██████▉   | 424/615 [02:34<00:53,  3.59it/s] 69%|██████▉   | 425/615 [02:34<00:52,  3.60it/s] 69%|██████▉   | 426/615 [02:34<00:52,  3.61it/s] 69%|██████▉   | 427/615 [02:35<00:52,  3.61it/s] 70%|██████▉   | 428/615 [02:35<00:51,  3.61it/s] 70%|██████▉   | 429/615 [02:35<00:51,  3.62it/s] 70%|██████▉   | 430/615 [02:35<00:51,  3.62it/s] 70%|███████   | 431/615 [02:36<00:50,  3.62it/s] 70%|███████   | 432/615 [02:36<00:50,  3.62it/s] 70%|███████   | 433/615 [02:36<00:50,  3.62it/s] 71%|███████   | 434/615 [02:36<00:49,  3.62it/s] 71%|███████   | 435/615 [02:37<00:49,  3.61it/s] 71%|███████   | 436/615 [02:37<00:49,  3.62it/s] 71%|███████   | 437/615 [02:37<00:49,  3.62it/s] 71%|███████   | 438/615 [02:38<00:48,  3.62it/s] 71%|███████▏  | 439/615 [02:38<00:48,  3.62it/s] 72%|███████▏  | 440/615 [02:38<00:48,  3.62it/s] 72%|███████▏  | 441/615 [02:38<00:48,  3.62it/s] 72%|███████▏  | 442/615 [02:39<00:47,  3.62it/s] 72%|███████▏  | 443/615 [02:39<00:47,  3.62it/s] 72%|███████▏  | 444/615 [02:39<00:47,  3.62it/s] 72%|███████▏  | 445/615 [02:39<00:46,  3.62it/s] 73%|███████▎  | 446/615 [02:40<00:46,  3.61it/s] 73%|███████▎  | 447/615 [02:40<00:46,  3.61it/s] 73%|███████▎  | 448/615 [02:40<00:46,  3.62it/s] 73%|███████▎  | 449/615 [02:41<00:45,  3.62it/s] 73%|███████▎  | 450/615 [02:41<00:45,  3.62it/s] 73%|███████▎  | 451/615 [02:41<00:45,  3.62it/s] 73%|███████▎  | 452/615 [02:41<00:44,  3.62it/s] 74%|███████▎  | 453/615 [02:42<00:44,  3.62it/s] 74%|███████▍  | 454/615 [02:42<00:44,  3.62it/s] 74%|███████▍  | 455/615 [02:42<00:44,  3.62it/s] 74%|███████▍  | 456/615 [02:43<00:43,  3.63it/s] 74%|███████▍  | 457/615 [02:43<00:43,  3.60it/s] 74%|███████▍  | 458/615 [02:43<00:43,  3.61it/s] 75%|███████▍  | 459/615 [02:43<00:43,  3.62it/s] 75%|███████▍  | 460/615 [02:44<00:42,  3.62it/s] 75%|███████▍  | 461/615 [02:44<00:42,  3.62it/s] 75%|███████▌  | 462/615 [02:44<00:42,  3.62it/s] 75%|███████▌  | 463/615 [02:44<00:42,  3.62it/s] 75%|███████▌  | 464/615 [02:45<00:41,  3.62it/s] 76%|███████▌  | 465/615 [02:45<00:41,  3.62it/s] 76%|███████▌  | 466/615 [02:45<00:41,  3.62it/s] 76%|███████▌  | 467/615 [02:46<00:40,  3.62it/s] 76%|███████▌  | 468/615 [02:46<00:40,  3.62it/s] 76%|███████▋  | 469/615 [02:46<00:40,  3.62it/s] 76%|███████▋  | 470/615 [02:46<00:40,  3.62it/s] 77%|███████▋  | 471/615 [02:47<00:39,  3.63it/s] 77%|███████▋  | 472/615 [02:47<00:39,  3.63it/s] 77%|███████▋  | 473/615 [02:47<00:39,  3.62it/s] 77%|███████▋  | 474/615 [02:48<00:38,  3.63it/s] 77%|███████▋  | 475/615 [02:48<00:38,  3.62it/s] 77%|███████▋  | 476/615 [02:48<00:38,  3.63it/s] 78%|███████▊  | 477/615 [02:48<00:38,  3.61it/s] 78%|███████▊  | 478/615 [02:49<00:37,  3.61it/s] 78%|███████▊  | 479/615 [02:49<00:37,  3.61it/s] 78%|███████▊  | 480/615 [02:49<00:37,  3.62it/s] 78%|███████▊  | 481/615 [02:49<00:36,  3.62it/s] 78%|███████▊  | 482/615 [02:50<00:36,  3.62it/s] 79%|███████▊  | 483/615 [02:50<00:36,  3.62it/s] 79%|███████▊  | 484/615 [02:50<00:36,  3.62it/s] 79%|███████▉  | 485/615 [02:51<00:35,  3.63it/s] 79%|███████▉  | 486/615 [02:51<00:35,  3.63it/s] 79%|███████▉  | 487/615 [02:51<00:35,  3.63it/s] 79%|███████▉  | 488/615 [02:51<00:35,  3.61it/s] 80%|███████▉  | 489/615 [02:52<00:34,  3.62it/s] 80%|███████▉  | 490/615 [02:52<00:34,  3.62it/s] 80%|███████▉  | 491/615 [02:52<00:34,  3.62it/s] 80%|████████  | 492/615 [02:52<00:33,  3.62it/s][INFO|trainer.py:2140] 2023-08-29 02:30:42,502 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:30:42,502 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 02:30:42,502 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9435, 'eval_samples_per_second': 351.287, 'eval_steps_per_second': 43.949, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.54it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.33it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.39it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.47it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.78it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.37it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.97it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.83it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.88it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.04it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.19it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.24it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.20it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.00it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.86it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.73it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.70it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.76it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.90it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.11it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.17it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.10it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.99it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.87it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.72it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.69it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.81it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.96it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.06it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.14it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.05it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.92it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.80it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.67it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.69it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.71it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.89it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.08it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.18it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.10it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.96it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.67it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.72it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.77it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.89it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.08it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.13it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.11it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.06it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.94it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.70it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 43.68it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.83it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.96it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.09it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.00it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.92it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.72it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.53it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.48it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.60it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.73it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.96it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.07it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.00it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.98it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.03it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.91it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.77it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.72it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.89it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.94it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.00it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.88it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.66it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.61it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.56it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.55it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.45it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.69it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.87it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.88it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.85it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.79it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.74it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.64it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.74it/s][A                                                 
                                                 [A 80%|████████  | 492/615 [03:03<00:33,  3.62it/s]
100%|██████████| 437/437 [00:09<00:00, 43.74it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:30:52,589 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-29 02:30:52,759 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:30:55,094 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:30:55,118 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:30:55,126 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-492/special_tokens_map.json
 80%|████████  | 493/615 [03:06<08:31,  4.19s/it] 80%|████████  | 494/615 [03:06<06:05,  3.02s/it] 80%|████████  | 495/615 [03:06<04:23,  2.20s/it] 81%|████████  | 496/615 [03:07<03:13,  1.62s/it] 81%|████████  | 497/615 [03:07<02:24,  1.22s/it] 81%|████████  | 498/615 [03:07<01:49,  1.06it/s] 81%|████████  | 499/615 [03:08<01:26,  1.35it/s] 81%|████████▏ | 500/615 [03:08<01:09,  1.66it/s]                                                  81%|████████▏ | 500/615 [03:08<01:09,  1.66it/s] 81%|████████▏ | 501/615 [03:08<00:58,  1.94it/s] 82%|████████▏ | 502/615 [03:08<00:50,  2.24it/s] 82%|████████▏ | 503/615 [03:09<00:44,  2.52it/s] 82%|████████▏ | 504/615 [03:09<00:40,  2.76it/s] 82%|████████▏ | 505/615 [03:09<00:37,  2.96it/s] 82%|████████▏ | 506/615 [03:09<00:34,  3.13it/s] 82%|████████▏ | 507/615 [03:10<00:33,  3.25it/s] 83%|████████▎ | 508/615 [03:10<00:32,  3.34it/s] 83%|████████▎ | 509/615 [03:10<00:31,  3.41it/s] 83%|████████▎ | 510/615 [03:11<00:30,  3.46it/s] 83%|████████▎ | 511/615 [03:11<00:29,  3.49it/s] 83%|████████▎ | 512/615 [03:11<00:29,  3.51it/s] 83%|████████▎ | 513/615 [03:11<00:28,  3.53it/s] 84%|████████▎ | 514/615 [03:12<00:28,  3.53it/s] 84%|████████▎ | 515/615 [03:12<00:28,  3.54it/s] 84%|████████▍ | 516/615 [03:12<00:27,  3.55it/s] 84%|████████▍ | 517/615 [03:13<00:27,  3.56it/s] 84%|████████▍ | 518/615 [03:13<00:27,  3.56it/s] 84%|████████▍ | 519/615 [03:13<00:26,  3.56it/s] 85%|████████▍ | 520/615 [03:13<00:26,  3.56it/s] 85%|████████▍ | 521/615 [03:14<00:26,  3.57it/s] 85%|████████▍ | 522/615 [03:14<00:26,  3.57it/s] 85%|████████▌ | 523/615 [03:14<00:25,  3.57it/s] 85%|████████▌ | 524/615 [03:15<00:25,  3.57it/s] 85%|████████▌ | 525/615 [03:15<00:25,  3.56it/s] 86%|████████▌ | 526/615 [03:15<00:24,  3.56it/s] 86%|████████▌ | 527/615 [03:15<00:24,  3.56it/s] 86%|████████▌ | 528/615 [03:16<00:24,  3.56it/s] 86%|████████▌ | 529/615 [03:16<00:24,  3.57it/s] 86%|████████▌ | 530/615 [03:16<00:23,  3.56it/s] 86%|████████▋ | 531/615 [03:17<00:23,  3.56it/s] 87%|████████▋ | 532/615 [03:17<00:23,  3.56it/s] 87%|████████▋ | 533/615 [03:17<00:23,  3.56it/s] 87%|████████▋ | 534/615 [03:17<00:22,  3.56it/s] 87%|████████▋ | 535/615 [03:18<00:22,  3.57it/s] 87%|████████▋ | 536/615 [03:18<00:22,  3.56it/s] 87%|████████▋ | 537/615 [03:18<00:21,  3.57it/s] 87%|████████▋ | 538/615 [03:18<00:21,  3.57it/s] 88%|████████▊ | 539/615 [03:19<00:21,  3.57it/s] 88%|████████▊ | 540/615 [03:19<00:21,  3.55it/s] 88%|████████▊ | 541/615 [03:19<00:20,  3.56it/s] 88%|████████▊ | 542/615 [03:20<00:20,  3.56it/s] 88%|████████▊ | 543/615 [03:20<00:20,  3.57it/s] 88%|████████▊ | 544/615 [03:20<00:19,  3.57it/s] 89%|████████▊ | 545/615 [03:20<00:19,  3.57it/s] 89%|████████▉ | 546/615 [03:21<00:19,  3.57it/s] 89%|████████▉ | 547/615 [03:21<00:19,  3.57it/s] 89%|████████▉ | 548/615 [03:21<00:18,  3.57it/s] 89%|████████▉ | 549/615 [03:22<00:18,  3.57it/s] 89%|████████▉ | 550/615 [03:22<00:18,  3.57it/s] 90%|████████▉ | 551/615 [03:22<00:17,  3.56it/s] 90%|████████▉ | 552/615 [03:22<00:17,  3.56it/s] 90%|████████▉ | 553/615 [03:23<00:17,  3.56it/s] 90%|█████████ | 554/615 [03:23<00:17,  3.57it/s] 90%|█████████ | 555/615 [03:23<00:16,  3.57it/s] 90%|█████████ | 556/615 [03:24<00:16,  3.57it/s] 91%|█████████ | 557/615 [03:24<00:16,  3.57it/s] 91%|█████████ | 558/615 [03:24<00:15,  3.56it/s] 91%|█████████ | 559/615 [03:24<00:15,  3.56it/s] 91%|█████████ | 560/615 [03:25<00:15,  3.57it/s] 91%|█████████ | 561/615 [03:25<00:15,  3.56it/s] 91%|█████████▏| 562/615 [03:25<00:14,  3.55it/s] 92%|█████████▏| 563/615 [03:25<00:14,  3.56it/s] 92%|█████████▏| 564/615 [03:26<00:14,  3.56it/s] 92%|█████████▏| 565/615 [03:26<00:14,  3.56it/s] 92%|█████████▏| 566/615 [03:26<00:13,  3.56it/s] 92%|█████████▏| 567/615 [03:27<00:13,  3.56it/s] 92%|█████████▏| 568/615 [03:27<00:13,  3.56it/s] 93%|█████████▎| 569/615 [03:27<00:12,  3.56it/s] 93%|█████████▎| 570/615 [03:27<00:12,  3.57it/s] 93%|█████████▎| 571/615 [03:28<00:12,  3.57it/s] 93%|█████████▎| 572/615 [03:28<00:12,  3.57it/s] 93%|█████████▎| 573/615 [03:28<00:11,  3.56it/s] 93%|█████████▎| 574/615 [03:29<00:11,  3.56it/s] 93%|█████████▎| 575/615 [03:29<00:11,  3.56it/s] 94%|█████████▎| 576/615 [03:29<00:10,  3.57it/s] 94%|█████████▍| 577/615 [03:29<00:10,  3.57it/s] 94%|█████████▍| 578/615 [03:30<00:10,  3.57it/s] 94%|█████████▍| 579/615 [03:30<00:10,  3.57it/s] 94%|█████████▍| 580/615 [03:30<00:09,  3.57it/s] 94%|█████████▍| 581/615 [03:31<00:09,  3.57it/s] 95%|█████████▍| 582/615 [03:31<00:09,  3.57it/s] 95%|█████████▍| 583/615 [03:31<00:08,  3.57it/s] 95%|█████████▍| 584/615 [03:31<00:08,  3.56it/s] 95%|█████████▌| 585/615 [03:32<00:08,  3.56it/s] 95%|█████████▌| 586/615 [03:32<00:08,  3.56it/s] 95%|█████████▌| 587/615 [03:32<00:07,  3.56it/s] 96%|█████████▌| 588/615 [03:32<00:07,  3.57it/s] 96%|█████████▌| 589/615 [03:33<00:07,  3.56it/s] 96%|█████████▌| 590/615 [03:33<00:07,  3.56it/s] 96%|█████████▌| 591/615 [03:33<00:06,  3.56it/s] 96%|█████████▋| 592/615 [03:34<00:06,  3.56it/s] 96%|█████████▋| 593/615 [03:34<00:06,  3.55it/s] 97%|█████████▋| 594/615 [03:34<00:05,  3.55it/s] 97%|█████████▋| 595/615 [03:34<00:05,  3.54it/s] 97%|█████████▋| 596/615 [03:35<00:05,  3.55it/s] 97%|█████████▋| 597/615 [03:35<00:05,  3.55it/s] 97%|█████████▋| 598/615 [03:35<00:04,  3.55it/s] 97%|█████████▋| 599/615 [03:36<00:04,  3.55it/s] 98%|█████████▊| 600/615 [03:36<00:04,  3.56it/s] 98%|█████████▊| 601/615 [03:36<00:03,  3.56it/s] 98%|█████████▊| 602/615 [03:36<00:03,  3.54it/s] 98%|█████████▊| 603/615 [03:37<00:03,  3.55it/s] 98%|█████████▊| 604/615 [03:37<00:03,  3.56it/s] 98%|█████████▊| 605/615 [03:37<00:02,  3.58it/s] 99%|█████████▊| 606/615 [03:38<00:02,  3.57it/s] 99%|█████████▊| 607/615 [03:38<00:02,  3.59it/s] 99%|█████████▉| 608/615 [03:38<00:01,  3.50it/s] 99%|█████████▉| 609/615 [03:38<00:01,  3.53it/s] 99%|█████████▉| 610/615 [03:39<00:01,  3.57it/s] 99%|█████████▉| 611/615 [03:39<00:01,  3.58it/s]100%|█████████▉| 612/615 [03:39<00:00,  3.60it/s]100%|█████████▉| 613/615 [03:40<00:00,  3.61it/s]100%|█████████▉| 614/615 [03:40<00:00,  3.61it/s]100%|██████████| 615/615 [03:40<00:00,  3.62it/s][INFO|trainer.py:2140] 2023-08-29 02:31:29,966 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:31:29,966 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 02:31:29,966 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9529, 'eval_samples_per_second': 350.952, 'eval_steps_per_second': 43.907, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7134146341463415e-05, 'epoch': 4.06}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.36it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.14it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.26it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.27it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.69it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.33it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.08it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.91it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.89it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.10it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.20it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.09it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.75it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.70it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.64it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.66it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.72it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.91it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.06it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.16it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.15it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.95it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.84it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.75it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.80it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.77it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.89it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.98it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.13it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.17it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.99it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.85it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.84it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.78it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.77it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.85it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.02it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.17it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.06it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.91it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.86it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.70it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.79it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.85it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.86it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.04it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.10it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.04it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.90it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.81it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.77it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 43.73it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.79it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.83it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.00it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.99it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.86it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.82it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.74it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.75it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.79it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.77it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.95it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.77it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.74it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.66it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.52it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.63it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.65it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.71it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.85it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.86it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.90it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.94it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.93it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.90it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.90it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.77it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.81it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.85it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.97it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.96it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.85it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.85it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.88it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.82it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.88it/s][A                                                 
                                                 [A100%|██████████| 615/615 [03:50<00:00,  3.62it/s]
100%|██████████| 437/437 [00:09<00:00, 43.88it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:31:39,938 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-615
[INFO|configuration_utils.py:351] 2023-08-29 02:31:39,959 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-615/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:31:41,691 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-615/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:31:41,704 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-615/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:31:41,712 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-615/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 02:31:41,989 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 02:31:41,989 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-123 (score: 1.1039679050445557).
                                                 100%|██████████| 615/615 [03:54<00:00,  3.62it/s]100%|██████████| 615/615 [03:54<00:00,  2.63it/s]
[INFO|trainer.py:1894] 2023-08-29 02:31:43,568 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 02:31:43,590 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:31:45,266 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:31:45,281 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:31:45,292 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:31:45,484 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:45,484 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:45,485 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:45,485 >>   train_runtime            = 0:03:54.16
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:45,485 >>   train_samples            =       7900
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:45,485 >>   train_samples_per_second =    168.685
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:45,485 >>   train_steps_per_second   =      2.626
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9537, 'eval_samples_per_second': 350.926, 'eval_steps_per_second': 43.903, 'epoch': 5.0}
{'train_runtime': 234.1638, 'train_samples_per_second': 168.685, 'train_steps_per_second': 2.626, 'train_loss': nan, 'epoch': 5.0}
08/29/2023 02:31:45 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 02:31:45,530 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:31:45,531 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 02:31:45,531 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 54.99it/s]  3%|▎         | 12/437 [00:00<00:08, 48.12it/s]  4%|▍         | 17/437 [00:00<00:09, 46.48it/s]  5%|▌         | 22/437 [00:00<00:09, 46.02it/s]  6%|▌         | 27/437 [00:00<00:08, 45.57it/s]  7%|▋         | 32/437 [00:00<00:08, 45.27it/s]  8%|▊         | 37/437 [00:00<00:08, 45.13it/s] 10%|▉         | 42/437 [00:00<00:08, 44.42it/s] 11%|█         | 47/437 [00:01<00:08, 43.88it/s] 12%|█▏        | 52/437 [00:01<00:08, 43.65it/s] 13%|█▎        | 57/437 [00:01<00:08, 43.75it/s] 14%|█▍        | 62/437 [00:01<00:08, 43.90it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.03it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.17it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.37it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.40it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.12it/s] 21%|██        | 92/437 [00:02<00:07, 43.75it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.60it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.68it/s] 24%|██▍       | 107/437 [00:02<00:07, 43.94it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.09it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.19it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.28it/s] 29%|██▉       | 127/437 [00:02<00:07, 44.23it/s] 30%|███       | 132/437 [00:02<00:06, 43.89it/s] 31%|███▏      | 137/437 [00:03<00:06, 43.78it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.67it/s] 34%|███▎      | 147/437 [00:03<00:06, 43.61it/s] 35%|███▍      | 152/437 [00:03<00:06, 43.86it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.06it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.16it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.32it/s] 39%|███▉      | 172/437 [00:03<00:06, 44.10it/s] 41%|████      | 177/437 [00:03<00:05, 43.99it/s] 42%|████▏     | 182/437 [00:04<00:05, 43.75it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.71it/s] 44%|████▍     | 192/437 [00:04<00:05, 43.77it/s] 45%|████▌     | 197/437 [00:04<00:05, 43.72it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.06it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.22it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.30it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.25it/s] 51%|█████     | 222/437 [00:05<00:04, 43.98it/s] 52%|█████▏    | 227/437 [00:05<00:04, 43.64it/s] 53%|█████▎    | 232/437 [00:05<00:04, 43.76it/s] 54%|█████▍    | 237/437 [00:05<00:04, 43.77it/s] 55%|█████▌    | 242/437 [00:05<00:04, 43.85it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.01it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.05it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.24it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.11it/s] 61%|██████    | 267/437 [00:06<00:03, 43.92it/s] 62%|██████▏   | 272/437 [00:06<00:03, 43.63it/s] 63%|██████▎   | 277/437 [00:06<00:03, 43.73it/s] 65%|██████▍   | 282/437 [00:06<00:03, 43.82it/s] 66%|██████▌   | 287/437 [00:06<00:03, 43.95it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.06it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.24it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.17it/s] 70%|███████   | 307/437 [00:06<00:02, 44.10it/s] 71%|███████▏  | 312/437 [00:07<00:02, 43.90it/s] 73%|███████▎  | 317/437 [00:07<00:02, 43.78it/s] 74%|███████▎  | 322/437 [00:07<00:02, 43.83it/s] 75%|███████▍  | 327/437 [00:07<00:02, 43.79it/s] 76%|███████▌  | 332/437 [00:07<00:02, 43.85it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.04it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.16it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.24it/s] 81%|████████  | 352/437 [00:07<00:01, 44.11it/s] 82%|████████▏ | 357/437 [00:08<00:01, 43.86it/s] 83%|████████▎ | 362/437 [00:08<00:01, 43.76it/s] 84%|████████▍ | 367/437 [00:08<00:01, 43.78it/s] 85%|████████▌ | 372/437 [00:08<00:01, 43.79it/s] 86%|████████▋ | 377/437 [00:08<00:01, 43.91it/s] 87%|████████▋ | 382/437 [00:08<00:01, 43.99it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.05it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.14it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.03it/s] 92%|█████████▏| 402/437 [00:09<00:00, 43.76it/s] 93%|█████████▎| 407/437 [00:09<00:00, 43.82it/s] 94%|█████████▍| 412/437 [00:09<00:00, 43.72it/s] 95%|█████████▌| 417/437 [00:09<00:00, 43.84it/s] 97%|█████████▋| 422/437 [00:09<00:00, 43.86it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.02it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.16it/s]100%|██████████| 437/437 [00:09<00:00, 44.04it/s]100%|██████████| 437/437 [00:09<00:00, 44.08it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:31:55,462 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:55,462 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:55,462 >>   eval_loss               =      1.104
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:55,462 >>   eval_runtime            = 0:00:09.93
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:55,462 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:55,462 >>   eval_samples_per_second =    351.718
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:55,462 >>   eval_steps_per_second   =     44.003
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:31:55,462 >>   perplexity              =     3.0161
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:01,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:01,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:01,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:01,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:01,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:32:02,332 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:32:02,333 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:32:02,905 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:32:03,935 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:32:03,935 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:06,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:06,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:06,819 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:06,819 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:06,819 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:32:07,431 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:32:07,432 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:32:07,987 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:32:08,149 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:32:08,149 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-615
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-123
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/checkpoint-369
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.79it/s]Extractor Predicting: 3it [00:01,  1.78it/s]Extractor Predicting: 4it [00:02,  1.84it/s]Extractor Predicting: 5it [00:02,  1.79it/s]Extractor Predicting: 6it [00:03,  1.74it/s]Extractor Predicting: 7it [00:03,  1.78it/s]Extractor Predicting: 8it [00:04,  1.77it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:06,  1.78it/s]Extractor Predicting: 13it [00:07,  1.80it/s]Extractor Predicting: 14it [00:07,  1.82it/s]Extractor Predicting: 15it [00:08,  1.79it/s]Extractor Predicting: 16it [00:08,  1.82it/s]Extractor Predicting: 17it [00:09,  1.83it/s]Extractor Predicting: 18it [00:10,  1.82it/s]Extractor Predicting: 19it [00:10,  1.82it/s]Extractor Predicting: 20it [00:11,  1.77it/s]Extractor Predicting: 21it [00:11,  1.75it/s]Extractor Predicting: 22it [00:12,  1.71it/s]Extractor Predicting: 23it [00:12,  1.76it/s]Extractor Predicting: 24it [00:13,  1.79it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:14,  1.86it/s]Extractor Predicting: 27it [00:15,  1.84it/s]Extractor Predicting: 28it [00:15,  1.88it/s]Extractor Predicting: 29it [00:16,  1.85it/s]Extractor Predicting: 30it [00:16,  1.80it/s]Extractor Predicting: 31it [00:17,  1.79it/s]Extractor Predicting: 32it [00:17,  1.76it/s]Extractor Predicting: 33it [00:18,  1.69it/s]Extractor Predicting: 34it [00:19,  1.67it/s]Extractor Predicting: 35it [00:19,  1.69it/s]Extractor Predicting: 36it [00:20,  1.68it/s]Extractor Predicting: 37it [00:20,  1.68it/s]Extractor Predicting: 38it [00:21,  1.66it/s]Extractor Predicting: 39it [00:22,  1.69it/s]Extractor Predicting: 40it [00:22,  1.69it/s]Extractor Predicting: 41it [00:23,  1.68it/s]Extractor Predicting: 42it [00:23,  1.70it/s]Extractor Predicting: 43it [00:24,  1.70it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:25,  1.75it/s]Extractor Predicting: 46it [00:26,  1.70it/s]Extractor Predicting: 47it [00:26,  1.68it/s]Extractor Predicting: 48it [00:27,  1.69it/s]Extractor Predicting: 49it [00:28,  1.68it/s]Extractor Predicting: 50it [00:28,  1.68it/s]Extractor Predicting: 51it [00:29,  1.57it/s]Extractor Predicting: 52it [00:29,  1.59it/s]Extractor Predicting: 53it [00:30,  1.66it/s]Extractor Predicting: 54it [00:31,  1.66it/s]Extractor Predicting: 55it [00:31,  1.63it/s]Extractor Predicting: 56it [00:32,  1.63it/s]Extractor Predicting: 57it [00:32,  1.62it/s]Extractor Predicting: 58it [00:33,  1.63it/s]Extractor Predicting: 59it [00:34,  1.62it/s]Extractor Predicting: 60it [00:34,  1.61it/s]Extractor Predicting: 61it [00:35,  1.63it/s]Extractor Predicting: 62it [00:36,  1.64it/s]Extractor Predicting: 63it [00:36,  1.68it/s]Extractor Predicting: 64it [00:37,  1.71it/s]Extractor Predicting: 65it [00:37,  1.67it/s]Extractor Predicting: 66it [00:38,  1.71it/s]Extractor Predicting: 67it [00:38,  1.69it/s]Extractor Predicting: 68it [00:39,  1.69it/s]Extractor Predicting: 69it [00:40,  1.68it/s]Extractor Predicting: 70it [00:40,  1.65it/s]Extractor Predicting: 71it [00:41,  1.67it/s]Extractor Predicting: 72it [00:41,  1.68it/s]Extractor Predicting: 73it [00:42,  1.68it/s]Extractor Predicting: 74it [00:43,  1.71it/s]Extractor Predicting: 75it [00:43,  1.72it/s]Extractor Predicting: 76it [00:44,  1.69it/s]Extractor Predicting: 77it [00:44,  1.68it/s]Extractor Predicting: 78it [00:45,  1.67it/s]Extractor Predicting: 79it [00:46,  1.67it/s]Extractor Predicting: 80it [00:46,  1.66it/s]Extractor Predicting: 81it [00:47,  1.71it/s]Extractor Predicting: 82it [00:47,  1.70it/s]Extractor Predicting: 83it [00:48,  1.66it/s]Extractor Predicting: 84it [00:49,  1.68it/s]Extractor Predicting: 85it [00:49,  1.71it/s]Extractor Predicting: 86it [00:50,  1.78it/s]Extractor Predicting: 87it [00:50,  1.83it/s]Extractor Predicting: 88it [00:51,  1.83it/s]Extractor Predicting: 89it [00:51,  1.82it/s]Extractor Predicting: 90it [00:52,  1.83it/s]Extractor Predicting: 91it [00:52,  1.81it/s]Extractor Predicting: 92it [00:53,  1.78it/s]Extractor Predicting: 93it [00:53,  1.83it/s]Extractor Predicting: 94it [00:54,  1.85it/s]Extractor Predicting: 95it [00:55,  1.84it/s]Extractor Predicting: 96it [00:55,  1.84it/s]Extractor Predicting: 97it [00:56,  1.82it/s]Extractor Predicting: 98it [00:56,  1.78it/s]Extractor Predicting: 99it [00:57,  1.75it/s]Extractor Predicting: 100it [00:57,  1.78it/s]Extractor Predicting: 101it [00:58,  1.82it/s]Extractor Predicting: 102it [00:58,  1.79it/s]Extractor Predicting: 103it [00:59,  1.80it/s]Extractor Predicting: 104it [01:00,  1.79it/s]Extractor Predicting: 105it [01:00,  1.84it/s]Extractor Predicting: 106it [01:01,  1.80it/s]Extractor Predicting: 107it [01:01,  1.82it/s]Extractor Predicting: 108it [01:02,  1.81it/s]Extractor Predicting: 109it [01:02,  1.85it/s]Extractor Predicting: 110it [01:03,  1.84it/s]Extractor Predicting: 111it [01:03,  1.82it/s]Extractor Predicting: 112it [01:04,  1.80it/s]Extractor Predicting: 113it [01:05,  1.80it/s]Extractor Predicting: 114it [01:05,  1.74it/s]Extractor Predicting: 115it [01:06,  1.76it/s]Extractor Predicting: 116it [01:06,  1.74it/s]Extractor Predicting: 117it [01:07,  1.72it/s]Extractor Predicting: 118it [01:07,  1.79it/s]Extractor Predicting: 119it [01:08,  1.77it/s]Extractor Predicting: 120it [01:09,  1.79it/s]Extractor Predicting: 121it [01:09,  1.76it/s]Extractor Predicting: 122it [01:10,  1.75it/s]Extractor Predicting: 123it [01:10,  1.74it/s]Extractor Predicting: 124it [01:11,  1.71it/s]Extractor Predicting: 125it [01:11,  1.70it/s]Extractor Predicting: 126it [01:12,  1.72it/s]Extractor Predicting: 127it [01:13,  1.69it/s]Extractor Predicting: 128it [01:13,  1.68it/s]Extractor Predicting: 129it [01:14,  1.50it/s]Extractor Predicting: 130it [01:15,  1.56it/s]Extractor Predicting: 131it [01:15,  1.60it/s]Extractor Predicting: 132it [01:16,  1.66it/s]Extractor Predicting: 133it [01:16,  1.68it/s]Extractor Predicting: 134it [01:17,  1.69it/s]Extractor Predicting: 135it [01:18,  1.69it/s]Extractor Predicting: 136it [01:18,  1.69it/s]Extractor Predicting: 137it [01:19,  1.69it/s]Extractor Predicting: 138it [01:19,  1.68it/s]Extractor Predicting: 139it [01:20,  1.67it/s]Extractor Predicting: 140it [01:20,  1.72it/s]Extractor Predicting: 141it [01:21,  1.69it/s]Extractor Predicting: 142it [01:22,  1.79it/s]Extractor Predicting: 142it [01:22,  1.73it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:38,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:38,289 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:38,290 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:38,290 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:38,290 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:33:38,898 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:33:38,899 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:33:39,448 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:33:40,505 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:33:40,505 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:43,451 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:43,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:43,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:43,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:43,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:33:44,101 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:33:44,102 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:33:44,676 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:33:44,842 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:33:44,842 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.79it/s]Extractor Predicting: 5it [00:02,  1.77it/s]Extractor Predicting: 6it [00:03,  1.73it/s]Extractor Predicting: 7it [00:04,  1.75it/s]Extractor Predicting: 8it [00:04,  1.80it/s]Extractor Predicting: 9it [00:05,  1.80it/s]Extractor Predicting: 10it [00:05,  1.83it/s]Extractor Predicting: 11it [00:06,  1.81it/s]Extractor Predicting: 12it [00:06,  1.83it/s]Extractor Predicting: 13it [00:07,  1.78it/s]Extractor Predicting: 14it [00:07,  1.81it/s]Extractor Predicting: 15it [00:08,  1.80it/s]Extractor Predicting: 16it [00:09,  1.76it/s]Extractor Predicting: 17it [00:09,  1.75it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:10,  1.78it/s]Extractor Predicting: 20it [00:11,  1.74it/s]Extractor Predicting: 21it [00:11,  1.76it/s]Extractor Predicting: 22it [00:12,  1.78it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:13,  1.73it/s]Extractor Predicting: 25it [00:14,  1.74it/s]Extractor Predicting: 26it [00:14,  1.74it/s]Extractor Predicting: 27it [00:15,  1.71it/s]Extractor Predicting: 28it [00:15,  1.74it/s]Extractor Predicting: 29it [00:16,  1.70it/s]Extractor Predicting: 30it [00:17,  1.65it/s]Extractor Predicting: 31it [00:17,  1.65it/s]Extractor Predicting: 32it [00:18,  1.65it/s]Extractor Predicting: 33it [00:18,  1.67it/s]Extractor Predicting: 34it [00:19,  1.69it/s]Extractor Predicting: 35it [00:20,  1.69it/s]Extractor Predicting: 36it [00:20,  1.73it/s]Extractor Predicting: 37it [00:21,  1.75it/s]Extractor Predicting: 38it [00:21,  1.77it/s]Extractor Predicting: 39it [00:22,  1.75it/s]Extractor Predicting: 40it [00:22,  1.75it/s]Extractor Predicting: 41it [00:23,  1.76it/s]Extractor Predicting: 42it [00:24,  1.73it/s]Extractor Predicting: 43it [00:24,  1.74it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:25,  1.71it/s]Extractor Predicting: 46it [00:26,  1.73it/s]Extractor Predicting: 47it [00:26,  1.74it/s]Extractor Predicting: 48it [00:27,  1.59it/s]Extractor Predicting: 49it [00:28,  1.63it/s]Extractor Predicting: 50it [00:28,  1.63it/s]Extractor Predicting: 51it [00:29,  1.68it/s]Extractor Predicting: 52it [00:30,  1.70it/s]Extractor Predicting: 53it [00:30,  1.69it/s]Extractor Predicting: 54it [00:31,  1.70it/s]Extractor Predicting: 55it [00:31,  1.66it/s]Extractor Predicting: 56it [00:32,  1.70it/s]Extractor Predicting: 57it [00:32,  1.74it/s]Extractor Predicting: 58it [00:33,  1.76it/s]Extractor Predicting: 59it [00:34,  1.76it/s]Extractor Predicting: 60it [00:34,  1.76it/s]Extractor Predicting: 61it [00:35,  1.76it/s]Extractor Predicting: 62it [00:35,  1.76it/s]Extractor Predicting: 63it [00:36,  1.76it/s]Extractor Predicting: 64it [00:36,  1.70it/s]Extractor Predicting: 65it [00:37,  1.71it/s]Extractor Predicting: 66it [00:38,  1.70it/s]Extractor Predicting: 67it [00:38,  1.73it/s]Extractor Predicting: 68it [00:39,  1.78it/s]Extractor Predicting: 69it [00:39,  1.77it/s]Extractor Predicting: 70it [00:40,  1.76it/s]Extractor Predicting: 71it [00:40,  1.74it/s]Extractor Predicting: 72it [00:41,  1.75it/s]Extractor Predicting: 73it [00:42,  1.69it/s]Extractor Predicting: 74it [00:42,  1.69it/s]Extractor Predicting: 75it [00:43,  1.71it/s]Extractor Predicting: 76it [00:43,  1.71it/s]Extractor Predicting: 77it [00:44,  1.67it/s]Extractor Predicting: 78it [00:45,  1.69it/s]Extractor Predicting: 79it [00:45,  1.69it/s]Extractor Predicting: 80it [00:46,  1.69it/s]Extractor Predicting: 81it [00:46,  1.72it/s]Extractor Predicting: 82it [00:47,  1.67it/s]Extractor Predicting: 83it [00:48,  1.68it/s]Extractor Predicting: 84it [00:48,  1.70it/s]Extractor Predicting: 85it [00:49,  1.72it/s]Extractor Predicting: 86it [00:49,  1.69it/s]Extractor Predicting: 87it [00:50,  1.70it/s]Extractor Predicting: 88it [00:51,  1.66it/s]Extractor Predicting: 89it [00:51,  1.68it/s]Extractor Predicting: 90it [00:52,  1.71it/s]Extractor Predicting: 91it [00:52,  1.68it/s]Extractor Predicting: 92it [00:53,  1.68it/s]Extractor Predicting: 93it [00:54,  1.65it/s]Extractor Predicting: 94it [00:54,  1.63it/s]Extractor Predicting: 95it [00:55,  1.65it/s]Extractor Predicting: 96it [00:55,  1.65it/s]Extractor Predicting: 97it [00:56,  1.66it/s]Extractor Predicting: 98it [00:57,  1.65it/s]Extractor Predicting: 99it [00:57,  1.66it/s]Extractor Predicting: 100it [00:58,  1.66it/s]Extractor Predicting: 101it [00:58,  1.66it/s]Extractor Predicting: 102it [00:59,  1.66it/s]Extractor Predicting: 103it [01:00,  1.68it/s]Extractor Predicting: 104it [01:00,  1.66it/s]Extractor Predicting: 105it [01:01,  1.65it/s]Extractor Predicting: 106it [01:01,  1.68it/s]Extractor Predicting: 107it [01:02,  1.66it/s]Extractor Predicting: 108it [01:03,  1.66it/s]Extractor Predicting: 109it [01:03,  1.66it/s]Extractor Predicting: 110it [01:04,  1.64it/s]Extractor Predicting: 111it [01:04,  1.64it/s]Extractor Predicting: 112it [01:05,  1.69it/s]Extractor Predicting: 113it [01:06,  1.72it/s]Extractor Predicting: 114it [01:06,  1.74it/s]Extractor Predicting: 115it [01:07,  1.74it/s]Extractor Predicting: 116it [01:07,  1.74it/s]Extractor Predicting: 117it [01:08,  1.75it/s]Extractor Predicting: 118it [01:08,  1.77it/s]Extractor Predicting: 119it [01:09,  1.75it/s]Extractor Predicting: 120it [01:10,  1.75it/s]Extractor Predicting: 121it [01:10,  1.73it/s]Extractor Predicting: 122it [01:11,  1.78it/s]Extractor Predicting: 123it [01:11,  1.77it/s]Extractor Predicting: 124it [01:12,  1.76it/s]Extractor Predicting: 125it [01:12,  1.76it/s]Extractor Predicting: 126it [01:13,  1.73it/s]Extractor Predicting: 127it [01:14,  1.74it/s]Extractor Predicting: 128it [01:14,  1.76it/s]Extractor Predicting: 129it [01:15,  1.72it/s]Extractor Predicting: 130it [01:15,  1.74it/s]Extractor Predicting: 131it [01:16,  1.72it/s]Extractor Predicting: 132it [01:16,  1.73it/s]Extractor Predicting: 133it [01:17,  1.72it/s]Extractor Predicting: 134it [01:18,  1.72it/s]Extractor Predicting: 135it [01:18,  1.76it/s]Extractor Predicting: 136it [01:19,  1.75it/s]Extractor Predicting: 137it [01:19,  1.74it/s]Extractor Predicting: 138it [01:20,  1.74it/s]Extractor Predicting: 139it [01:20,  1.79it/s]Extractor Predicting: 140it [01:21,  1.75it/s]Extractor Predicting: 141it [01:22,  1.74it/s]Extractor Predicting: 142it [01:22,  1.60it/s]Extractor Predicting: 143it [01:23,  1.63it/s]Extractor Predicting: 144it [01:24,  1.64it/s]Extractor Predicting: 145it [01:24,  1.67it/s]Extractor Predicting: 146it [01:25,  1.69it/s]Extractor Predicting: 147it [01:25,  1.67it/s]Extractor Predicting: 148it [01:26,  1.68it/s]Extractor Predicting: 149it [01:26,  1.67it/s]Extractor Predicting: 150it [01:27,  1.69it/s]Extractor Predicting: 151it [01:28,  1.71it/s]Extractor Predicting: 152it [01:28,  1.73it/s]Extractor Predicting: 153it [01:29,  1.73it/s]Extractor Predicting: 154it [01:29,  1.71it/s]Extractor Predicting: 155it [01:30,  1.71it/s]Extractor Predicting: 156it [01:31,  1.69it/s]Extractor Predicting: 157it [01:31,  1.67it/s]Extractor Predicting: 158it [01:32,  1.67it/s]Extractor Predicting: 159it [01:32,  1.67it/s]Extractor Predicting: 160it [01:33,  1.68it/s]Extractor Predicting: 161it [01:34,  1.71it/s]Extractor Predicting: 162it [01:34,  1.73it/s]Extractor Predicting: 163it [01:35,  1.73it/s]Extractor Predicting: 164it [01:35,  1.73it/s]Extractor Predicting: 165it [01:36,  1.69it/s]Extractor Predicting: 166it [01:36,  1.72it/s]Extractor Predicting: 167it [01:37,  1.74it/s]Extractor Predicting: 168it [01:38,  1.73it/s]Extractor Predicting: 169it [01:38,  1.71it/s]Extractor Predicting: 170it [01:39,  1.68it/s]Extractor Predicting: 171it [01:39,  1.64it/s]Extractor Predicting: 172it [01:40,  1.66it/s]Extractor Predicting: 173it [01:41,  1.67it/s]Extractor Predicting: 174it [01:41,  1.61it/s]Extractor Predicting: 175it [01:42,  1.56it/s]Extractor Predicting: 176it [01:43,  1.59it/s]Extractor Predicting: 177it [01:43,  1.61it/s]Extractor Predicting: 178it [01:44,  1.66it/s]Extractor Predicting: 179it [01:44,  1.66it/s]Extractor Predicting: 180it [01:45,  1.70it/s]Extractor Predicting: 181it [01:45,  1.68it/s]Extractor Predicting: 182it [01:46,  1.72it/s]Extractor Predicting: 183it [01:47,  1.72it/s]Extractor Predicting: 184it [01:47,  1.76it/s]Extractor Predicting: 185it [01:48,  1.78it/s]Extractor Predicting: 186it [01:48,  1.78it/s]Extractor Predicting: 187it [01:49,  1.79it/s]Extractor Predicting: 188it [01:49,  1.78it/s]Extractor Predicting: 189it [01:50,  1.77it/s]Extractor Predicting: 190it [01:51,  1.74it/s]Extractor Predicting: 191it [01:51,  1.68it/s]Extractor Predicting: 192it [01:52,  1.71it/s]Extractor Predicting: 193it [01:52,  1.76it/s]Extractor Predicting: 194it [01:53,  1.74it/s]Extractor Predicting: 195it [01:53,  1.74it/s]Extractor Predicting: 196it [01:54,  1.77it/s]Extractor Predicting: 197it [01:55,  1.78it/s]Extractor Predicting: 198it [01:55,  1.74it/s]Extractor Predicting: 199it [01:56,  1.75it/s]Extractor Predicting: 200it [01:56,  1.75it/s]Extractor Predicting: 201it [01:57,  1.75it/s]Extractor Predicting: 202it [01:57,  1.79it/s]Extractor Predicting: 203it [01:58,  1.80it/s]Extractor Predicting: 204it [01:59,  1.78it/s]Extractor Predicting: 205it [01:59,  1.72it/s]Extractor Predicting: 206it [02:00,  1.72it/s]Extractor Predicting: 207it [02:00,  1.75it/s]Extractor Predicting: 208it [02:01,  1.76it/s]Extractor Predicting: 209it [02:01,  1.70it/s]Extractor Predicting: 210it [02:02,  1.68it/s]Extractor Predicting: 211it [02:03,  1.68it/s]Extractor Predicting: 212it [02:03,  1.71it/s]Extractor Predicting: 213it [02:04,  1.73it/s]Extractor Predicting: 214it [02:04,  1.68it/s]Extractor Predicting: 215it [02:05,  1.68it/s]Extractor Predicting: 216it [02:06,  1.72it/s]Extractor Predicting: 217it [02:06,  1.55it/s]Extractor Predicting: 218it [02:07,  1.54it/s]Extractor Predicting: 219it [02:08,  1.59it/s]Extractor Predicting: 220it [02:08,  1.62it/s]Extractor Predicting: 221it [02:09,  1.61it/s]Extractor Predicting: 222it [02:09,  1.62it/s]Extractor Predicting: 223it [02:10,  1.63it/s]Extractor Predicting: 224it [02:11,  1.68it/s]Extractor Predicting: 225it [02:11,  1.68it/s]Extractor Predicting: 226it [02:12,  1.73it/s]Extractor Predicting: 227it [02:12,  1.76it/s]Extractor Predicting: 228it [02:13,  1.73it/s]Extractor Predicting: 229it [02:13,  1.73it/s]Extractor Predicting: 230it [02:14,  1.70it/s]Extractor Predicting: 231it [02:15,  1.69it/s]Extractor Predicting: 232it [02:15,  1.69it/s]Extractor Predicting: 233it [02:16,  1.74it/s]Extractor Predicting: 234it [02:16,  1.72it/s]Extractor Predicting: 235it [02:17,  1.73it/s]Extractor Predicting: 236it [02:18,  1.70it/s]Extractor Predicting: 237it [02:18,  1.69it/s]Extractor Predicting: 238it [02:19,  1.73it/s]Extractor Predicting: 239it [02:19,  1.74it/s]Extractor Predicting: 240it [02:20,  1.73it/s]Extractor Predicting: 241it [02:20,  1.71it/s]Extractor Predicting: 242it [02:21,  1.70it/s]Extractor Predicting: 243it [02:22,  1.66it/s]Extractor Predicting: 244it [02:22,  1.68it/s]Extractor Predicting: 245it [02:23,  1.74it/s]Extractor Predicting: 246it [02:23,  1.72it/s]Extractor Predicting: 247it [02:24,  1.74it/s]Extractor Predicting: 248it [02:25,  1.74it/s]Extractor Predicting: 249it [02:25,  1.73it/s]Extractor Predicting: 250it [02:26,  1.72it/s]Extractor Predicting: 251it [02:26,  1.68it/s]Extractor Predicting: 252it [02:27,  1.69it/s]Extractor Predicting: 253it [02:28,  1.70it/s]Extractor Predicting: 254it [02:28,  1.73it/s]Extractor Predicting: 255it [02:29,  1.72it/s]Extractor Predicting: 256it [02:29,  1.70it/s]Extractor Predicting: 257it [02:30,  1.72it/s]Extractor Predicting: 258it [02:30,  1.70it/s]Extractor Predicting: 259it [02:31,  1.65it/s]Extractor Predicting: 260it [02:32,  1.68it/s]Extractor Predicting: 261it [02:32,  1.70it/s]Extractor Predicting: 262it [02:33,  1.69it/s]Extractor Predicting: 263it [02:33,  1.67it/s]Extractor Predicting: 264it [02:34,  1.67it/s]Extractor Predicting: 265it [02:35,  1.65it/s]Extractor Predicting: 266it [02:35,  1.63it/s]Extractor Predicting: 267it [02:36,  1.61it/s]Extractor Predicting: 268it [02:37,  1.62it/s]Extractor Predicting: 269it [02:37,  1.62it/s]Extractor Predicting: 270it [02:38,  1.62it/s]Extractor Predicting: 271it [02:38,  1.63it/s]Extractor Predicting: 272it [02:39,  1.64it/s]Extractor Predicting: 273it [02:40,  1.63it/s]Extractor Predicting: 274it [02:40,  1.63it/s]Extractor Predicting: 275it [02:41,  1.68it/s]Extractor Predicting: 276it [02:41,  1.67it/s]Extractor Predicting: 277it [02:42,  1.65it/s]Extractor Predicting: 278it [02:43,  1.64it/s]Extractor Predicting: 279it [02:43,  1.65it/s]Extractor Predicting: 280it [02:44,  1.64it/s]Extractor Predicting: 281it [02:44,  1.63it/s]Extractor Predicting: 282it [02:45,  1.64it/s]Extractor Predicting: 283it [02:46,  1.58it/s]Extractor Predicting: 284it [02:46,  1.57it/s]Extractor Predicting: 285it [02:47,  1.58it/s]Extractor Predicting: 286it [02:48,  1.57it/s]Extractor Predicting: 287it [02:48,  1.60it/s]Extractor Predicting: 288it [02:48,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:40,470 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:40,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:40,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:40,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:40,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:36:40,773 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:36:40,774 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:36:41,447 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:36:42,466 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:36:42,467 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:44,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:44,570 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:44,570 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:44,570 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:44,570 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:36:45,301 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:36:45,302 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:36:45,556 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:36:45,712 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:36:45,712 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:01,  1.98it/s]Extractor Predicting: 3it [00:01,  1.77it/s]
[INFO|configuration_utils.py:515] 2023-08-29 02:36:47,779 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:36:47,780 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:36:47,784 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:36:47,785 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 02:36:47,787 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:36:50,839 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 02:36:50,842 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 02:36:50,858 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:36:50,858 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:36:50,864 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:36:50,869 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:36:50,869 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:36:50,869 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:36:50,869 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:36:50,869 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:36:50,869 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 02:36:51,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:51,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:52,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:53,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:53,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:54,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:55,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:55,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:56,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:56,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:57,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:58,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:59,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:59,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:00,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:00,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:01,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:02,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:02,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:03,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:04,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:04,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:05,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:06,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:39, 15.71s/it][WARNING|generation_utils.py:914] 2023-08-29 02:37:06,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:07,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:08,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:08,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:09,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:10,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:10,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:11,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:12,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:12,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:13,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:14,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:14,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:15,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:16,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:16,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:17,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:18,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:18,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:19,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:20,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:20,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:30<03:16, 15.08s/it][WARNING|generation_utils.py:914] 2023-08-29 02:37:21,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:22,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:22,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:23,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:23,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:24,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:25,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:25,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:26,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:27,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:27,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:28,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:29,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:29,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:30,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:31,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:31,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:32,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:33,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:33,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:34,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:34,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:35,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:36,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:36,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:37,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:38,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:38,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:39,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:48<03:20, 16.70s/it][WARNING|generation_utils.py:914] 2023-08-29 02:37:40,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:40,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:41,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:42,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:42,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:43,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:44,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:44,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:45,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:45,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:46,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:47,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:47,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:48,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:49,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:49,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:50,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:51,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:52,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:52,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:53,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:54,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:03<02:55, 15.93s/it][WARNING|generation_utils.py:914] 2023-08-29 02:37:54,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:55,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:56,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:56,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:57,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:57,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:58,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:58,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:37:59,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:00,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:00,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:01,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:01,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:02,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:02,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:03,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:04,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:04,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:05,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:06,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:06,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:07,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:07,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:08,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:08,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:09,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:19<02:37, 15.73s/it][WARNING|generation_utils.py:914] 2023-08-29 02:38:10,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:10,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:11,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:12,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:12,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:13,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:13,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:14,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:15,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:16,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:16,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:17,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:18,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:18,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:19,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:20,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:20,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:21,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:21,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:22,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:23,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:23,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:24,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:25,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:34<02:21, 15.72s/it][WARNING|generation_utils.py:914] 2023-08-29 02:38:25,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:26,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:27,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:27,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:28,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:29,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:29,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:30,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:31,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:31,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:32,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:33,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:34,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:34,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:35,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:36,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:36,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:37,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:38,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:38,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:39,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:40,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:41,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:50<02:06, 15.76s/it][WARNING|generation_utils.py:914] 2023-08-29 02:38:41,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:42,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:43,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:43,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:44,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:44,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:45,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:46,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:47,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:47,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:48,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:49,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:49,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:50,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:51,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:51,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:52,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:53,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:53,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:54,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:55,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:55,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:56,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:57,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:58,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:59,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:08<01:55, 16.48s/it][WARNING|generation_utils.py:914] 2023-08-29 02:38:59,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:00,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:00,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:01,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:02,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:02,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:03,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:04,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:04,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:05,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:05,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:06,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:07,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:07,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:08,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:08,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:09,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:10,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:10,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:11,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:11,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:12,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:13,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:13,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:23<01:34, 15.81s/it][WARNING|generation_utils.py:914] 2023-08-29 02:39:14,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:14,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:15,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:16,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:17,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:17,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:18,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:19,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:19,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:20,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:21,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:21,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:22,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:23,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:23,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:24,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:24,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:25,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:26,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:26,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:27,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:28,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:28,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:38<01:18, 15.70s/it][WARNING|generation_utils.py:914] 2023-08-29 02:39:29,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:30,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:30,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:31,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:32,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:32,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:33,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:34,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:34,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:35,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:36,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:36,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:37,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:37,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:38,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:39,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:39,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:40,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:41,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:41,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:42,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:43,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:52<01:00, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-29 02:39:43,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:44,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:44,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:45,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:46,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:46,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:47,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:48,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:48,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:49,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:50,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:51,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:51,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:52,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:52,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:53,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:54,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:54,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:55,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:55,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:56,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:57,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:57,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:58,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:58,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:08<00:46, 15.39s/it][WARNING|generation_utils.py:914] 2023-08-29 02:39:59,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:00,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:00,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:01,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:02,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:02,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:03,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:03,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:04,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:05,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:05,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:06,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:07,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:07,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:08,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:09,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:09,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:10,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:10,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:11,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:12,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:13,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:13,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:23<00:30, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-29 02:40:14,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:15,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:15,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:16,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:17,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:17,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:18,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:19,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:19,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:20,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:20,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:21,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:22,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:22,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:23,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:24,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:24,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:25,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:26,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:26,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:27,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:27,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:28,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:38<00:15, 15.15s/it][WARNING|generation_utils.py:914] 2023-08-29 02:40:29,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:29,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:30,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:31,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:31,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:32,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:32,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:33,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:33,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:34,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:35,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:35,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:36,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:36,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:37,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:38,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:38,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:39,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:40,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:40,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:41,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:41,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:42,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:43,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:43,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:44,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:44,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:45,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:46,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:46,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:47,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:47,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:48,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:48,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:58<00:00, 16.66s/it]Generating: 100%|██████████| 15/15 [03:58<00:00, 15.89s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:55,994 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:55,999 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:56,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:56,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:56,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:40:56,687 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:40:56,688 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:40:56,934 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:40:57,998 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:40:57,998 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:41:00,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:41:00,098 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:41:00,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:41:00,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:41:00,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:41:00,823 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:41:00,824 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:41:01,504 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:41:01,673 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:41:01,673 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.859375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 100, 'raw': 160}
{'target': 600, 'success': 124, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 180, 'raw': 288}
{'target': 600, 'success': 200, 'raw': 320}
{'target': 600, 'success': 218, 'raw': 352}
{'target': 600, 'success': 240, 'raw': 384}
{'target': 600, 'success': 258, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 307, 'raw': 480}
{'target': 600, 'success': 328, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 374, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 418, 'raw': 640}
{'target': 600, 'success': 440, 'raw': 672}
{'target': 600, 'success': 460, 'raw': 704}
{'target': 600, 'success': 487, 'raw': 736}
{'target': 600, 'success': 506, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 569, 'raw': 864}
{'target': 600, 'success': 595, 'raw': 896}
{'target': 600, 'success': 617, 'raw': 928}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.6648706896551724, 'errors': {'', '(\'2012 MTV Video Music Video Awards\', \'nominated for\', \'\', \'He was in a songwriting competition on " The Simpsons " at the 2012 MTV Video Music Video Awards , winning in one of his four categories .\')', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 402, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 579, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.7271634615384616, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : architect .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 496, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 551, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 594, 'raw': 800}
{'target': 600, 'success': 616, 'raw': 832}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7403846153846154, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a new development project called " Bifurcio " , which was developed by the Swiss developers , EMC , for Bifurcio . Head Entity : Bifurcio , Tail Entity : Ericsson .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.80078125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8152173913043478, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.77375, 'errors': {'', '(\'Pristiniki\', \'member of political party\', \'\', \'In 2005 , she became the first female politician to serve in the parliament of Bulgaria , as first and leader of " Pristiniki " in the new parliament .\')', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : operator . Context : Later in the year , the company built a high - speed commuter railway crossing the Bordeaux via Bordeaux Station to Marseille , France . Head Entity : Marseille , Tail Entity : Ralf Rummel .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 76, 'raw': 128}
{'target': 600, 'success': 92, 'raw': 160}
{'target': 600, 'success': 111, 'raw': 192}
{'target': 600, 'success': 130, 'raw': 224}
{'target': 600, 'success': 144, 'raw': 256}
{'target': 600, 'success': 158, 'raw': 288}
{'target': 600, 'success': 176, 'raw': 320}
{'target': 600, 'success': 191, 'raw': 352}
{'target': 600, 'success': 208, 'raw': 384}
{'target': 600, 'success': 226, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 267, 'raw': 480}
{'target': 600, 'success': 284, 'raw': 512}
{'target': 600, 'success': 298, 'raw': 544}
{'target': 600, 'success': 315, 'raw': 576}
{'target': 600, 'success': 332, 'raw': 608}
{'target': 600, 'success': 349, 'raw': 640}
{'target': 600, 'success': 372, 'raw': 672}
{'target': 600, 'success': 388, 'raw': 704}
{'target': 600, 'success': 406, 'raw': 736}
{'target': 600, 'success': 424, 'raw': 768}
{'target': 600, 'success': 439, 'raw': 800}
{'target': 600, 'success': 454, 'raw': 832}
{'target': 600, 'success': 473, 'raw': 864}
{'target': 600, 'success': 490, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 530, 'raw': 960}
{'target': 600, 'success': 551, 'raw': 992}
{'target': 600, 'success': 567, 'raw': 1024}
{'target': 600, 'success': 586, 'raw': 1056}
{'target': 600, 'success': 608, 'raw': 1088}
{'prompt': 'Relation : position held .', 'success_rate': 0.5588235294117647, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 14871
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14971, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.44it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:02,  1.43it/s]Extractor Estimating: 4it [00:02,  1.45it/s]Extractor Estimating: 5it [00:03,  1.55it/s]Extractor Estimating: 6it [00:03,  1.54it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:05,  1.58it/s]Extractor Estimating: 9it [00:05,  1.54it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:07,  1.48it/s]Extractor Estimating: 12it [00:07,  1.52it/s]Extractor Estimating: 13it [00:08,  1.48it/s]Extractor Estimating: 14it [00:09,  1.49it/s]Extractor Estimating: 15it [00:09,  1.49it/s]Extractor Estimating: 16it [00:10,  1.47it/s]Extractor Estimating: 17it [00:11,  1.52it/s]Extractor Estimating: 18it [00:11,  1.47it/s]Extractor Estimating: 19it [00:12,  1.54it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:13,  1.51it/s]Extractor Estimating: 22it [00:14,  1.57it/s]Extractor Estimating: 23it [00:15,  1.56it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:16,  1.62it/s]Extractor Estimating: 26it [00:17,  1.56it/s]Extractor Estimating: 27it [00:17,  1.56it/s]Extractor Estimating: 28it [00:18,  1.60it/s]Extractor Estimating: 29it [00:18,  1.63it/s]Extractor Estimating: 30it [00:19,  1.62it/s]Extractor Estimating: 31it [00:20,  1.65it/s]Extractor Estimating: 32it [00:20,  1.66it/s]Extractor Estimating: 33it [00:21,  1.60it/s]Extractor Estimating: 34it [00:22,  1.43it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:23,  1.55it/s]Extractor Estimating: 37it [00:24,  1.46it/s]Extractor Estimating: 38it [00:24,  1.51it/s]Extractor Estimating: 39it [00:25,  1.52it/s]Extractor Estimating: 40it [00:26,  1.51it/s]Extractor Estimating: 41it [00:26,  1.49it/s]Extractor Estimating: 42it [00:27,  1.48it/s]Extractor Estimating: 43it [00:28,  1.50it/s]Extractor Estimating: 44it [00:28,  1.46it/s]Extractor Estimating: 45it [00:29,  1.47it/s]Extractor Estimating: 46it [00:30,  1.51it/s]Extractor Estimating: 47it [00:30,  1.53it/s]Extractor Estimating: 48it [00:31,  1.51it/s]Extractor Estimating: 49it [00:32,  1.56it/s]Extractor Estimating: 50it [00:32,  1.54it/s]Extractor Estimating: 51it [00:33,  1.52it/s]Extractor Estimating: 52it [00:34,  1.51it/s]Extractor Estimating: 53it [00:34,  1.51it/s]Extractor Estimating: 54it [00:35,  1.48it/s]Extractor Estimating: 55it [00:36,  1.45it/s]Extractor Estimating: 56it [00:36,  1.46it/s]Extractor Estimating: 57it [00:37,  1.45it/s]Extractor Estimating: 58it [00:38,  1.46it/s]Extractor Estimating: 59it [00:38,  1.43it/s]Extractor Estimating: 60it [00:39,  1.47it/s]Extractor Estimating: 61it [00:40,  1.42it/s]Extractor Estimating: 62it [00:40,  1.45it/s]Extractor Estimating: 63it [00:41,  1.43it/s]Extractor Estimating: 64it [00:42,  1.48it/s]Extractor Estimating: 65it [00:43,  1.44it/s]Extractor Estimating: 66it [00:43,  1.45it/s]Extractor Estimating: 67it [00:44,  1.44it/s]Extractor Estimating: 68it [00:45,  1.50it/s]Extractor Estimating: 69it [00:45,  1.53it/s]Extractor Estimating: 70it [00:46,  1.52it/s]Extractor Estimating: 71it [00:47,  1.49it/s]Extractor Estimating: 72it [00:47,  1.50it/s]Extractor Estimating: 73it [00:48,  1.48it/s]Extractor Estimating: 74it [00:48,  1.53it/s]Extractor Estimating: 75it [00:49,  1.51it/s]Extractor Estimating: 76it [00:50,  1.57it/s]Extractor Estimating: 77it [00:50,  1.59it/s]Extractor Estimating: 78it [00:51,  1.66it/s]Extractor Estimating: 79it [00:51,  1.67it/s]Extractor Estimating: 80it [00:52,  1.72it/s]Extractor Estimating: 81it [00:53,  1.73it/s]Extractor Estimating: 82it [00:53,  1.70it/s]Extractor Estimating: 83it [00:54,  1.69it/s]Extractor Estimating: 84it [00:54,  1.69it/s]Extractor Estimating: 85it [00:55,  1.66it/s]Extractor Estimating: 86it [00:56,  1.68it/s]Extractor Estimating: 87it [00:56,  1.66it/s]Extractor Estimating: 88it [00:57,  1.71it/s]Extractor Estimating: 89it [00:57,  1.72it/s]Extractor Estimating: 90it [00:58,  1.74it/s]Extractor Estimating: 91it [00:59,  1.69it/s]Extractor Estimating: 92it [00:59,  1.71it/s]Extractor Estimating: 93it [01:00,  1.71it/s]Extractor Estimating: 94it [01:00,  1.60it/s]Extractor Estimating: 95it [01:01,  1.53it/s]Extractor Estimating: 96it [01:02,  1.56it/s]Extractor Estimating: 97it [01:02,  1.60it/s]Extractor Estimating: 98it [01:03,  1.64it/s]Extractor Estimating: 99it [01:04,  1.64it/s]Extractor Estimating: 100it [01:04,  1.69it/s]Extractor Estimating: 101it [01:05,  1.71it/s]Extractor Estimating: 102it [01:05,  1.70it/s]Extractor Estimating: 103it [01:06,  1.71it/s]Extractor Estimating: 104it [01:06,  1.75it/s]Extractor Estimating: 105it [01:07,  1.73it/s]Extractor Estimating: 106it [01:08,  1.70it/s]Extractor Estimating: 107it [01:08,  1.75it/s]Extractor Estimating: 108it [01:09,  1.82it/s]Extractor Estimating: 109it [01:09,  1.78it/s]Extractor Estimating: 110it [01:10,  1.80it/s]Extractor Estimating: 111it [01:10,  1.64it/s]Extractor Estimating: 112it [01:11,  1.68it/s]Extractor Estimating: 113it [01:12,  1.66it/s]Extractor Estimating: 114it [01:12,  1.70it/s]Extractor Estimating: 115it [01:13,  1.71it/s]Extractor Estimating: 116it [01:13,  1.72it/s]Extractor Estimating: 117it [01:14,  1.69it/s]Extractor Estimating: 118it [01:15,  1.68it/s]Extractor Estimating: 119it [01:15,  1.63it/s]Extractor Estimating: 120it [01:16,  1.71it/s]Extractor Estimating: 121it [01:16,  1.67it/s]Extractor Estimating: 122it [01:17,  1.59it/s]Extractor Estimating: 123it [01:18,  1.65it/s]Extractor Estimating: 124it [01:18,  1.70it/s]Extractor Estimating: 125it [01:19,  1.69it/s]Extractor Estimating: 126it [01:19,  1.68it/s]Extractor Estimating: 127it [01:20,  1.63it/s]Extractor Estimating: 128it [01:21,  1.67it/s]Extractor Estimating: 129it [01:21,  1.69it/s]Extractor Estimating: 130it [01:22,  1.71it/s]Extractor Estimating: 131it [01:22,  1.69it/s]Extractor Estimating: 132it [01:23,  1.69it/s]Extractor Estimating: 133it [01:24,  1.65it/s]Extractor Estimating: 134it [01:24,  1.70it/s]Extractor Estimating: 135it [01:25,  1.64it/s]Extractor Estimating: 136it [01:25,  1.62it/s]Extractor Estimating: 137it [01:26,  1.61it/s]Extractor Estimating: 138it [01:27,  1.64it/s]Extractor Estimating: 139it [01:27,  1.63it/s]Extractor Estimating: 140it [01:28,  1.63it/s]Extractor Estimating: 141it [01:29,  1.60it/s]Extractor Estimating: 142it [01:29,  1.62it/s]Extractor Estimating: 143it [01:30,  1.68it/s]Extractor Estimating: 144it [01:30,  1.67it/s]Extractor Estimating: 145it [01:31,  1.68it/s]Extractor Estimating: 146it [01:31,  1.69it/s]Extractor Estimating: 147it [01:32,  1.65it/s]Extractor Estimating: 148it [01:33,  1.60it/s]Extractor Estimating: 149it [01:33,  1.59it/s]Extractor Estimating: 150it [01:34,  1.59it/s]Extractor Estimating: 151it [01:35,  1.56it/s]Extractor Estimating: 152it [01:35,  1.53it/s]Extractor Estimating: 153it [01:36,  1.55it/s]Extractor Estimating: 154it [01:37,  1.59it/s]Extractor Estimating: 155it [01:37,  1.62it/s]Extractor Estimating: 156it [01:38,  1.65it/s]Extractor Estimating: 157it [01:38,  1.69it/s]Extractor Estimating: 158it [01:39,  1.61it/s]Extractor Estimating: 159it [01:40,  1.69it/s]Extractor Estimating: 160it [01:40,  1.66it/s]Extractor Estimating: 161it [01:41,  1.63it/s]Extractor Estimating: 162it [01:41,  1.62it/s]Extractor Estimating: 163it [01:42,  1.55it/s]Extractor Estimating: 164it [01:43,  1.58it/s]Extractor Estimating: 165it [01:43,  1.51it/s]Extractor Estimating: 166it [01:44,  1.52it/s]Extractor Estimating: 167it [01:45,  1.56it/s]Extractor Estimating: 168it [01:45,  1.51it/s]Extractor Estimating: 169it [01:46,  1.52it/s]Extractor Estimating: 170it [01:47,  1.59it/s]Extractor Estimating: 171it [01:47,  1.53it/s]Extractor Estimating: 172it [01:48,  1.54it/s]Extractor Estimating: 173it [01:49,  1.55it/s]Extractor Estimating: 174it [01:49,  1.49it/s]Extractor Estimating: 175it [01:50,  1.54it/s]Extractor Estimating: 176it [01:51,  1.48it/s]Extractor Estimating: 177it [01:51,  1.43it/s]Extractor Estimating: 178it [01:52,  1.45it/s]Extractor Estimating: 179it [01:53,  1.53it/s]Extractor Estimating: 180it [01:53,  1.60it/s]Extractor Estimating: 181it [01:54,  1.59it/s]Extractor Estimating: 182it [01:54,  1.59it/s]Extractor Estimating: 183it [01:55,  1.54it/s]Extractor Estimating: 184it [01:56,  1.48it/s]Extractor Estimating: 185it [01:57,  1.51it/s]Extractor Estimating: 186it [01:57,  1.56it/s]Extractor Estimating: 187it [01:58,  1.59it/s]Extractor Estimating: 188it [01:58,  1.55it/s]Extractor Estimating: 189it [01:59,  1.62it/s]Extractor Estimating: 190it [02:00,  1.64it/s]Extractor Estimating: 191it [02:00,  1.59it/s]Extractor Estimating: 192it [02:01,  1.62it/s]Extractor Estimating: 193it [02:01,  1.60it/s]Extractor Estimating: 194it [02:02,  1.58it/s]Extractor Estimating: 195it [02:03,  1.60it/s]Extractor Estimating: 196it [02:03,  1.63it/s]Extractor Estimating: 197it [02:04,  1.63it/s]Extractor Estimating: 198it [02:05,  1.59it/s]Extractor Estimating: 199it [02:05,  1.54it/s]Extractor Estimating: 200it [02:06,  1.62it/s]Extractor Estimating: 201it [02:06,  1.60it/s]Extractor Estimating: 202it [02:07,  1.59it/s]Extractor Estimating: 203it [02:08,  1.60it/s]Extractor Estimating: 204it [02:08,  1.52it/s]Extractor Estimating: 205it [02:09,  1.54it/s]Extractor Estimating: 206it [02:10,  1.58it/s]Extractor Estimating: 207it [02:10,  1.49it/s]Extractor Estimating: 208it [02:11,  1.48it/s]Extractor Estimating: 209it [02:12,  1.47it/s]Extractor Estimating: 210it [02:12,  1.50it/s]Extractor Estimating: 211it [02:13,  1.55it/s]Extractor Estimating: 212it [02:14,  1.53it/s]Extractor Estimating: 213it [02:14,  1.53it/s]Extractor Estimating: 214it [02:15,  1.50it/s]Extractor Estimating: 215it [02:16,  1.56it/s]Extractor Estimating: 216it [02:16,  1.57it/s]Extractor Estimating: 217it [02:17,  1.54it/s]Extractor Estimating: 218it [02:18,  1.57it/s]Extractor Estimating: 219it [02:18,  1.55it/s]Extractor Estimating: 220it [02:19,  1.57it/s]Extractor Estimating: 221it [02:20,  1.53it/s]Extractor Estimating: 222it [02:20,  1.58it/s]Extractor Estimating: 223it [02:21,  1.63it/s]Extractor Estimating: 224it [02:21,  1.64it/s]Extractor Estimating: 225it [02:22,  1.60it/s]Extractor Estimating: 226it [02:23,  1.56it/s]Extractor Estimating: 227it [02:23,  1.54it/s]Extractor Estimating: 228it [02:24,  1.55it/s]Extractor Estimating: 229it [02:25,  1.51it/s]Extractor Estimating: 230it [02:25,  1.48it/s]Extractor Estimating: 231it [02:26,  1.47it/s]Extractor Estimating: 232it [02:27,  1.49it/s]Extractor Estimating: 233it [02:27,  1.49it/s]Extractor Estimating: 234it [02:28,  1.52it/s]Extractor Estimating: 235it [02:29,  1.49it/s]Extractor Estimating: 236it [02:29,  1.48it/s]Extractor Estimating: 237it [02:30,  1.50it/s]Extractor Estimating: 238it [02:31,  1.46it/s]Extractor Estimating: 239it [02:32,  1.42it/s]Extractor Estimating: 240it [02:32,  1.46it/s]Extractor Estimating: 241it [02:33,  1.48it/s]Extractor Estimating: 242it [02:33,  1.49it/s]Extractor Estimating: 243it [02:34,  1.50it/s]Extractor Estimating: 244it [02:35,  1.46it/s]Extractor Estimating: 245it [02:36,  1.41it/s]Extractor Estimating: 246it [02:36,  1.47it/s]Extractor Estimating: 247it [02:37,  1.47it/s]Extractor Estimating: 248it [02:38,  1.43it/s]Extractor Estimating: 249it [02:38,  1.45it/s]Extractor Estimating: 250it [02:39,  1.48it/s]Extractor Estimating: 251it [02:40,  1.44it/s]Extractor Estimating: 252it [02:40,  1.55it/s]Extractor Estimating: 253it [02:41,  1.51it/s]Extractor Estimating: 254it [02:41,  1.59it/s]Extractor Estimating: 255it [02:42,  1.65it/s]Extractor Estimating: 256it [02:43,  1.66it/s]Extractor Estimating: 257it [02:43,  1.69it/s]Extractor Estimating: 258it [02:44,  1.68it/s]Extractor Estimating: 259it [02:44,  1.66it/s]Extractor Estimating: 260it [02:45,  1.58it/s]Extractor Estimating: 261it [02:46,  1.61it/s]Extractor Estimating: 262it [02:46,  1.66it/s]Extractor Estimating: 263it [02:47,  1.65it/s]Extractor Estimating: 264it [02:47,  1.70it/s]Extractor Estimating: 265it [02:48,  1.69it/s]Extractor Estimating: 266it [02:49,  1.77it/s]Extractor Estimating: 267it [02:49,  1.76it/s]Extractor Estimating: 268it [02:50,  1.76it/s]Extractor Estimating: 269it [02:50,  1.76it/s]Extractor Estimating: 270it [02:51,  1.72it/s]Extractor Estimating: 271it [02:51,  1.72it/s]Extractor Estimating: 272it [02:52,  1.72it/s]Extractor Estimating: 273it [02:53,  1.76it/s]Extractor Estimating: 274it [02:53,  1.72it/s]Extractor Estimating: 275it [02:54,  1.74it/s]Extractor Estimating: 276it [02:54,  1.70it/s]Extractor Estimating: 277it [02:55,  1.63it/s]Extractor Estimating: 278it [02:56,  1.60it/s]Extractor Estimating: 279it [02:56,  1.57it/s]Extractor Estimating: 280it [02:57,  1.60it/s]Extractor Estimating: 281it [02:58,  1.61it/s]Extractor Estimating: 282it [02:58,  1.54it/s]Extractor Estimating: 283it [02:59,  1.56it/s]Extractor Estimating: 284it [03:00,  1.51it/s]Extractor Estimating: 285it [03:00,  1.52it/s]Extractor Estimating: 286it [03:01,  1.57it/s]Extractor Estimating: 287it [03:01,  1.59it/s]Extractor Estimating: 288it [03:02,  1.64it/s]Extractor Estimating: 289it [03:03,  1.60it/s]Extractor Estimating: 290it [03:03,  1.64it/s]Extractor Estimating: 291it [03:04,  1.65it/s]Extractor Estimating: 292it [03:04,  1.66it/s]Extractor Estimating: 293it [03:05,  1.63it/s]Extractor Estimating: 294it [03:06,  1.61it/s]Extractor Estimating: 295it [03:06,  1.59it/s]Extractor Estimating: 296it [03:07,  1.59it/s]Extractor Estimating: 297it [03:08,  1.59it/s]Extractor Estimating: 298it [03:08,  1.60it/s]Extractor Estimating: 299it [03:09,  1.59it/s]Extractor Estimating: 300it [03:09,  1.59it/s]Extractor Estimating: 301it [03:10,  1.59it/s]Extractor Estimating: 302it [03:11,  1.55it/s]Extractor Estimating: 303it [03:11,  1.54it/s]Extractor Estimating: 304it [03:12,  1.55it/s]Extractor Estimating: 305it [03:13,  1.54it/s]Extractor Estimating: 306it [03:13,  1.60it/s]Extractor Estimating: 307it [03:14,  1.60it/s]Extractor Estimating: 308it [03:15,  1.62it/s]Extractor Estimating: 309it [03:15,  1.61it/s]Extractor Estimating: 310it [03:16,  1.59it/s]Extractor Estimating: 311it [03:17,  1.54it/s]Extractor Estimating: 312it [03:17,  1.46it/s]Extractor Estimating: 313it [03:18,  1.44it/s]Extractor Estimating: 314it [03:19,  1.49it/s]Extractor Estimating: 315it [03:19,  1.49it/s]Extractor Estimating: 316it [03:20,  1.52it/s]Extractor Estimating: 317it [03:21,  1.51it/s]Extractor Estimating: 318it [03:21,  1.53it/s]Extractor Estimating: 319it [03:22,  1.59it/s]Extractor Estimating: 320it [03:22,  1.63it/s]Extractor Estimating: 321it [03:23,  1.60it/s]Extractor Estimating: 322it [03:24,  1.55it/s]Extractor Estimating: 323it [03:24,  1.56it/s]Extractor Estimating: 324it [03:25,  1.55it/s]Extractor Estimating: 325it [03:26,  1.52it/s]Extractor Estimating: 326it [03:26,  1.55it/s]Extractor Estimating: 327it [03:27,  1.49it/s]Extractor Estimating: 328it [03:28,  1.49it/s]Extractor Estimating: 329it [03:28,  1.46it/s]Extractor Estimating: 330it [03:29,  1.47it/s]Extractor Estimating: 331it [03:30,  1.47it/s]Extractor Estimating: 332it [03:30,  1.55it/s]Extractor Estimating: 333it [03:31,  1.54it/s]Extractor Estimating: 334it [03:32,  1.58it/s]Extractor Estimating: 335it [03:32,  1.41it/s]Extractor Estimating: 336it [03:33,  1.46it/s]Extractor Estimating: 337it [03:34,  1.42it/s]Extractor Estimating: 338it [03:35,  1.46it/s]Extractor Estimating: 339it [03:35,  1.49it/s]Extractor Estimating: 340it [03:36,  1.55it/s]Extractor Estimating: 341it [03:36,  1.57it/s]Extractor Estimating: 342it [03:37,  1.57it/s]Extractor Estimating: 343it [03:38,  1.58it/s]Extractor Estimating: 344it [03:38,  1.54it/s]Extractor Estimating: 345it [03:39,  1.53it/s]Extractor Estimating: 346it [03:40,  1.43it/s]Extractor Estimating: 347it [03:40,  1.46it/s]Extractor Estimating: 348it [03:41,  1.53it/s]Extractor Estimating: 349it [03:42,  1.53it/s]Extractor Estimating: 350it [03:42,  1.51it/s]Extractor Estimating: 351it [03:43,  1.54it/s]Extractor Estimating: 352it [03:44,  1.52it/s]Extractor Estimating: 353it [03:44,  1.57it/s]Extractor Estimating: 354it [03:45,  1.57it/s]Extractor Estimating: 355it [03:45,  1.58it/s]Extractor Estimating: 356it [03:46,  1.55it/s]Extractor Estimating: 357it [03:47,  1.59it/s]Extractor Estimating: 358it [03:47,  1.60it/s]Extractor Estimating: 359it [03:48,  1.61it/s]Extractor Estimating: 360it [03:49,  1.58it/s]Extractor Estimating: 361it [03:49,  1.59it/s]Extractor Estimating: 362it [03:50,  1.63it/s]Extractor Estimating: 363it [03:51,  1.59it/s]Extractor Estimating: 364it [03:51,  1.60it/s]Extractor Estimating: 365it [03:52,  1.59it/s]Extractor Estimating: 366it [03:52,  1.56it/s]Extractor Estimating: 367it [03:53,  1.55it/s]Extractor Estimating: 368it [03:54,  1.60it/s]Extractor Estimating: 369it [03:54,  1.62it/s]Extractor Estimating: 370it [03:55,  1.61it/s]Extractor Estimating: 371it [03:56,  1.59it/s]Extractor Estimating: 372it [03:56,  1.68it/s]Extractor Estimating: 373it [03:57,  1.70it/s]Extractor Estimating: 374it [03:57,  1.71it/s]Extractor Estimating: 375it [03:58,  1.61it/s]Extractor Estimating: 375it [03:58,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:45:12,290 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:45:12,295 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:45:12,295 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:45:12,295 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:45:12,295 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:45:12,689 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:45:12,690 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:45:12,945 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:45:14,021 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:45:14,021 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:45:16,771 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:45:16,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:45:16,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:45:16,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:45:16,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:45:17,424 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:45:17,425 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:45:17,992 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:45:18,166 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:45:18,166 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 05:00:34,599 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 05:00:34,631 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7911 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 24475
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24575, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24575, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.032, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.014, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.016, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 70, avg_time 1.038, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 170, avg_time 1.042, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 270, avg_time 2.045, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 40, avg_time 1.019, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 140, avg_time 1.029, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 240, avg_time 1.036, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 10, avg_time 1.038, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 110, avg_time 2.041, loss:nan
g_step 1200, step 210, avg_time 1.049, loss:nan
g_step 1300, step 310, avg_time 1.017, loss:nan
g_step 1400, step 80, avg_time 1.028, loss:nan
g_step 1500, step 180, avg_time 1.015, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 280, avg_time 2.059, loss:nan
g_step 1700, step 50, avg_time 1.012, loss:nan
g_step 1800, step 150, avg_time 1.033, loss:nan
g_step 1900, step 250, avg_time 1.033, loss:nan
g_step 2000, step 20, avg_time 1.032, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 120, avg_time 2.037, loss:nan
g_step 2200, step 220, avg_time 1.029, loss:nan
g_step 2300, step 320, avg_time 1.034, loss:nan
g_step 2400, step 90, avg_time 1.006, loss:nan
g_step 2500, step 190, avg_time 1.042, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 290, avg_time 2.039, loss:nan
g_step 2700, step 60, avg_time 1.029, loss:nan
g_step 2800, step 160, avg_time 1.041, loss:nan
g_step 2900, step 260, avg_time 1.039, loss:nan
g_step 3000, step 30, avg_time 1.016, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 130, avg_time 2.070, loss:nan
g_step 3200, step 230, avg_time 1.024, loss:nan
g_step 3300, step 330, avg_time 1.020, loss:nan
g_step 3400, step 100, avg_time 1.015, loss:nan
g_step 3500, step 200, avg_time 1.043, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 300, avg_time 2.055, loss:nan
g_step 3700, step 70, avg_time 1.023, loss:nan
g_step 3800, step 170, avg_time 1.055, loss:nan
g_step 3900, step 270, avg_time 1.008, loss:nan
g_step 4000, step 40, avg_time 1.033, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 140, avg_time 2.041, loss:nan
g_step 4200, step 240, avg_time 1.054, loss:nan
g_step 4300, step 10, avg_time 1.001, loss:nan
g_step 4400, step 110, avg_time 1.025, loss:nan
g_step 4500, step 210, avg_time 1.026, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 310, avg_time 2.070, loss:nan
g_step 4700, step 80, avg_time 1.042, loss:nan
g_step 4800, step 180, avg_time 1.013, loss:nan
g_step 4900, step 280, avg_time 1.027, loss:nan
g_step 5000, step 50, avg_time 1.009, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 150, avg_time 2.062, loss:nan
g_step 5200, step 250, avg_time 1.024, loss:nan
g_step 5300, step 20, avg_time 1.044, loss:nan
g_step 5400, step 120, avg_time 1.027, loss:nan
g_step 5500, step 220, avg_time 1.036, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 320, avg_time 2.030, loss:nan
g_step 5700, step 90, avg_time 1.011, loss:nan
g_step 5800, step 190, avg_time 1.037, loss:nan
g_step 5900, step 290, avg_time 1.037, loss:nan
g_step 6000, step 60, avg_time 1.027, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 160, avg_time 2.048, loss:nan
g_step 6200, step 260, avg_time 1.046, loss:nan
g_step 6300, step 30, avg_time 1.027, loss:nan
g_step 6400, step 130, avg_time 1.034, loss:nan
g_step 6500, step 230, avg_time 1.016, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 330, avg_time 2.037, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 05:00:34 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 05:00:34 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_05-00-34_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 05:00:35 - WARNING - datasets.builder -   Using custom data configuration default-ab1298b58f651b26
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ab1298b58f651b26/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 05:00:35,910 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:00:35,911 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:00:35,912 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:00:35,913 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:00:35,921 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:00:35,926 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:00:35,926 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:00:35,926 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:00:35,926 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:00:35,926 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:00:35,926 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 05:00:36,082 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:00:39,120 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 05:00:39,123 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ab1298b58f651b26/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.19ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.22ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.73ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.04ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.24ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.37ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.47ba/s]100%|██████████| 8/8 [00:01<00:00,  4.63ba/s]100%|██████████| 8/8 [00:01<00:00,  4.12ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.09ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.33ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.41ba/s]100%|██████████| 4/4 [00:00<00:00,  5.52ba/s]100%|██████████| 4/4 [00:00<00:00,  5.00ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.13ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.96ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 11.24ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 11.41ba/s]100%|██████████| 8/8 [00:00<00:00, 11.40ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.61ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 11.06ba/s]100%|██████████| 4/4 [00:00<00:00, 12.47ba/s]
[INFO|trainer.py:414] 2023-08-29 05:00:43,277 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 05:00:43,291 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 05:00:43,291 >>   Num examples = 7920
[INFO|trainer.py:1149] 2023-08-29 05:00:43,291 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 05:00:43,291 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 05:00:43,292 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 05:00:43,292 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 05:00:43,292 >>   Total optimization steps = 620
  0%|          | 0/620 [00:00<?, ?it/s]  0%|          | 1/620 [00:00<02:56,  3.52it/s]  0%|          | 2/620 [00:00<02:52,  3.59it/s]  0%|          | 3/620 [00:00<02:51,  3.60it/s]  1%|          | 4/620 [00:01<02:50,  3.61it/s]  1%|          | 5/620 [00:01<02:49,  3.62it/s]  1%|          | 6/620 [00:01<02:49,  3.62it/s]  1%|          | 7/620 [00:01<02:49,  3.63it/s]  1%|▏         | 8/620 [00:02<02:48,  3.62it/s]  1%|▏         | 9/620 [00:02<02:50,  3.59it/s]  2%|▏         | 10/620 [00:02<02:49,  3.60it/s]  2%|▏         | 11/620 [00:03<02:48,  3.61it/s]  2%|▏         | 12/620 [00:03<02:48,  3.61it/s]  2%|▏         | 13/620 [00:03<02:47,  3.61it/s]  2%|▏         | 14/620 [00:03<02:47,  3.61it/s]  2%|▏         | 15/620 [00:04<02:47,  3.62it/s]  3%|▎         | 16/620 [00:04<02:46,  3.62it/s]  3%|▎         | 17/620 [00:04<02:46,  3.61it/s]  3%|▎         | 18/620 [00:04<02:46,  3.61it/s]  3%|▎         | 19/620 [00:05<02:46,  3.61it/s]  3%|▎         | 20/620 [00:05<02:46,  3.60it/s]  3%|▎         | 21/620 [00:05<02:46,  3.60it/s]  4%|▎         | 22/620 [00:06<02:45,  3.61it/s]  4%|▎         | 23/620 [00:06<02:45,  3.61it/s]  4%|▍         | 24/620 [00:06<02:44,  3.61it/s]  4%|▍         | 25/620 [00:06<02:44,  3.61it/s]  4%|▍         | 26/620 [00:07<02:44,  3.61it/s]  4%|▍         | 27/620 [00:07<02:44,  3.61it/s]  5%|▍         | 28/620 [00:07<02:43,  3.61it/s]  5%|▍         | 29/620 [00:08<02:43,  3.61it/s]  5%|▍         | 30/620 [00:08<02:43,  3.62it/s]  5%|▌         | 31/620 [00:08<02:43,  3.60it/s]  5%|▌         | 32/620 [00:08<02:43,  3.60it/s]  5%|▌         | 33/620 [00:09<02:43,  3.60it/s]  5%|▌         | 34/620 [00:09<02:42,  3.61it/s]  6%|▌         | 35/620 [00:09<02:42,  3.61it/s]  6%|▌         | 36/620 [00:09<02:41,  3.61it/s]  6%|▌         | 37/620 [00:10<02:41,  3.61it/s]  6%|▌         | 38/620 [00:10<02:41,  3.61it/s]  6%|▋         | 39/620 [00:10<02:40,  3.61it/s]  6%|▋         | 40/620 [00:11<02:40,  3.61it/s]  7%|▋         | 41/620 [00:11<02:40,  3.61it/s]  7%|▋         | 42/620 [00:11<02:41,  3.59it/s]  7%|▋         | 43/620 [00:11<02:40,  3.59it/s]  7%|▋         | 44/620 [00:12<02:40,  3.60it/s]  7%|▋         | 45/620 [00:12<02:39,  3.61it/s]  7%|▋         | 46/620 [00:12<02:38,  3.61it/s]  8%|▊         | 47/620 [00:13<02:38,  3.61it/s]  8%|▊         | 48/620 [00:13<02:38,  3.61it/s]  8%|▊         | 49/620 [00:13<02:38,  3.61it/s]  8%|▊         | 50/620 [00:13<02:37,  3.61it/s]  8%|▊         | 51/620 [00:14<02:37,  3.61it/s]  8%|▊         | 52/620 [00:14<02:37,  3.61it/s]  9%|▊         | 53/620 [00:14<02:37,  3.59it/s]  9%|▊         | 54/620 [00:14<02:37,  3.60it/s]  9%|▉         | 55/620 [00:15<02:36,  3.60it/s]  9%|▉         | 56/620 [00:15<02:36,  3.60it/s]  9%|▉         | 57/620 [00:15<02:36,  3.60it/s]  9%|▉         | 58/620 [00:16<02:35,  3.60it/s] 10%|▉         | 59/620 [00:16<02:35,  3.61it/s] 10%|▉         | 60/620 [00:16<02:35,  3.61it/s] 10%|▉         | 61/620 [00:16<02:34,  3.61it/s] 10%|█         | 62/620 [00:17<02:34,  3.61it/s] 10%|█         | 63/620 [00:17<02:34,  3.61it/s] 10%|█         | 64/620 [00:17<02:34,  3.60it/s] 10%|█         | 65/620 [00:18<02:34,  3.60it/s] 11%|█         | 66/620 [00:18<02:33,  3.60it/s] 11%|█         | 67/620 [00:18<02:33,  3.60it/s] 11%|█         | 68/620 [00:18<02:33,  3.60it/s] 11%|█         | 69/620 [00:19<02:32,  3.61it/s] 11%|█▏        | 70/620 [00:19<02:32,  3.61it/s] 11%|█▏        | 71/620 [00:19<02:32,  3.61it/s] 12%|█▏        | 72/620 [00:19<02:31,  3.61it/s] 12%|█▏        | 73/620 [00:20<02:31,  3.61it/s] 12%|█▏        | 74/620 [00:20<02:31,  3.61it/s] 12%|█▏        | 75/620 [00:20<02:31,  3.60it/s] 12%|█▏        | 76/620 [00:21<02:30,  3.60it/s] 12%|█▏        | 77/620 [00:21<02:30,  3.60it/s] 13%|█▎        | 78/620 [00:21<02:30,  3.61it/s] 13%|█▎        | 79/620 [00:21<02:29,  3.61it/s] 13%|█▎        | 80/620 [00:22<02:29,  3.61it/s] 13%|█▎        | 81/620 [00:22<02:29,  3.61it/s] 13%|█▎        | 82/620 [00:22<02:29,  3.61it/s] 13%|█▎        | 83/620 [00:23<02:28,  3.61it/s] 14%|█▎        | 84/620 [00:23<02:28,  3.61it/s] 14%|█▎        | 85/620 [00:23<02:28,  3.61it/s] 14%|█▍        | 86/620 [00:23<02:27,  3.61it/s] 14%|█▍        | 87/620 [00:24<02:27,  3.61it/s] 14%|█▍        | 88/620 [00:24<02:27,  3.61it/s] 14%|█▍        | 89/620 [00:24<02:27,  3.61it/s] 15%|█▍        | 90/620 [00:24<02:26,  3.61it/s] 15%|█▍        | 91/620 [00:25<02:27,  3.59it/s] 15%|█▍        | 92/620 [00:25<02:26,  3.60it/s] 15%|█▌        | 93/620 [00:25<02:26,  3.60it/s] 15%|█▌        | 94/620 [00:26<02:25,  3.60it/s] 15%|█▌        | 95/620 [00:26<02:25,  3.60it/s] 15%|█▌        | 96/620 [00:26<02:25,  3.60it/s] 16%|█▌        | 97/620 [00:26<02:25,  3.60it/s] 16%|█▌        | 98/620 [00:27<02:24,  3.61it/s] 16%|█▌        | 99/620 [00:27<02:24,  3.61it/s] 16%|█▌        | 100/620 [00:27<02:24,  3.61it/s] 16%|█▋        | 101/620 [00:28<02:23,  3.61it/s] 16%|█▋        | 102/620 [00:28<02:23,  3.60it/s] 17%|█▋        | 103/620 [00:28<02:23,  3.60it/s] 17%|█▋        | 104/620 [00:28<02:23,  3.60it/s] 17%|█▋        | 105/620 [00:29<02:23,  3.60it/s] 17%|█▋        | 106/620 [00:29<02:22,  3.60it/s] 17%|█▋        | 107/620 [00:29<02:22,  3.60it/s] 17%|█▋        | 108/620 [00:29<02:22,  3.60it/s] 18%|█▊        | 109/620 [00:30<02:21,  3.60it/s] 18%|█▊        | 110/620 [00:30<02:21,  3.61it/s] 18%|█▊        | 111/620 [00:30<02:21,  3.60it/s] 18%|█▊        | 112/620 [00:31<02:20,  3.60it/s] 18%|█▊        | 113/620 [00:31<02:21,  3.59it/s] 18%|█▊        | 114/620 [00:31<02:20,  3.59it/s] 19%|█▊        | 115/620 [00:31<02:20,  3.60it/s] 19%|█▊        | 116/620 [00:32<02:20,  3.60it/s] 19%|█▉        | 117/620 [00:32<02:19,  3.60it/s] 19%|█▉        | 118/620 [00:32<02:19,  3.60it/s] 19%|█▉        | 119/620 [00:33<02:18,  3.61it/s] 19%|█▉        | 120/620 [00:33<02:18,  3.60it/s] 20%|█▉        | 121/620 [00:33<02:18,  3.60it/s] 20%|█▉        | 122/620 [00:33<02:18,  3.60it/s] 20%|█▉        | 123/620 [00:34<02:18,  3.60it/s] 20%|██        | 124/620 [00:34<02:09,  3.84it/s][INFO|trainer.py:2140] 2023-08-29 05:01:17,628 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:01:17,628 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 05:01:17,628 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.77it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.80it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.82it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.77it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.03it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.54it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.12it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.98it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.10it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.27it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.40it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.48it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.35it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.19it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.96it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.81it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.78it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.97it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.01it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.32it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.41it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.27it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.16it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.90it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.81it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.71it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.92it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.09it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.30it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.32it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.27it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.21it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.10it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.83it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.85it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.98it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.11it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.27it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.30it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.16it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.13it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.98it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.90it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.86it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.01it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.12it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.26it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.25it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.29it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.03it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.99it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.90it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.85it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.02it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.20it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.30it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.36it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.22it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.06it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.96it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.87it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.88it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.04it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.15it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.25it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.34it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.13it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.04it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.88it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.85it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.82it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.06it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.17it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.24it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.31it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.14it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.03it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.85it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.88it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.91it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.09it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.24it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.32it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.34it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.15it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.97it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.93it/s][A                                                 
                                                 [A 20%|██        | 124/620 [00:44<02:09,  3.84it/s]
100%|██████████| 437/437 [00:09<00:00, 43.93it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:01:27,547 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-124
[INFO|configuration_utils.py:351] 2023-08-29 05:01:27,573 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-124/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:01:29,228 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-124/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:01:29,240 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-124/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:01:29,250 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-124/special_tokens_map.json
 20%|██        | 125/620 [00:46<31:38,  3.84s/it] 20%|██        | 126/620 [00:46<22:48,  2.77s/it] 20%|██        | 127/620 [00:47<16:37,  2.02s/it] 21%|██        | 128/620 [00:47<12:18,  1.50s/it] 21%|██        | 129/620 [00:47<09:17,  1.14s/it] 21%|██        | 130/620 [00:47<07:10,  1.14it/s] 21%|██        | 131/620 [00:48<05:42,  1.43it/s] 21%|██▏       | 132/620 [00:48<04:39,  1.74it/s] 21%|██▏       | 133/620 [00:48<03:55,  2.06it/s] 22%|██▏       | 134/620 [00:49<03:25,  2.37it/s] 22%|██▏       | 135/620 [00:49<03:03,  2.64it/s] 22%|██▏       | 136/620 [00:49<02:48,  2.87it/s] 22%|██▏       | 137/620 [00:49<02:38,  3.04it/s] 22%|██▏       | 138/620 [00:50<02:31,  3.19it/s] 22%|██▏       | 139/620 [00:50<02:25,  3.30it/s] 23%|██▎       | 140/620 [00:50<02:22,  3.38it/s] 23%|██▎       | 141/620 [00:50<02:19,  3.44it/s] 23%|██▎       | 142/620 [00:51<02:16,  3.49it/s] 23%|██▎       | 143/620 [00:51<02:18,  3.44it/s] 23%|██▎       | 144/620 [00:51<02:16,  3.48it/s] 23%|██▎       | 145/620 [00:52<02:15,  3.52it/s] 24%|██▎       | 146/620 [00:52<02:13,  3.55it/s] 24%|██▎       | 147/620 [00:52<02:13,  3.55it/s] 24%|██▍       | 148/620 [00:52<02:12,  3.55it/s] 24%|██▍       | 149/620 [00:53<02:12,  3.55it/s] 24%|██▍       | 150/620 [00:53<02:12,  3.55it/s] 24%|██▍       | 151/620 [00:53<02:11,  3.55it/s] 25%|██▍       | 152/620 [00:54<02:11,  3.55it/s] 25%|██▍       | 153/620 [00:54<02:11,  3.55it/s] 25%|██▍       | 154/620 [00:54<02:11,  3.55it/s] 25%|██▌       | 155/620 [00:54<02:11,  3.54it/s] 25%|██▌       | 156/620 [00:55<02:10,  3.55it/s] 25%|██▌       | 157/620 [00:55<02:10,  3.55it/s] 25%|██▌       | 158/620 [00:55<02:10,  3.55it/s] 26%|██▌       | 159/620 [00:56<02:10,  3.52it/s] 26%|██▌       | 160/620 [00:56<02:10,  3.53it/s] 26%|██▌       | 161/620 [00:56<02:10,  3.53it/s] 26%|██▌       | 162/620 [00:56<02:09,  3.53it/s] 26%|██▋       | 163/620 [00:57<02:09,  3.54it/s] 26%|██▋       | 164/620 [00:57<02:08,  3.54it/s] 27%|██▋       | 165/620 [00:57<02:08,  3.55it/s] 27%|██▋       | 166/620 [00:58<02:07,  3.55it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/620 [00:58<02:07,  3.55it/s] 27%|██▋       | 168/620 [00:58<02:07,  3.55it/s] 27%|██▋       | 169/620 [00:58<02:07,  3.55it/s] 27%|██▋       | 170/620 [00:59<02:07,  3.54it/s] 28%|██▊       | 171/620 [00:59<02:06,  3.54it/s] 28%|██▊       | 172/620 [00:59<02:06,  3.54it/s] 28%|██▊       | 173/620 [01:00<02:06,  3.54it/s] 28%|██▊       | 174/620 [01:00<02:05,  3.55it/s] 28%|██▊       | 175/620 [01:00<02:05,  3.55it/s] 28%|██▊       | 176/620 [01:00<02:04,  3.55it/s] 29%|██▊       | 177/620 [01:01<02:04,  3.55it/s] 29%|██▊       | 178/620 [01:01<02:04,  3.55it/s] 29%|██▉       | 179/620 [01:01<02:04,  3.55it/s] 29%|██▉       | 180/620 [01:01<02:04,  3.55it/s] 29%|██▉       | 181/620 [01:02<02:04,  3.53it/s] 29%|██▉       | 182/620 [01:02<02:03,  3.54it/s] 30%|██▉       | 183/620 [01:02<02:03,  3.54it/s] 30%|██▉       | 184/620 [01:03<02:02,  3.55it/s] 30%|██▉       | 185/620 [01:03<02:02,  3.55it/s] 30%|███       | 186/620 [01:03<02:02,  3.55it/s] 30%|███       | 187/620 [01:03<02:02,  3.55it/s] 30%|███       | 188/620 [01:04<02:01,  3.55it/s] 30%|███       | 189/620 [01:04<02:01,  3.55it/s] 31%|███       | 190/620 [01:04<02:01,  3.55it/s] 31%|███       | 191/620 [01:05<02:00,  3.55it/s] 31%|███       | 192/620 [01:05<02:00,  3.54it/s] 31%|███       | 193/620 [01:05<02:00,  3.54it/s] 31%|███▏      | 194/620 [01:05<02:00,  3.54it/s] 31%|███▏      | 195/620 [01:06<01:59,  3.55it/s] 32%|███▏      | 196/620 [01:06<01:59,  3.55it/s] 32%|███▏      | 197/620 [01:06<01:59,  3.55it/s] 32%|███▏      | 198/620 [01:07<01:59,  3.54it/s] 32%|███▏      | 199/620 [01:07<01:58,  3.54it/s] 32%|███▏      | 200/620 [01:07<01:58,  3.54it/s] 32%|███▏      | 201/620 [01:07<01:58,  3.55it/s] 33%|███▎      | 202/620 [01:08<01:57,  3.55it/s] 33%|███▎      | 203/620 [01:08<01:58,  3.52it/s] 33%|███▎      | 204/620 [01:08<01:57,  3.53it/s] 33%|███▎      | 205/620 [01:09<01:57,  3.53it/s] 33%|███▎      | 206/620 [01:09<01:57,  3.54it/s] 33%|███▎      | 207/620 [01:09<01:56,  3.54it/s] 34%|███▎      | 208/620 [01:09<01:56,  3.54it/s] 34%|███▎      | 209/620 [01:10<01:55,  3.55it/s] 34%|███▍      | 210/620 [01:10<01:55,  3.55it/s] 34%|███▍      | 211/620 [01:10<01:55,  3.55it/s] 34%|███▍      | 212/620 [01:11<01:54,  3.55it/s] 34%|███▍      | 213/620 [01:11<01:54,  3.55it/s] 35%|███▍      | 214/620 [01:11<01:54,  3.53it/s] 35%|███▍      | 215/620 [01:11<01:54,  3.54it/s] 35%|███▍      | 216/620 [01:12<01:54,  3.54it/s] 35%|███▌      | 217/620 [01:12<01:53,  3.55it/s] 35%|███▌      | 218/620 [01:12<01:53,  3.55it/s] 35%|███▌      | 219/620 [01:12<01:52,  3.55it/s] 35%|███▌      | 220/620 [01:13<01:52,  3.55it/s] 36%|███▌      | 221/620 [01:13<01:52,  3.55it/s] 36%|███▌      | 222/620 [01:13<01:52,  3.55it/s] 36%|███▌      | 223/620 [01:14<01:51,  3.55it/s] 36%|███▌      | 224/620 [01:14<01:51,  3.55it/s] 36%|███▋      | 225/620 [01:14<01:52,  3.52it/s] 36%|███▋      | 226/620 [01:14<01:51,  3.53it/s] 37%|███▋      | 227/620 [01:15<01:51,  3.54it/s] 37%|███▋      | 228/620 [01:15<01:50,  3.54it/s] 37%|███▋      | 229/620 [01:15<01:50,  3.55it/s] 37%|███▋      | 230/620 [01:16<01:49,  3.55it/s] 37%|███▋      | 231/620 [01:16<01:49,  3.55it/s] 37%|███▋      | 232/620 [01:16<01:49,  3.55it/s] 38%|███▊      | 233/620 [01:16<01:49,  3.55it/s] 38%|███▊      | 234/620 [01:17<01:48,  3.54it/s] 38%|███▊      | 235/620 [01:17<01:48,  3.54it/s] 38%|███▊      | 236/620 [01:17<01:49,  3.50it/s] 38%|███▊      | 237/620 [01:18<01:49,  3.51it/s] 38%|███▊      | 238/620 [01:18<01:48,  3.52it/s] 39%|███▊      | 239/620 [01:18<01:47,  3.53it/s] 39%|███▊      | 240/620 [01:18<01:47,  3.54it/s] 39%|███▉      | 241/620 [01:19<01:47,  3.54it/s] 39%|███▉      | 242/620 [01:19<01:46,  3.55it/s] 39%|███▉      | 243/620 [01:19<01:46,  3.55it/s] 39%|███▉      | 244/620 [01:20<01:46,  3.55it/s] 40%|███▉      | 245/620 [01:20<01:45,  3.55it/s] 40%|███▉      | 246/620 [01:20<01:45,  3.54it/s] 40%|███▉      | 247/620 [01:20<01:45,  3.54it/s] 40%|████      | 248/620 [01:21<01:37,  3.80it/s][INFO|trainer.py:2140] 2023-08-29 05:02:04,420 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:02:04,420 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 05:02:04,420 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9044, 'eval_samples_per_second': 352.672, 'eval_steps_per_second': 44.122, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.47it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.43it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.02it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.77it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.94it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.40it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.16it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.87it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.98it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.20it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.26it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.37it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.28it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.18it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.93it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.94it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.84it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.93it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.11it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.24it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.23it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.20it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.05it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.95it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.86it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.97it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.02it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.18it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.24it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.15it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.08it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.01it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.82it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.84it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.92it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.94it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.14it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.22it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.12it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.02it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.91it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.83it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.80it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.92it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.08it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.17it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.24it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.19it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.04it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.03it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.86it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.82it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.87it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.07it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.11it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.29it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.17it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.08it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.02it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.92it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.91it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.89it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.00it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.10it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.22it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.20it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.14it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.04it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.01it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.90it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.90it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.05it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.14it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.17it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.12it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.03it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.92it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.99it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.93it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.94it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.05it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.19it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.13it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.15it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.00it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.99it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.87it/s][A                                                 
                                                 [A 40%|████      | 248/620 [01:31<01:37,  3.80it/s]
100%|██████████| 437/437 [00:09<00:00, 43.87it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:02:14,367 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-248
[INFO|configuration_utils.py:351] 2023-08-29 05:02:14,423 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-248/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:02:16,503 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-248/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:02:16,528 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-248/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:02:16,540 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-248/special_tokens_map.json
 40%|████      | 249/620 [01:33<24:42,  4.00s/it] 40%|████      | 250/620 [01:34<17:46,  2.88s/it] 40%|████      | 251/620 [01:34<12:55,  2.10s/it] 41%|████      | 252/620 [01:34<09:32,  1.56s/it] 41%|████      | 253/620 [01:34<07:10,  1.17s/it] 41%|████      | 254/620 [01:35<05:31,  1.10it/s] 41%|████      | 255/620 [01:35<04:22,  1.39it/s] 41%|████▏     | 256/620 [01:35<03:33,  1.70it/s] 41%|████▏     | 257/620 [01:36<02:59,  2.02it/s] 42%|████▏     | 258/620 [01:36<02:35,  2.33it/s] 42%|████▏     | 259/620 [01:36<02:19,  2.60it/s] 42%|████▏     | 260/620 [01:36<02:07,  2.83it/s] 42%|████▏     | 261/620 [01:37<01:58,  3.03it/s] 42%|████▏     | 262/620 [01:37<01:52,  3.18it/s] 42%|████▏     | 263/620 [01:37<01:48,  3.30it/s] 43%|████▎     | 264/620 [01:38<01:45,  3.39it/s] 43%|████▎     | 265/620 [01:38<01:42,  3.45it/s] 43%|████▎     | 266/620 [01:38<01:41,  3.50it/s] 43%|████▎     | 267/620 [01:38<01:40,  3.53it/s] 43%|████▎     | 268/620 [01:39<01:39,  3.55it/s] 43%|████▎     | 269/620 [01:39<01:38,  3.57it/s] 44%|████▎     | 270/620 [01:39<01:38,  3.55it/s] 44%|████▎     | 271/620 [01:39<01:37,  3.57it/s] 44%|████▍     | 272/620 [01:40<01:37,  3.58it/s] 44%|████▍     | 273/620 [01:40<01:36,  3.59it/s] 44%|████▍     | 274/620 [01:40<01:36,  3.60it/s] 44%|████▍     | 275/620 [01:41<01:35,  3.60it/s] 45%|████▍     | 276/620 [01:41<01:35,  3.60it/s] 45%|████▍     | 277/620 [01:41<01:35,  3.60it/s] 45%|████▍     | 278/620 [01:41<01:34,  3.60it/s] 45%|████▌     | 279/620 [01:42<01:34,  3.60it/s] 45%|████▌     | 280/620 [01:42<01:34,  3.60it/s] 45%|████▌     | 281/620 [01:42<01:34,  3.59it/s] 45%|████▌     | 282/620 [01:43<01:33,  3.60it/s] 46%|████▌     | 283/620 [01:43<01:33,  3.60it/s] 46%|████▌     | 284/620 [01:43<01:33,  3.60it/s] 46%|████▌     | 285/620 [01:43<01:32,  3.60it/s] 46%|████▌     | 286/620 [01:44<01:32,  3.60it/s] 46%|████▋     | 287/620 [01:44<01:32,  3.60it/s] 46%|████▋     | 288/620 [01:44<01:32,  3.60it/s] 47%|████▋     | 289/620 [01:44<01:31,  3.60it/s] 47%|████▋     | 290/620 [01:45<01:31,  3.61it/s] 47%|████▋     | 291/620 [01:45<01:31,  3.61it/s] 47%|████▋     | 292/620 [01:45<01:31,  3.59it/s] 47%|████▋     | 293/620 [01:46<01:30,  3.60it/s] 47%|████▋     | 294/620 [01:46<01:30,  3.60it/s] 48%|████▊     | 295/620 [01:46<01:30,  3.60it/s] 48%|████▊     | 296/620 [01:46<01:30,  3.60it/s] 48%|████▊     | 297/620 [01:47<01:29,  3.60it/s] 48%|████▊     | 298/620 [01:47<01:29,  3.60it/s] 48%|████▊     | 299/620 [01:47<01:29,  3.60it/s] 48%|████▊     | 300/620 [01:48<01:28,  3.60it/s] 49%|████▊     | 301/620 [01:48<01:28,  3.60it/s] 49%|████▊     | 302/620 [01:48<01:28,  3.60it/s] 49%|████▉     | 303/620 [01:48<01:28,  3.59it/s] 49%|████▉     | 304/620 [01:49<01:27,  3.59it/s] 49%|████▉     | 305/620 [01:49<01:27,  3.59it/s] 49%|████▉     | 306/620 [01:49<01:27,  3.58it/s] 50%|████▉     | 307/620 [01:49<01:27,  3.57it/s] 50%|████▉     | 308/620 [01:50<01:27,  3.56it/s] 50%|████▉     | 309/620 [01:50<01:27,  3.54it/s] 50%|█████     | 310/620 [01:50<01:27,  3.54it/s] 50%|█████     | 311/620 [01:51<01:27,  3.54it/s] 50%|█████     | 312/620 [01:51<01:26,  3.54it/s] 50%|█████     | 313/620 [01:51<01:28,  3.47it/s] 51%|█████     | 314/620 [01:51<01:28,  3.47it/s] 51%|█████     | 315/620 [01:52<01:27,  3.50it/s] 51%|█████     | 316/620 [01:52<01:26,  3.52it/s] 51%|█████     | 317/620 [01:52<01:25,  3.52it/s] 51%|█████▏    | 318/620 [01:53<01:25,  3.53it/s] 51%|█████▏    | 319/620 [01:53<01:25,  3.54it/s] 52%|█████▏    | 320/620 [01:53<01:24,  3.54it/s] 52%|█████▏    | 321/620 [01:53<01:24,  3.53it/s] 52%|█████▏    | 322/620 [01:54<01:24,  3.54it/s] 52%|█████▏    | 323/620 [01:54<01:23,  3.54it/s] 52%|█████▏    | 324/620 [01:54<01:23,  3.54it/s] 52%|█████▏    | 325/620 [01:55<01:23,  3.54it/s] 53%|█████▎    | 326/620 [01:55<01:22,  3.55it/s] 53%|█████▎    | 327/620 [01:55<01:22,  3.57it/s] 53%|█████▎    | 328/620 [01:55<01:21,  3.58it/s] 53%|█████▎    | 329/620 [01:56<01:21,  3.59it/s] 53%|█████▎    | 330/620 [01:56<01:20,  3.59it/s] 53%|█████▎    | 331/620 [01:56<01:20,  3.59it/s] 54%|█████▎    | 332/620 [01:57<01:20,  3.60it/s] 54%|█████▎    | 333/620 [01:57<01:19,  3.60it/s] 54%|█████▍    | 334/620 [01:57<01:19,  3.60it/s] 54%|█████▍    | 335/620 [01:57<01:19,  3.60it/s] 54%|█████▍    | 336/620 [01:58<01:19,  3.58it/s] 54%|█████▍    | 337/620 [01:58<01:18,  3.59it/s] 55%|█████▍    | 338/620 [01:58<01:18,  3.60it/s] 55%|█████▍    | 339/620 [01:58<01:18,  3.60it/s] 55%|█████▍    | 340/620 [01:59<01:17,  3.60it/s] 55%|█████▌    | 341/620 [01:59<01:17,  3.60it/s] 55%|█████▌    | 342/620 [01:59<01:17,  3.60it/s] 55%|█████▌    | 343/620 [02:00<01:16,  3.60it/s] 55%|█████▌    | 344/620 [02:00<01:16,  3.61it/s] 56%|█████▌    | 345/620 [02:00<01:16,  3.61it/s] 56%|█████▌    | 346/620 [02:00<01:15,  3.61it/s] 56%|█████▌    | 347/620 [02:01<01:15,  3.61it/s] 56%|█████▌    | 348/620 [02:01<01:15,  3.61it/s] 56%|█████▋    | 349/620 [02:01<01:15,  3.60it/s] 56%|█████▋    | 350/620 [02:02<01:14,  3.60it/s] 57%|█████▋    | 351/620 [02:02<01:14,  3.60it/s] 57%|█████▋    | 352/620 [02:02<01:14,  3.59it/s] 57%|█████▋    | 353/620 [02:02<01:14,  3.60it/s] 57%|█████▋    | 354/620 [02:03<01:13,  3.60it/s] 57%|█████▋    | 355/620 [02:03<01:13,  3.60it/s] 57%|█████▋    | 356/620 [02:03<01:13,  3.60it/s] 58%|█████▊    | 357/620 [02:03<01:12,  3.60it/s] 58%|█████▊    | 358/620 [02:04<01:12,  3.60it/s] 58%|█████▊    | 359/620 [02:04<01:12,  3.60it/s] 58%|█████▊    | 360/620 [02:04<01:12,  3.60it/s] 58%|█████▊    | 361/620 [02:05<01:11,  3.60it/s] 58%|█████▊    | 362/620 [02:05<01:11,  3.60it/s] 59%|█████▊    | 363/620 [02:05<01:11,  3.59it/s] 59%|█████▊    | 364/620 [02:05<01:11,  3.59it/s] 59%|█████▉    | 365/620 [02:06<01:10,  3.60it/s] 59%|█████▉    | 366/620 [02:06<01:10,  3.60it/s] 59%|█████▉    | 367/620 [02:06<01:10,  3.60it/s] 59%|█████▉    | 368/620 [02:07<01:09,  3.60it/s] 60%|█████▉    | 369/620 [02:07<01:09,  3.60it/s] 60%|█████▉    | 370/620 [02:07<01:09,  3.60it/s] 60%|█████▉    | 371/620 [02:07<01:09,  3.60it/s] 60%|██████    | 372/620 [02:08<01:04,  3.86it/s][INFO|trainer.py:2140] 2023-08-29 05:02:51,371 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:02:51,371 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 05:02:51,371 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.917, 'eval_samples_per_second': 352.224, 'eval_steps_per_second': 44.066, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.88it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.72it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.70it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.53it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.78it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.36it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.09it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.97it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.05it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.27it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.35it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.42it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.30it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.16it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.89it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.85it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.80it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.86it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.02it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.17it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.31it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.30it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.14it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.04it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.89it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.92it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.94it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.08it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.24it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.34it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.25it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.17it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.01it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.92it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.93it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.73it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.06it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.26it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.34it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.30it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.17it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.07it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.91it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.93it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.94it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.10it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.28it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.36it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.28it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.21it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.06it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.91it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.93it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.00it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.01it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.26it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.22it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.25it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.16it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.08it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.96it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.95it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.96it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.09it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.90it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.21it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.13it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.09it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.02it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.92it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.93it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.98it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.08it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.16it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.28it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.23it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.21it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.05it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.00it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.92it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.98it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.98it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.15it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.15it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.18it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.11it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.06it/s][A                                                 
                                                 [A 60%|██████    | 372/620 [02:17<01:04,  3.86it/s]
100%|██████████| 437/437 [00:09<00:00, 44.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:03:01,289 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-372
[INFO|configuration_utils.py:351] 2023-08-29 05:03:01,305 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-372/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:03:03,069 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-372/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:03:03,083 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-372/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:03:03,093 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-372/special_tokens_map.json
 60%|██████    | 373/620 [02:20<15:55,  3.87s/it] 60%|██████    | 374/620 [02:20<11:27,  2.79s/it] 60%|██████    | 375/620 [02:20<08:19,  2.04s/it] 61%|██████    | 376/620 [02:21<06:09,  1.51s/it] 61%|██████    | 377/620 [02:21<04:37,  1.14s/it] 61%|██████    | 378/620 [02:21<03:33,  1.13it/s] 61%|██████    | 379/620 [02:22<02:48,  1.43it/s] 61%|██████▏   | 380/620 [02:22<02:17,  1.74it/s] 61%|██████▏   | 381/620 [02:22<01:55,  2.06it/s] 62%|██████▏   | 382/620 [02:22<01:40,  2.36it/s] 62%|██████▏   | 383/620 [02:23<01:29,  2.64it/s] 62%|██████▏   | 384/620 [02:23<01:22,  2.87it/s] 62%|██████▏   | 385/620 [02:23<01:16,  3.05it/s] 62%|██████▏   | 386/620 [02:23<01:13,  3.19it/s] 62%|██████▏   | 387/620 [02:24<01:10,  3.30it/s] 63%|██████▎   | 388/620 [02:24<01:08,  3.39it/s] 63%|██████▎   | 389/620 [02:24<01:06,  3.45it/s] 63%|██████▎   | 390/620 [02:25<01:05,  3.50it/s] 63%|██████▎   | 391/620 [02:25<01:04,  3.53it/s] 63%|██████▎   | 392/620 [02:25<01:04,  3.55it/s] 63%|██████▎   | 393/620 [02:25<01:03,  3.57it/s] 64%|██████▎   | 394/620 [02:26<01:03,  3.58it/s] 64%|██████▎   | 395/620 [02:26<01:02,  3.59it/s] 64%|██████▍   | 396/620 [02:26<01:02,  3.59it/s] 64%|██████▍   | 397/620 [02:27<01:02,  3.54it/s] 64%|██████▍   | 398/620 [02:27<01:02,  3.56it/s] 64%|██████▍   | 399/620 [02:27<01:01,  3.58it/s] 65%|██████▍   | 400/620 [02:27<01:01,  3.58it/s] 65%|██████▍   | 401/620 [02:28<01:00,  3.59it/s] 65%|██████▍   | 402/620 [02:28<01:00,  3.60it/s] 65%|██████▌   | 403/620 [02:28<01:00,  3.60it/s] 65%|██████▌   | 404/620 [02:29<00:59,  3.60it/s] 65%|██████▌   | 405/620 [02:29<00:59,  3.60it/s] 65%|██████▌   | 406/620 [02:29<00:59,  3.60it/s] 66%|██████▌   | 407/620 [02:29<00:59,  3.60it/s] 66%|██████▌   | 408/620 [02:30<00:59,  3.59it/s] 66%|██████▌   | 409/620 [02:30<00:58,  3.60it/s] 66%|██████▌   | 410/620 [02:30<00:58,  3.60it/s] 66%|██████▋   | 411/620 [02:30<00:57,  3.60it/s] 66%|██████▋   | 412/620 [02:31<00:57,  3.61it/s] 67%|██████▋   | 413/620 [02:31<00:57,  3.60it/s] 67%|██████▋   | 414/620 [02:31<00:57,  3.60it/s] 67%|██████▋   | 415/620 [02:32<00:56,  3.60it/s] 67%|██████▋   | 416/620 [02:32<00:56,  3.60it/s] 67%|██████▋   | 417/620 [02:32<00:56,  3.60it/s] 67%|██████▋   | 418/620 [02:32<00:55,  3.61it/s] 68%|██████▊   | 419/620 [02:33<00:55,  3.59it/s] 68%|██████▊   | 420/620 [02:33<00:55,  3.60it/s] 68%|██████▊   | 421/620 [02:33<00:55,  3.60it/s] 68%|██████▊   | 422/620 [02:34<00:54,  3.60it/s] 68%|██████▊   | 423/620 [02:34<00:54,  3.60it/s] 68%|██████▊   | 424/620 [02:34<00:54,  3.60it/s] 69%|██████▊   | 425/620 [02:34<00:54,  3.61it/s] 69%|██████▊   | 426/620 [02:35<00:53,  3.61it/s] 69%|██████▉   | 427/620 [02:35<00:53,  3.61it/s] 69%|██████▉   | 428/620 [02:35<00:53,  3.61it/s] 69%|██████▉   | 429/620 [02:35<00:52,  3.61it/s] 69%|██████▉   | 430/620 [02:36<00:53,  3.58it/s] 70%|██████▉   | 431/620 [02:36<00:52,  3.59it/s] 70%|██████▉   | 432/620 [02:36<00:52,  3.59it/s] 70%|██████▉   | 433/620 [02:37<00:52,  3.60it/s] 70%|███████   | 434/620 [02:37<00:51,  3.60it/s] 70%|███████   | 435/620 [02:37<00:51,  3.60it/s] 70%|███████   | 436/620 [02:37<00:51,  3.61it/s] 70%|███████   | 437/620 [02:38<00:50,  3.61it/s] 71%|███████   | 438/620 [02:38<00:50,  3.61it/s] 71%|███████   | 439/620 [02:38<00:50,  3.60it/s] 71%|███████   | 440/620 [02:38<00:49,  3.60it/s] 71%|███████   | 441/620 [02:39<00:49,  3.59it/s] 71%|███████▏  | 442/620 [02:39<00:49,  3.59it/s] 71%|███████▏  | 443/620 [02:39<00:49,  3.59it/s] 72%|███████▏  | 444/620 [02:40<00:48,  3.60it/s] 72%|███████▏  | 445/620 [02:40<00:48,  3.60it/s] 72%|███████▏  | 446/620 [02:40<00:48,  3.60it/s] 72%|███████▏  | 447/620 [02:40<00:48,  3.60it/s] 72%|███████▏  | 448/620 [02:41<00:47,  3.61it/s] 72%|███████▏  | 449/620 [02:41<00:47,  3.61it/s] 73%|███████▎  | 450/620 [02:41<00:47,  3.61it/s] 73%|███████▎  | 451/620 [02:42<00:46,  3.61it/s] 73%|███████▎  | 452/620 [02:42<00:46,  3.59it/s] 73%|███████▎  | 453/620 [02:42<00:46,  3.59it/s] 73%|███████▎  | 454/620 [02:42<00:46,  3.60it/s] 73%|███████▎  | 455/620 [02:43<00:45,  3.60it/s] 74%|███████▎  | 456/620 [02:43<00:45,  3.60it/s] 74%|███████▎  | 457/620 [02:43<00:45,  3.60it/s] 74%|███████▍  | 458/620 [02:43<00:44,  3.61it/s] 74%|███████▍  | 459/620 [02:44<00:44,  3.60it/s] 74%|███████▍  | 460/620 [02:44<00:44,  3.60it/s] 74%|███████▍  | 461/620 [02:44<00:44,  3.60it/s] 75%|███████▍  | 462/620 [02:45<00:43,  3.60it/s] 75%|███████▍  | 463/620 [02:45<00:43,  3.59it/s] 75%|███████▍  | 464/620 [02:45<00:43,  3.59it/s] 75%|███████▌  | 465/620 [02:45<00:43,  3.60it/s] 75%|███████▌  | 466/620 [02:46<00:42,  3.60it/s] 75%|███████▌  | 467/620 [02:46<00:42,  3.60it/s] 75%|███████▌  | 468/620 [02:46<00:42,  3.60it/s] 76%|███████▌  | 469/620 [02:47<00:41,  3.60it/s] 76%|███████▌  | 470/620 [02:47<00:41,  3.60it/s] 76%|███████▌  | 471/620 [02:47<00:41,  3.60it/s] 76%|███████▌  | 472/620 [02:47<00:41,  3.60it/s] 76%|███████▋  | 473/620 [02:48<00:40,  3.60it/s] 76%|███████▋  | 474/620 [02:48<00:40,  3.60it/s] 77%|███████▋  | 475/620 [02:48<00:40,  3.59it/s] 77%|███████▋  | 476/620 [02:48<00:40,  3.60it/s] 77%|███████▋  | 477/620 [02:49<00:39,  3.60it/s] 77%|███████▋  | 478/620 [02:49<00:39,  3.60it/s] 77%|███████▋  | 479/620 [02:49<00:39,  3.60it/s] 77%|███████▋  | 480/620 [02:50<00:38,  3.60it/s] 78%|███████▊  | 481/620 [02:50<00:38,  3.60it/s] 78%|███████▊  | 482/620 [02:50<00:38,  3.60it/s] 78%|███████▊  | 483/620 [02:50<00:38,  3.60it/s] 78%|███████▊  | 484/620 [02:51<00:37,  3.60it/s] 78%|███████▊  | 485/620 [02:51<00:37,  3.61it/s] 78%|███████▊  | 486/620 [02:51<00:38,  3.51it/s] 79%|███████▊  | 487/620 [02:52<00:37,  3.53it/s] 79%|███████▊  | 488/620 [02:52<00:37,  3.55it/s] 79%|███████▉  | 489/620 [02:52<00:36,  3.57it/s] 79%|███████▉  | 490/620 [02:52<00:36,  3.58it/s] 79%|███████▉  | 491/620 [02:53<00:35,  3.59it/s] 79%|███████▉  | 492/620 [02:53<00:35,  3.59it/s] 80%|███████▉  | 493/620 [02:53<00:35,  3.58it/s] 80%|███████▉  | 494/620 [02:54<00:35,  3.59it/s] 80%|███████▉  | 495/620 [02:54<00:34,  3.59it/s] 80%|████████  | 496/620 [02:54<00:32,  3.85it/s][INFO|trainer.py:2140] 2023-08-29 05:03:37,813 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:03:37,813 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 05:03:37,813 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.906, 'eval_samples_per_second': 352.616, 'eval_steps_per_second': 44.115, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.71it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.69it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.69it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.59it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.82it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.52it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.17it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.83it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.96it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.20it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.33it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.41it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.36it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.27it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.02it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.89it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.83it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.82it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.06it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.16it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.26it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.23it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.11it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.95it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.85it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.79it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.86it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.07it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.25it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.26it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.22it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.10it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.98it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.87it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.87it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.88it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.03it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.16it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.31it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.27it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.20it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.97it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.85it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.92it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.93it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.06it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.22it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.25it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.22it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.10it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.94it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.77it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.89it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.02it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.12it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.25it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.23it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.22it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.09it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.96it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.80it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.91it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.09it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.20it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.20it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.22it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.12it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.01it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.85it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.78it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.87it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.14it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.22it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.34it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.20it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.00it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.94it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.80it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.82it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.93it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.05it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.29it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.39it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.28it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.14it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.02it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.89it/s][A                                                 
                                                 [A 80%|████████  | 496/620 [03:04<00:32,  3.85it/s]
100%|██████████| 437/437 [00:09<00:00, 43.89it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:03:47,752 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-496
[INFO|configuration_utils.py:351] 2023-08-29 05:03:47,776 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-496/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:03:49,571 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-496/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:03:49,584 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-496/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:03:49,601 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-496/special_tokens_map.json
 80%|████████  | 497/620 [03:06<07:57,  3.88s/it] 80%|████████  | 498/620 [03:07<05:41,  2.80s/it] 80%|████████  | 499/620 [03:07<04:07,  2.05s/it] 81%|████████  | 500/620 [03:07<03:02,  1.52s/it]                                                  81%|████████  | 500/620 [03:07<03:02,  1.52s/it] 81%|████████  | 501/620 [03:07<02:16,  1.15s/it] 81%|████████  | 502/620 [03:08<01:44,  1.13it/s] 81%|████████  | 503/620 [03:08<01:22,  1.42it/s] 81%|████████▏ | 504/620 [03:08<01:07,  1.73it/s] 81%|████████▏ | 505/620 [03:09<00:56,  2.04it/s] 82%|████████▏ | 506/620 [03:09<00:48,  2.34it/s] 82%|████████▏ | 507/620 [03:09<00:43,  2.60it/s] 82%|████████▏ | 508/620 [03:09<00:39,  2.83it/s] 82%|████████▏ | 509/620 [03:10<00:36,  3.01it/s] 82%|████████▏ | 510/620 [03:10<00:34,  3.15it/s] 82%|████████▏ | 511/620 [03:10<00:33,  3.26it/s] 83%|████████▎ | 512/620 [03:11<00:32,  3.34it/s] 83%|████████▎ | 513/620 [03:11<00:31,  3.40it/s] 83%|████████▎ | 514/620 [03:11<00:30,  3.44it/s] 83%|████████▎ | 515/620 [03:11<00:30,  3.47it/s] 83%|████████▎ | 516/620 [03:12<00:29,  3.49it/s] 83%|████████▎ | 517/620 [03:12<00:29,  3.51it/s] 84%|████████▎ | 518/620 [03:12<00:28,  3.53it/s] 84%|████████▎ | 519/620 [03:13<00:28,  3.53it/s] 84%|████████▍ | 520/620 [03:13<00:28,  3.52it/s] 84%|████████▍ | 521/620 [03:13<00:28,  3.53it/s] 84%|████████▍ | 522/620 [03:13<00:27,  3.54it/s] 84%|████████▍ | 523/620 [03:14<00:27,  3.54it/s] 85%|████████▍ | 524/620 [03:14<00:27,  3.54it/s] 85%|████████▍ | 525/620 [03:14<00:26,  3.54it/s] 85%|████████▍ | 526/620 [03:15<00:26,  3.55it/s] 85%|████████▌ | 527/620 [03:15<00:26,  3.57it/s] 85%|████████▌ | 528/620 [03:15<00:25,  3.58it/s] 85%|████████▌ | 529/620 [03:15<00:25,  3.59it/s] 85%|████████▌ | 530/620 [03:16<00:25,  3.59it/s] 86%|████████▌ | 531/620 [03:16<00:24,  3.59it/s] 86%|████████▌ | 532/620 [03:16<00:24,  3.59it/s] 86%|████████▌ | 533/620 [03:16<00:24,  3.60it/s] 86%|████████▌ | 534/620 [03:17<00:23,  3.60it/s] 86%|████████▋ | 535/620 [03:17<00:23,  3.61it/s] 86%|████████▋ | 536/620 [03:17<00:23,  3.61it/s] 87%|████████▋ | 537/620 [03:18<00:23,  3.60it/s] 87%|████████▋ | 538/620 [03:18<00:22,  3.60it/s] 87%|████████▋ | 539/620 [03:18<00:22,  3.60it/s] 87%|████████▋ | 540/620 [03:18<00:22,  3.60it/s] 87%|████████▋ | 541/620 [03:19<00:21,  3.61it/s] 87%|████████▋ | 542/620 [03:19<00:21,  3.60it/s] 88%|████████▊ | 543/620 [03:19<00:21,  3.60it/s] 88%|████████▊ | 544/620 [03:20<00:21,  3.60it/s] 88%|████████▊ | 545/620 [03:20<00:20,  3.60it/s] 88%|████████▊ | 546/620 [03:20<00:20,  3.61it/s] 88%|████████▊ | 547/620 [03:20<00:20,  3.61it/s] 88%|████████▊ | 548/620 [03:21<00:19,  3.61it/s] 89%|████████▊ | 549/620 [03:21<00:19,  3.61it/s] 89%|████████▊ | 550/620 [03:21<00:19,  3.61it/s] 89%|████████▉ | 551/620 [03:21<00:19,  3.60it/s] 89%|████████▉ | 552/620 [03:22<00:18,  3.60it/s] 89%|████████▉ | 553/620 [03:22<00:18,  3.59it/s] 89%|████████▉ | 554/620 [03:22<00:18,  3.60it/s] 90%|████████▉ | 555/620 [03:23<00:18,  3.60it/s] 90%|████████▉ | 556/620 [03:23<00:17,  3.60it/s] 90%|████████▉ | 557/620 [03:23<00:17,  3.61it/s] 90%|█████████ | 558/620 [03:23<00:17,  3.61it/s] 90%|█████████ | 559/620 [03:24<00:16,  3.61it/s] 90%|█████████ | 560/620 [03:24<00:16,  3.61it/s] 90%|█████████ | 561/620 [03:24<00:16,  3.61it/s] 91%|█████████ | 562/620 [03:25<00:16,  3.61it/s] 91%|█████████ | 563/620 [03:25<00:15,  3.61it/s] 91%|█████████ | 564/620 [03:25<00:15,  3.59it/s] 91%|█████████ | 565/620 [03:25<00:15,  3.60it/s] 91%|█████████▏| 566/620 [03:26<00:14,  3.60it/s] 91%|█████████▏| 567/620 [03:26<00:14,  3.61it/s] 92%|█████████▏| 568/620 [03:26<00:14,  3.61it/s] 92%|█████████▏| 569/620 [03:26<00:14,  3.61it/s] 92%|█████████▏| 570/620 [03:27<00:13,  3.61it/s] 92%|█████████▏| 571/620 [03:27<00:13,  3.61it/s] 92%|█████████▏| 572/620 [03:27<00:13,  3.61it/s] 92%|█████████▏| 573/620 [03:28<00:13,  3.61it/s] 93%|█████████▎| 574/620 [03:28<00:12,  3.61it/s] 93%|█████████▎| 575/620 [03:28<00:12,  3.59it/s] 93%|█████████▎| 576/620 [03:28<00:12,  3.60it/s] 93%|█████████▎| 577/620 [03:29<00:11,  3.60it/s] 93%|█████████▎| 578/620 [03:29<00:11,  3.60it/s] 93%|█████████▎| 579/620 [03:29<00:11,  3.60it/s] 94%|█████████▎| 580/620 [03:30<00:11,  3.60it/s] 94%|█████████▎| 581/620 [03:30<00:10,  3.61it/s] 94%|█████████▍| 582/620 [03:30<00:10,  3.61it/s] 94%|█████████▍| 583/620 [03:30<00:10,  3.61it/s] 94%|█████████▍| 584/620 [03:31<00:09,  3.61it/s] 94%|█████████▍| 585/620 [03:31<00:09,  3.61it/s] 95%|█████████▍| 586/620 [03:31<00:09,  3.61it/s] 95%|█████████▍| 587/620 [03:31<00:09,  3.61it/s] 95%|█████████▍| 588/620 [03:32<00:08,  3.61it/s] 95%|█████████▌| 589/620 [03:32<00:08,  3.61it/s] 95%|█████████▌| 590/620 [03:32<00:08,  3.61it/s] 95%|█████████▌| 591/620 [03:33<00:08,  3.61it/s] 95%|█████████▌| 592/620 [03:33<00:07,  3.61it/s] 96%|█████████▌| 593/620 [03:33<00:07,  3.60it/s] 96%|█████████▌| 594/620 [03:33<00:07,  3.61it/s] 96%|█████████▌| 595/620 [03:34<00:06,  3.61it/s] 96%|█████████▌| 596/620 [03:34<00:06,  3.61it/s] 96%|█████████▋| 597/620 [03:34<00:06,  3.59it/s] 96%|█████████▋| 598/620 [03:35<00:06,  3.60it/s] 97%|█████████▋| 599/620 [03:35<00:05,  3.60it/s] 97%|█████████▋| 600/620 [03:35<00:05,  3.60it/s] 97%|█████████▋| 601/620 [03:35<00:05,  3.60it/s] 97%|█████████▋| 602/620 [03:36<00:05,  3.60it/s] 97%|█████████▋| 603/620 [03:36<00:04,  3.60it/s] 97%|█████████▋| 604/620 [03:36<00:04,  3.61it/s] 98%|█████████▊| 605/620 [03:36<00:04,  3.61it/s] 98%|█████████▊| 606/620 [03:37<00:03,  3.61it/s] 98%|█████████▊| 607/620 [03:37<00:03,  3.61it/s] 98%|█████████▊| 608/620 [03:37<00:03,  3.61it/s] 98%|█████████▊| 609/620 [03:38<00:03,  3.61it/s] 98%|█████████▊| 610/620 [03:38<00:02,  3.61it/s] 99%|█████████▊| 611/620 [03:38<00:02,  3.61it/s] 99%|█████████▊| 612/620 [03:38<00:02,  3.61it/s] 99%|█████████▉| 613/620 [03:39<00:01,  3.61it/s] 99%|█████████▉| 614/620 [03:39<00:01,  3.61it/s] 99%|█████████▉| 615/620 [03:39<00:01,  3.60it/s] 99%|█████████▉| 616/620 [03:39<00:01,  3.60it/s]100%|█████████▉| 617/620 [03:40<00:00,  3.61it/s]100%|█████████▉| 618/620 [03:40<00:00,  3.61it/s]100%|█████████▉| 619/620 [03:40<00:00,  3.61it/s]100%|██████████| 620/620 [03:41<00:00,  3.87it/s][INFO|trainer.py:2140] 2023-08-29 05:04:24,334 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:04:24,334 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 05:04:24,334 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9112, 'eval_samples_per_second': 352.43, 'eval_steps_per_second': 44.092, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7298387096774197e-05, 'epoch': 4.03}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.81it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.59it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.77it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.43it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.01it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.57it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.26it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.97it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.22it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.40it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.44it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.38it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.12it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.03it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.94it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.83it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.81it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.02it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.23it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.37it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.30it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.19it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.09it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.98it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.80it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.87it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.06it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.21it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.21it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.27it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.17it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.01it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.92it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.84it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.99it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.12it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.17it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.29it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.28it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.16it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.05it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.87it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.93it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.04it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.20it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.28it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.34it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.30it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.18it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.02it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.90it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.86it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.00it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.18it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.22it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.16it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.19it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.15it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.99it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.85it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.05it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.14it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.14it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.20it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.10it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.09it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.08it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.00it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.79it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.02it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.15it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.21it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.27it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.19it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.02it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.99it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.94it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.90it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.03it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.02it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.22it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.29it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.25it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.16it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.05it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.96it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.96it/s][A                                                 
                                                 [A100%|██████████| 620/620 [03:50<00:00,  3.87it/s]
100%|██████████| 437/437 [00:09<00:00, 43.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:04:34,256 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-620
[INFO|configuration_utils.py:351] 2023-08-29 05:04:34,274 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-620/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:04:35,807 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-620/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:04:35,819 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-620/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:04:35,830 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-620/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 05:04:36,135 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 05:04:36,137 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-124 (score: 1.1039679050445557).
                                                 100%|██████████| 620/620 [03:54<00:00,  3.87it/s]100%|██████████| 620/620 [03:54<00:00,  2.64it/s]
[INFO|trainer.py:1894] 2023-08-29 05:04:37,922 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 05:04:37,937 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:04:39,579 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:04:39,595 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:04:39,606 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:04:39,799 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:39,800 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:39,800 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:39,800 >>   train_runtime            = 0:03:54.62
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:39,800 >>   train_samples            =       7920
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:39,800 >>   train_samples_per_second =    168.778
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:39,800 >>   train_steps_per_second   =      2.642
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9042, 'eval_samples_per_second': 352.679, 'eval_steps_per_second': 44.123, 'epoch': 5.0}
{'train_runtime': 234.6272, 'train_samples_per_second': 168.778, 'train_steps_per_second': 2.642, 'train_loss': nan, 'epoch': 5.0}
08/29/2023 05:04:39 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 05:04:39,843 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:04:39,843 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 05:04:39,843 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.17it/s]  3%|▎         | 12/437 [00:00<00:08, 48.43it/s]  4%|▍         | 17/437 [00:00<00:08, 46.69it/s]  5%|▌         | 22/437 [00:00<00:08, 46.17it/s]  6%|▌         | 27/437 [00:00<00:08, 45.65it/s]  7%|▋         | 32/437 [00:00<00:08, 45.36it/s]  8%|▊         | 37/437 [00:00<00:08, 45.19it/s] 10%|▉         | 42/437 [00:00<00:08, 44.67it/s] 11%|█         | 47/437 [00:01<00:08, 44.07it/s] 12%|█▏        | 52/437 [00:01<00:08, 43.73it/s] 13%|█▎        | 57/437 [00:01<00:08, 43.88it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.13it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.32it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.42it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.50it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.53it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.29it/s] 21%|██        | 92/437 [00:02<00:07, 43.90it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.80it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.86it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.03it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.24it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.40it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.46it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.50it/s] 30%|███       | 132/437 [00:02<00:06, 44.34it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.09it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.98it/s] 34%|███▎      | 147/437 [00:03<00:06, 43.97it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.03it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.25it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.36it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.49it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.54it/s] 41%|████      | 177/437 [00:03<00:05, 44.29it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.06it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.02it/s] 44%|████▍     | 192/437 [00:04<00:05, 43.99it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.03it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.16it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.26it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.42it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.45it/s] 51%|█████     | 222/437 [00:04<00:04, 44.28it/s] 52%|█████▏    | 227/437 [00:05<00:04, 43.99it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.07it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.07it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.01it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.20it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.29it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.37it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.43it/s] 61%|██████    | 267/437 [00:06<00:03, 44.23it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.10it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.09it/s] 65%|██████▍   | 282/437 [00:06<00:03, 43.98it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.16it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.32it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.34it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.26it/s] 70%|███████   | 307/437 [00:06<00:02, 44.33it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.26it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.14it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.04it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.02it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.17it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.30it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.32it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.37it/s] 81%|████████  | 352/437 [00:07<00:01, 44.35it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.13it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.06it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.02it/s] 85%|████████▌ | 372/437 [00:08<00:01, 43.95it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.09it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.28it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.38it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.41it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.11it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.19it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.04it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.01it/s] 95%|█████████▌| 417/437 [00:09<00:00, 43.98it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.14it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.27it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.33it/s]100%|██████████| 437/437 [00:09<00:00, 44.33it/s]100%|██████████| 437/437 [00:09<00:00, 44.31it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:04:49,723 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:49,723 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:49,723 >>   eval_loss               =      1.104
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:49,723 >>   eval_runtime            = 0:00:09.88
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:49,723 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:49,723 >>   eval_samples_per_second =    353.539
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:49,723 >>   eval_steps_per_second   =      44.23
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:04:49,723 >>   perplexity              =     3.0161
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:04:55,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:04:55,331 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:04:55,331 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:04:55,331 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:04:55,331 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:04:55,640 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:04:55,640 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:04:56,320 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:04:57,353 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:04:57,354 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:04:59,948 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:04:59,956 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:04:59,956 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:04:59,956 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:04:59,956 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:05:00,283 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:05:00,284 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:05:00,555 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:05:00,732 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:05:00,732 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-496
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-372
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-248
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-620
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/checkpoint-124
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.76it/s]Extractor Predicting: 4it [00:02,  1.82it/s]Extractor Predicting: 5it [00:02,  1.77it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 7it [00:03,  1.76it/s]Extractor Predicting: 8it [00:04,  1.76it/s]Extractor Predicting: 9it [00:05,  1.77it/s]Extractor Predicting: 10it [00:05,  1.70it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:06,  1.77it/s]Extractor Predicting: 13it [00:07,  1.79it/s]Extractor Predicting: 14it [00:07,  1.81it/s]Extractor Predicting: 15it [00:08,  1.78it/s]Extractor Predicting: 16it [00:09,  1.80it/s]Extractor Predicting: 17it [00:09,  1.82it/s]Extractor Predicting: 18it [00:10,  1.81it/s]Extractor Predicting: 19it [00:10,  1.81it/s]Extractor Predicting: 20it [00:11,  1.74it/s]Extractor Predicting: 21it [00:11,  1.72it/s]Extractor Predicting: 22it [00:12,  1.68it/s]Extractor Predicting: 23it [00:13,  1.73it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.76it/s]Extractor Predicting: 26it [00:14,  1.84it/s]Extractor Predicting: 27it [00:15,  1.83it/s]Extractor Predicting: 28it [00:15,  1.87it/s]Extractor Predicting: 29it [00:16,  1.84it/s]Extractor Predicting: 30it [00:16,  1.80it/s]Extractor Predicting: 31it [00:17,  1.78it/s]Extractor Predicting: 32it [00:18,  1.75it/s]Extractor Predicting: 33it [00:18,  1.70it/s]Extractor Predicting: 34it [00:19,  1.68it/s]Extractor Predicting: 35it [00:19,  1.70it/s]Extractor Predicting: 36it [00:20,  1.69it/s]Extractor Predicting: 37it [00:21,  1.68it/s]Extractor Predicting: 38it [00:21,  1.67it/s]Extractor Predicting: 39it [00:22,  1.69it/s]Extractor Predicting: 40it [00:22,  1.70it/s]Extractor Predicting: 41it [00:23,  1.68it/s]Extractor Predicting: 42it [00:24,  1.70it/s]Extractor Predicting: 43it [00:24,  1.70it/s]Extractor Predicting: 44it [00:25,  1.73it/s]Extractor Predicting: 45it [00:25,  1.75it/s]Extractor Predicting: 46it [00:26,  1.70it/s]Extractor Predicting: 47it [00:26,  1.68it/s]Extractor Predicting: 48it [00:27,  1.68it/s]Extractor Predicting: 49it [00:28,  1.68it/s]Extractor Predicting: 50it [00:28,  1.68it/s]Extractor Predicting: 51it [00:29,  1.70it/s]Extractor Predicting: 52it [00:29,  1.67it/s]Extractor Predicting: 53it [00:30,  1.72it/s]Extractor Predicting: 54it [00:31,  1.70it/s]Extractor Predicting: 55it [00:31,  1.67it/s]Extractor Predicting: 56it [00:32,  1.67it/s]Extractor Predicting: 57it [00:32,  1.64it/s]Extractor Predicting: 58it [00:33,  1.64it/s]Extractor Predicting: 59it [00:34,  1.63it/s]Extractor Predicting: 60it [00:34,  1.63it/s]Extractor Predicting: 61it [00:35,  1.65it/s]Extractor Predicting: 62it [00:35,  1.65it/s]Extractor Predicting: 63it [00:36,  1.69it/s]Extractor Predicting: 64it [00:37,  1.72it/s]Extractor Predicting: 65it [00:37,  1.69it/s]Extractor Predicting: 66it [00:38,  1.58it/s]Extractor Predicting: 67it [00:39,  1.60it/s]Extractor Predicting: 68it [00:39,  1.62it/s]Extractor Predicting: 69it [00:40,  1.63it/s]Extractor Predicting: 70it [00:40,  1.62it/s]Extractor Predicting: 71it [00:41,  1.64it/s]Extractor Predicting: 72it [00:42,  1.66it/s]Extractor Predicting: 73it [00:42,  1.67it/s]Extractor Predicting: 74it [00:43,  1.70it/s]Extractor Predicting: 75it [00:43,  1.72it/s]Extractor Predicting: 76it [00:44,  1.69it/s]Extractor Predicting: 77it [00:45,  1.67it/s]Extractor Predicting: 78it [00:45,  1.66it/s]Extractor Predicting: 79it [00:46,  1.67it/s]Extractor Predicting: 80it [00:46,  1.65it/s]Extractor Predicting: 81it [00:47,  1.71it/s]Extractor Predicting: 82it [00:47,  1.69it/s]Extractor Predicting: 83it [00:48,  1.66it/s]Extractor Predicting: 84it [00:49,  1.67it/s]Extractor Predicting: 85it [00:49,  1.69it/s]Extractor Predicting: 86it [00:50,  1.76it/s]Extractor Predicting: 87it [00:50,  1.81it/s]Extractor Predicting: 88it [00:51,  1.82it/s]Extractor Predicting: 89it [00:51,  1.82it/s]Extractor Predicting: 90it [00:52,  1.83it/s]Extractor Predicting: 91it [00:53,  1.81it/s]Extractor Predicting: 92it [00:53,  1.78it/s]Extractor Predicting: 93it [00:54,  1.82it/s]Extractor Predicting: 94it [00:54,  1.85it/s]Extractor Predicting: 95it [00:55,  1.84it/s]Extractor Predicting: 96it [00:55,  1.85it/s]Extractor Predicting: 97it [00:56,  1.82it/s]Extractor Predicting: 98it [00:56,  1.77it/s]Extractor Predicting: 99it [00:57,  1.74it/s]Extractor Predicting: 100it [00:58,  1.77it/s]Extractor Predicting: 101it [00:58,  1.81it/s]Extractor Predicting: 102it [00:59,  1.80it/s]Extractor Predicting: 103it [00:59,  1.79it/s]Extractor Predicting: 104it [01:00,  1.78it/s]Extractor Predicting: 105it [01:00,  1.83it/s]Extractor Predicting: 106it [01:01,  1.80it/s]Extractor Predicting: 107it [01:01,  1.82it/s]Extractor Predicting: 108it [01:02,  1.81it/s]Extractor Predicting: 109it [01:02,  1.85it/s]Extractor Predicting: 110it [01:03,  1.83it/s]Extractor Predicting: 111it [01:04,  1.81it/s]Extractor Predicting: 112it [01:04,  1.80it/s]Extractor Predicting: 113it [01:05,  1.79it/s]Extractor Predicting: 114it [01:05,  1.74it/s]Extractor Predicting: 115it [01:06,  1.75it/s]Extractor Predicting: 116it [01:06,  1.74it/s]Extractor Predicting: 117it [01:07,  1.72it/s]Extractor Predicting: 118it [01:08,  1.80it/s]Extractor Predicting: 119it [01:08,  1.77it/s]Extractor Predicting: 120it [01:09,  1.79it/s]Extractor Predicting: 121it [01:09,  1.77it/s]Extractor Predicting: 122it [01:10,  1.75it/s]Extractor Predicting: 123it [01:10,  1.75it/s]Extractor Predicting: 124it [01:11,  1.71it/s]Extractor Predicting: 125it [01:12,  1.71it/s]Extractor Predicting: 126it [01:12,  1.72it/s]Extractor Predicting: 127it [01:13,  1.70it/s]Extractor Predicting: 128it [01:13,  1.70it/s]Extractor Predicting: 129it [01:14,  1.64it/s]Extractor Predicting: 130it [01:15,  1.65it/s]Extractor Predicting: 131it [01:15,  1.67it/s]Extractor Predicting: 132it [01:16,  1.72it/s]Extractor Predicting: 133it [01:16,  1.72it/s]Extractor Predicting: 134it [01:17,  1.72it/s]Extractor Predicting: 135it [01:18,  1.72it/s]Extractor Predicting: 136it [01:18,  1.71it/s]Extractor Predicting: 137it [01:19,  1.70it/s]Extractor Predicting: 138it [01:19,  1.70it/s]Extractor Predicting: 139it [01:20,  1.68it/s]Extractor Predicting: 140it [01:21,  1.58it/s]Extractor Predicting: 141it [01:21,  1.60it/s]Extractor Predicting: 142it [01:22,  1.70it/s]Extractor Predicting: 142it [01:22,  1.73it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:31,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:31,145 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:31,145 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:31,145 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:31,145 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:06:31,773 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:06:31,774 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:06:32,334 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:06:33,331 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:06:33,331 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:36,358 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:36,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:36,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:36,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:06:36,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:06:36,984 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:06:36,985 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:06:37,554 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:06:37,724 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:06:37,724 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.80it/s]Extractor Predicting: 5it [00:02,  1.78it/s]Extractor Predicting: 6it [00:03,  1.74it/s]Extractor Predicting: 7it [00:04,  1.75it/s]Extractor Predicting: 8it [00:04,  1.80it/s]Extractor Predicting: 9it [00:05,  1.80it/s]Extractor Predicting: 10it [00:05,  1.83it/s]Extractor Predicting: 11it [00:06,  1.80it/s]Extractor Predicting: 12it [00:06,  1.81it/s]Extractor Predicting: 13it [00:07,  1.76it/s]Extractor Predicting: 14it [00:07,  1.79it/s]Extractor Predicting: 15it [00:08,  1.78it/s]Extractor Predicting: 16it [00:09,  1.74it/s]Extractor Predicting: 17it [00:09,  1.74it/s]Extractor Predicting: 18it [00:10,  1.73it/s]Extractor Predicting: 19it [00:10,  1.77it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:11,  1.75it/s]Extractor Predicting: 22it [00:12,  1.77it/s]Extractor Predicting: 23it [00:13,  1.73it/s]Extractor Predicting: 24it [00:13,  1.72it/s]Extractor Predicting: 25it [00:14,  1.74it/s]Extractor Predicting: 26it [00:14,  1.75it/s]Extractor Predicting: 27it [00:15,  1.73it/s]Extractor Predicting: 28it [00:15,  1.76it/s]Extractor Predicting: 29it [00:16,  1.71it/s]Extractor Predicting: 30it [00:17,  1.67it/s]Extractor Predicting: 31it [00:17,  1.65it/s]Extractor Predicting: 32it [00:18,  1.65it/s]Extractor Predicting: 33it [00:18,  1.67it/s]Extractor Predicting: 34it [00:19,  1.69it/s]Extractor Predicting: 35it [00:20,  1.70it/s]Extractor Predicting: 36it [00:20,  1.74it/s]Extractor Predicting: 37it [00:21,  1.76it/s]Extractor Predicting: 38it [00:21,  1.78it/s]Extractor Predicting: 39it [00:22,  1.75it/s]Extractor Predicting: 40it [00:22,  1.75it/s]Extractor Predicting: 41it [00:23,  1.76it/s]Extractor Predicting: 42it [00:24,  1.73it/s]Extractor Predicting: 43it [00:24,  1.74it/s]Extractor Predicting: 44it [00:25,  1.73it/s]Extractor Predicting: 45it [00:25,  1.72it/s]Extractor Predicting: 46it [00:26,  1.73it/s]Extractor Predicting: 47it [00:26,  1.74it/s]Extractor Predicting: 48it [00:27,  1.74it/s]Extractor Predicting: 49it [00:28,  1.73it/s]Extractor Predicting: 50it [00:28,  1.70it/s]Extractor Predicting: 51it [00:29,  1.72it/s]Extractor Predicting: 52it [00:29,  1.73it/s]Extractor Predicting: 53it [00:30,  1.72it/s]Extractor Predicting: 54it [00:31,  1.73it/s]Extractor Predicting: 55it [00:31,  1.68it/s]Extractor Predicting: 56it [00:32,  1.71it/s]Extractor Predicting: 57it [00:32,  1.74it/s]Extractor Predicting: 58it [00:33,  1.77it/s]Extractor Predicting: 59it [00:33,  1.77it/s]Extractor Predicting: 60it [00:34,  1.77it/s]Extractor Predicting: 61it [00:35,  1.76it/s]Extractor Predicting: 62it [00:35,  1.77it/s]Extractor Predicting: 63it [00:36,  1.77it/s]Extractor Predicting: 64it [00:36,  1.72it/s]Extractor Predicting: 65it [00:37,  1.56it/s]Extractor Predicting: 66it [00:38,  1.59it/s]Extractor Predicting: 67it [00:38,  1.64it/s]Extractor Predicting: 68it [00:39,  1.72it/s]Extractor Predicting: 69it [00:39,  1.72it/s]Extractor Predicting: 70it [00:40,  1.72it/s]Extractor Predicting: 71it [00:40,  1.72it/s]Extractor Predicting: 72it [00:41,  1.73it/s]Extractor Predicting: 73it [00:42,  1.68it/s]Extractor Predicting: 74it [00:42,  1.69it/s]Extractor Predicting: 75it [00:43,  1.70it/s]Extractor Predicting: 76it [00:43,  1.71it/s]Extractor Predicting: 77it [00:44,  1.68it/s]Extractor Predicting: 78it [00:45,  1.70it/s]Extractor Predicting: 79it [00:45,  1.70it/s]Extractor Predicting: 80it [00:46,  1.70it/s]Extractor Predicting: 81it [00:46,  1.72it/s]Extractor Predicting: 82it [00:47,  1.68it/s]Extractor Predicting: 83it [00:48,  1.69it/s]Extractor Predicting: 84it [00:48,  1.70it/s]Extractor Predicting: 85it [00:49,  1.73it/s]Extractor Predicting: 86it [00:49,  1.69it/s]Extractor Predicting: 87it [00:50,  1.71it/s]Extractor Predicting: 88it [00:51,  1.66it/s]Extractor Predicting: 89it [00:51,  1.68it/s]Extractor Predicting: 90it [00:52,  1.72it/s]Extractor Predicting: 91it [00:52,  1.68it/s]Extractor Predicting: 92it [00:53,  1.69it/s]Extractor Predicting: 93it [00:54,  1.65it/s]Extractor Predicting: 94it [00:54,  1.64it/s]Extractor Predicting: 95it [00:55,  1.65it/s]Extractor Predicting: 96it [00:55,  1.65it/s]Extractor Predicting: 97it [00:56,  1.67it/s]Extractor Predicting: 98it [00:57,  1.66it/s]Extractor Predicting: 99it [00:57,  1.66it/s]Extractor Predicting: 100it [00:58,  1.67it/s]Extractor Predicting: 101it [00:58,  1.66it/s]Extractor Predicting: 102it [00:59,  1.66it/s]Extractor Predicting: 103it [01:00,  1.67it/s]Extractor Predicting: 104it [01:00,  1.66it/s]Extractor Predicting: 105it [01:01,  1.65it/s]Extractor Predicting: 106it [01:01,  1.67it/s]Extractor Predicting: 107it [01:02,  1.63it/s]Extractor Predicting: 108it [01:03,  1.63it/s]Extractor Predicting: 109it [01:03,  1.64it/s]Extractor Predicting: 110it [01:04,  1.62it/s]Extractor Predicting: 111it [01:04,  1.62it/s]Extractor Predicting: 112it [01:05,  1.67it/s]Extractor Predicting: 113it [01:06,  1.70it/s]Extractor Predicting: 114it [01:06,  1.72it/s]Extractor Predicting: 115it [01:07,  1.71it/s]Extractor Predicting: 116it [01:07,  1.71it/s]Extractor Predicting: 117it [01:08,  1.72it/s]Extractor Predicting: 118it [01:08,  1.74it/s]Extractor Predicting: 119it [01:09,  1.73it/s]Extractor Predicting: 120it [01:10,  1.75it/s]Extractor Predicting: 121it [01:10,  1.74it/s]Extractor Predicting: 122it [01:11,  1.80it/s]Extractor Predicting: 123it [01:11,  1.78it/s]Extractor Predicting: 124it [01:12,  1.78it/s]Extractor Predicting: 125it [01:12,  1.77it/s]Extractor Predicting: 126it [01:13,  1.74it/s]Extractor Predicting: 127it [01:14,  1.75it/s]Extractor Predicting: 128it [01:14,  1.76it/s]Extractor Predicting: 129it [01:15,  1.72it/s]Extractor Predicting: 130it [01:15,  1.74it/s]Extractor Predicting: 131it [01:16,  1.72it/s]Extractor Predicting: 132it [01:16,  1.73it/s]Extractor Predicting: 133it [01:17,  1.72it/s]Extractor Predicting: 134it [01:18,  1.73it/s]Extractor Predicting: 135it [01:18,  1.77it/s]Extractor Predicting: 136it [01:19,  1.76it/s]Extractor Predicting: 137it [01:19,  1.74it/s]Extractor Predicting: 138it [01:20,  1.75it/s]Extractor Predicting: 139it [01:20,  1.80it/s]Extractor Predicting: 140it [01:21,  1.76it/s]Extractor Predicting: 141it [01:22,  1.74it/s]Extractor Predicting: 142it [01:22,  1.78it/s]Extractor Predicting: 143it [01:23,  1.77it/s]Extractor Predicting: 144it [01:23,  1.74it/s]Extractor Predicting: 145it [01:24,  1.75it/s]Extractor Predicting: 146it [01:24,  1.74it/s]Extractor Predicting: 147it [01:25,  1.71it/s]Extractor Predicting: 148it [01:26,  1.71it/s]Extractor Predicting: 149it [01:26,  1.70it/s]Extractor Predicting: 150it [01:27,  1.73it/s]Extractor Predicting: 151it [01:27,  1.76it/s]Extractor Predicting: 152it [01:28,  1.76it/s]Extractor Predicting: 153it [01:28,  1.75it/s]Extractor Predicting: 154it [01:29,  1.72it/s]Extractor Predicting: 155it [01:30,  1.72it/s]Extractor Predicting: 156it [01:30,  1.70it/s]Extractor Predicting: 157it [01:31,  1.69it/s]Extractor Predicting: 158it [01:31,  1.69it/s]Extractor Predicting: 159it [01:32,  1.68it/s]Extractor Predicting: 160it [01:33,  1.69it/s]Extractor Predicting: 161it [01:33,  1.73it/s]Extractor Predicting: 162it [01:34,  1.74it/s]Extractor Predicting: 163it [01:35,  1.57it/s]Extractor Predicting: 164it [01:35,  1.62it/s]Extractor Predicting: 165it [01:36,  1.63it/s]Extractor Predicting: 166it [01:36,  1.67it/s]Extractor Predicting: 167it [01:37,  1.71it/s]Extractor Predicting: 168it [01:37,  1.70it/s]Extractor Predicting: 169it [01:38,  1.70it/s]Extractor Predicting: 170it [01:39,  1.50it/s]Extractor Predicting: 171it [01:40,  1.52it/s]Extractor Predicting: 172it [01:40,  1.56it/s]Extractor Predicting: 173it [01:41,  1.60it/s]Extractor Predicting: 174it [01:41,  1.57it/s]Extractor Predicting: 175it [01:42,  1.54it/s]Extractor Predicting: 176it [01:43,  1.58it/s]Extractor Predicting: 177it [01:43,  1.59it/s]Extractor Predicting: 178it [01:44,  1.64it/s]Extractor Predicting: 179it [01:44,  1.64it/s]Extractor Predicting: 180it [01:45,  1.70it/s]Extractor Predicting: 181it [01:46,  1.68it/s]Extractor Predicting: 182it [01:46,  1.72it/s]Extractor Predicting: 183it [01:47,  1.72it/s]Extractor Predicting: 184it [01:47,  1.76it/s]Extractor Predicting: 185it [01:48,  1.78it/s]Extractor Predicting: 186it [01:48,  1.77it/s]Extractor Predicting: 187it [01:49,  1.78it/s]Extractor Predicting: 188it [01:50,  1.77it/s]Extractor Predicting: 189it [01:50,  1.77it/s]Extractor Predicting: 190it [01:51,  1.74it/s]Extractor Predicting: 191it [01:51,  1.67it/s]Extractor Predicting: 192it [01:52,  1.69it/s]Extractor Predicting: 193it [01:52,  1.73it/s]Extractor Predicting: 194it [01:53,  1.73it/s]Extractor Predicting: 195it [01:54,  1.73it/s]Extractor Predicting: 196it [01:54,  1.76it/s]Extractor Predicting: 197it [01:55,  1.77it/s]Extractor Predicting: 198it [01:55,  1.74it/s]Extractor Predicting: 199it [01:56,  1.75it/s]Extractor Predicting: 200it [01:56,  1.74it/s]Extractor Predicting: 201it [01:57,  1.75it/s]Extractor Predicting: 202it [01:58,  1.79it/s]Extractor Predicting: 203it [01:58,  1.80it/s]Extractor Predicting: 204it [01:59,  1.78it/s]Extractor Predicting: 205it [01:59,  1.72it/s]Extractor Predicting: 206it [02:00,  1.72it/s]Extractor Predicting: 207it [02:00,  1.75it/s]Extractor Predicting: 208it [02:01,  1.76it/s]Extractor Predicting: 209it [02:02,  1.69it/s]Extractor Predicting: 210it [02:02,  1.67it/s]Extractor Predicting: 211it [02:03,  1.68it/s]Extractor Predicting: 212it [02:03,  1.71it/s]Extractor Predicting: 213it [02:04,  1.73it/s]Extractor Predicting: 214it [02:05,  1.68it/s]Extractor Predicting: 215it [02:05,  1.68it/s]Extractor Predicting: 216it [02:06,  1.72it/s]Extractor Predicting: 217it [02:06,  1.73it/s]Extractor Predicting: 218it [02:07,  1.65it/s]Extractor Predicting: 219it [02:08,  1.67it/s]Extractor Predicting: 220it [02:08,  1.68it/s]Extractor Predicting: 221it [02:09,  1.65it/s]Extractor Predicting: 222it [02:09,  1.66it/s]Extractor Predicting: 223it [02:10,  1.66it/s]Extractor Predicting: 224it [02:11,  1.70it/s]Extractor Predicting: 225it [02:11,  1.70it/s]Extractor Predicting: 226it [02:12,  1.74it/s]Extractor Predicting: 227it [02:12,  1.78it/s]Extractor Predicting: 228it [02:13,  1.75it/s]Extractor Predicting: 229it [02:13,  1.75it/s]Extractor Predicting: 230it [02:14,  1.71it/s]Extractor Predicting: 231it [02:15,  1.70it/s]Extractor Predicting: 232it [02:15,  1.70it/s]Extractor Predicting: 233it [02:16,  1.75it/s]Extractor Predicting: 234it [02:16,  1.73it/s]Extractor Predicting: 235it [02:17,  1.74it/s]Extractor Predicting: 236it [02:17,  1.71it/s]Extractor Predicting: 237it [02:18,  1.70it/s]Extractor Predicting: 238it [02:19,  1.73it/s]Extractor Predicting: 239it [02:19,  1.75it/s]Extractor Predicting: 240it [02:20,  1.73it/s]Extractor Predicting: 241it [02:20,  1.72it/s]Extractor Predicting: 242it [02:21,  1.70it/s]Extractor Predicting: 243it [02:22,  1.66it/s]Extractor Predicting: 244it [02:22,  1.67it/s]Extractor Predicting: 245it [02:23,  1.73it/s]Extractor Predicting: 246it [02:23,  1.71it/s]Extractor Predicting: 247it [02:24,  1.73it/s]Extractor Predicting: 248it [02:24,  1.74it/s]Extractor Predicting: 249it [02:25,  1.73it/s]Extractor Predicting: 250it [02:26,  1.72it/s]Extractor Predicting: 251it [02:26,  1.68it/s]Extractor Predicting: 252it [02:27,  1.68it/s]Extractor Predicting: 253it [02:27,  1.69it/s]Extractor Predicting: 254it [02:28,  1.72it/s]Extractor Predicting: 255it [02:29,  1.71it/s]Extractor Predicting: 256it [02:29,  1.69it/s]Extractor Predicting: 257it [02:30,  1.72it/s]Extractor Predicting: 258it [02:30,  1.71it/s]Extractor Predicting: 259it [02:31,  1.66it/s]Extractor Predicting: 260it [02:32,  1.69it/s]Extractor Predicting: 261it [02:32,  1.53it/s]Extractor Predicting: 262it [02:33,  1.56it/s]Extractor Predicting: 263it [02:34,  1.58it/s]Extractor Predicting: 264it [02:34,  1.60it/s]Extractor Predicting: 265it [02:35,  1.60it/s]Extractor Predicting: 266it [02:35,  1.59it/s]Extractor Predicting: 267it [02:36,  1.58it/s]Extractor Predicting: 268it [02:37,  1.60it/s]Extractor Predicting: 269it [02:37,  1.60it/s]Extractor Predicting: 270it [02:38,  1.59it/s]Extractor Predicting: 271it [02:39,  1.62it/s]Extractor Predicting: 272it [02:39,  1.63it/s]Extractor Predicting: 273it [02:40,  1.62it/s]Extractor Predicting: 274it [02:40,  1.61it/s]Extractor Predicting: 275it [02:41,  1.66it/s]Extractor Predicting: 276it [02:42,  1.66it/s]Extractor Predicting: 277it [02:42,  1.64it/s]Extractor Predicting: 278it [02:43,  1.64it/s]Extractor Predicting: 279it [02:43,  1.64it/s]Extractor Predicting: 280it [02:44,  1.63it/s]Extractor Predicting: 281it [02:45,  1.62it/s]Extractor Predicting: 282it [02:45,  1.65it/s]Extractor Predicting: 283it [02:46,  1.59it/s]Extractor Predicting: 284it [02:47,  1.58it/s]Extractor Predicting: 285it [02:47,  1.58it/s]Extractor Predicting: 286it [02:48,  1.58it/s]Extractor Predicting: 287it [02:48,  1.60it/s]Extractor Predicting: 288it [02:49,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:34,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:34,154 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:34,154 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:34,154 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:34,154 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:09:34,876 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:09:34,876 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:09:35,457 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:09:36,503 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:09:36,503 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:39,361 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:39,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:39,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:39,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:39,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:09:40,084 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:09:40,085 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:09:40,633 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:09:40,801 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:09:40,801 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:01,  1.98it/s]Extractor Predicting: 3it [00:01,  1.78it/s]
[INFO|configuration_utils.py:515] 2023-08-29 05:09:42,867 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:09:42,868 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:09:42,873 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:09:42,874 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 05:09:42,877 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:09:45,760 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 05:09:45,762 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 05:09:45,782 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:09:45,783 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:09:45,789 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:45,792 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:45,792 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:45,792 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:45,792 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:45,792 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:45,792 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 05:09:46,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:46,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:47,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:47,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:48,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:49,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:49,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:50,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:51,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:51,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:52,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:53,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:53,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:54,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:55,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:55,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:56,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:57,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:57,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:58,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:58,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:59,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:00,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:00,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:35, 15.40s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:01,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:02,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:02,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:03,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:03,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:04,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:05,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:06,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:06,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:07,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:08,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:08,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:09,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:10,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:10,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:11,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:12,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:12,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:13,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:14,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:14,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:15,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:13, 14.89s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:15,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:16,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:17,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:17,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:18,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:19,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:19,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:20,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:21,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:21,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:22,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:22,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:23,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:24,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:25,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:25,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:26,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:26,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:27,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:28,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:28,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:29,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:29,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:30,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:31,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:31,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:32,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:33,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:33,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:48<03:18, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:34,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:35,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:35,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:36,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:36,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:37,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:38,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:38,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:39,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:40,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:40,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:41,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:42,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:42,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:43,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:43,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:44,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:45,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:46,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:47,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:47,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:48,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:02<02:52, 15.71s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:48,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:49,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:50,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:50,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:51,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:51,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:52,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:52,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:53,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:54,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:54,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:55,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:55,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:56,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:56,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:57,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:58,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:58,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:59,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:00,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:00,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:01,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:01,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:02,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:03,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:03,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:18<02:36, 15.62s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:04,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:05,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:05,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:06,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:06,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:07,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:08,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:09,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:09,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:10,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:11,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:11,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:12,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:12,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:13,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:14,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:14,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:15,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:16,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:16,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:17,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:18,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:18,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:19,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:34<02:20, 15.66s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:20,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:20,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:21,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:21,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:22,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:23,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:23,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:24,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:25,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:26,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:26,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:27,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:28,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:28,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:29,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:30,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:30,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:31,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:32,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:32,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:33,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:34,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:35,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:49<02:05, 15.68s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:35,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:36,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:37,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:37,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:38,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:38,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:39,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:40,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:41,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:41,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:42,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:43,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:43,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:44,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:44,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:45,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:46,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:47,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:47,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:48,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:49,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:49,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:50,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:51,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:52,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:52,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:07<01:54, 16.34s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:53,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:54,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:54,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:55,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:55,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:56,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:57,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:57,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:58,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:58,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:59,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:00,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:00,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:01,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:01,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:02,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:03,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:03,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:04,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:04,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:05,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:05,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:06,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:06,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:21<01:33, 15.59s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:07,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:08,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:08,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:09,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:10,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:10,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:11,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:12,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:12,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:13,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:14,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:14,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:15,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:16,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:16,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:17,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:17,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:18,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:19,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:19,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:20,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:21,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:21,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:36<01:17, 15.42s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:22,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:23,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:23,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:24,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:25,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:25,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:26,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:27,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:27,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:28,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:29,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:29,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:30,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:31,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:31,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:32,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:33,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:33,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:34,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:35,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:35,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:36,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:50<01:00, 15.13s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:37,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:37,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:38,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:38,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:39,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:40,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:40,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:41,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:41,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:42,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:43,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:44,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:44,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:45,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:45,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:46,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:47,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:47,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:48,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:48,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:49,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:50,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:50,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:51,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:51,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:06<00:45, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:52,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:53,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:53,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:54,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:55,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:55,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:56,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:56,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:57,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:58,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:58,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:59,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:00,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:00,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:01,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:02,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:02,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:03,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:03,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:04,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:05,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:06,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:06,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:21<00:30, 15.10s/it][WARNING|generation_utils.py:914] 2023-08-29 05:13:07,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:07,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:08,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:09,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:09,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:10,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:11,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:11,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:12,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:12,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:13,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:14,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:14,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:15,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:16,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:16,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:17,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:17,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:18,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:19,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:19,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:20,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:21,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:35<00:14, 14.95s/it][WARNING|generation_utils.py:914] 2023-08-29 05:13:21,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:22,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:23,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:23,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:24,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:24,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:25,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:25,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:26,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:27,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:27,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:28,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:28,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:29,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:30,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:30,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:31,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:31,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:32,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:33,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:33,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:34,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:34,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:35,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:36,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:36,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:37,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:37,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:38,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:39,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:39,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:40,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:40,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:41,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:55<00:00, 16.47s/it]Generating: 100%|██████████| 15/15 [03:55<00:00, 15.72s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:47,659 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:47,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:47,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:47,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:47,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:13:48,381 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:13:48,381 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:13:48,640 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:13:49,699 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:13:49,699 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:52,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:52,629 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:52,629 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:52,629 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:52,629 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:13:52,951 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:13:52,952 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:13:53,206 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:13:53,379 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:13:53,379 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.859375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 100, 'raw': 160}
{'target': 600, 'success': 124, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 180, 'raw': 288}
{'target': 600, 'success': 200, 'raw': 320}
{'target': 600, 'success': 218, 'raw': 352}
{'target': 600, 'success': 240, 'raw': 384}
{'target': 600, 'success': 258, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 307, 'raw': 480}
{'target': 600, 'success': 328, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 374, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 418, 'raw': 640}
{'target': 600, 'success': 440, 'raw': 672}
{'target': 600, 'success': 460, 'raw': 704}
{'target': 600, 'success': 487, 'raw': 736}
{'target': 600, 'success': 506, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 569, 'raw': 864}
{'target': 600, 'success': 595, 'raw': 896}
{'target': 600, 'success': 617, 'raw': 928}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.6648706896551724, 'errors': {'', '(\'2012 MTV Video Music Video Awards\', \'nominated for\', \'\', \'He was in a songwriting competition on " The Simpsons " at the 2012 MTV Video Music Video Awards , winning in one of his four categories .\')', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 402, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 579, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.7271634615384616, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : architect .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 496, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 551, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 594, 'raw': 800}
{'target': 600, 'success': 616, 'raw': 832}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7403846153846154, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a new development project called " Bifurcio " , which was developed by the Swiss developers , EMC , for Bifurcio . Head Entity : Bifurcio , Tail Entity : Ericsson .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.80078125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8152173913043478, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.77375, 'errors': {'', '(\'Pristiniki\', \'member of political party\', \'\', \'In 2005 , she became the first female politician to serve in the parliament of Bulgaria , as first and leader of " Pristiniki " in the new parliament .\')', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : operator . Context : Later in the year , the company built a high - speed commuter railway crossing the Bordeaux via Bordeaux Station to Marseille , France . Head Entity : Marseille , Tail Entity : Ralf Rummel .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 76, 'raw': 128}
{'target': 600, 'success': 92, 'raw': 160}
{'target': 600, 'success': 111, 'raw': 192}
{'target': 600, 'success': 130, 'raw': 224}
{'target': 600, 'success': 144, 'raw': 256}
{'target': 600, 'success': 158, 'raw': 288}
{'target': 600, 'success': 176, 'raw': 320}
{'target': 600, 'success': 191, 'raw': 352}
{'target': 600, 'success': 208, 'raw': 384}
{'target': 600, 'success': 226, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 267, 'raw': 480}
{'target': 600, 'success': 284, 'raw': 512}
{'target': 600, 'success': 298, 'raw': 544}
{'target': 600, 'success': 315, 'raw': 576}
{'target': 600, 'success': 332, 'raw': 608}
{'target': 600, 'success': 349, 'raw': 640}
{'target': 600, 'success': 372, 'raw': 672}
{'target': 600, 'success': 388, 'raw': 704}
{'target': 600, 'success': 406, 'raw': 736}
{'target': 600, 'success': 424, 'raw': 768}
{'target': 600, 'success': 439, 'raw': 800}
{'target': 600, 'success': 454, 'raw': 832}
{'target': 600, 'success': 473, 'raw': 864}
{'target': 600, 'success': 490, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 530, 'raw': 960}
{'target': 600, 'success': 551, 'raw': 992}
{'target': 600, 'success': 567, 'raw': 1024}
{'target': 600, 'success': 586, 'raw': 1056}
{'target': 600, 'success': 608, 'raw': 1088}
{'prompt': 'Relation : position held .', 'success_rate': 0.5588235294117647, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 14871
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14971, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.24it/s]Extractor Estimating: 2it [00:01,  1.40it/s]Extractor Estimating: 3it [00:02,  1.36it/s]Extractor Estimating: 4it [00:02,  1.40it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:04,  1.52it/s]Extractor Estimating: 7it [00:04,  1.52it/s]Extractor Estimating: 8it [00:05,  1.57it/s]Extractor Estimating: 9it [00:06,  1.53it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.48it/s]Extractor Estimating: 12it [00:08,  1.52it/s]Extractor Estimating: 13it [00:08,  1.47it/s]Extractor Estimating: 14it [00:09,  1.47it/s]Extractor Estimating: 15it [00:10,  1.48it/s]Extractor Estimating: 16it [00:10,  1.46it/s]Extractor Estimating: 17it [00:11,  1.52it/s]Extractor Estimating: 18it [00:12,  1.47it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:13,  1.53it/s]Extractor Estimating: 21it [00:13,  1.56it/s]Extractor Estimating: 22it [00:14,  1.61it/s]Extractor Estimating: 23it [00:15,  1.58it/s]Extractor Estimating: 24it [00:15,  1.60it/s]Extractor Estimating: 25it [00:16,  1.63it/s]Extractor Estimating: 26it [00:17,  1.57it/s]Extractor Estimating: 27it [00:17,  1.57it/s]Extractor Estimating: 28it [00:18,  1.60it/s]Extractor Estimating: 29it [00:18,  1.63it/s]Extractor Estimating: 30it [00:19,  1.63it/s]Extractor Estimating: 31it [00:20,  1.66it/s]Extractor Estimating: 32it [00:20,  1.67it/s]Extractor Estimating: 33it [00:21,  1.61it/s]Extractor Estimating: 34it [00:22,  1.51it/s]Extractor Estimating: 35it [00:22,  1.58it/s]Extractor Estimating: 36it [00:23,  1.60it/s]Extractor Estimating: 37it [00:24,  1.49it/s]Extractor Estimating: 38it [00:24,  1.53it/s]Extractor Estimating: 39it [00:25,  1.54it/s]Extractor Estimating: 40it [00:26,  1.53it/s]Extractor Estimating: 41it [00:26,  1.51it/s]Extractor Estimating: 42it [00:27,  1.49it/s]Extractor Estimating: 43it [00:28,  1.50it/s]Extractor Estimating: 44it [00:28,  1.46it/s]Extractor Estimating: 45it [00:29,  1.47it/s]Extractor Estimating: 46it [00:30,  1.52it/s]Extractor Estimating: 47it [00:30,  1.54it/s]Extractor Estimating: 48it [00:31,  1.51it/s]Extractor Estimating: 49it [00:31,  1.57it/s]Extractor Estimating: 50it [00:32,  1.54it/s]Extractor Estimating: 51it [00:33,  1.52it/s]Extractor Estimating: 52it [00:33,  1.51it/s]Extractor Estimating: 53it [00:34,  1.51it/s]Extractor Estimating: 54it [00:35,  1.47it/s]Extractor Estimating: 55it [00:36,  1.44it/s]Extractor Estimating: 56it [00:36,  1.46it/s]Extractor Estimating: 57it [00:37,  1.44it/s]Extractor Estimating: 58it [00:38,  1.45it/s]Extractor Estimating: 59it [00:38,  1.42it/s]Extractor Estimating: 60it [00:39,  1.46it/s]Extractor Estimating: 61it [00:40,  1.42it/s]Extractor Estimating: 62it [00:40,  1.45it/s]Extractor Estimating: 63it [00:41,  1.43it/s]Extractor Estimating: 64it [00:42,  1.48it/s]Extractor Estimating: 65it [00:43,  1.44it/s]Extractor Estimating: 66it [00:43,  1.45it/s]Extractor Estimating: 67it [00:44,  1.44it/s]Extractor Estimating: 68it [00:44,  1.50it/s]Extractor Estimating: 69it [00:45,  1.53it/s]Extractor Estimating: 70it [00:46,  1.52it/s]Extractor Estimating: 71it [00:46,  1.49it/s]Extractor Estimating: 72it [00:47,  1.50it/s]Extractor Estimating: 73it [00:48,  1.36it/s]Extractor Estimating: 74it [00:49,  1.44it/s]Extractor Estimating: 75it [00:49,  1.44it/s]Extractor Estimating: 76it [00:50,  1.51it/s]Extractor Estimating: 77it [00:51,  1.55it/s]Extractor Estimating: 78it [00:51,  1.62it/s]Extractor Estimating: 79it [00:52,  1.64it/s]Extractor Estimating: 80it [00:52,  1.70it/s]Extractor Estimating: 81it [00:53,  1.71it/s]Extractor Estimating: 82it [00:53,  1.68it/s]Extractor Estimating: 83it [00:54,  1.68it/s]Extractor Estimating: 84it [00:55,  1.68it/s]Extractor Estimating: 85it [00:55,  1.65it/s]Extractor Estimating: 86it [00:56,  1.67it/s]Extractor Estimating: 87it [00:56,  1.66it/s]Extractor Estimating: 88it [00:57,  1.71it/s]Extractor Estimating: 89it [00:58,  1.72it/s]Extractor Estimating: 90it [00:58,  1.74it/s]Extractor Estimating: 91it [00:59,  1.69it/s]Extractor Estimating: 92it [00:59,  1.71it/s]Extractor Estimating: 93it [01:00,  1.71it/s]Extractor Estimating: 94it [01:01,  1.60it/s]Extractor Estimating: 95it [01:01,  1.53it/s]Extractor Estimating: 96it [01:02,  1.56it/s]Extractor Estimating: 97it [01:03,  1.59it/s]Extractor Estimating: 98it [01:03,  1.63it/s]Extractor Estimating: 99it [01:04,  1.64it/s]Extractor Estimating: 100it [01:04,  1.69it/s]Extractor Estimating: 101it [01:05,  1.71it/s]Extractor Estimating: 102it [01:05,  1.70it/s]Extractor Estimating: 103it [01:06,  1.71it/s]Extractor Estimating: 104it [01:07,  1.75it/s]Extractor Estimating: 105it [01:07,  1.73it/s]Extractor Estimating: 106it [01:08,  1.71it/s]Extractor Estimating: 107it [01:08,  1.74it/s]Extractor Estimating: 108it [01:09,  1.82it/s]Extractor Estimating: 109it [01:09,  1.78it/s]Extractor Estimating: 110it [01:10,  1.81it/s]Extractor Estimating: 111it [01:10,  1.80it/s]Extractor Estimating: 112it [01:11,  1.80it/s]Extractor Estimating: 113it [01:12,  1.75it/s]Extractor Estimating: 114it [01:12,  1.76it/s]Extractor Estimating: 115it [01:13,  1.76it/s]Extractor Estimating: 116it [01:13,  1.76it/s]Extractor Estimating: 117it [01:14,  1.71it/s]Extractor Estimating: 118it [01:15,  1.71it/s]Extractor Estimating: 119it [01:15,  1.65it/s]Extractor Estimating: 120it [01:16,  1.73it/s]Extractor Estimating: 121it [01:16,  1.68it/s]Extractor Estimating: 122it [01:17,  1.60it/s]Extractor Estimating: 123it [01:18,  1.67it/s]Extractor Estimating: 124it [01:18,  1.71it/s]Extractor Estimating: 125it [01:19,  1.68it/s]Extractor Estimating: 126it [01:19,  1.66it/s]Extractor Estimating: 127it [01:20,  1.62it/s]Extractor Estimating: 128it [01:21,  1.66it/s]Extractor Estimating: 129it [01:21,  1.68it/s]Extractor Estimating: 130it [01:22,  1.70it/s]Extractor Estimating: 131it [01:22,  1.68it/s]Extractor Estimating: 132it [01:23,  1.68it/s]Extractor Estimating: 133it [01:24,  1.65it/s]Extractor Estimating: 134it [01:24,  1.69it/s]Extractor Estimating: 135it [01:25,  1.64it/s]Extractor Estimating: 136it [01:25,  1.62it/s]Extractor Estimating: 137it [01:26,  1.61it/s]Extractor Estimating: 138it [01:27,  1.64it/s]Extractor Estimating: 139it [01:27,  1.63it/s]Extractor Estimating: 140it [01:28,  1.64it/s]Extractor Estimating: 141it [01:28,  1.61it/s]Extractor Estimating: 142it [01:29,  1.63it/s]Extractor Estimating: 143it [01:30,  1.68it/s]Extractor Estimating: 144it [01:30,  1.67it/s]Extractor Estimating: 145it [01:31,  1.53it/s]Extractor Estimating: 146it [01:32,  1.58it/s]Extractor Estimating: 147it [01:32,  1.57it/s]Extractor Estimating: 148it [01:33,  1.54it/s]Extractor Estimating: 149it [01:34,  1.54it/s]Extractor Estimating: 150it [01:34,  1.56it/s]Extractor Estimating: 151it [01:35,  1.53it/s]Extractor Estimating: 152it [01:36,  1.52it/s]Extractor Estimating: 153it [01:36,  1.53it/s]Extractor Estimating: 154it [01:37,  1.57it/s]Extractor Estimating: 155it [01:37,  1.61it/s]Extractor Estimating: 156it [01:38,  1.64it/s]Extractor Estimating: 157it [01:39,  1.68it/s]Extractor Estimating: 158it [01:39,  1.61it/s]Extractor Estimating: 159it [01:40,  1.68it/s]Extractor Estimating: 160it [01:40,  1.65it/s]Extractor Estimating: 161it [01:41,  1.63it/s]Extractor Estimating: 162it [01:42,  1.62it/s]Extractor Estimating: 163it [01:42,  1.54it/s]Extractor Estimating: 164it [01:43,  1.57it/s]Extractor Estimating: 165it [01:44,  1.52it/s]Extractor Estimating: 166it [01:44,  1.51it/s]Extractor Estimating: 167it [01:45,  1.56it/s]Extractor Estimating: 168it [01:46,  1.51it/s]Extractor Estimating: 169it [01:46,  1.52it/s]Extractor Estimating: 170it [01:47,  1.59it/s]Extractor Estimating: 171it [01:48,  1.53it/s]Extractor Estimating: 172it [01:48,  1.54it/s]Extractor Estimating: 173it [01:49,  1.55it/s]Extractor Estimating: 174it [01:50,  1.49it/s]Extractor Estimating: 175it [01:50,  1.54it/s]Extractor Estimating: 176it [01:51,  1.48it/s]Extractor Estimating: 177it [01:51,  1.55it/s]Extractor Estimating: 178it [01:52,  1.54it/s]Extractor Estimating: 179it [01:53,  1.61it/s]Extractor Estimating: 180it [01:53,  1.66it/s]Extractor Estimating: 181it [01:54,  1.63it/s]Extractor Estimating: 182it [01:55,  1.62it/s]Extractor Estimating: 183it [01:55,  1.56it/s]Extractor Estimating: 184it [01:56,  1.50it/s]Extractor Estimating: 185it [01:57,  1.52it/s]Extractor Estimating: 186it [01:57,  1.57it/s]Extractor Estimating: 187it [01:58,  1.60it/s]Extractor Estimating: 188it [01:58,  1.58it/s]Extractor Estimating: 189it [01:59,  1.64it/s]Extractor Estimating: 190it [02:00,  1.65it/s]Extractor Estimating: 191it [02:00,  1.60it/s]Extractor Estimating: 192it [02:01,  1.63it/s]Extractor Estimating: 193it [02:01,  1.59it/s]Extractor Estimating: 194it [02:02,  1.57it/s]Extractor Estimating: 195it [02:03,  1.58it/s]Extractor Estimating: 196it [02:03,  1.59it/s]Extractor Estimating: 197it [02:04,  1.59it/s]Extractor Estimating: 198it [02:05,  1.56it/s]Extractor Estimating: 199it [02:05,  1.52it/s]Extractor Estimating: 200it [02:06,  1.59it/s]Extractor Estimating: 201it [02:07,  1.58it/s]Extractor Estimating: 202it [02:07,  1.56it/s]Extractor Estimating: 203it [02:08,  1.58it/s]Extractor Estimating: 204it [02:09,  1.49it/s]Extractor Estimating: 205it [02:09,  1.51it/s]Extractor Estimating: 206it [02:10,  1.55it/s]Extractor Estimating: 207it [02:11,  1.48it/s]Extractor Estimating: 208it [02:11,  1.48it/s]Extractor Estimating: 209it [02:12,  1.47it/s]Extractor Estimating: 210it [02:13,  1.50it/s]Extractor Estimating: 211it [02:13,  1.56it/s]Extractor Estimating: 212it [02:14,  1.53it/s]Extractor Estimating: 213it [02:15,  1.54it/s]Extractor Estimating: 214it [02:15,  1.50it/s]Extractor Estimating: 215it [02:16,  1.56it/s]Extractor Estimating: 216it [02:16,  1.58it/s]Extractor Estimating: 217it [02:17,  1.53it/s]Extractor Estimating: 218it [02:18,  1.57it/s]Extractor Estimating: 219it [02:18,  1.56it/s]Extractor Estimating: 220it [02:19,  1.57it/s]Extractor Estimating: 221it [02:20,  1.53it/s]Extractor Estimating: 222it [02:20,  1.58it/s]Extractor Estimating: 223it [02:21,  1.63it/s]Extractor Estimating: 224it [02:21,  1.63it/s]Extractor Estimating: 225it [02:22,  1.46it/s]Extractor Estimating: 226it [02:23,  1.45it/s]Extractor Estimating: 227it [02:24,  1.47it/s]Extractor Estimating: 228it [02:24,  1.49it/s]Extractor Estimating: 229it [02:25,  1.47it/s]Extractor Estimating: 230it [02:26,  1.45it/s]Extractor Estimating: 231it [02:26,  1.44it/s]Extractor Estimating: 232it [02:27,  1.46it/s]Extractor Estimating: 233it [02:28,  1.47it/s]Extractor Estimating: 234it [02:28,  1.51it/s]Extractor Estimating: 235it [02:29,  1.49it/s]Extractor Estimating: 236it [02:30,  1.47it/s]Extractor Estimating: 237it [02:30,  1.49it/s]Extractor Estimating: 238it [02:31,  1.45it/s]Extractor Estimating: 239it [02:32,  1.42it/s]Extractor Estimating: 240it [02:33,  1.46it/s]Extractor Estimating: 241it [02:33,  1.48it/s]Extractor Estimating: 242it [02:34,  1.49it/s]Extractor Estimating: 243it [02:34,  1.50it/s]Extractor Estimating: 244it [02:35,  1.46it/s]Extractor Estimating: 245it [02:36,  1.41it/s]Extractor Estimating: 246it [02:37,  1.46it/s]Extractor Estimating: 247it [02:37,  1.47it/s]Extractor Estimating: 248it [02:38,  1.42it/s]Extractor Estimating: 249it [02:39,  1.45it/s]Extractor Estimating: 250it [02:39,  1.48it/s]Extractor Estimating: 251it [02:40,  1.57it/s]Extractor Estimating: 252it [02:40,  1.65it/s]Extractor Estimating: 253it [02:41,  1.57it/s]Extractor Estimating: 254it [02:42,  1.64it/s]Extractor Estimating: 255it [02:42,  1.68it/s]Extractor Estimating: 256it [02:43,  1.68it/s]Extractor Estimating: 257it [02:43,  1.72it/s]Extractor Estimating: 258it [02:44,  1.69it/s]Extractor Estimating: 259it [02:45,  1.67it/s]Extractor Estimating: 260it [02:45,  1.59it/s]Extractor Estimating: 261it [02:46,  1.60it/s]Extractor Estimating: 262it [02:46,  1.66it/s]Extractor Estimating: 263it [02:47,  1.65it/s]Extractor Estimating: 264it [02:48,  1.70it/s]Extractor Estimating: 265it [02:48,  1.70it/s]Extractor Estimating: 266it [02:49,  1.76it/s]Extractor Estimating: 267it [02:49,  1.76it/s]Extractor Estimating: 268it [02:50,  1.76it/s]Extractor Estimating: 269it [02:50,  1.75it/s]Extractor Estimating: 270it [02:51,  1.72it/s]Extractor Estimating: 271it [02:52,  1.71it/s]Extractor Estimating: 272it [02:52,  1.71it/s]Extractor Estimating: 273it [02:53,  1.76it/s]Extractor Estimating: 274it [02:53,  1.73it/s]Extractor Estimating: 275it [02:54,  1.74it/s]Extractor Estimating: 276it [02:55,  1.70it/s]Extractor Estimating: 277it [02:55,  1.62it/s]Extractor Estimating: 278it [02:56,  1.59it/s]Extractor Estimating: 279it [02:57,  1.57it/s]Extractor Estimating: 280it [02:57,  1.60it/s]Extractor Estimating: 281it [02:58,  1.61it/s]Extractor Estimating: 282it [02:58,  1.54it/s]Extractor Estimating: 283it [02:59,  1.56it/s]Extractor Estimating: 284it [03:00,  1.51it/s]Extractor Estimating: 285it [03:00,  1.52it/s]Extractor Estimating: 286it [03:01,  1.57it/s]Extractor Estimating: 287it [03:02,  1.58it/s]Extractor Estimating: 288it [03:02,  1.64it/s]Extractor Estimating: 289it [03:03,  1.61it/s]Extractor Estimating: 290it [03:03,  1.64it/s]Extractor Estimating: 291it [03:04,  1.65it/s]Extractor Estimating: 292it [03:05,  1.66it/s]Extractor Estimating: 293it [03:05,  1.63it/s]Extractor Estimating: 294it [03:06,  1.60it/s]Extractor Estimating: 295it [03:07,  1.59it/s]Extractor Estimating: 296it [03:07,  1.58it/s]Extractor Estimating: 297it [03:08,  1.59it/s]Extractor Estimating: 298it [03:08,  1.60it/s]Extractor Estimating: 299it [03:09,  1.60it/s]Extractor Estimating: 300it [03:10,  1.60it/s]Extractor Estimating: 301it [03:10,  1.60it/s]Extractor Estimating: 302it [03:11,  1.55it/s]Extractor Estimating: 303it [03:12,  1.41it/s]Extractor Estimating: 304it [03:13,  1.45it/s]Extractor Estimating: 305it [03:13,  1.47it/s]Extractor Estimating: 306it [03:14,  1.54it/s]Extractor Estimating: 307it [03:14,  1.55it/s]Extractor Estimating: 308it [03:15,  1.58it/s]Extractor Estimating: 309it [03:16,  1.58it/s]Extractor Estimating: 310it [03:16,  1.56it/s]Extractor Estimating: 311it [03:17,  1.51it/s]Extractor Estimating: 312it [03:18,  1.45it/s]Extractor Estimating: 313it [03:19,  1.43it/s]Extractor Estimating: 314it [03:19,  1.48it/s]Extractor Estimating: 315it [03:20,  1.48it/s]Extractor Estimating: 316it [03:20,  1.51it/s]Extractor Estimating: 317it [03:21,  1.51it/s]Extractor Estimating: 318it [03:22,  1.52it/s]Extractor Estimating: 319it [03:22,  1.58it/s]Extractor Estimating: 320it [03:23,  1.63it/s]Extractor Estimating: 321it [03:24,  1.60it/s]Extractor Estimating: 322it [03:24,  1.56it/s]Extractor Estimating: 323it [03:25,  1.56it/s]Extractor Estimating: 324it [03:26,  1.55it/s]Extractor Estimating: 325it [03:26,  1.51it/s]Extractor Estimating: 326it [03:27,  1.54it/s]Extractor Estimating: 327it [03:28,  1.49it/s]Extractor Estimating: 328it [03:28,  1.48it/s]Extractor Estimating: 329it [03:29,  1.47it/s]Extractor Estimating: 330it [03:30,  1.47it/s]Extractor Estimating: 331it [03:30,  1.47it/s]Extractor Estimating: 332it [03:31,  1.55it/s]Extractor Estimating: 333it [03:32,  1.54it/s]Extractor Estimating: 334it [03:32,  1.58it/s]Extractor Estimating: 335it [03:33,  1.54it/s]Extractor Estimating: 336it [03:33,  1.55it/s]Extractor Estimating: 337it [03:34,  1.49it/s]Extractor Estimating: 338it [03:35,  1.51it/s]Extractor Estimating: 339it [03:35,  1.52it/s]Extractor Estimating: 340it [03:36,  1.57it/s]Extractor Estimating: 341it [03:37,  1.58it/s]Extractor Estimating: 342it [03:37,  1.58it/s]Extractor Estimating: 343it [03:38,  1.59it/s]Extractor Estimating: 344it [03:39,  1.55it/s]Extractor Estimating: 345it [03:39,  1.53it/s]Extractor Estimating: 346it [03:40,  1.44it/s]Extractor Estimating: 347it [03:41,  1.46it/s]Extractor Estimating: 348it [03:41,  1.53it/s]Extractor Estimating: 349it [03:42,  1.53it/s]Extractor Estimating: 350it [03:43,  1.51it/s]Extractor Estimating: 351it [03:43,  1.55it/s]Extractor Estimating: 352it [03:44,  1.53it/s]Extractor Estimating: 353it [03:45,  1.58it/s]Extractor Estimating: 354it [03:45,  1.59it/s]Extractor Estimating: 355it [03:46,  1.59it/s]Extractor Estimating: 356it [03:46,  1.57it/s]Extractor Estimating: 357it [03:47,  1.59it/s]Extractor Estimating: 358it [03:48,  1.60it/s]Extractor Estimating: 359it [03:48,  1.61it/s]Extractor Estimating: 360it [03:49,  1.58it/s]Extractor Estimating: 361it [03:50,  1.60it/s]Extractor Estimating: 362it [03:50,  1.62it/s]Extractor Estimating: 363it [03:51,  1.59it/s]Extractor Estimating: 364it [03:51,  1.59it/s]Extractor Estimating: 365it [03:52,  1.59it/s]Extractor Estimating: 366it [03:53,  1.55it/s]Extractor Estimating: 367it [03:53,  1.55it/s]Extractor Estimating: 368it [03:54,  1.59it/s]Extractor Estimating: 369it [03:55,  1.62it/s]Extractor Estimating: 370it [03:55,  1.61it/s]Extractor Estimating: 371it [03:56,  1.59it/s]Extractor Estimating: 372it [03:56,  1.67it/s]Extractor Estimating: 373it [03:57,  1.69it/s]Extractor Estimating: 374it [03:57,  1.70it/s]Extractor Estimating: 375it [03:58,  1.60it/s]Extractor Estimating: 375it [03:58,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:03,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:03,430 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:03,430 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:03,430 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:03,430 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:18:04,030 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:18:04,031 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:18:04,596 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:18:05,653 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:18:05,653 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:08,644 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:08,649 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:08,649 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:08,649 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:18:08,649 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:18:09,287 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:18:09,288 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:18:09,871 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:18:10,046 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:18:10,047 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 07:36:48,860 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 07:36:48,885 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7895 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 22510
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22610, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22610, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.061, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.083, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.051, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 71, avg_time 1.062, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 171, avg_time 1.070, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 271, avg_time 2.067, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 42, avg_time 1.046, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 142, avg_time 1.058, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 242, avg_time 1.048, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 13, avg_time 1.078, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 113, avg_time 2.086, loss:nan
g_step 1200, step 213, avg_time 1.062, loss:nan
g_step 1300, step 313, avg_time 1.051, loss:nan
g_step 1400, step 84, avg_time 1.073, loss:nan
g_step 1500, step 184, avg_time 1.055, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 284, avg_time 2.090, loss:nan
g_step 1700, step 55, avg_time 1.067, loss:nan
g_step 1800, step 155, avg_time 1.052, loss:nan
g_step 1900, step 255, avg_time 1.080, loss:nan
g_step 2000, step 26, avg_time 1.053, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 126, avg_time 2.097, loss:nan
g_step 2200, step 226, avg_time 1.065, loss:nan
g_step 2300, step 326, avg_time 1.051, loss:nan
g_step 2400, step 97, avg_time 1.061, loss:nan
g_step 2500, step 197, avg_time 1.061, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 297, avg_time 2.097, loss:nan
g_step 2700, step 68, avg_time 1.049, loss:nan
g_step 2800, step 168, avg_time 1.060, loss:nan
g_step 2900, step 268, avg_time 1.073, loss:nan
g_step 3000, step 39, avg_time 1.067, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 139, avg_time 2.102, loss:nan
g_step 3200, step 239, avg_time 1.059, loss:nan
g_step 3300, step 10, avg_time 1.059, loss:nan
g_step 3400, step 110, avg_time 1.052, loss:nan
g_step 3500, step 210, avg_time 1.061, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 310, avg_time 2.093, loss:nan
g_step 3700, step 81, avg_time 1.088, loss:nan
g_step 3800, step 181, avg_time 1.061, loss:nan
g_step 3900, step 281, avg_time 1.050, loss:nan
g_step 4000, step 52, avg_time 1.055, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 152, avg_time 2.071, loss:nan
g_step 4200, step 252, avg_time 1.063, loss:nan
g_step 4300, step 23, avg_time 1.074, loss:nan
g_step 4400, step 123, avg_time 1.059, loss:nan
g_step 4500, step 223, avg_time 1.070, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 323, avg_time 2.077, loss:nan
g_step 4700, step 94, avg_time 1.058, loss:nan
g_step 4800, step 194, avg_time 1.054, loss:nan
g_step 4900, step 294, avg_time 1.064, loss:nan
g_step 5000, step 65, avg_time 1.065, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 165, avg_time 2.080, loss:nan
g_step 5200, step 265, avg_time 1.053, loss:nan
g_step 5300, step 36, avg_time 1.057, loss:nan
g_step 5400, step 136, avg_time 1.054, loss:nan
g_step 5500, step 236, avg_time 1.078, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 7, avg_time 2.070, loss:nan
g_step 5700, step 107, avg_time 1.063, loss:nan
g_step 5800, step 207, avg_time 1.052, loss:nan
g_step 5900, step 307, avg_time 1.067, loss:nan
g_step 6000, step 78, avg_time 1.054, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 178, avg_time 2.096, loss:nan
g_step 6200, step 278, avg_time 1.076, loss:nan
g_step 6300, step 49, avg_time 1.052, loss:nan
g_step 6400, step 149, avg_time 1.043, loss:nan
g_step 6500, step 249, avg_time 1.072, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 07:36:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 07:36:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_07-36-48_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 07:36:49 - WARNING - datasets.builder -   Using custom data configuration default-8bf560d7c0010708
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8bf560d7c0010708/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 07:36:50,063 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:36:50,064 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:36:50,064 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:36:50,065 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:36:50,075 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:36:50,079 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:36:50,079 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:36:50,079 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:36:50,079 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:36:50,079 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:36:50,079 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 07:36:50,216 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:36:53,248 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 07:36:53,250 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8bf560d7c0010708/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.26ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.30ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.83ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.13ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.30ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.44ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.49ba/s]100%|██████████| 8/8 [00:01<00:00,  4.68ba/s]100%|██████████| 8/8 [00:01<00:00,  4.19ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.15ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.34ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.42ba/s]100%|██████████| 4/4 [00:00<00:00,  5.54ba/s]100%|██████████| 4/4 [00:00<00:00,  5.02ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.20ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.82ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 11.08ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 11.33ba/s]100%|██████████| 8/8 [00:00<00:00, 11.30ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.33ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 11.01ba/s]100%|██████████| 4/4 [00:00<00:00, 12.49ba/s]
[INFO|trainer.py:414] 2023-08-29 07:36:57,362 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 07:36:57,375 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 07:36:57,375 >>   Num examples = 7899
[INFO|trainer.py:1149] 2023-08-29 07:36:57,375 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 07:36:57,375 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 07:36:57,375 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 07:36:57,375 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 07:36:57,375 >>   Total optimization steps = 615
  0%|          | 0/615 [00:00<?, ?it/s]  0%|          | 1/615 [00:00<02:54,  3.51it/s]  0%|          | 2/615 [00:00<02:51,  3.58it/s]  0%|          | 3/615 [00:00<02:49,  3.60it/s]  1%|          | 4/615 [00:01<02:49,  3.61it/s]  1%|          | 5/615 [00:01<02:48,  3.62it/s]  1%|          | 6/615 [00:01<02:48,  3.62it/s]  1%|          | 7/615 [00:01<02:47,  3.62it/s]  1%|▏         | 8/615 [00:02<02:47,  3.63it/s]  1%|▏         | 9/615 [00:02<02:48,  3.61it/s]  2%|▏         | 10/615 [00:02<02:47,  3.62it/s]  2%|▏         | 11/615 [00:03<02:47,  3.62it/s]  2%|▏         | 12/615 [00:03<02:46,  3.62it/s]  2%|▏         | 13/615 [00:03<02:46,  3.62it/s]  2%|▏         | 14/615 [00:03<02:45,  3.62it/s]  2%|▏         | 15/615 [00:04<02:45,  3.62it/s]  3%|▎         | 16/615 [00:04<02:45,  3.62it/s]  3%|▎         | 17/615 [00:04<02:45,  3.62it/s]  3%|▎         | 18/615 [00:04<02:44,  3.62it/s]  3%|▎         | 19/615 [00:05<02:44,  3.62it/s]  3%|▎         | 20/615 [00:05<02:44,  3.61it/s]  3%|▎         | 21/615 [00:05<02:44,  3.61it/s]  4%|▎         | 22/615 [00:06<02:44,  3.61it/s]  4%|▎         | 23/615 [00:06<02:44,  3.61it/s]  4%|▍         | 24/615 [00:06<02:43,  3.61it/s]  4%|▍         | 25/615 [00:06<02:43,  3.62it/s]  4%|▍         | 26/615 [00:07<02:42,  3.62it/s]  4%|▍         | 27/615 [00:07<02:42,  3.62it/s]  5%|▍         | 28/615 [00:07<02:42,  3.62it/s]  5%|▍         | 29/615 [00:08<02:41,  3.62it/s]  5%|▍         | 30/615 [00:08<02:41,  3.62it/s]  5%|▌         | 31/615 [00:08<02:42,  3.60it/s]  5%|▌         | 32/615 [00:08<02:41,  3.61it/s]  5%|▌         | 33/615 [00:09<02:41,  3.61it/s]  6%|▌         | 34/615 [00:09<02:40,  3.61it/s]  6%|▌         | 35/615 [00:09<02:40,  3.62it/s]  6%|▌         | 36/615 [00:09<02:40,  3.62it/s]  6%|▌         | 37/615 [00:10<02:39,  3.62it/s]  6%|▌         | 38/615 [00:10<02:39,  3.62it/s]  6%|▋         | 39/615 [00:10<02:39,  3.62it/s]  7%|▋         | 40/615 [00:11<02:39,  3.61it/s]  7%|▋         | 41/615 [00:11<02:38,  3.61it/s]  7%|▋         | 42/615 [00:11<02:40,  3.57it/s]  7%|▋         | 43/615 [00:11<02:39,  3.58it/s]  7%|▋         | 44/615 [00:12<02:38,  3.60it/s]  7%|▋         | 45/615 [00:12<02:38,  3.60it/s]  7%|▋         | 46/615 [00:12<02:37,  3.61it/s]  8%|▊         | 47/615 [00:13<02:37,  3.61it/s]  8%|▊         | 48/615 [00:13<02:37,  3.61it/s]  8%|▊         | 49/615 [00:13<02:36,  3.61it/s]  8%|▊         | 50/615 [00:13<02:36,  3.61it/s]  8%|▊         | 51/615 [00:14<02:36,  3.61it/s]  8%|▊         | 52/615 [00:14<02:35,  3.62it/s]  9%|▊         | 53/615 [00:14<02:36,  3.60it/s]  9%|▉         | 54/615 [00:14<02:35,  3.61it/s]  9%|▉         | 55/615 [00:15<02:35,  3.61it/s]  9%|▉         | 56/615 [00:15<02:34,  3.61it/s]  9%|▉         | 57/615 [00:15<02:34,  3.61it/s]  9%|▉         | 58/615 [00:16<02:34,  3.61it/s] 10%|▉         | 59/615 [00:16<02:34,  3.61it/s] 10%|▉         | 60/615 [00:16<02:33,  3.61it/s] 10%|▉         | 61/615 [00:16<02:33,  3.61it/s] 10%|█         | 62/615 [00:17<02:33,  3.61it/s] 10%|█         | 63/615 [00:17<02:33,  3.61it/s] 10%|█         | 64/615 [00:17<02:33,  3.60it/s] 11%|█         | 65/615 [00:18<02:32,  3.60it/s] 11%|█         | 66/615 [00:18<02:32,  3.60it/s] 11%|█         | 67/615 [00:18<02:31,  3.61it/s] 11%|█         | 68/615 [00:18<02:31,  3.61it/s] 11%|█         | 69/615 [00:19<02:31,  3.61it/s] 11%|█▏        | 70/615 [00:19<02:31,  3.61it/s] 12%|█▏        | 71/615 [00:19<02:30,  3.60it/s] 12%|█▏        | 72/615 [00:19<02:30,  3.61it/s] 12%|█▏        | 73/615 [00:20<02:30,  3.61it/s] 12%|█▏        | 74/615 [00:20<02:29,  3.61it/s] 12%|█▏        | 75/615 [00:20<02:30,  3.59it/s] 12%|█▏        | 76/615 [00:21<02:29,  3.60it/s] 13%|█▎        | 77/615 [00:21<02:29,  3.61it/s] 13%|█▎        | 78/615 [00:21<02:28,  3.61it/s] 13%|█▎        | 79/615 [00:21<02:28,  3.61it/s] 13%|█▎        | 80/615 [00:22<02:28,  3.61it/s] 13%|█▎        | 81/615 [00:22<02:27,  3.61it/s] 13%|█▎        | 82/615 [00:22<02:27,  3.61it/s] 13%|█▎        | 83/615 [00:22<02:26,  3.62it/s] 14%|█▎        | 84/615 [00:23<02:26,  3.62it/s] 14%|█▍        | 85/615 [00:23<02:26,  3.61it/s] 14%|█▍        | 86/615 [00:23<02:26,  3.61it/s] 14%|█▍        | 87/615 [00:24<02:25,  3.62it/s] 14%|█▍        | 88/615 [00:24<02:25,  3.61it/s] 14%|█▍        | 89/615 [00:24<02:25,  3.61it/s] 15%|█▍        | 90/615 [00:24<02:25,  3.61it/s] 15%|█▍        | 91/615 [00:25<02:24,  3.61it/s] 15%|█▍        | 92/615 [00:25<02:24,  3.61it/s] 15%|█▌        | 93/615 [00:25<02:25,  3.60it/s] 15%|█▌        | 94/615 [00:26<02:24,  3.60it/s] 15%|█▌        | 95/615 [00:26<02:24,  3.60it/s] 16%|█▌        | 96/615 [00:26<02:23,  3.61it/s] 16%|█▌        | 97/615 [00:26<02:23,  3.61it/s] 16%|█▌        | 98/615 [00:27<02:23,  3.61it/s] 16%|█▌        | 99/615 [00:27<02:22,  3.61it/s] 16%|█▋        | 100/615 [00:27<02:22,  3.61it/s] 16%|█▋        | 101/615 [00:27<02:22,  3.61it/s] 17%|█▋        | 102/615 [00:28<02:21,  3.62it/s] 17%|█▋        | 103/615 [00:28<02:21,  3.61it/s] 17%|█▋        | 104/615 [00:28<02:21,  3.61it/s] 17%|█▋        | 105/615 [00:29<02:21,  3.60it/s] 17%|█▋        | 106/615 [00:29<02:21,  3.61it/s] 17%|█▋        | 107/615 [00:29<02:20,  3.61it/s] 18%|█▊        | 108/615 [00:29<02:20,  3.60it/s] 18%|█▊        | 109/615 [00:30<02:20,  3.60it/s] 18%|█▊        | 110/615 [00:30<02:20,  3.60it/s] 18%|█▊        | 111/615 [00:30<02:19,  3.61it/s] 18%|█▊        | 112/615 [00:31<02:19,  3.61it/s] 18%|█▊        | 113/615 [00:31<02:19,  3.61it/s] 19%|█▊        | 114/615 [00:31<02:18,  3.61it/s] 19%|█▊        | 115/615 [00:31<02:19,  3.59it/s] 19%|█▉        | 116/615 [00:32<02:18,  3.60it/s] 19%|█▉        | 117/615 [00:32<02:18,  3.60it/s] 19%|█▉        | 118/615 [00:32<02:17,  3.61it/s] 19%|█▉        | 119/615 [00:32<02:17,  3.61it/s] 20%|█▉        | 120/615 [00:33<02:17,  3.61it/s] 20%|█▉        | 121/615 [00:33<02:16,  3.62it/s] 20%|█▉        | 122/615 [00:33<02:16,  3.61it/s] 20%|██        | 123/615 [00:34<02:16,  3.61it/s][INFO|trainer.py:2140] 2023-08-29 07:37:31,572 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:37:31,572 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 07:37:31,572 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.19it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.81it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.75it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.73it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.91it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.48it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.27it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.15it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.18it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.43it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.53it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.61it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.34it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.16it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.01it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.91it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.95it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.06it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.29it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.43it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.47it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.33it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.16it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.97it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.95it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.97it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.96it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.26it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.42it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.53it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.27it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.07it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.92it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.94it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.00it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.03it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.27it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.40it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.34it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.21it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.97it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.94it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.99it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.01it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.13it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.35it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.46it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.48it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.22it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.08it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.00it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.03it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.99it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.19it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.32it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.38it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.37it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.19it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.00it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.91it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.92it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.99it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.14it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.31it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.31it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.29it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.20it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.04it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.95it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.89it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.02it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.21it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.25it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.26it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.21it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.16it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.02it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.93it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.01it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.11it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.17it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.24it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.30it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.25it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.23it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.06it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.96it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.96it/s][A 20%|██        | 123/615 [00:44<02:16,  3.61it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:37:41,480 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-123
[INFO|configuration_utils.py:351] 2023-08-29 07:37:41,501 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-123/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:37:43,129 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-123/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:37:43,150 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-123/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:37:43,161 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-123/special_tokens_map.json
 20%|██        | 124/615 [00:46<31:43,  3.88s/it] 20%|██        | 125/615 [00:46<22:51,  2.80s/it] 20%|██        | 126/615 [00:46<16:39,  2.04s/it] 21%|██        | 127/615 [00:47<12:20,  1.52s/it] 21%|██        | 128/615 [00:47<09:18,  1.15s/it] 21%|██        | 129/615 [00:47<07:11,  1.13it/s] 21%|██        | 130/615 [00:48<05:42,  1.42it/s] 21%|██▏       | 131/615 [00:48<04:40,  1.73it/s] 21%|██▏       | 132/615 [00:48<03:56,  2.04it/s] 22%|██▏       | 133/615 [00:48<03:26,  2.34it/s] 22%|██▏       | 134/615 [00:49<03:04,  2.61it/s] 22%|██▏       | 135/615 [00:49<02:49,  2.83it/s] 22%|██▏       | 136/615 [00:49<02:38,  3.01it/s] 22%|██▏       | 137/615 [00:50<02:31,  3.16it/s] 22%|██▏       | 138/615 [00:50<02:26,  3.26it/s] 23%|██▎       | 139/615 [00:50<02:23,  3.33it/s] 23%|██▎       | 140/615 [00:50<02:20,  3.39it/s] 23%|██▎       | 141/615 [00:51<02:17,  3.44it/s] 23%|██▎       | 142/615 [00:51<02:16,  3.47it/s] 23%|██▎       | 143/615 [00:51<02:19,  3.38it/s] 23%|██▎       | 144/615 [00:52<02:17,  3.42it/s] 24%|██▎       | 145/615 [00:52<02:15,  3.46it/s] 24%|██▎       | 146/615 [00:52<02:14,  3.49it/s] 24%|██▍       | 147/615 [00:52<02:12,  3.52it/s] 24%|██▍       | 148/615 [00:53<02:11,  3.55it/s] 24%|██▍       | 149/615 [00:53<02:10,  3.56it/s] 24%|██▍       | 150/615 [00:53<02:10,  3.57it/s] 25%|██▍       | 151/615 [00:53<02:09,  3.58it/s] 25%|██▍       | 152/615 [00:54<02:09,  3.58it/s] 25%|██▍       | 153/615 [00:54<02:08,  3.59it/s] 25%|██▌       | 154/615 [00:54<02:08,  3.59it/s] 25%|██▌       | 155/615 [00:55<02:07,  3.60it/s] 25%|██▌       | 156/615 [00:55<02:07,  3.60it/s] 26%|██▌       | 157/615 [00:55<02:07,  3.60it/s] 26%|██▌       | 158/615 [00:55<02:06,  3.60it/s] 26%|██▌       | 159/615 [00:56<02:06,  3.60it/s] 26%|██▌       | 160/615 [00:56<02:06,  3.59it/s] 26%|██▌       | 161/615 [00:56<02:06,  3.59it/s] 26%|██▋       | 162/615 [00:57<02:06,  3.59it/s] 27%|██▋       | 163/615 [00:57<02:05,  3.59it/s] 27%|██▋       | 164/615 [00:57<02:05,  3.59it/s] 27%|██▋       | 165/615 [00:57<02:05,  3.60it/s] 27%|██▋       | 166/615 [00:58<02:04,  3.60it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/615 [00:58<02:04,  3.60it/s] 27%|██▋       | 168/615 [00:58<02:04,  3.60it/s] 27%|██▋       | 169/615 [00:58<02:03,  3.60it/s] 28%|██▊       | 170/615 [00:59<02:03,  3.60it/s] 28%|██▊       | 171/615 [00:59<02:03,  3.59it/s] 28%|██▊       | 172/615 [00:59<02:03,  3.59it/s] 28%|██▊       | 173/615 [01:00<02:03,  3.59it/s] 28%|██▊       | 174/615 [01:00<02:02,  3.59it/s] 28%|██▊       | 175/615 [01:00<02:02,  3.60it/s] 29%|██▊       | 176/615 [01:00<02:02,  3.60it/s] 29%|██▉       | 177/615 [01:01<02:01,  3.60it/s] 29%|██▉       | 178/615 [01:01<02:01,  3.60it/s] 29%|██▉       | 179/615 [01:01<02:01,  3.60it/s] 29%|██▉       | 180/615 [01:02<02:00,  3.60it/s] 29%|██▉       | 181/615 [01:02<02:00,  3.60it/s] 30%|██▉       | 182/615 [01:02<02:00,  3.58it/s] 30%|██▉       | 183/615 [01:02<02:00,  3.59it/s] 30%|██▉       | 184/615 [01:03<02:00,  3.59it/s] 30%|███       | 185/615 [01:03<01:59,  3.59it/s] 30%|███       | 186/615 [01:03<01:59,  3.59it/s] 30%|███       | 187/615 [01:04<01:59,  3.60it/s] 31%|███       | 188/615 [01:04<01:58,  3.59it/s] 31%|███       | 189/615 [01:04<01:58,  3.60it/s] 31%|███       | 190/615 [01:04<01:58,  3.60it/s] 31%|███       | 191/615 [01:05<01:57,  3.60it/s] 31%|███       | 192/615 [01:05<01:57,  3.60it/s] 31%|███▏      | 193/615 [01:05<01:57,  3.59it/s] 32%|███▏      | 194/615 [01:05<01:57,  3.59it/s] 32%|███▏      | 195/615 [01:06<01:56,  3.59it/s] 32%|███▏      | 196/615 [01:06<01:56,  3.60it/s] 32%|███▏      | 197/615 [01:06<01:56,  3.59it/s] 32%|███▏      | 198/615 [01:07<01:56,  3.59it/s] 32%|███▏      | 199/615 [01:07<01:55,  3.60it/s] 33%|███▎      | 200/615 [01:07<01:55,  3.60it/s] 33%|███▎      | 201/615 [01:07<01:54,  3.60it/s] 33%|███▎      | 202/615 [01:08<01:54,  3.60it/s] 33%|███▎      | 203/615 [01:08<01:54,  3.60it/s] 33%|███▎      | 204/615 [01:08<01:54,  3.58it/s] 33%|███▎      | 205/615 [01:09<01:54,  3.59it/s] 33%|███▎      | 206/615 [01:09<01:53,  3.59it/s] 34%|███▎      | 207/615 [01:09<01:53,  3.60it/s] 34%|███▍      | 208/615 [01:09<01:53,  3.59it/s] 34%|███▍      | 209/615 [01:10<01:52,  3.60it/s] 34%|███▍      | 210/615 [01:10<01:52,  3.60it/s] 34%|███▍      | 211/615 [01:10<01:52,  3.60it/s] 34%|███▍      | 212/615 [01:10<01:51,  3.60it/s] 35%|███▍      | 213/615 [01:11<01:51,  3.60it/s] 35%|███▍      | 214/615 [01:11<01:51,  3.60it/s] 35%|███▍      | 215/615 [01:11<01:51,  3.59it/s] 35%|███▌      | 216/615 [01:12<01:51,  3.59it/s] 35%|███▌      | 217/615 [01:12<01:50,  3.59it/s] 35%|███▌      | 218/615 [01:12<01:50,  3.59it/s] 36%|███▌      | 219/615 [01:12<01:50,  3.59it/s] 36%|███▌      | 220/615 [01:13<01:49,  3.59it/s] 36%|███▌      | 221/615 [01:13<01:49,  3.60it/s] 36%|███▌      | 222/615 [01:13<01:49,  3.60it/s] 36%|███▋      | 223/615 [01:14<01:48,  3.60it/s] 36%|███▋      | 224/615 [01:14<01:48,  3.60it/s] 37%|███▋      | 225/615 [01:14<01:48,  3.60it/s] 37%|███▋      | 226/615 [01:14<01:48,  3.58it/s] 37%|███▋      | 227/615 [01:15<01:48,  3.59it/s] 37%|███▋      | 228/615 [01:15<01:47,  3.59it/s] 37%|███▋      | 229/615 [01:15<01:47,  3.59it/s] 37%|███▋      | 230/615 [01:15<01:47,  3.59it/s] 38%|███▊      | 231/615 [01:16<01:46,  3.60it/s] 38%|███▊      | 232/615 [01:16<01:46,  3.59it/s] 38%|███▊      | 233/615 [01:16<01:46,  3.60it/s] 38%|███▊      | 234/615 [01:17<01:45,  3.60it/s] 38%|███▊      | 235/615 [01:17<01:45,  3.60it/s] 38%|███▊      | 236/615 [01:17<01:45,  3.60it/s] 39%|███▊      | 237/615 [01:17<01:45,  3.59it/s] 39%|███▊      | 238/615 [01:18<01:44,  3.60it/s] 39%|███▉      | 239/615 [01:18<01:44,  3.59it/s] 39%|███▉      | 240/615 [01:18<01:44,  3.60it/s] 39%|███▉      | 241/615 [01:19<01:44,  3.60it/s] 39%|███▉      | 242/615 [01:19<01:43,  3.60it/s] 40%|███▉      | 243/615 [01:19<01:43,  3.60it/s] 40%|███▉      | 244/615 [01:19<01:43,  3.60it/s] 40%|███▉      | 245/615 [01:20<01:42,  3.60it/s] 40%|████      | 246/615 [01:20<01:42,  3.60it/s][INFO|trainer.py:2140] 2023-08-29 07:38:17,908 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:38:17,908 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 07:38:17,908 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.8908, 'eval_samples_per_second': 353.158, 'eval_steps_per_second': 44.183, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.31it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.42it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.79it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.72it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.96it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.51it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.15it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.88it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.06it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.27it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.38it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.50it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.35it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.18it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.05it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.88it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.80it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.01it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.18it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.32it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.38it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.22it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.08it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.94it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.83it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.82it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.03it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.18it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.30it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.38it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.28it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.91it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.91it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.89it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.91it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.09it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.23it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.31it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.37it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.26it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.04it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.96it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.88it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.97it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.15it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.22it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.30it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.32it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.14it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.94it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.98it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.93it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.97it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.04it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.30it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.33it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.25it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.07it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.02it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.00it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.96it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.97it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.08it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.28it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.27it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.27it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.11it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.00it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.94it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.93it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.99it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.10it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.25it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.26it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.17it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.00it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.01it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.98it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.02it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.11it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.08it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.17it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.18it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.13it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.06it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.06it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.99it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.99it/s][A 40%|████      | 246/615 [01:30<01:42,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:38:27,836 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-29 07:38:27,860 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:38:29,506 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:38:29,524 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:38:29,533 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-246/special_tokens_map.json
 40%|████      | 247/615 [01:32<23:49,  3.88s/it] 40%|████      | 248/615 [01:32<17:08,  2.80s/it] 40%|████      | 249/615 [01:33<12:28,  2.05s/it] 41%|████      | 250/615 [01:33<09:13,  1.52s/it] 41%|████      | 251/615 [01:33<06:57,  1.15s/it] 41%|████      | 252/615 [01:34<05:21,  1.13it/s] 41%|████      | 253/615 [01:34<04:15,  1.42it/s] 41%|████▏     | 254/615 [01:34<03:28,  1.73it/s] 41%|████▏     | 255/615 [01:34<02:56,  2.04it/s] 42%|████▏     | 256/615 [01:35<02:33,  2.34it/s] 42%|████▏     | 257/615 [01:35<02:17,  2.60it/s] 42%|████▏     | 258/615 [01:35<02:06,  2.83it/s] 42%|████▏     | 259/615 [01:36<01:58,  3.01it/s] 42%|████▏     | 260/615 [01:36<01:52,  3.16it/s] 42%|████▏     | 261/615 [01:36<01:48,  3.26it/s] 43%|████▎     | 262/615 [01:36<01:45,  3.34it/s] 43%|████▎     | 263/615 [01:37<01:43,  3.40it/s] 43%|████▎     | 264/615 [01:37<01:41,  3.44it/s] 43%|████▎     | 265/615 [01:37<01:40,  3.47it/s] 43%|████▎     | 266/615 [01:38<01:39,  3.50it/s] 43%|████▎     | 267/615 [01:38<01:39,  3.51it/s] 44%|████▎     | 268/615 [01:38<01:38,  3.51it/s] 44%|████▎     | 269/615 [01:38<01:38,  3.52it/s] 44%|████▍     | 270/615 [01:39<01:37,  3.53it/s] 44%|████▍     | 271/615 [01:39<01:37,  3.53it/s] 44%|████▍     | 272/615 [01:39<01:37,  3.53it/s] 44%|████▍     | 273/615 [01:40<01:36,  3.54it/s] 45%|████▍     | 274/615 [01:40<01:36,  3.54it/s] 45%|████▍     | 275/615 [01:40<01:35,  3.54it/s] 45%|████▍     | 276/615 [01:40<01:35,  3.55it/s] 45%|████▌     | 277/615 [01:41<01:35,  3.55it/s] 45%|████▌     | 278/615 [01:41<01:34,  3.55it/s] 45%|████▌     | 279/615 [01:41<01:35,  3.53it/s] 46%|████▌     | 280/615 [01:42<01:34,  3.53it/s] 46%|████▌     | 281/615 [01:42<01:34,  3.54it/s] 46%|████▌     | 282/615 [01:42<01:34,  3.54it/s] 46%|████▌     | 283/615 [01:42<01:33,  3.55it/s] 46%|████▌     | 284/615 [01:43<01:33,  3.55it/s] 46%|████▋     | 285/615 [01:43<01:32,  3.55it/s] 47%|████▋     | 286/615 [01:43<01:32,  3.55it/s] 47%|████▋     | 287/615 [01:43<01:32,  3.55it/s] 47%|████▋     | 288/615 [01:44<01:32,  3.55it/s] 47%|████▋     | 289/615 [01:44<01:31,  3.56it/s] 47%|████▋     | 290/615 [01:44<01:31,  3.56it/s] 47%|████▋     | 291/615 [01:45<01:30,  3.58it/s] 47%|████▋     | 292/615 [01:45<01:29,  3.59it/s] 48%|████▊     | 293/615 [01:45<01:29,  3.59it/s] 48%|████▊     | 294/615 [01:45<01:29,  3.60it/s] 48%|████▊     | 295/615 [01:46<01:28,  3.60it/s] 48%|████▊     | 296/615 [01:46<01:28,  3.60it/s] 48%|████▊     | 297/615 [01:46<01:28,  3.60it/s] 48%|████▊     | 298/615 [01:47<01:27,  3.60it/s] 49%|████▊     | 299/615 [01:47<01:27,  3.60it/s] 49%|████▉     | 300/615 [01:47<01:27,  3.61it/s] 49%|████▉     | 301/615 [01:47<01:27,  3.59it/s] 49%|████▉     | 302/615 [01:48<01:27,  3.60it/s] 49%|████▉     | 303/615 [01:48<01:26,  3.60it/s] 49%|████▉     | 304/615 [01:48<01:26,  3.60it/s] 50%|████▉     | 305/615 [01:48<01:26,  3.60it/s] 50%|████▉     | 306/615 [01:49<01:25,  3.60it/s] 50%|████▉     | 307/615 [01:49<01:25,  3.60it/s] 50%|█████     | 308/615 [01:49<01:25,  3.60it/s] 50%|█████     | 309/615 [01:50<01:24,  3.60it/s] 50%|█████     | 310/615 [01:50<01:24,  3.61it/s] 51%|█████     | 311/615 [01:50<01:24,  3.58it/s] 51%|█████     | 312/615 [01:50<01:24,  3.57it/s] 51%|█████     | 313/615 [01:51<01:24,  3.58it/s] 51%|█████     | 314/615 [01:51<01:23,  3.59it/s] 51%|█████     | 315/615 [01:51<01:25,  3.50it/s] 51%|█████▏    | 316/615 [01:52<01:24,  3.53it/s] 52%|█████▏    | 317/615 [01:52<01:23,  3.55it/s] 52%|█████▏    | 318/615 [01:52<01:23,  3.57it/s] 52%|█████▏    | 319/615 [01:52<01:22,  3.58it/s] 52%|█████▏    | 320/615 [01:53<01:22,  3.59it/s] 52%|█████▏    | 321/615 [01:53<01:21,  3.60it/s] 52%|█████▏    | 322/615 [01:53<01:21,  3.60it/s] 53%|█████▎    | 323/615 [01:54<01:21,  3.59it/s] 53%|█████▎    | 324/615 [01:54<01:20,  3.60it/s] 53%|█████▎    | 325/615 [01:54<01:20,  3.60it/s] 53%|█████▎    | 326/615 [01:54<01:20,  3.60it/s] 53%|█████▎    | 327/615 [01:55<01:19,  3.60it/s] 53%|█████▎    | 328/615 [01:55<01:19,  3.61it/s] 53%|█████▎    | 329/615 [01:55<01:19,  3.60it/s] 54%|█████▎    | 330/615 [01:55<01:19,  3.61it/s] 54%|█████▍    | 331/615 [01:56<01:18,  3.61it/s] 54%|█████▍    | 332/615 [01:56<01:18,  3.61it/s] 54%|█████▍    | 333/615 [01:56<01:18,  3.61it/s] 54%|█████▍    | 334/615 [01:57<01:18,  3.59it/s] 54%|█████▍    | 335/615 [01:57<01:17,  3.59it/s] 55%|█████▍    | 336/615 [01:57<01:17,  3.60it/s] 55%|█████▍    | 337/615 [01:57<01:17,  3.60it/s] 55%|█████▍    | 338/615 [01:58<01:16,  3.60it/s] 55%|█████▌    | 339/615 [01:58<01:16,  3.60it/s] 55%|█████▌    | 340/615 [01:58<01:16,  3.60it/s] 55%|█████▌    | 341/615 [01:59<01:15,  3.61it/s] 56%|█████▌    | 342/615 [01:59<01:15,  3.61it/s] 56%|█████▌    | 343/615 [01:59<01:15,  3.61it/s] 56%|█████▌    | 344/615 [01:59<01:15,  3.61it/s] 56%|█████▌    | 345/615 [02:00<01:14,  3.61it/s] 56%|█████▋    | 346/615 [02:00<01:14,  3.61it/s] 56%|█████▋    | 347/615 [02:00<01:14,  3.60it/s] 57%|█████▋    | 348/615 [02:00<01:14,  3.60it/s] 57%|█████▋    | 349/615 [02:01<01:13,  3.61it/s] 57%|█████▋    | 350/615 [02:01<01:13,  3.61it/s] 57%|█████▋    | 351/615 [02:01<01:13,  3.61it/s] 57%|█████▋    | 352/615 [02:02<01:12,  3.60it/s] 57%|█████▋    | 353/615 [02:02<01:12,  3.61it/s] 58%|█████▊    | 354/615 [02:02<01:12,  3.60it/s] 58%|█████▊    | 355/615 [02:02<01:12,  3.59it/s] 58%|█████▊    | 356/615 [02:03<01:12,  3.59it/s] 58%|█████▊    | 357/615 [02:03<01:11,  3.60it/s] 58%|█████▊    | 358/615 [02:03<01:11,  3.61it/s] 58%|█████▊    | 359/615 [02:04<01:10,  3.61it/s] 59%|█████▊    | 360/615 [02:04<01:10,  3.60it/s] 59%|█████▊    | 361/615 [02:04<01:10,  3.60it/s] 59%|█████▉    | 362/615 [02:04<01:10,  3.60it/s] 59%|█████▉    | 363/615 [02:05<01:09,  3.60it/s] 59%|█████▉    | 364/615 [02:05<01:09,  3.60it/s] 59%|█████▉    | 365/615 [02:05<01:09,  3.61it/s] 60%|█████▉    | 366/615 [02:05<01:09,  3.61it/s] 60%|█████▉    | 367/615 [02:06<01:08,  3.61it/s] 60%|█████▉    | 368/615 [02:06<01:08,  3.61it/s] 60%|██████    | 369/615 [02:06<01:08,  3.61it/s][INFO|trainer.py:2140] 2023-08-29 07:39:04,286 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:39:04,287 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 07:39:04,287 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9022, 'eval_samples_per_second': 352.749, 'eval_steps_per_second': 44.131, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.15it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.76it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.75it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.82it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.06it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.45it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.10it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.96it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.14it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.26it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.38it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.43it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.43it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.13it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.01it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.82it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.84it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.94it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.06it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.32it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.42it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.33it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.11it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.95it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.79it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.87it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.94it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.12it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.32it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.38it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.22it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.09it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.96it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.85it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.84it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.99it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.09it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.33it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.36it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.16it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.14it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.04it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.93it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.91it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.04it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.14it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.28it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.18it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.17it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.11it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.01it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.91it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.83it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.05it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.17it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.25it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.26it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.13it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.98it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.01it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.88it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.89it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.10it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.20it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.26it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.17it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.06it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.03it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.00it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.84it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.92it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.06it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.23it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.20it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.23it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.08it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.97it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.90it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.89it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.94it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.08it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.20it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.31it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.22it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.17it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.01it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.97it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.97it/s][A 60%|██████    | 369/615 [02:16<01:08,  3.61it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:39:14,204 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-369
[INFO|configuration_utils.py:351] 2023-08-29 07:39:14,223 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-369/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:39:15,932 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-369/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:39:15,965 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-369/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:39:15,986 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-369/special_tokens_map.json
 60%|██████    | 370/615 [02:19<15:56,  3.90s/it] 60%|██████    | 371/615 [02:19<11:27,  2.82s/it] 60%|██████    | 372/615 [02:19<08:19,  2.06s/it] 61%|██████    | 373/615 [02:20<06:08,  1.52s/it] 61%|██████    | 374/615 [02:20<04:37,  1.15s/it] 61%|██████    | 375/615 [02:20<03:33,  1.12it/s] 61%|██████    | 376/615 [02:20<02:49,  1.41it/s] 61%|██████▏   | 377/615 [02:21<02:18,  1.72it/s] 61%|██████▏   | 378/615 [02:21<01:56,  2.04it/s] 62%|██████▏   | 379/615 [02:21<01:40,  2.35it/s] 62%|██████▏   | 380/615 [02:21<01:29,  2.62it/s] 62%|██████▏   | 381/615 [02:22<01:22,  2.85it/s] 62%|██████▏   | 382/615 [02:22<01:16,  3.04it/s] 62%|██████▏   | 383/615 [02:22<01:12,  3.19it/s] 62%|██████▏   | 384/615 [02:23<01:09,  3.31it/s] 63%|██████▎   | 385/615 [02:23<01:07,  3.39it/s] 63%|██████▎   | 386/615 [02:23<01:06,  3.45it/s] 63%|██████▎   | 387/615 [02:23<01:05,  3.49it/s] 63%|██████▎   | 388/615 [02:24<01:04,  3.53it/s] 63%|██████▎   | 389/615 [02:24<01:03,  3.55it/s] 63%|██████▎   | 390/615 [02:24<01:03,  3.56it/s] 64%|██████▎   | 391/615 [02:25<01:02,  3.56it/s] 64%|██████▎   | 392/615 [02:25<01:02,  3.57it/s] 64%|██████▍   | 393/615 [02:25<01:02,  3.58it/s] 64%|██████▍   | 394/615 [02:25<01:01,  3.59it/s] 64%|██████▍   | 395/615 [02:26<01:01,  3.59it/s] 64%|██████▍   | 396/615 [02:26<01:00,  3.60it/s] 65%|██████▍   | 397/615 [02:26<01:00,  3.60it/s] 65%|██████▍   | 398/615 [02:26<01:00,  3.60it/s] 65%|██████▍   | 399/615 [02:27<01:00,  3.60it/s] 65%|██████▌   | 400/615 [02:27<00:59,  3.60it/s] 65%|██████▌   | 401/615 [02:27<00:59,  3.60it/s] 65%|██████▌   | 402/615 [02:28<00:59,  3.58it/s] 66%|██████▌   | 403/615 [02:28<00:59,  3.58it/s] 66%|██████▌   | 404/615 [02:28<00:58,  3.59it/s] 66%|██████▌   | 405/615 [02:28<00:58,  3.59it/s] 66%|██████▌   | 406/615 [02:29<00:58,  3.60it/s] 66%|██████▌   | 407/615 [02:29<00:57,  3.60it/s] 66%|██████▋   | 408/615 [02:29<00:57,  3.60it/s] 67%|██████▋   | 409/615 [02:30<00:57,  3.61it/s] 67%|██████▋   | 410/615 [02:30<00:56,  3.60it/s] 67%|██████▋   | 411/615 [02:30<00:56,  3.60it/s] 67%|██████▋   | 412/615 [02:30<00:56,  3.60it/s] 67%|██████▋   | 413/615 [02:31<00:56,  3.59it/s] 67%|██████▋   | 414/615 [02:31<00:55,  3.59it/s] 67%|██████▋   | 415/615 [02:31<00:55,  3.60it/s] 68%|██████▊   | 416/615 [02:31<00:55,  3.60it/s] 68%|██████▊   | 417/615 [02:32<00:54,  3.60it/s] 68%|██████▊   | 418/615 [02:32<00:54,  3.61it/s] 68%|██████▊   | 419/615 [02:32<00:54,  3.60it/s] 68%|██████▊   | 420/615 [02:33<00:54,  3.60it/s] 68%|██████▊   | 421/615 [02:33<00:53,  3.60it/s] 69%|██████▊   | 422/615 [02:33<00:53,  3.61it/s] 69%|██████▉   | 423/615 [02:33<00:53,  3.61it/s] 69%|██████▉   | 424/615 [02:34<00:53,  3.59it/s] 69%|██████▉   | 425/615 [02:34<00:52,  3.60it/s] 69%|██████▉   | 426/615 [02:34<00:52,  3.60it/s] 69%|██████▉   | 427/615 [02:35<00:52,  3.60it/s] 70%|██████▉   | 428/615 [02:35<00:51,  3.61it/s] 70%|██████▉   | 429/615 [02:35<00:51,  3.61it/s] 70%|██████▉   | 430/615 [02:35<00:51,  3.61it/s] 70%|███████   | 431/615 [02:36<00:51,  3.61it/s] 70%|███████   | 432/615 [02:36<00:50,  3.60it/s] 70%|███████   | 433/615 [02:36<00:50,  3.61it/s] 71%|███████   | 434/615 [02:36<00:50,  3.61it/s] 71%|███████   | 435/615 [02:37<00:50,  3.60it/s] 71%|███████   | 436/615 [02:37<00:49,  3.60it/s] 71%|███████   | 437/615 [02:37<00:49,  3.60it/s] 71%|███████   | 438/615 [02:38<00:49,  3.60it/s] 71%|███████▏  | 439/615 [02:38<00:48,  3.60it/s] 72%|███████▏  | 440/615 [02:38<00:48,  3.60it/s] 72%|███████▏  | 441/615 [02:38<00:48,  3.60it/s] 72%|███████▏  | 442/615 [02:39<00:48,  3.60it/s] 72%|███████▏  | 443/615 [02:39<00:47,  3.61it/s] 72%|███████▏  | 444/615 [02:39<00:47,  3.60it/s] 72%|███████▏  | 445/615 [02:40<00:47,  3.60it/s] 73%|███████▎  | 446/615 [02:40<00:47,  3.58it/s] 73%|███████▎  | 447/615 [02:40<00:46,  3.59it/s] 73%|███████▎  | 448/615 [02:40<00:46,  3.59it/s] 73%|███████▎  | 449/615 [02:41<00:46,  3.59it/s] 73%|███████▎  | 450/615 [02:41<00:45,  3.60it/s] 73%|███████▎  | 451/615 [02:41<00:45,  3.60it/s] 73%|███████▎  | 452/615 [02:41<00:45,  3.60it/s] 74%|███████▎  | 453/615 [02:42<00:44,  3.61it/s] 74%|███████▍  | 454/615 [02:42<00:44,  3.61it/s] 74%|███████▍  | 455/615 [02:42<00:44,  3.60it/s] 74%|███████▍  | 456/615 [02:43<00:44,  3.60it/s] 74%|███████▍  | 457/615 [02:43<00:44,  3.58it/s] 74%|███████▍  | 458/615 [02:43<00:43,  3.59it/s] 75%|███████▍  | 459/615 [02:43<00:43,  3.59it/s] 75%|███████▍  | 460/615 [02:44<00:43,  3.60it/s] 75%|███████▍  | 461/615 [02:44<00:42,  3.60it/s] 75%|███████▌  | 462/615 [02:44<00:42,  3.60it/s] 75%|███████▌  | 463/615 [02:45<00:42,  3.60it/s] 75%|███████▌  | 464/615 [02:45<00:41,  3.60it/s] 76%|███████▌  | 465/615 [02:45<00:41,  3.60it/s] 76%|███████▌  | 466/615 [02:45<00:41,  3.60it/s] 76%|███████▌  | 467/615 [02:46<00:41,  3.60it/s] 76%|███████▌  | 468/615 [02:46<00:40,  3.60it/s] 76%|███████▋  | 469/615 [02:46<00:40,  3.60it/s] 76%|███████▋  | 470/615 [02:46<00:40,  3.60it/s] 77%|███████▋  | 471/615 [02:47<00:39,  3.61it/s] 77%|███████▋  | 472/615 [02:47<00:39,  3.61it/s] 77%|███████▋  | 473/615 [02:47<00:39,  3.61it/s] 77%|███████▋  | 474/615 [02:48<00:39,  3.61it/s] 77%|███████▋  | 475/615 [02:48<00:38,  3.61it/s] 77%|███████▋  | 476/615 [02:48<00:38,  3.60it/s] 78%|███████▊  | 477/615 [02:48<00:38,  3.58it/s] 78%|███████▊  | 478/615 [02:49<00:38,  3.59it/s] 78%|███████▊  | 479/615 [02:49<00:37,  3.59it/s] 78%|███████▊  | 480/615 [02:49<00:37,  3.59it/s] 78%|███████▊  | 481/615 [02:50<00:37,  3.59it/s] 78%|███████▊  | 482/615 [02:50<00:36,  3.60it/s] 79%|███████▊  | 483/615 [02:50<00:36,  3.60it/s] 79%|███████▊  | 484/615 [02:50<00:36,  3.59it/s] 79%|███████▉  | 485/615 [02:51<00:36,  3.59it/s] 79%|███████▉  | 486/615 [02:51<00:35,  3.60it/s] 79%|███████▉  | 487/615 [02:51<00:35,  3.60it/s] 79%|███████▉  | 488/615 [02:52<00:36,  3.51it/s] 80%|███████▉  | 489/615 [02:52<00:35,  3.52it/s] 80%|███████▉  | 490/615 [02:52<00:35,  3.54it/s] 80%|███████▉  | 491/615 [02:52<00:34,  3.56it/s] 80%|████████  | 492/615 [02:53<00:34,  3.57it/s][INFO|trainer.py:2140] 2023-08-29 07:39:50,614 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:39:50,614 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 07:39:50,614 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9052, 'eval_samples_per_second': 352.642, 'eval_steps_per_second': 44.118, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.73it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.53it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.67it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.80it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.96it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.51it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.12it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.02it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.95it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.15it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.30it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.48it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.42it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.23it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.99it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.78it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.79it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.91it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.11it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.31it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.42it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.32it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.19it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.88it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.77it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.76it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.94it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.10it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.33it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.36it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.26it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.13it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.98it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.86it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.83it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.02it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.14it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.30it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.36it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.28it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.99it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.80it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.81it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.84it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.04it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.19it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.30it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.34it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.24it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.04it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.92it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.83it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.00it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.07it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.22it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.31it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.38it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.32it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.15it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.96it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.93it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.91it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.97it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.12it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.24it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.29it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.25it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.09it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.06it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.96it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.97it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.12it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.20it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.22it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.31it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.14it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.06it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.93it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.89it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.87it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.12it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.19it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.22it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.24it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.16it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.06it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.04it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.04it/s][A 80%|████████  | 492/615 [03:03<00:34,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:40:00,539 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-29 07:40:00,563 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:40:02,217 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:40:02,228 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:40:02,240 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-492/special_tokens_map.json
 80%|████████  | 493/615 [03:05<07:54,  3.89s/it] 80%|████████  | 494/615 [03:05<05:39,  2.81s/it] 80%|████████  | 495/615 [03:05<04:06,  2.05s/it] 81%|████████  | 496/615 [03:06<03:00,  1.52s/it] 81%|████████  | 497/615 [03:06<02:15,  1.15s/it] 81%|████████  | 498/615 [03:06<01:43,  1.13it/s] 81%|████████  | 499/615 [03:07<01:21,  1.42it/s] 81%|████████▏ | 500/615 [03:07<01:06,  1.73it/s]                                                  81%|████████▏ | 500/615 [03:07<01:06,  1.73it/s] 81%|████████▏ | 501/615 [03:07<00:55,  2.04it/s] 82%|████████▏ | 502/615 [03:07<00:48,  2.34it/s] 82%|████████▏ | 503/615 [03:08<00:43,  2.60it/s] 82%|████████▏ | 504/615 [03:08<00:39,  2.82it/s] 82%|████████▏ | 505/615 [03:08<00:36,  3.01it/s] 82%|████████▏ | 506/615 [03:09<00:34,  3.15it/s] 82%|████████▏ | 507/615 [03:09<00:33,  3.26it/s] 83%|████████▎ | 508/615 [03:09<00:31,  3.34it/s] 83%|████████▎ | 509/615 [03:09<00:31,  3.40it/s] 83%|████████▎ | 510/615 [03:10<00:30,  3.45it/s] 83%|████████▎ | 511/615 [03:10<00:29,  3.48it/s] 83%|████████▎ | 512/615 [03:10<00:29,  3.50it/s] 83%|████████▎ | 513/615 [03:11<00:29,  3.51it/s] 84%|████████▎ | 514/615 [03:11<00:28,  3.51it/s] 84%|████████▎ | 515/615 [03:11<00:28,  3.52it/s] 84%|████████▍ | 516/615 [03:11<00:28,  3.53it/s] 84%|████████▍ | 517/615 [03:12<00:27,  3.54it/s] 84%|████████▍ | 518/615 [03:12<00:27,  3.54it/s] 84%|████████▍ | 519/615 [03:12<00:27,  3.54it/s] 85%|████████▍ | 520/615 [03:13<00:26,  3.55it/s] 85%|████████▍ | 521/615 [03:13<00:26,  3.55it/s] 85%|████████▍ | 522/615 [03:13<00:26,  3.54it/s] 85%|████████▌ | 523/615 [03:13<00:25,  3.54it/s] 85%|████████▌ | 524/615 [03:14<00:25,  3.54it/s] 85%|████████▌ | 525/615 [03:14<00:25,  3.53it/s] 86%|████████▌ | 526/615 [03:14<00:25,  3.54it/s] 86%|████████▌ | 527/615 [03:15<00:24,  3.54it/s] 86%|████████▌ | 528/615 [03:15<00:24,  3.54it/s] 86%|████████▌ | 529/615 [03:15<00:24,  3.54it/s] 86%|████████▌ | 530/615 [03:15<00:23,  3.54it/s] 86%|████████▋ | 531/615 [03:16<00:23,  3.54it/s] 87%|████████▋ | 532/615 [03:16<00:23,  3.54it/s] 87%|████████▋ | 533/615 [03:16<00:23,  3.55it/s] 87%|████████▋ | 534/615 [03:17<00:22,  3.55it/s] 87%|████████▋ | 535/615 [03:17<00:22,  3.55it/s] 87%|████████▋ | 536/615 [03:17<00:22,  3.53it/s] 87%|████████▋ | 537/615 [03:17<00:22,  3.53it/s] 87%|████████▋ | 538/615 [03:18<00:21,  3.54it/s] 88%|████████▊ | 539/615 [03:18<00:21,  3.54it/s] 88%|████████▊ | 540/615 [03:18<00:21,  3.54it/s] 88%|████████▊ | 541/615 [03:18<00:20,  3.54it/s] 88%|████████▊ | 542/615 [03:19<00:20,  3.55it/s] 88%|████████▊ | 543/615 [03:19<00:20,  3.55it/s] 88%|████████▊ | 544/615 [03:19<00:20,  3.55it/s] 89%|████████▊ | 545/615 [03:20<00:19,  3.55it/s] 89%|████████▉ | 546/615 [03:20<00:19,  3.54it/s] 89%|████████▉ | 547/615 [03:20<00:19,  3.53it/s] 89%|████████▉ | 548/615 [03:20<00:18,  3.54it/s] 89%|████████▉ | 549/615 [03:21<00:18,  3.54it/s] 89%|████████▉ | 550/615 [03:21<00:18,  3.55it/s] 90%|████████▉ | 551/615 [03:21<00:18,  3.55it/s] 90%|████████▉ | 552/615 [03:22<00:17,  3.55it/s] 90%|████████▉ | 553/615 [03:22<00:17,  3.55it/s] 90%|█████████ | 554/615 [03:22<00:17,  3.55it/s] 90%|█████████ | 555/615 [03:22<00:16,  3.55it/s] 90%|█████████ | 556/615 [03:23<00:16,  3.55it/s] 91%|█████████ | 557/615 [03:23<00:16,  3.54it/s] 91%|█████████ | 558/615 [03:23<00:16,  3.53it/s] 91%|█████████ | 559/615 [03:24<00:15,  3.54it/s] 91%|█████████ | 560/615 [03:24<00:15,  3.54it/s] 91%|█████████ | 561/615 [03:24<00:15,  3.54it/s] 91%|█████████▏| 562/615 [03:24<00:14,  3.54it/s] 92%|█████████▏| 563/615 [03:25<00:14,  3.54it/s] 92%|█████████▏| 564/615 [03:25<00:14,  3.54it/s] 92%|█████████▏| 565/615 [03:25<00:14,  3.54it/s] 92%|█████████▏| 566/615 [03:26<00:13,  3.54it/s] 92%|█████████▏| 567/615 [03:26<00:13,  3.54it/s] 92%|█████████▏| 568/615 [03:26<00:13,  3.55it/s] 93%|█████████▎| 569/615 [03:26<00:13,  3.53it/s] 93%|█████████▎| 570/615 [03:27<00:12,  3.54it/s] 93%|█████████▎| 571/615 [03:27<00:12,  3.54it/s] 93%|█████████▎| 572/615 [03:27<00:12,  3.54it/s] 93%|█████████▎| 573/615 [03:28<00:11,  3.54it/s] 93%|█████████▎| 574/615 [03:28<00:11,  3.54it/s] 93%|█████████▎| 575/615 [03:28<00:11,  3.55it/s] 94%|█████████▎| 576/615 [03:28<00:10,  3.55it/s] 94%|█████████▍| 577/615 [03:29<00:10,  3.55it/s] 94%|█████████▍| 578/615 [03:29<00:10,  3.55it/s] 94%|█████████▍| 579/615 [03:29<00:10,  3.56it/s] 94%|█████████▍| 580/615 [03:29<00:09,  3.53it/s] 94%|█████████▍| 581/615 [03:30<00:09,  3.53it/s] 95%|█████████▍| 582/615 [03:30<00:09,  3.54it/s] 95%|█████████▍| 583/615 [03:30<00:09,  3.54it/s] 95%|█████████▍| 584/615 [03:31<00:08,  3.55it/s] 95%|█████████▌| 585/615 [03:31<00:08,  3.55it/s] 95%|█████████▌| 586/615 [03:31<00:08,  3.55it/s] 95%|█████████▌| 587/615 [03:31<00:07,  3.55it/s] 96%|█████████▌| 588/615 [03:32<00:07,  3.54it/s] 96%|█████████▌| 589/615 [03:32<00:07,  3.56it/s] 96%|█████████▌| 590/615 [03:32<00:06,  3.58it/s] 96%|█████████▌| 591/615 [03:33<00:06,  3.59it/s] 96%|█████████▋| 592/615 [03:33<00:06,  3.60it/s] 96%|█████████▋| 593/615 [03:33<00:06,  3.60it/s] 97%|█████████▋| 594/615 [03:33<00:05,  3.61it/s] 97%|█████████▋| 595/615 [03:34<00:05,  3.60it/s] 97%|█████████▋| 596/615 [03:34<00:05,  3.61it/s] 97%|█████████▋| 597/615 [03:34<00:04,  3.61it/s] 97%|█████████▋| 598/615 [03:35<00:04,  3.59it/s] 97%|█████████▋| 599/615 [03:35<00:04,  3.60it/s] 98%|█████████▊| 600/615 [03:35<00:04,  3.60it/s] 98%|█████████▊| 601/615 [03:35<00:03,  3.61it/s] 98%|█████████▊| 602/615 [03:36<00:03,  3.61it/s] 98%|█████████▊| 603/615 [03:36<00:03,  3.61it/s] 98%|█████████▊| 604/615 [03:36<00:03,  3.61it/s] 98%|█████████▊| 605/615 [03:36<00:02,  3.61it/s] 99%|█████████▊| 606/615 [03:37<00:02,  3.61it/s] 99%|█████████▊| 607/615 [03:37<00:02,  3.61it/s] 99%|█████████▉| 608/615 [03:37<00:01,  3.61it/s] 99%|█████████▉| 609/615 [03:38<00:01,  3.61it/s] 99%|█████████▉| 610/615 [03:38<00:01,  3.61it/s] 99%|█████████▉| 611/615 [03:38<00:01,  3.61it/s]100%|█████████▉| 612/615 [03:38<00:00,  3.61it/s]100%|█████████▉| 613/615 [03:39<00:00,  3.62it/s]100%|█████████▉| 614/615 [03:39<00:00,  3.62it/s]100%|██████████| 615/615 [03:39<00:00,  3.61it/s][INFO|trainer.py:2140] 2023-08-29 07:40:37,101 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:40:37,101 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 07:40:37,101 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9027, 'eval_samples_per_second': 352.732, 'eval_steps_per_second': 44.129, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7134146341463415e-05, 'epoch': 4.06}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.08it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.80it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.80it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.60it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.10it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.56it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.21it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.08it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.12it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.28it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.33it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.43it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.44it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.24it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.02it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.88it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.91it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.06it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.07it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.16it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.25it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.29it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.15it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.91it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.88it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.90it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.05it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.11it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.24it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.34it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.28it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.06it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.02it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.99it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.01it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.96it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.02it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.26it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.37it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.22it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.04it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.99it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.97it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.90it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.00it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.11it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.28it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.35it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.20it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.00it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.96it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.98it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.90it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.08it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.21it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.31it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.30it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.17it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.04it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.04it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.86it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.88it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.10it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.22it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.23it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.28it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.17it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.03it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.98it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.04it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.05it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.19it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.24it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.24it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.19it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.07it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.00it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.00it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.01it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.98it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.19it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.23it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.97it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.06it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.02it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.96it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.06it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.06it/s][A100%|██████████| 615/615 [03:49<00:00,  3.61it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:40:47,016 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-615
[INFO|configuration_utils.py:351] 2023-08-29 07:40:47,040 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-615/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:40:48,628 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-615/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:40:48,648 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-615/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:40:48,662 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-615/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 07:40:48,948 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 07:40:48,948 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-123 (score: 1.1039679050445557).
                                                 100%|██████████| 615/615 [03:53<00:00,  3.61it/s]100%|██████████| 615/615 [03:53<00:00,  2.64it/s]
[INFO|trainer.py:1894] 2023-08-29 07:40:50,629 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 07:40:50,652 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:40:52,467 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:40:52,491 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:40:52,506 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:40:52,706 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:40:52,706 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:40:52,706 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:40:52,706 >>   train_runtime            = 0:03:53.22
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:40:52,706 >>   train_samples            =       7899
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:40:52,706 >>   train_samples_per_second =    169.343
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:40:52,706 >>   train_steps_per_second   =      2.637
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9006, 'eval_samples_per_second': 352.808, 'eval_steps_per_second': 44.139, 'epoch': 5.0}
{'train_runtime': 233.2251, 'train_samples_per_second': 169.343, 'train_steps_per_second': 2.637, 'train_loss': nan, 'epoch': 5.0}
08/29/2023 07:40:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 07:40:52,747 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:40:52,747 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 07:40:52,748 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 56.46it/s]  3%|▎         | 12/437 [00:00<00:08, 48.90it/s]  4%|▍         | 17/437 [00:00<00:08, 47.04it/s]  5%|▌         | 22/437 [00:00<00:08, 46.38it/s]  6%|▌         | 27/437 [00:00<00:08, 45.83it/s]  7%|▋         | 32/437 [00:00<00:08, 45.52it/s]  8%|▊         | 37/437 [00:00<00:08, 45.27it/s] 10%|▉         | 42/437 [00:00<00:08, 44.61it/s] 11%|█         | 47/437 [00:01<00:08, 44.11it/s] 12%|█▏        | 52/437 [00:01<00:08, 43.85it/s] 13%|█▎        | 57/437 [00:01<00:08, 43.97it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.21it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.38it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.49it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.57it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.58it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.25it/s] 21%|██        | 92/437 [00:02<00:07, 43.97it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.77it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.92it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.12it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.35it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.50it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.51it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.54it/s] 30%|███       | 132/437 [00:02<00:06, 44.31it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.02it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.74it/s] 34%|███▎      | 147/437 [00:03<00:06, 43.91it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.15it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.30it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.31it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.44it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.36it/s] 41%|████      | 177/437 [00:03<00:05, 44.26it/s] 42%|████▏     | 182/437 [00:04<00:05, 43.91it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.89it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.00it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.20it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.28it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.31it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.41it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.30it/s] 51%|█████     | 222/437 [00:04<00:04, 44.20it/s] 52%|█████▏    | 227/437 [00:05<00:04, 43.96it/s] 53%|█████▎    | 232/437 [00:05<00:04, 43.92it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.14it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.24it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.27it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.37it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.38it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.21it/s] 61%|██████    | 267/437 [00:06<00:03, 44.07it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.00it/s] 63%|██████▎   | 277/437 [00:06<00:03, 43.91it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.15it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.29it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.27it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.36it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.31it/s] 70%|███████   | 307/437 [00:06<00:02, 44.15it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.09it/s] 73%|███████▎  | 317/437 [00:07<00:02, 43.96it/s] 74%|███████▎  | 322/437 [00:07<00:02, 43.98it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.13it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.27it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.31it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.35it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.30it/s] 81%|████████  | 352/437 [00:07<00:01, 44.17it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.07it/s] 83%|████████▎ | 362/437 [00:08<00:01, 43.93it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.09it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.19it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.22it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.36it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.33it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.24it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.00it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.03it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.04it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.18it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.20it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.21it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.30it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.41it/s]100%|██████████| 437/437 [00:09<00:00, 44.30it/s]100%|██████████| 437/437 [00:09<00:00, 44.32it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:41:02,627 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:41:02,627 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:41:02,627 >>   eval_loss               =      1.104
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:41:02,627 >>   eval_runtime            = 0:00:09.87
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:41:02,627 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:41:02,627 >>   eval_samples_per_second =    353.573
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:41:02,627 >>   eval_steps_per_second   =     44.235
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:41:02,627 >>   perplexity              =     3.0161
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:41:08,200 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:41:08,204 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:41:08,204 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:41:08,204 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:41:08,204 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:41:08,945 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:41:08,946 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:41:09,202 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:41:10,236 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:41:10,236 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:41:12,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:41:12,832 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:41:12,832 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:41:12,833 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:41:12,833 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:41:13,582 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:41:13,583 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:41:14,257 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:41:14,427 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:41:14,427 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-123
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-615
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/checkpoint-369
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.75it/s]Extractor Predicting: 4it [00:02,  1.82it/s]Extractor Predicting: 5it [00:02,  1.77it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 7it [00:03,  1.76it/s]Extractor Predicting: 8it [00:04,  1.76it/s]Extractor Predicting: 9it [00:05,  1.77it/s]Extractor Predicting: 10it [00:05,  1.70it/s]Extractor Predicting: 11it [00:06,  1.71it/s]Extractor Predicting: 12it [00:06,  1.76it/s]Extractor Predicting: 13it [00:07,  1.79it/s]Extractor Predicting: 14it [00:07,  1.80it/s]Extractor Predicting: 15it [00:08,  1.78it/s]Extractor Predicting: 16it [00:09,  1.80it/s]Extractor Predicting: 17it [00:09,  1.81it/s]Extractor Predicting: 18it [00:10,  1.80it/s]Extractor Predicting: 19it [00:10,  1.80it/s]Extractor Predicting: 20it [00:11,  1.75it/s]Extractor Predicting: 21it [00:11,  1.72it/s]Extractor Predicting: 22it [00:12,  1.69it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.75it/s]Extractor Predicting: 26it [00:14,  1.84it/s]Extractor Predicting: 27it [00:15,  1.83it/s]Extractor Predicting: 28it [00:15,  1.86it/s]Extractor Predicting: 29it [00:16,  1.82it/s]Extractor Predicting: 30it [00:16,  1.78it/s]Extractor Predicting: 31it [00:17,  1.77it/s]Extractor Predicting: 32it [00:18,  1.74it/s]Extractor Predicting: 33it [00:18,  1.69it/s]Extractor Predicting: 34it [00:19,  1.67it/s]Extractor Predicting: 35it [00:19,  1.69it/s]Extractor Predicting: 36it [00:20,  1.68it/s]Extractor Predicting: 37it [00:21,  1.67it/s]Extractor Predicting: 38it [00:21,  1.66it/s]Extractor Predicting: 39it [00:22,  1.69it/s]Extractor Predicting: 40it [00:22,  1.69it/s]Extractor Predicting: 41it [00:23,  1.68it/s]Extractor Predicting: 42it [00:24,  1.70it/s]Extractor Predicting: 43it [00:24,  1.69it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:25,  1.74it/s]Extractor Predicting: 46it [00:26,  1.70it/s]Extractor Predicting: 47it [00:27,  1.68it/s]Extractor Predicting: 48it [00:27,  1.68it/s]Extractor Predicting: 49it [00:28,  1.67it/s]Extractor Predicting: 50it [00:28,  1.68it/s]Extractor Predicting: 51it [00:29,  1.68it/s]Extractor Predicting: 52it [00:30,  1.66it/s]Extractor Predicting: 53it [00:30,  1.71it/s]Extractor Predicting: 54it [00:31,  1.68it/s]Extractor Predicting: 55it [00:31,  1.65it/s]Extractor Predicting: 56it [00:32,  1.66it/s]Extractor Predicting: 57it [00:33,  1.63it/s]Extractor Predicting: 58it [00:33,  1.63it/s]Extractor Predicting: 59it [00:34,  1.61it/s]Extractor Predicting: 60it [00:34,  1.61it/s]Extractor Predicting: 61it [00:35,  1.64it/s]Extractor Predicting: 62it [00:36,  1.64it/s]Extractor Predicting: 63it [00:36,  1.68it/s]Extractor Predicting: 64it [00:37,  1.71it/s]Extractor Predicting: 65it [00:37,  1.69it/s]Extractor Predicting: 66it [00:38,  1.72it/s]Extractor Predicting: 67it [00:39,  1.70it/s]Extractor Predicting: 68it [00:39,  1.69it/s]Extractor Predicting: 69it [00:40,  1.68it/s]Extractor Predicting: 70it [00:41,  1.53it/s]Extractor Predicting: 71it [00:41,  1.57it/s]Extractor Predicting: 72it [00:42,  1.61it/s]Extractor Predicting: 73it [00:42,  1.63it/s]Extractor Predicting: 74it [00:43,  1.66it/s]Extractor Predicting: 75it [00:43,  1.69it/s]Extractor Predicting: 76it [00:44,  1.66it/s]Extractor Predicting: 77it [00:45,  1.66it/s]Extractor Predicting: 78it [00:45,  1.65it/s]Extractor Predicting: 79it [00:46,  1.65it/s]Extractor Predicting: 80it [00:47,  1.63it/s]Extractor Predicting: 81it [00:47,  1.69it/s]Extractor Predicting: 82it [00:48,  1.68it/s]Extractor Predicting: 83it [00:48,  1.65it/s]Extractor Predicting: 84it [00:49,  1.66it/s]Extractor Predicting: 85it [00:49,  1.69it/s]Extractor Predicting: 86it [00:50,  1.76it/s]Extractor Predicting: 87it [00:50,  1.81it/s]Extractor Predicting: 88it [00:51,  1.81it/s]Extractor Predicting: 89it [00:52,  1.80it/s]Extractor Predicting: 90it [00:52,  1.83it/s]Extractor Predicting: 91it [00:53,  1.80it/s]Extractor Predicting: 92it [00:53,  1.75it/s]Extractor Predicting: 93it [00:54,  1.79it/s]Extractor Predicting: 94it [00:54,  1.83it/s]Extractor Predicting: 95it [00:55,  1.82it/s]Extractor Predicting: 96it [00:55,  1.84it/s]Extractor Predicting: 97it [00:56,  1.80it/s]Extractor Predicting: 98it [00:57,  1.76it/s]Extractor Predicting: 99it [00:57,  1.74it/s]Extractor Predicting: 100it [00:58,  1.76it/s]Extractor Predicting: 101it [00:58,  1.81it/s]Extractor Predicting: 102it [00:59,  1.79it/s]Extractor Predicting: 103it [00:59,  1.79it/s]Extractor Predicting: 104it [01:00,  1.78it/s]Extractor Predicting: 105it [01:01,  1.82it/s]Extractor Predicting: 106it [01:01,  1.79it/s]Extractor Predicting: 107it [01:02,  1.81it/s]Extractor Predicting: 108it [01:02,  1.81it/s]Extractor Predicting: 109it [01:03,  1.85it/s]Extractor Predicting: 110it [01:03,  1.84it/s]Extractor Predicting: 111it [01:04,  1.82it/s]Extractor Predicting: 112it [01:04,  1.79it/s]Extractor Predicting: 113it [01:05,  1.79it/s]Extractor Predicting: 114it [01:06,  1.72it/s]Extractor Predicting: 115it [01:06,  1.74it/s]Extractor Predicting: 116it [01:07,  1.73it/s]Extractor Predicting: 117it [01:07,  1.71it/s]Extractor Predicting: 118it [01:08,  1.78it/s]Extractor Predicting: 119it [01:08,  1.76it/s]Extractor Predicting: 120it [01:09,  1.79it/s]Extractor Predicting: 121it [01:10,  1.76it/s]Extractor Predicting: 122it [01:10,  1.74it/s]Extractor Predicting: 123it [01:11,  1.74it/s]Extractor Predicting: 124it [01:11,  1.69it/s]Extractor Predicting: 125it [01:12,  1.69it/s]Extractor Predicting: 126it [01:13,  1.70it/s]Extractor Predicting: 127it [01:13,  1.69it/s]Extractor Predicting: 128it [01:14,  1.68it/s]Extractor Predicting: 129it [01:14,  1.63it/s]Extractor Predicting: 130it [01:15,  1.65it/s]Extractor Predicting: 131it [01:16,  1.67it/s]Extractor Predicting: 132it [01:16,  1.71it/s]Extractor Predicting: 133it [01:17,  1.71it/s]Extractor Predicting: 134it [01:17,  1.71it/s]Extractor Predicting: 135it [01:18,  1.71it/s]Extractor Predicting: 136it [01:18,  1.70it/s]Extractor Predicting: 137it [01:19,  1.70it/s]Extractor Predicting: 138it [01:20,  1.69it/s]Extractor Predicting: 139it [01:20,  1.67it/s]Extractor Predicting: 140it [01:21,  1.73it/s]Extractor Predicting: 141it [01:21,  1.70it/s]Extractor Predicting: 142it [01:22,  1.78it/s]Extractor Predicting: 142it [01:22,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:45,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:45,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:45,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:45,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:45,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:42:45,640 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:42:45,641 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:42:46,201 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:42:47,228 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:42:47,228 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:50,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:50,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:50,234 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:50,234 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:50,234 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:42:50,873 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:42:50,874 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:42:51,434 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:42:51,591 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:42:51,592 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.79it/s]Extractor Predicting: 5it [00:02,  1.76it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 7it [00:04,  1.74it/s]Extractor Predicting: 8it [00:04,  1.78it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.81it/s]Extractor Predicting: 11it [00:06,  1.79it/s]Extractor Predicting: 12it [00:06,  1.81it/s]Extractor Predicting: 13it [00:07,  1.77it/s]Extractor Predicting: 14it [00:07,  1.79it/s]Extractor Predicting: 15it [00:08,  1.77it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:09,  1.73it/s]Extractor Predicting: 18it [00:10,  1.73it/s]Extractor Predicting: 19it [00:10,  1.75it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:11,  1.75it/s]Extractor Predicting: 22it [00:12,  1.76it/s]Extractor Predicting: 23it [00:13,  1.72it/s]Extractor Predicting: 24it [00:13,  1.70it/s]Extractor Predicting: 25it [00:14,  1.71it/s]Extractor Predicting: 26it [00:14,  1.72it/s]Extractor Predicting: 27it [00:15,  1.70it/s]Extractor Predicting: 28it [00:16,  1.74it/s]Extractor Predicting: 29it [00:16,  1.69it/s]Extractor Predicting: 30it [00:17,  1.65it/s]Extractor Predicting: 31it [00:17,  1.63it/s]Extractor Predicting: 32it [00:18,  1.63it/s]Extractor Predicting: 33it [00:19,  1.65it/s]Extractor Predicting: 34it [00:19,  1.66it/s]Extractor Predicting: 35it [00:20,  1.67it/s]Extractor Predicting: 36it [00:20,  1.71it/s]Extractor Predicting: 37it [00:21,  1.73it/s]Extractor Predicting: 38it [00:22,  1.75it/s]Extractor Predicting: 39it [00:22,  1.73it/s]Extractor Predicting: 40it [00:23,  1.73it/s]Extractor Predicting: 41it [00:23,  1.73it/s]Extractor Predicting: 42it [00:24,  1.71it/s]Extractor Predicting: 43it [00:24,  1.71it/s]Extractor Predicting: 44it [00:25,  1.70it/s]Extractor Predicting: 45it [00:26,  1.70it/s]Extractor Predicting: 46it [00:26,  1.71it/s]Extractor Predicting: 47it [00:27,  1.72it/s]Extractor Predicting: 48it [00:27,  1.72it/s]Extractor Predicting: 49it [00:28,  1.71it/s]Extractor Predicting: 50it [00:29,  1.69it/s]Extractor Predicting: 51it [00:29,  1.71it/s]Extractor Predicting: 52it [00:30,  1.72it/s]Extractor Predicting: 53it [00:30,  1.72it/s]Extractor Predicting: 54it [00:31,  1.72it/s]Extractor Predicting: 55it [00:31,  1.68it/s]Extractor Predicting: 56it [00:32,  1.70it/s]Extractor Predicting: 57it [00:33,  1.74it/s]Extractor Predicting: 58it [00:33,  1.76it/s]Extractor Predicting: 59it [00:34,  1.76it/s]Extractor Predicting: 60it [00:34,  1.75it/s]Extractor Predicting: 61it [00:35,  1.75it/s]Extractor Predicting: 62it [00:35,  1.75it/s]Extractor Predicting: 63it [00:36,  1.75it/s]Extractor Predicting: 64it [00:37,  1.70it/s]Extractor Predicting: 65it [00:37,  1.71it/s]Extractor Predicting: 66it [00:38,  1.70it/s]Extractor Predicting: 67it [00:38,  1.72it/s]Extractor Predicting: 68it [00:39,  1.78it/s]Extractor Predicting: 69it [00:39,  1.76it/s]Extractor Predicting: 70it [00:40,  1.59it/s]Extractor Predicting: 71it [00:41,  1.62it/s]Extractor Predicting: 72it [00:41,  1.66it/s]Extractor Predicting: 73it [00:42,  1.63it/s]Extractor Predicting: 74it [00:43,  1.65it/s]Extractor Predicting: 75it [00:43,  1.68it/s]Extractor Predicting: 76it [00:44,  1.69it/s]Extractor Predicting: 77it [00:44,  1.66it/s]Extractor Predicting: 78it [00:45,  1.68it/s]Extractor Predicting: 79it [00:46,  1.68it/s]Extractor Predicting: 80it [00:46,  1.69it/s]Extractor Predicting: 81it [00:47,  1.70it/s]Extractor Predicting: 82it [00:47,  1.66it/s]Extractor Predicting: 83it [00:48,  1.67it/s]Extractor Predicting: 84it [00:49,  1.68it/s]Extractor Predicting: 85it [00:49,  1.71it/s]Extractor Predicting: 86it [00:50,  1.67it/s]Extractor Predicting: 87it [00:50,  1.69it/s]Extractor Predicting: 88it [00:51,  1.64it/s]Extractor Predicting: 89it [00:52,  1.66it/s]Extractor Predicting: 90it [00:52,  1.69it/s]Extractor Predicting: 91it [00:53,  1.67it/s]Extractor Predicting: 92it [00:53,  1.66it/s]Extractor Predicting: 93it [00:54,  1.63it/s]Extractor Predicting: 94it [00:55,  1.62it/s]Extractor Predicting: 95it [00:55,  1.63it/s]Extractor Predicting: 96it [00:56,  1.63it/s]Extractor Predicting: 97it [00:56,  1.65it/s]Extractor Predicting: 98it [00:57,  1.64it/s]Extractor Predicting: 99it [00:58,  1.66it/s]Extractor Predicting: 100it [00:58,  1.66it/s]Extractor Predicting: 101it [00:59,  1.65it/s]Extractor Predicting: 102it [00:59,  1.65it/s]Extractor Predicting: 103it [01:00,  1.67it/s]Extractor Predicting: 104it [01:01,  1.65it/s]Extractor Predicting: 105it [01:01,  1.65it/s]Extractor Predicting: 106it [01:02,  1.66it/s]Extractor Predicting: 107it [01:03,  1.62it/s]Extractor Predicting: 108it [01:03,  1.62it/s]Extractor Predicting: 109it [01:04,  1.64it/s]Extractor Predicting: 110it [01:04,  1.61it/s]Extractor Predicting: 111it [01:05,  1.61it/s]Extractor Predicting: 112it [01:06,  1.66it/s]Extractor Predicting: 113it [01:06,  1.70it/s]Extractor Predicting: 114it [01:07,  1.70it/s]Extractor Predicting: 115it [01:07,  1.70it/s]Extractor Predicting: 116it [01:08,  1.70it/s]Extractor Predicting: 117it [01:08,  1.71it/s]Extractor Predicting: 118it [01:09,  1.72it/s]Extractor Predicting: 119it [01:10,  1.72it/s]Extractor Predicting: 120it [01:10,  1.72it/s]Extractor Predicting: 121it [01:11,  1.72it/s]Extractor Predicting: 122it [01:11,  1.78it/s]Extractor Predicting: 123it [01:12,  1.77it/s]Extractor Predicting: 124it [01:12,  1.76it/s]Extractor Predicting: 125it [01:13,  1.76it/s]Extractor Predicting: 126it [01:14,  1.73it/s]Extractor Predicting: 127it [01:14,  1.74it/s]Extractor Predicting: 128it [01:15,  1.76it/s]Extractor Predicting: 129it [01:15,  1.72it/s]Extractor Predicting: 130it [01:16,  1.73it/s]Extractor Predicting: 131it [01:17,  1.71it/s]Extractor Predicting: 132it [01:17,  1.73it/s]Extractor Predicting: 133it [01:18,  1.71it/s]Extractor Predicting: 134it [01:18,  1.72it/s]Extractor Predicting: 135it [01:19,  1.76it/s]Extractor Predicting: 136it [01:19,  1.75it/s]Extractor Predicting: 137it [01:20,  1.74it/s]Extractor Predicting: 138it [01:21,  1.74it/s]Extractor Predicting: 139it [01:21,  1.79it/s]Extractor Predicting: 140it [01:22,  1.75it/s]Extractor Predicting: 141it [01:22,  1.74it/s]Extractor Predicting: 142it [01:23,  1.79it/s]Extractor Predicting: 143it [01:23,  1.77it/s]Extractor Predicting: 144it [01:24,  1.73it/s]Extractor Predicting: 145it [01:25,  1.75it/s]Extractor Predicting: 146it [01:25,  1.74it/s]Extractor Predicting: 147it [01:26,  1.71it/s]Extractor Predicting: 148it [01:26,  1.70it/s]Extractor Predicting: 149it [01:27,  1.69it/s]Extractor Predicting: 150it [01:27,  1.72it/s]Extractor Predicting: 151it [01:28,  1.75it/s]Extractor Predicting: 152it [01:29,  1.77it/s]Extractor Predicting: 153it [01:29,  1.75it/s]Extractor Predicting: 154it [01:30,  1.72it/s]Extractor Predicting: 155it [01:31,  1.53it/s]Extractor Predicting: 156it [01:31,  1.57it/s]Extractor Predicting: 157it [01:32,  1.58it/s]Extractor Predicting: 158it [01:32,  1.61it/s]Extractor Predicting: 159it [01:33,  1.62it/s]Extractor Predicting: 160it [01:34,  1.64it/s]Extractor Predicting: 161it [01:34,  1.69it/s]Extractor Predicting: 162it [01:35,  1.71it/s]Extractor Predicting: 163it [01:35,  1.70it/s]Extractor Predicting: 164it [01:36,  1.72it/s]Extractor Predicting: 165it [01:36,  1.69it/s]Extractor Predicting: 166it [01:37,  1.71it/s]Extractor Predicting: 167it [01:38,  1.73it/s]Extractor Predicting: 168it [01:38,  1.71it/s]Extractor Predicting: 169it [01:39,  1.70it/s]Extractor Predicting: 170it [01:39,  1.68it/s]Extractor Predicting: 171it [01:40,  1.63it/s]Extractor Predicting: 172it [01:41,  1.65it/s]Extractor Predicting: 173it [01:41,  1.65it/s]Extractor Predicting: 174it [01:42,  1.60it/s]Extractor Predicting: 175it [01:43,  1.56it/s]Extractor Predicting: 176it [01:43,  1.59it/s]Extractor Predicting: 177it [01:44,  1.60it/s]Extractor Predicting: 178it [01:44,  1.65it/s]Extractor Predicting: 179it [01:45,  1.64it/s]Extractor Predicting: 180it [01:46,  1.70it/s]Extractor Predicting: 181it [01:46,  1.68it/s]Extractor Predicting: 182it [01:47,  1.71it/s]Extractor Predicting: 183it [01:47,  1.71it/s]Extractor Predicting: 184it [01:48,  1.74it/s]Extractor Predicting: 185it [01:48,  1.77it/s]Extractor Predicting: 186it [01:49,  1.77it/s]Extractor Predicting: 187it [01:50,  1.78it/s]Extractor Predicting: 188it [01:50,  1.74it/s]Extractor Predicting: 189it [01:51,  1.75it/s]Extractor Predicting: 190it [01:51,  1.73it/s]Extractor Predicting: 191it [01:52,  1.67it/s]Extractor Predicting: 192it [01:52,  1.70it/s]Extractor Predicting: 193it [01:53,  1.74it/s]Extractor Predicting: 194it [01:54,  1.73it/s]Extractor Predicting: 195it [01:54,  1.72it/s]Extractor Predicting: 196it [01:55,  1.74it/s]Extractor Predicting: 197it [01:55,  1.76it/s]Extractor Predicting: 198it [01:56,  1.72it/s]Extractor Predicting: 199it [01:56,  1.74it/s]Extractor Predicting: 200it [01:57,  1.73it/s]Extractor Predicting: 201it [01:58,  1.74it/s]Extractor Predicting: 202it [01:58,  1.78it/s]Extractor Predicting: 203it [01:59,  1.78it/s]Extractor Predicting: 204it [01:59,  1.77it/s]Extractor Predicting: 205it [02:00,  1.71it/s]Extractor Predicting: 206it [02:01,  1.71it/s]Extractor Predicting: 207it [02:01,  1.74it/s]Extractor Predicting: 208it [02:02,  1.76it/s]Extractor Predicting: 209it [02:02,  1.69it/s]Extractor Predicting: 210it [02:03,  1.67it/s]Extractor Predicting: 211it [02:03,  1.68it/s]Extractor Predicting: 212it [02:04,  1.71it/s]Extractor Predicting: 213it [02:05,  1.73it/s]Extractor Predicting: 214it [02:05,  1.67it/s]Extractor Predicting: 215it [02:06,  1.68it/s]Extractor Predicting: 216it [02:06,  1.70it/s]Extractor Predicting: 217it [02:07,  1.72it/s]Extractor Predicting: 218it [02:08,  1.65it/s]Extractor Predicting: 219it [02:08,  1.66it/s]Extractor Predicting: 220it [02:09,  1.68it/s]Extractor Predicting: 221it [02:09,  1.64it/s]Extractor Predicting: 222it [02:10,  1.65it/s]Extractor Predicting: 223it [02:11,  1.65it/s]Extractor Predicting: 224it [02:11,  1.70it/s]Extractor Predicting: 225it [02:12,  1.69it/s]Extractor Predicting: 226it [02:12,  1.74it/s]Extractor Predicting: 227it [02:13,  1.77it/s]Extractor Predicting: 228it [02:13,  1.74it/s]Extractor Predicting: 229it [02:14,  1.75it/s]Extractor Predicting: 230it [02:15,  1.71it/s]Extractor Predicting: 231it [02:15,  1.70it/s]Extractor Predicting: 232it [02:16,  1.70it/s]Extractor Predicting: 233it [02:17,  1.56it/s]Extractor Predicting: 234it [02:17,  1.59it/s]Extractor Predicting: 235it [02:18,  1.63it/s]Extractor Predicting: 236it [02:18,  1.62it/s]Extractor Predicting: 237it [02:19,  1.64it/s]Extractor Predicting: 238it [02:20,  1.67it/s]Extractor Predicting: 239it [02:20,  1.69it/s]Extractor Predicting: 240it [02:21,  1.68it/s]Extractor Predicting: 241it [02:21,  1.68it/s]Extractor Predicting: 242it [02:22,  1.66it/s]Extractor Predicting: 243it [02:23,  1.63it/s]Extractor Predicting: 244it [02:23,  1.64it/s]Extractor Predicting: 245it [02:24,  1.71it/s]Extractor Predicting: 246it [02:24,  1.69it/s]Extractor Predicting: 247it [02:25,  1.72it/s]Extractor Predicting: 248it [02:25,  1.72it/s]Extractor Predicting: 249it [02:26,  1.71it/s]Extractor Predicting: 250it [02:27,  1.70it/s]Extractor Predicting: 251it [02:27,  1.66it/s]Extractor Predicting: 252it [02:28,  1.67it/s]Extractor Predicting: 253it [02:28,  1.68it/s]Extractor Predicting: 254it [02:29,  1.71it/s]Extractor Predicting: 255it [02:30,  1.70it/s]Extractor Predicting: 256it [02:30,  1.68it/s]Extractor Predicting: 257it [02:31,  1.71it/s]Extractor Predicting: 258it [02:31,  1.70it/s]Extractor Predicting: 259it [02:32,  1.65it/s]Extractor Predicting: 260it [02:33,  1.67it/s]Extractor Predicting: 261it [02:33,  1.70it/s]Extractor Predicting: 262it [02:34,  1.68it/s]Extractor Predicting: 263it [02:34,  1.66it/s]Extractor Predicting: 264it [02:35,  1.66it/s]Extractor Predicting: 265it [02:36,  1.65it/s]Extractor Predicting: 266it [02:36,  1.62it/s]Extractor Predicting: 267it [02:37,  1.60it/s]Extractor Predicting: 268it [02:38,  1.62it/s]Extractor Predicting: 269it [02:38,  1.62it/s]Extractor Predicting: 270it [02:39,  1.60it/s]Extractor Predicting: 271it [02:39,  1.62it/s]Extractor Predicting: 272it [02:40,  1.63it/s]Extractor Predicting: 273it [02:41,  1.61it/s]Extractor Predicting: 274it [02:41,  1.62it/s]Extractor Predicting: 275it [02:42,  1.67it/s]Extractor Predicting: 276it [02:42,  1.66it/s]Extractor Predicting: 277it [02:43,  1.65it/s]Extractor Predicting: 278it [02:44,  1.64it/s]Extractor Predicting: 279it [02:44,  1.65it/s]Extractor Predicting: 280it [02:45,  1.64it/s]Extractor Predicting: 281it [02:45,  1.63it/s]Extractor Predicting: 282it [02:46,  1.65it/s]Extractor Predicting: 283it [02:47,  1.59it/s]Extractor Predicting: 284it [02:47,  1.58it/s]Extractor Predicting: 285it [02:48,  1.58it/s]Extractor Predicting: 286it [02:49,  1.57it/s]Extractor Predicting: 287it [02:49,  1.58it/s]Extractor Predicting: 288it [02:49,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:45:48,830 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:45:48,832 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:45:48,832 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:45:48,832 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:45:48,832 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:45:49,570 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:45:49,572 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:45:49,833 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:45:50,876 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:45:50,876 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:45:52,971 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:45:52,973 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:45:52,973 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:45:52,973 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:45:52,973 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:45:53,305 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:45:53,305 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:45:53,556 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:45:53,721 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:45:53,721 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:01,  1.95it/s]Extractor Predicting: 3it [00:01,  1.75it/s]
[INFO|configuration_utils.py:515] 2023-08-29 07:45:55,814 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:45:55,815 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:45:55,818 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:45:55,819 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 07:45:55,821 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:45:58,922 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 07:45:58,922 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 07:45:58,934 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:45:58,935 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:45:58,941 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:45:58,944 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:45:58,944 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:45:58,944 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:45:58,944 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:45:58,944 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:45:58,944 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 07:45:59,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:59,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:00,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:01,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:01,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:02,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:03,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:03,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:04,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:05,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:05,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:06,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:07,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:07,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:08,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:09,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:09,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:10,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:11,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:11,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:12,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:13,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:13,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:14,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:38, 15.64s/it][WARNING|generation_utils.py:914] 2023-08-29 07:46:14,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:15,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:16,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:16,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:17,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:18,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:18,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:19,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:20,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:20,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:21,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:22,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:22,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:23,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:24,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:25,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:25,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:26,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:27,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:27,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:28,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:28,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:30<03:15, 15.06s/it][WARNING|generation_utils.py:914] 2023-08-29 07:46:29,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:30,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:30,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:31,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:32,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:32,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:33,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:33,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:34,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:35,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:35,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:36,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:37,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:37,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:38,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:39,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:39,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:40,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:41,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:41,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:42,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:42,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:43,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:44,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:44,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:45,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:46,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:46,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:47,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:48<03:20, 16.68s/it][WARNING|generation_utils.py:914] 2023-08-29 07:46:48,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:48,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:49,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:50,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:50,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:51,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:51,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:52,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:53,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:53,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:54,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:55,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:55,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:56,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:57,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:57,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:58,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:59,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:00,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:00,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:01,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:02,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:03<02:54, 15.90s/it][WARNING|generation_utils.py:914] 2023-08-29 07:47:02,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:03,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:04,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:04,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:05,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:05,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:06,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:07,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:07,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:08,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:08,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:09,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:09,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:10,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:11,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:11,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:12,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:13,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:13,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:14,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:14,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:15,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:15,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:16,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:17,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:17,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:19<02:38, 15.82s/it][WARNING|generation_utils.py:914] 2023-08-29 07:47:18,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:19,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:19,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:20,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:21,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:21,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:22,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:23,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:23,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:24,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:25,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:25,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:26,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:27,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:27,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:28,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:29,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:29,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:30,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:31,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:31,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:32,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:32,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:33,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:35<02:22, 15.83s/it][WARNING|generation_utils.py:914] 2023-08-29 07:47:34,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:35,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:35,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:36,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:36,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:37,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:38,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:38,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:39,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:40,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:40,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:41,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:42,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:43,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:44,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:44,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:45,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:45,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:46,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:47,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:48,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:48,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:49,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:51<02:06, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-29 07:47:50,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:51,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:51,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:52,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:52,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:53,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:54,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:54,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:55,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:56,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:57,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:57,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:58,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:58,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:59,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:00,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:01,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:01,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:02,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:02,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:03,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:04,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:05,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:05,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:06,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:07,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:08<01:55, 16.48s/it][WARNING|generation_utils.py:914] 2023-08-29 07:48:08,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:08,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:09,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:09,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:10,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:10,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:11,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:12,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:12,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:13,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:13,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:14,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:15,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:15,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:16,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:16,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:17,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:18,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:18,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:19,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:19,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:20,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:20,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:21,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:22<01:34, 15.69s/it][WARNING|generation_utils.py:914] 2023-08-29 07:48:21,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:22,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:23,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:24,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:24,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:25,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:26,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:26,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:27,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:28,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:28,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:29,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:30,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:30,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:31,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:31,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:32,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:33,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:33,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:34,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:35,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:35,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:36,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:37<01:17, 15.50s/it][WARNING|generation_utils.py:914] 2023-08-29 07:48:37,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:37,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:38,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:39,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:39,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:40,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:40,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:41,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:42,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:43,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:43,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:44,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:44,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:45,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:46,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:46,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:47,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:48,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:49,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:49,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:50,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:50,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:52<01:00, 15.19s/it][WARNING|generation_utils.py:914] 2023-08-29 07:48:51,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:52,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:52,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:53,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:54,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:54,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:55,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:55,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:56,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:57,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:57,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:58,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:59,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:59,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:00,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:01,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:01,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:02,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:02,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:03,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:03,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:04,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:05,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:05,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:06,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:07<00:45, 15.25s/it][WARNING|generation_utils.py:914] 2023-08-29 07:49:06,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:07,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:08,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:09,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:09,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:10,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:10,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:11,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:12,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:12,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:13,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:13,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:14,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:15,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:15,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:16,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:17,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:17,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:18,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:19,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:19,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:20,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:21,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:22<00:30, 15.12s/it][WARNING|generation_utils.py:914] 2023-08-29 07:49:21,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:22,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:23,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:23,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:24,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:25,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:25,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:26,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:26,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:27,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:28,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:28,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:29,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:30,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:30,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:31,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:31,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:32,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:33,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:33,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:34,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:35,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:35,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:37<00:14, 14.99s/it][WARNING|generation_utils.py:914] 2023-08-29 07:49:36,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:37,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:37,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:38,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:38,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:39,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:39,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:40,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:41,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:41,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:42,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:42,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:43,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:44,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:44,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:45,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:45,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:46,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:47,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:47,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:48,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:48,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:49,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:50,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:50,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:51,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:52,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:52,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:53,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:53,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:54,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:54,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:55,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:56,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:57<00:00, 16.55s/it]Generating: 100%|██████████| 15/15 [03:57<00:00, 15.83s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:02,647 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:02,652 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:02,652 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:02,652 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:02,652 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:50:02,952 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:50:02,952 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:50:03,206 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:50:04,271 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:50:04,272 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:06,760 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:06,765 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:06,765 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:06,765 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:06,765 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:50:07,499 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:50:07,500 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:50:07,752 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:50:07,923 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:50:07,923 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.859375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 100, 'raw': 160}
{'target': 600, 'success': 124, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 180, 'raw': 288}
{'target': 600, 'success': 200, 'raw': 320}
{'target': 600, 'success': 218, 'raw': 352}
{'target': 600, 'success': 240, 'raw': 384}
{'target': 600, 'success': 258, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 307, 'raw': 480}
{'target': 600, 'success': 328, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 374, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 418, 'raw': 640}
{'target': 600, 'success': 440, 'raw': 672}
{'target': 600, 'success': 460, 'raw': 704}
{'target': 600, 'success': 487, 'raw': 736}
{'target': 600, 'success': 506, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 569, 'raw': 864}
{'target': 600, 'success': 595, 'raw': 896}
{'target': 600, 'success': 617, 'raw': 928}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.6648706896551724, 'errors': {'', '(\'2012 MTV Video Music Video Awards\', \'nominated for\', \'\', \'He was in a songwriting competition on " The Simpsons " at the 2012 MTV Video Music Video Awards , winning in one of his four categories .\')', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 402, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 579, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.7271634615384616, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : architect .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 496, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 551, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 594, 'raw': 800}
{'target': 600, 'success': 616, 'raw': 832}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7403846153846154, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a new development project called " Bifurcio " , which was developed by the Swiss developers , EMC , for Bifurcio . Head Entity : Bifurcio , Tail Entity : Ericsson .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.80078125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8152173913043478, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.77375, 'errors': {'', '(\'Pristiniki\', \'member of political party\', \'\', \'In 2005 , she became the first female politician to serve in the parliament of Bulgaria , as first and leader of " Pristiniki " in the new parliament .\')', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : operator . Context : Later in the year , the company built a high - speed commuter railway crossing the Bordeaux via Bordeaux Station to Marseille , France . Head Entity : Marseille , Tail Entity : Ralf Rummel .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 76, 'raw': 128}
{'target': 600, 'success': 92, 'raw': 160}
{'target': 600, 'success': 111, 'raw': 192}
{'target': 600, 'success': 130, 'raw': 224}
{'target': 600, 'success': 144, 'raw': 256}
{'target': 600, 'success': 158, 'raw': 288}
{'target': 600, 'success': 176, 'raw': 320}
{'target': 600, 'success': 191, 'raw': 352}
{'target': 600, 'success': 208, 'raw': 384}
{'target': 600, 'success': 226, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 267, 'raw': 480}
{'target': 600, 'success': 284, 'raw': 512}
{'target': 600, 'success': 298, 'raw': 544}
{'target': 600, 'success': 315, 'raw': 576}
{'target': 600, 'success': 332, 'raw': 608}
{'target': 600, 'success': 349, 'raw': 640}
{'target': 600, 'success': 372, 'raw': 672}
{'target': 600, 'success': 388, 'raw': 704}
{'target': 600, 'success': 406, 'raw': 736}
{'target': 600, 'success': 424, 'raw': 768}
{'target': 600, 'success': 439, 'raw': 800}
{'target': 600, 'success': 454, 'raw': 832}
{'target': 600, 'success': 473, 'raw': 864}
{'target': 600, 'success': 490, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 530, 'raw': 960}
{'target': 600, 'success': 551, 'raw': 992}
{'target': 600, 'success': 567, 'raw': 1024}
{'target': 600, 'success': 586, 'raw': 1056}
{'target': 600, 'success': 608, 'raw': 1088}
{'prompt': 'Relation : position held .', 'success_rate': 0.5588235294117647, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 14871
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14971, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.40it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:02,  1.40it/s]Extractor Estimating: 4it [00:02,  1.43it/s]Extractor Estimating: 5it [00:03,  1.53it/s]Extractor Estimating: 6it [00:04,  1.49it/s]Extractor Estimating: 7it [00:04,  1.49it/s]Extractor Estimating: 8it [00:05,  1.53it/s]Extractor Estimating: 9it [00:06,  1.50it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:07,  1.46it/s]Extractor Estimating: 12it [00:08,  1.50it/s]Extractor Estimating: 13it [00:08,  1.46it/s]Extractor Estimating: 14it [00:09,  1.46it/s]Extractor Estimating: 15it [00:10,  1.47it/s]Extractor Estimating: 16it [00:10,  1.45it/s]Extractor Estimating: 17it [00:11,  1.50it/s]Extractor Estimating: 18it [00:12,  1.46it/s]Extractor Estimating: 19it [00:12,  1.53it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:14,  1.55it/s]Extractor Estimating: 22it [00:14,  1.60it/s]Extractor Estimating: 23it [00:15,  1.57it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:16,  1.61it/s]Extractor Estimating: 26it [00:17,  1.56it/s]Extractor Estimating: 27it [00:17,  1.55it/s]Extractor Estimating: 28it [00:18,  1.59it/s]Extractor Estimating: 29it [00:19,  1.62it/s]Extractor Estimating: 30it [00:19,  1.61it/s]Extractor Estimating: 31it [00:20,  1.64it/s]Extractor Estimating: 32it [00:20,  1.65it/s]Extractor Estimating: 33it [00:21,  1.60it/s]Extractor Estimating: 34it [00:22,  1.51it/s]Extractor Estimating: 35it [00:22,  1.57it/s]Extractor Estimating: 36it [00:23,  1.60it/s]Extractor Estimating: 37it [00:24,  1.49it/s]Extractor Estimating: 38it [00:24,  1.52it/s]Extractor Estimating: 39it [00:25,  1.53it/s]Extractor Estimating: 40it [00:26,  1.52it/s]Extractor Estimating: 41it [00:26,  1.50it/s]Extractor Estimating: 42it [00:27,  1.48it/s]Extractor Estimating: 43it [00:28,  1.50it/s]Extractor Estimating: 44it [00:28,  1.45it/s]Extractor Estimating: 45it [00:29,  1.47it/s]Extractor Estimating: 46it [00:30,  1.52it/s]Extractor Estimating: 47it [00:30,  1.53it/s]Extractor Estimating: 48it [00:31,  1.50it/s]Extractor Estimating: 49it [00:32,  1.56it/s]Extractor Estimating: 50it [00:32,  1.53it/s]Extractor Estimating: 51it [00:33,  1.51it/s]Extractor Estimating: 52it [00:34,  1.50it/s]Extractor Estimating: 53it [00:34,  1.50it/s]Extractor Estimating: 54it [00:35,  1.47it/s]Extractor Estimating: 55it [00:36,  1.44it/s]Extractor Estimating: 56it [00:36,  1.46it/s]Extractor Estimating: 57it [00:37,  1.44it/s]Extractor Estimating: 58it [00:38,  1.45it/s]Extractor Estimating: 59it [00:39,  1.42it/s]Extractor Estimating: 60it [00:39,  1.46it/s]Extractor Estimating: 61it [00:40,  1.40it/s]Extractor Estimating: 62it [00:41,  1.44it/s]Extractor Estimating: 63it [00:41,  1.42it/s]Extractor Estimating: 64it [00:42,  1.47it/s]Extractor Estimating: 65it [00:43,  1.43it/s]Extractor Estimating: 66it [00:43,  1.44it/s]Extractor Estimating: 67it [00:44,  1.44it/s]Extractor Estimating: 68it [00:45,  1.40it/s]Extractor Estimating: 69it [00:45,  1.45it/s]Extractor Estimating: 70it [00:46,  1.46it/s]Extractor Estimating: 71it [00:47,  1.45it/s]Extractor Estimating: 72it [00:48,  1.46it/s]Extractor Estimating: 73it [00:48,  1.45it/s]Extractor Estimating: 74it [00:49,  1.50it/s]Extractor Estimating: 75it [00:50,  1.48it/s]Extractor Estimating: 76it [00:50,  1.54it/s]Extractor Estimating: 77it [00:51,  1.57it/s]Extractor Estimating: 78it [00:51,  1.63it/s]Extractor Estimating: 79it [00:52,  1.64it/s]Extractor Estimating: 80it [00:52,  1.69it/s]Extractor Estimating: 81it [00:53,  1.70it/s]Extractor Estimating: 82it [00:54,  1.67it/s]Extractor Estimating: 83it [00:54,  1.68it/s]Extractor Estimating: 84it [00:55,  1.67it/s]Extractor Estimating: 85it [00:55,  1.64it/s]Extractor Estimating: 86it [00:56,  1.66it/s]Extractor Estimating: 87it [00:57,  1.65it/s]Extractor Estimating: 88it [00:57,  1.70it/s]Extractor Estimating: 89it [00:58,  1.71it/s]Extractor Estimating: 90it [00:58,  1.73it/s]Extractor Estimating: 91it [00:59,  1.68it/s]Extractor Estimating: 92it [01:00,  1.70it/s]Extractor Estimating: 93it [01:00,  1.70it/s]Extractor Estimating: 94it [01:01,  1.58it/s]Extractor Estimating: 95it [01:02,  1.52it/s]Extractor Estimating: 96it [01:02,  1.55it/s]Extractor Estimating: 97it [01:03,  1.58it/s]Extractor Estimating: 98it [01:03,  1.62it/s]Extractor Estimating: 99it [01:04,  1.64it/s]Extractor Estimating: 100it [01:05,  1.68it/s]Extractor Estimating: 101it [01:05,  1.70it/s]Extractor Estimating: 102it [01:06,  1.69it/s]Extractor Estimating: 103it [01:06,  1.70it/s]Extractor Estimating: 104it [01:07,  1.74it/s]Extractor Estimating: 105it [01:07,  1.72it/s]Extractor Estimating: 106it [01:08,  1.69it/s]Extractor Estimating: 107it [01:09,  1.74it/s]Extractor Estimating: 108it [01:09,  1.81it/s]Extractor Estimating: 109it [01:10,  1.78it/s]Extractor Estimating: 110it [01:10,  1.80it/s]Extractor Estimating: 111it [01:11,  1.80it/s]Extractor Estimating: 112it [01:11,  1.80it/s]Extractor Estimating: 113it [01:12,  1.74it/s]Extractor Estimating: 114it [01:13,  1.76it/s]Extractor Estimating: 115it [01:13,  1.75it/s]Extractor Estimating: 116it [01:14,  1.76it/s]Extractor Estimating: 117it [01:14,  1.71it/s]Extractor Estimating: 118it [01:15,  1.71it/s]Extractor Estimating: 119it [01:16,  1.65it/s]Extractor Estimating: 120it [01:16,  1.72it/s]Extractor Estimating: 121it [01:17,  1.68it/s]Extractor Estimating: 122it [01:17,  1.60it/s]Extractor Estimating: 123it [01:18,  1.66it/s]Extractor Estimating: 124it [01:18,  1.70it/s]Extractor Estimating: 125it [01:19,  1.69it/s]Extractor Estimating: 126it [01:20,  1.67it/s]Extractor Estimating: 127it [01:20,  1.63it/s]Extractor Estimating: 128it [01:21,  1.67it/s]Extractor Estimating: 129it [01:21,  1.69it/s]Extractor Estimating: 130it [01:22,  1.70it/s]Extractor Estimating: 131it [01:23,  1.68it/s]Extractor Estimating: 132it [01:23,  1.68it/s]Extractor Estimating: 133it [01:24,  1.64it/s]Extractor Estimating: 134it [01:24,  1.69it/s]Extractor Estimating: 135it [01:25,  1.64it/s]Extractor Estimating: 136it [01:26,  1.61it/s]Extractor Estimating: 137it [01:26,  1.61it/s]Extractor Estimating: 138it [01:27,  1.63it/s]Extractor Estimating: 139it [01:28,  1.62it/s]Extractor Estimating: 140it [01:28,  1.63it/s]Extractor Estimating: 141it [01:29,  1.60it/s]Extractor Estimating: 142it [01:29,  1.62it/s]Extractor Estimating: 143it [01:30,  1.67it/s]Extractor Estimating: 144it [01:31,  1.67it/s]Extractor Estimating: 145it [01:31,  1.68it/s]Extractor Estimating: 146it [01:32,  1.70it/s]Extractor Estimating: 147it [01:32,  1.65it/s]Extractor Estimating: 148it [01:33,  1.59it/s]Extractor Estimating: 149it [01:34,  1.59it/s]Extractor Estimating: 150it [01:34,  1.59it/s]Extractor Estimating: 151it [01:35,  1.55it/s]Extractor Estimating: 152it [01:36,  1.53it/s]Extractor Estimating: 153it [01:36,  1.54it/s]Extractor Estimating: 154it [01:37,  1.58it/s]Extractor Estimating: 155it [01:38,  1.61it/s]Extractor Estimating: 156it [01:38,  1.64it/s]Extractor Estimating: 157it [01:39,  1.68it/s]Extractor Estimating: 158it [01:39,  1.61it/s]Extractor Estimating: 159it [01:40,  1.69it/s]Extractor Estimating: 160it [01:41,  1.65it/s]Extractor Estimating: 161it [01:41,  1.63it/s]Extractor Estimating: 162it [01:42,  1.62it/s]Extractor Estimating: 163it [01:43,  1.55it/s]Extractor Estimating: 164it [01:43,  1.57it/s]Extractor Estimating: 165it [01:44,  1.51it/s]Extractor Estimating: 166it [01:45,  1.51it/s]Extractor Estimating: 167it [01:45,  1.56it/s]Extractor Estimating: 168it [01:46,  1.51it/s]Extractor Estimating: 169it [01:46,  1.52it/s]Extractor Estimating: 170it [01:47,  1.59it/s]Extractor Estimating: 171it [01:48,  1.41it/s]Extractor Estimating: 172it [01:49,  1.45it/s]Extractor Estimating: 173it [01:49,  1.47it/s]Extractor Estimating: 174it [01:50,  1.43it/s]Extractor Estimating: 175it [01:51,  1.49it/s]Extractor Estimating: 176it [01:51,  1.44it/s]Extractor Estimating: 177it [01:52,  1.51it/s]Extractor Estimating: 178it [01:53,  1.51it/s]Extractor Estimating: 179it [01:53,  1.58it/s]Extractor Estimating: 180it [01:54,  1.64it/s]Extractor Estimating: 181it [01:54,  1.60it/s]Extractor Estimating: 182it [01:55,  1.60it/s]Extractor Estimating: 183it [01:56,  1.54it/s]Extractor Estimating: 184it [01:56,  1.47it/s]Extractor Estimating: 185it [01:57,  1.50it/s]Extractor Estimating: 186it [01:58,  1.56it/s]Extractor Estimating: 187it [01:58,  1.59it/s]Extractor Estimating: 188it [01:59,  1.56it/s]Extractor Estimating: 189it [01:59,  1.62it/s]Extractor Estimating: 190it [02:00,  1.63it/s]Extractor Estimating: 191it [02:01,  1.58it/s]Extractor Estimating: 192it [02:01,  1.60it/s]Extractor Estimating: 193it [02:02,  1.57it/s]Extractor Estimating: 194it [02:03,  1.54it/s]Extractor Estimating: 195it [02:03,  1.56it/s]Extractor Estimating: 196it [02:04,  1.58it/s]Extractor Estimating: 197it [02:05,  1.58it/s]Extractor Estimating: 198it [02:05,  1.54it/s]Extractor Estimating: 199it [02:06,  1.51it/s]Extractor Estimating: 200it [02:07,  1.58it/s]Extractor Estimating: 201it [02:07,  1.57it/s]Extractor Estimating: 202it [02:08,  1.55it/s]Extractor Estimating: 203it [02:08,  1.57it/s]Extractor Estimating: 204it [02:09,  1.49it/s]Extractor Estimating: 205it [02:10,  1.50it/s]Extractor Estimating: 206it [02:10,  1.55it/s]Extractor Estimating: 207it [02:11,  1.47it/s]Extractor Estimating: 208it [02:12,  1.48it/s]Extractor Estimating: 209it [02:13,  1.47it/s]Extractor Estimating: 210it [02:13,  1.50it/s]Extractor Estimating: 211it [02:14,  1.56it/s]Extractor Estimating: 212it [02:14,  1.53it/s]Extractor Estimating: 213it [02:15,  1.54it/s]Extractor Estimating: 214it [02:16,  1.50it/s]Extractor Estimating: 215it [02:16,  1.56it/s]Extractor Estimating: 216it [02:17,  1.58it/s]Extractor Estimating: 217it [02:18,  1.54it/s]Extractor Estimating: 218it [02:18,  1.57it/s]Extractor Estimating: 219it [02:19,  1.55it/s]Extractor Estimating: 220it [02:20,  1.56it/s]Extractor Estimating: 221it [02:20,  1.53it/s]Extractor Estimating: 222it [02:21,  1.58it/s]Extractor Estimating: 223it [02:21,  1.63it/s]Extractor Estimating: 224it [02:22,  1.63it/s]Extractor Estimating: 225it [02:23,  1.60it/s]Extractor Estimating: 226it [02:23,  1.55it/s]Extractor Estimating: 227it [02:24,  1.54it/s]Extractor Estimating: 228it [02:25,  1.53it/s]Extractor Estimating: 229it [02:25,  1.50it/s]Extractor Estimating: 230it [02:26,  1.48it/s]Extractor Estimating: 231it [02:27,  1.46it/s]Extractor Estimating: 232it [02:27,  1.48it/s]Extractor Estimating: 233it [02:28,  1.48it/s]Extractor Estimating: 234it [02:29,  1.52it/s]Extractor Estimating: 235it [02:29,  1.49it/s]Extractor Estimating: 236it [02:30,  1.47it/s]Extractor Estimating: 237it [02:31,  1.50it/s]Extractor Estimating: 238it [02:32,  1.45it/s]Extractor Estimating: 239it [02:32,  1.42it/s]Extractor Estimating: 240it [02:33,  1.46it/s]Extractor Estimating: 241it [02:34,  1.47it/s]Extractor Estimating: 242it [02:34,  1.49it/s]Extractor Estimating: 243it [02:35,  1.50it/s]Extractor Estimating: 244it [02:36,  1.46it/s]Extractor Estimating: 245it [02:36,  1.41it/s]Extractor Estimating: 246it [02:37,  1.46it/s]Extractor Estimating: 247it [02:38,  1.47it/s]Extractor Estimating: 248it [02:38,  1.42it/s]Extractor Estimating: 249it [02:39,  1.44it/s]Extractor Estimating: 250it [02:40,  1.35it/s]Extractor Estimating: 251it [02:41,  1.46it/s]Extractor Estimating: 252it [02:41,  1.56it/s]Extractor Estimating: 253it [02:42,  1.51it/s]Extractor Estimating: 254it [02:42,  1.58it/s]Extractor Estimating: 255it [02:43,  1.65it/s]Extractor Estimating: 256it [02:44,  1.66it/s]Extractor Estimating: 257it [02:44,  1.69it/s]Extractor Estimating: 258it [02:45,  1.67it/s]Extractor Estimating: 259it [02:45,  1.64it/s]Extractor Estimating: 260it [02:46,  1.57it/s]Extractor Estimating: 261it [02:47,  1.59it/s]Extractor Estimating: 262it [02:47,  1.64it/s]Extractor Estimating: 263it [02:48,  1.64it/s]Extractor Estimating: 264it [02:48,  1.69it/s]Extractor Estimating: 265it [02:49,  1.68it/s]Extractor Estimating: 266it [02:49,  1.75it/s]Extractor Estimating: 267it [02:50,  1.74it/s]Extractor Estimating: 268it [02:51,  1.72it/s]Extractor Estimating: 269it [02:51,  1.72it/s]Extractor Estimating: 270it [02:52,  1.69it/s]Extractor Estimating: 271it [02:52,  1.69it/s]Extractor Estimating: 272it [02:53,  1.70it/s]Extractor Estimating: 273it [02:54,  1.74it/s]Extractor Estimating: 274it [02:54,  1.71it/s]Extractor Estimating: 275it [02:55,  1.73it/s]Extractor Estimating: 276it [02:55,  1.68it/s]Extractor Estimating: 277it [02:56,  1.61it/s]Extractor Estimating: 278it [02:57,  1.59it/s]Extractor Estimating: 279it [02:57,  1.57it/s]Extractor Estimating: 280it [02:58,  1.59it/s]Extractor Estimating: 281it [02:59,  1.60it/s]Extractor Estimating: 282it [02:59,  1.54it/s]Extractor Estimating: 283it [03:00,  1.56it/s]Extractor Estimating: 284it [03:01,  1.51it/s]Extractor Estimating: 285it [03:01,  1.52it/s]Extractor Estimating: 286it [03:02,  1.57it/s]Extractor Estimating: 287it [03:02,  1.58it/s]Extractor Estimating: 288it [03:03,  1.64it/s]Extractor Estimating: 289it [03:04,  1.61it/s]Extractor Estimating: 290it [03:04,  1.64it/s]Extractor Estimating: 291it [03:05,  1.65it/s]Extractor Estimating: 292it [03:05,  1.67it/s]Extractor Estimating: 293it [03:06,  1.63it/s]Extractor Estimating: 294it [03:07,  1.60it/s]Extractor Estimating: 295it [03:07,  1.59it/s]Extractor Estimating: 296it [03:08,  1.58it/s]Extractor Estimating: 297it [03:09,  1.59it/s]Extractor Estimating: 298it [03:09,  1.59it/s]Extractor Estimating: 299it [03:10,  1.60it/s]Extractor Estimating: 300it [03:11,  1.59it/s]Extractor Estimating: 301it [03:11,  1.59it/s]Extractor Estimating: 302it [03:12,  1.55it/s]Extractor Estimating: 303it [03:13,  1.54it/s]Extractor Estimating: 304it [03:13,  1.55it/s]Extractor Estimating: 305it [03:14,  1.55it/s]Extractor Estimating: 306it [03:14,  1.60it/s]Extractor Estimating: 307it [03:15,  1.60it/s]Extractor Estimating: 308it [03:16,  1.62it/s]Extractor Estimating: 309it [03:16,  1.61it/s]Extractor Estimating: 310it [03:17,  1.59it/s]Extractor Estimating: 311it [03:18,  1.53it/s]Extractor Estimating: 312it [03:18,  1.47it/s]Extractor Estimating: 313it [03:19,  1.45it/s]Extractor Estimating: 314it [03:20,  1.50it/s]Extractor Estimating: 315it [03:20,  1.50it/s]Extractor Estimating: 316it [03:21,  1.52it/s]Extractor Estimating: 317it [03:22,  1.51it/s]Extractor Estimating: 318it [03:22,  1.52it/s]Extractor Estimating: 319it [03:23,  1.58it/s]Extractor Estimating: 320it [03:23,  1.62it/s]Extractor Estimating: 321it [03:24,  1.60it/s]Extractor Estimating: 322it [03:25,  1.56it/s]Extractor Estimating: 323it [03:25,  1.56it/s]Extractor Estimating: 324it [03:26,  1.55it/s]Extractor Estimating: 325it [03:27,  1.52it/s]Extractor Estimating: 326it [03:27,  1.55it/s]Extractor Estimating: 327it [03:28,  1.50it/s]Extractor Estimating: 328it [03:29,  1.49it/s]Extractor Estimating: 329it [03:29,  1.47it/s]Extractor Estimating: 330it [03:30,  1.47it/s]Extractor Estimating: 331it [03:31,  1.35it/s]Extractor Estimating: 332it [03:32,  1.44it/s]Extractor Estimating: 333it [03:32,  1.46it/s]Extractor Estimating: 334it [03:33,  1.52it/s]Extractor Estimating: 335it [03:34,  1.49it/s]Extractor Estimating: 336it [03:34,  1.51it/s]Extractor Estimating: 337it [03:35,  1.44it/s]Extractor Estimating: 338it [03:36,  1.47it/s]Extractor Estimating: 339it [03:36,  1.49it/s]Extractor Estimating: 340it [03:37,  1.54it/s]Extractor Estimating: 341it [03:37,  1.55it/s]Extractor Estimating: 342it [03:38,  1.56it/s]Extractor Estimating: 343it [03:39,  1.56it/s]Extractor Estimating: 344it [03:39,  1.53it/s]Extractor Estimating: 345it [03:40,  1.51it/s]Extractor Estimating: 346it [03:41,  1.42it/s]Extractor Estimating: 347it [03:42,  1.44it/s]Extractor Estimating: 348it [03:42,  1.51it/s]Extractor Estimating: 349it [03:43,  1.52it/s]Extractor Estimating: 350it [03:44,  1.50it/s]Extractor Estimating: 351it [03:44,  1.53it/s]Extractor Estimating: 352it [03:45,  1.51it/s]Extractor Estimating: 353it [03:45,  1.57it/s]Extractor Estimating: 354it [03:46,  1.58it/s]Extractor Estimating: 355it [03:47,  1.58it/s]Extractor Estimating: 356it [03:47,  1.55it/s]Extractor Estimating: 357it [03:48,  1.58it/s]Extractor Estimating: 358it [03:49,  1.59it/s]Extractor Estimating: 359it [03:49,  1.61it/s]Extractor Estimating: 360it [03:50,  1.58it/s]Extractor Estimating: 361it [03:50,  1.59it/s]Extractor Estimating: 362it [03:51,  1.63it/s]Extractor Estimating: 363it [03:52,  1.59it/s]Extractor Estimating: 364it [03:52,  1.59it/s]Extractor Estimating: 365it [03:53,  1.59it/s]Extractor Estimating: 366it [03:54,  1.56it/s]Extractor Estimating: 367it [03:54,  1.55it/s]Extractor Estimating: 368it [03:55,  1.59it/s]Extractor Estimating: 369it [03:55,  1.62it/s]Extractor Estimating: 370it [03:56,  1.60it/s]Extractor Estimating: 371it [03:57,  1.59it/s]Extractor Estimating: 372it [03:57,  1.66it/s]Extractor Estimating: 373it [03:58,  1.69it/s]Extractor Estimating: 374it [03:58,  1.70it/s]Extractor Estimating: 375it [03:59,  1.59it/s]Extractor Estimating: 375it [03:59,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:54:19,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:54:19,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:54:19,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:54:19,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:54:19,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:54:19,802 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:54:19,803 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:54:20,365 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:54:21,418 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:54:21,418 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:54:24,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:54:24,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:54:24,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:54:24,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:54:24,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:54:24,858 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:54:24,860 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:54:25,425 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:54:25,598 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:54:25,598 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 10:10:14,245 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 10:10:14,273 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7699 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 20524
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20624, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20624, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.087, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.061, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.099, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 79, avg_time 1.085, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 179, avg_time 1.070, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 279, avg_time 2.100, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 58, avg_time 1.095, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 158, avg_time 1.072, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 258, avg_time 1.084, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 37, avg_time 1.076, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 137, avg_time 2.119, loss:nan
g_step 1200, step 237, avg_time 1.072, loss:nan
g_step 1300, step 16, avg_time 1.066, loss:nan
g_step 1400, step 116, avg_time 1.074, loss:nan
g_step 1500, step 216, avg_time 1.077, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 316, avg_time 2.112, loss:nan
g_step 1700, step 95, avg_time 1.070, loss:nan
g_step 1800, step 195, avg_time 1.070, loss:nan
g_step 1900, step 295, avg_time 1.087, loss:nan
g_step 2000, step 74, avg_time 1.079, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 174, avg_time 2.101, loss:nan
g_step 2200, step 274, avg_time 1.069, loss:nan
g_step 2300, step 53, avg_time 1.081, loss:nan
g_step 2400, step 153, avg_time 1.084, loss:nan
g_step 2500, step 253, avg_time 1.075, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 32, avg_time 2.112, loss:nan
g_step 2700, step 132, avg_time 1.079, loss:nan
g_step 2800, step 232, avg_time 1.074, loss:nan
g_step 2900, step 11, avg_time 1.064, loss:nan
g_step 3000, step 111, avg_time 1.081, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 211, avg_time 2.093, loss:nan
g_step 3200, step 311, avg_time 1.078, loss:nan
g_step 3300, step 90, avg_time 1.076, loss:nan
g_step 3400, step 190, avg_time 1.069, loss:nan
g_step 3500, step 290, avg_time 1.088, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 69, avg_time 2.110, loss:nan
g_step 3700, step 169, avg_time 1.073, loss:nan
g_step 3800, step 269, avg_time 1.075, loss:nan
g_step 3900, step 48, avg_time 1.055, loss:nan
g_step 4000, step 148, avg_time 1.101, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 248, avg_time 2.116, loss:nan
g_step 4200, step 27, avg_time 1.062, loss:nan
g_step 4300, step 127, avg_time 1.088, loss:nan
g_step 4400, step 227, avg_time 1.076, loss:nan
g_step 4500, step 6, avg_time 1.071, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 106, avg_time 2.106, loss:nan
g_step 4700, step 206, avg_time 1.080, loss:nan
g_step 4800, step 306, avg_time 1.075, loss:nan
g_step 4900, step 85, avg_time 1.077, loss:nan
g_step 5000, step 185, avg_time 1.065, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 285, avg_time 2.101, loss:nan
g_step 5200, step 64, avg_time 1.091, loss:nan
g_step 5300, step 164, avg_time 1.081, loss:nan
g_step 5400, step 264, avg_time 1.062, loss:nan
g_step 5500, step 43, avg_time 1.071, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 143, avg_time 2.097, loss:nan
g_step 5700, step 243, avg_time 1.083, loss:nan
g_step 5800, step 22, avg_time 1.086, loss:nan
g_step 5900, step 122, avg_time 1.089, loss:nan
g_step 6000, step 222, avg_time 1.060, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 1, avg_time 2.088, loss:nan
g_step 6200, step 101, avg_time 1.077, loss:nan
g_step 6300, step 201, avg_time 1.092, loss:nan
g_step 6400, step 301, avg_time 1.053, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 10:10:14 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 10:10:14 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_10-10-14_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 10:10:15 - WARNING - datasets.builder -   Using custom data configuration default-1ccdf11b5bcd739c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-1ccdf11b5bcd739c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 10:10:15,477 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:10:15,478 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 10:10:15,478 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:10:15,479 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 10:10:15,487 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:10:15,490 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:10:15,490 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:10:15,490 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:10:15,490 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:10:15,490 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:10:15,490 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 10:10:15,629 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 10:10:18,676 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 10:10:18,682 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-1ccdf11b5bcd739c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.11ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.91ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.29ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.42ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.52ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.58ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.62ba/s]100%|██████████| 8/8 [00:01<00:00,  5.08ba/s]100%|██████████| 8/8 [00:01<00:00,  4.58ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.11ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.27ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.74ba/s]100%|██████████| 4/4 [00:00<00:00,  4.86ba/s]100%|██████████| 4/4 [00:00<00:00,  4.32ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.55ba/s] 38%|███▊      | 3/8 [00:00<00:00, 11.05ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 11.05ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 11.25ba/s]100%|██████████| 8/8 [00:00<00:00, 11.57ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.74ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.95ba/s]100%|██████████| 4/4 [00:00<00:00, 12.45ba/s]
[INFO|trainer.py:414] 2023-08-29 10:10:22,713 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 10:10:22,731 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 10:10:22,731 >>   Num examples = 7700
[INFO|trainer.py:1149] 2023-08-29 10:10:22,731 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 10:10:22,731 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 10:10:22,731 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 10:10:22,731 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 10:10:22,731 >>   Total optimization steps = 600
  0%|          | 0/600 [00:00<?, ?it/s]  0%|          | 1/600 [00:00<02:52,  3.48it/s]  0%|          | 2/600 [00:00<02:47,  3.56it/s]  0%|          | 3/600 [00:00<02:46,  3.58it/s]  1%|          | 4/600 [00:01<02:45,  3.59it/s]  1%|          | 5/600 [00:01<02:45,  3.60it/s]  1%|          | 6/600 [00:01<02:44,  3.60it/s]  1%|          | 7/600 [00:01<02:44,  3.61it/s]  1%|▏         | 8/600 [00:02<02:43,  3.61it/s]  2%|▏         | 9/600 [00:02<02:44,  3.59it/s]  2%|▏         | 10/600 [00:02<02:43,  3.60it/s]  2%|▏         | 11/600 [00:03<02:43,  3.61it/s]  2%|▏         | 12/600 [00:03<02:43,  3.61it/s]  2%|▏         | 13/600 [00:03<02:42,  3.61it/s]  2%|▏         | 14/600 [00:03<02:42,  3.61it/s]  2%|▎         | 15/600 [00:04<02:42,  3.61it/s]  3%|▎         | 16/600 [00:04<02:41,  3.61it/s]  3%|▎         | 17/600 [00:04<02:41,  3.62it/s]  3%|▎         | 18/600 [00:04<02:40,  3.62it/s]  3%|▎         | 19/600 [00:05<02:40,  3.61it/s]  3%|▎         | 20/600 [00:05<02:41,  3.59it/s]  4%|▎         | 21/600 [00:05<02:41,  3.59it/s]  4%|▎         | 22/600 [00:06<02:40,  3.60it/s]  4%|▍         | 23/600 [00:06<02:40,  3.60it/s]  4%|▍         | 24/600 [00:06<02:39,  3.61it/s]  4%|▍         | 25/600 [00:06<02:39,  3.61it/s]  4%|▍         | 26/600 [00:07<02:39,  3.61it/s]  4%|▍         | 27/600 [00:07<02:38,  3.61it/s]  5%|▍         | 28/600 [00:07<02:38,  3.61it/s]  5%|▍         | 29/600 [00:08<02:38,  3.61it/s]  5%|▌         | 30/600 [00:08<02:37,  3.61it/s]  5%|▌         | 31/600 [00:08<02:37,  3.61it/s]  5%|▌         | 32/600 [00:08<02:37,  3.61it/s]  6%|▌         | 33/600 [00:09<02:37,  3.61it/s]  6%|▌         | 34/600 [00:09<02:36,  3.61it/s]  6%|▌         | 35/600 [00:09<02:36,  3.61it/s]  6%|▌         | 36/600 [00:09<02:36,  3.61it/s]  6%|▌         | 37/600 [00:10<02:36,  3.59it/s]  6%|▋         | 38/600 [00:10<02:36,  3.60it/s]  6%|▋         | 39/600 [00:10<02:35,  3.61it/s]  7%|▋         | 40/600 [00:11<02:35,  3.61it/s]  7%|▋         | 41/600 [00:11<02:34,  3.61it/s]  7%|▋         | 42/600 [00:11<02:34,  3.60it/s]  7%|▋         | 43/600 [00:11<02:34,  3.60it/s]  7%|▋         | 44/600 [00:12<02:34,  3.61it/s]  8%|▊         | 45/600 [00:12<02:33,  3.61it/s]  8%|▊         | 46/600 [00:12<02:33,  3.61it/s]  8%|▊         | 47/600 [00:13<02:33,  3.61it/s]  8%|▊         | 48/600 [00:13<02:32,  3.61it/s]  8%|▊         | 49/600 [00:13<02:32,  3.61it/s]  8%|▊         | 50/600 [00:13<02:32,  3.60it/s]  8%|▊         | 51/600 [00:14<02:32,  3.60it/s]  9%|▊         | 52/600 [00:14<02:31,  3.61it/s]  9%|▉         | 53/600 [00:14<02:31,  3.61it/s]  9%|▉         | 54/600 [00:14<02:31,  3.61it/s]  9%|▉         | 55/600 [00:15<02:31,  3.59it/s]  9%|▉         | 56/600 [00:15<02:31,  3.60it/s] 10%|▉         | 57/600 [00:15<02:30,  3.60it/s] 10%|▉         | 58/600 [00:16<02:30,  3.60it/s] 10%|▉         | 59/600 [00:16<02:30,  3.60it/s] 10%|█         | 60/600 [00:16<02:29,  3.60it/s] 10%|█         | 61/600 [00:16<02:29,  3.60it/s] 10%|█         | 62/600 [00:17<02:29,  3.60it/s] 10%|█         | 63/600 [00:17<02:28,  3.60it/s] 11%|█         | 64/600 [00:17<02:28,  3.60it/s] 11%|█         | 65/600 [00:18<02:28,  3.61it/s] 11%|█         | 66/600 [00:18<02:28,  3.61it/s] 11%|█         | 67/600 [00:18<02:27,  3.60it/s] 11%|█▏        | 68/600 [00:18<02:27,  3.60it/s] 12%|█▏        | 69/600 [00:19<02:27,  3.60it/s] 12%|█▏        | 70/600 [00:19<02:27,  3.60it/s] 12%|█▏        | 71/600 [00:19<02:26,  3.60it/s] 12%|█▏        | 72/600 [00:19<02:26,  3.61it/s] 12%|█▏        | 73/600 [00:20<02:26,  3.61it/s] 12%|█▏        | 74/600 [00:20<02:26,  3.59it/s] 12%|█▎        | 75/600 [00:20<02:25,  3.60it/s] 13%|█▎        | 76/600 [00:21<02:25,  3.60it/s] 13%|█▎        | 77/600 [00:21<02:25,  3.60it/s] 13%|█▎        | 78/600 [00:21<02:24,  3.60it/s] 13%|█▎        | 79/600 [00:21<02:24,  3.60it/s] 13%|█▎        | 80/600 [00:22<02:24,  3.61it/s] 14%|█▎        | 81/600 [00:22<02:23,  3.60it/s] 14%|█▎        | 82/600 [00:22<02:23,  3.61it/s] 14%|█▍        | 83/600 [00:23<02:23,  3.60it/s] 14%|█▍        | 84/600 [00:23<02:23,  3.60it/s] 14%|█▍        | 85/600 [00:23<02:23,  3.59it/s] 14%|█▍        | 86/600 [00:23<02:22,  3.60it/s] 14%|█▍        | 87/600 [00:24<02:22,  3.59it/s] 15%|█▍        | 88/600 [00:24<02:22,  3.60it/s] 15%|█▍        | 89/600 [00:24<02:21,  3.60it/s] 15%|█▌        | 90/600 [00:24<02:21,  3.60it/s] 15%|█▌        | 91/600 [00:25<02:21,  3.60it/s] 15%|█▌        | 92/600 [00:25<02:21,  3.59it/s] 16%|█▌        | 93/600 [00:25<02:21,  3.60it/s] 16%|█▌        | 94/600 [00:26<02:20,  3.60it/s] 16%|█▌        | 95/600 [00:26<02:20,  3.60it/s] 16%|█▌        | 96/600 [00:26<02:20,  3.60it/s] 16%|█▌        | 97/600 [00:26<02:19,  3.60it/s] 16%|█▋        | 98/600 [00:27<02:19,  3.60it/s] 16%|█▋        | 99/600 [00:27<02:19,  3.60it/s] 17%|█▋        | 100/600 [00:27<02:18,  3.60it/s] 17%|█▋        | 101/600 [00:28<02:18,  3.60it/s] 17%|█▋        | 102/600 [00:28<02:18,  3.60it/s] 17%|█▋        | 103/600 [00:28<02:17,  3.60it/s] 17%|█▋        | 104/600 [00:28<02:17,  3.60it/s] 18%|█▊        | 105/600 [00:29<02:17,  3.60it/s] 18%|█▊        | 106/600 [00:29<02:17,  3.60it/s] 18%|█▊        | 107/600 [00:29<02:16,  3.60it/s] 18%|█▊        | 108/600 [00:29<02:16,  3.60it/s] 18%|█▊        | 109/600 [00:30<02:16,  3.60it/s] 18%|█▊        | 110/600 [00:30<02:16,  3.59it/s] 18%|█▊        | 111/600 [00:30<02:15,  3.60it/s] 19%|█▊        | 112/600 [00:31<02:15,  3.60it/s] 19%|█▉        | 113/600 [00:31<02:15,  3.60it/s] 19%|█▉        | 114/600 [00:31<02:15,  3.60it/s] 19%|█▉        | 115/600 [00:31<02:14,  3.60it/s] 19%|█▉        | 116/600 [00:32<02:14,  3.60it/s] 20%|█▉        | 117/600 [00:32<02:14,  3.60it/s] 20%|█▉        | 118/600 [00:32<02:13,  3.60it/s] 20%|█▉        | 119/600 [00:33<02:13,  3.60it/s] 20%|██        | 120/600 [00:33<02:13,  3.60it/s][INFO|trainer.py:2140] 2023-08-29 10:10:56,086 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:10:56,087 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 10:10:56,087 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.76it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.58it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.56it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.59it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.77it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.33it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.19it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.12it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.03it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.19it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.30it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.48it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.43it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.24it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.02it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.94it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.90it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.99it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.09it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.34it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.47it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.40it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.16it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.02it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.87it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.92it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.97it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.10it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.31it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.38it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.33it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.18it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.91it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.87it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.92it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.94it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.10it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.29it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.30it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.26it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.13it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.03it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.88it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.82it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.93it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.09it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.11it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.38it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.36it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.16it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.98it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.90it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.98it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.08it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.19it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.30it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.36it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.31it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.16it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.99it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.96it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.99it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.12it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.24it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.32it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.28it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.24it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.10it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.95it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.89it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.07it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.23it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.24it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.24it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.27it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.21it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.07it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.92it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.79it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.07it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.09it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.18it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.24it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.25it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.16it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.07it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.98it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.98it/s][A 20%|██        | 120/600 [00:43<02:13,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:11:06,050 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-29 10:11:06,074 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:11:08,516 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:11:08,541 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:11:08,551 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-120/special_tokens_map.json
 20%|██        | 121/600 [00:46<32:53,  4.12s/it] 20%|██        | 122/600 [00:46<23:39,  2.97s/it] 20%|██        | 123/600 [00:46<17:11,  2.16s/it] 21%|██        | 124/600 [00:47<12:41,  1.60s/it] 21%|██        | 125/600 [00:47<09:31,  1.20s/it] 21%|██        | 126/600 [00:47<07:19,  1.08it/s] 21%|██        | 127/600 [00:48<05:46,  1.36it/s] 21%|██▏       | 128/600 [00:48<04:41,  1.67it/s] 22%|██▏       | 129/600 [00:48<03:56,  1.99it/s] 22%|██▏       | 130/600 [00:48<03:24,  2.29it/s] 22%|██▏       | 131/600 [00:49<03:03,  2.56it/s] 22%|██▏       | 132/600 [00:49<02:47,  2.79it/s] 22%|██▏       | 133/600 [00:49<02:36,  2.98it/s] 22%|██▏       | 134/600 [00:50<02:28,  3.13it/s] 22%|██▎       | 135/600 [00:50<02:24,  3.23it/s] 23%|██▎       | 136/600 [00:50<02:20,  3.31it/s] 23%|██▎       | 137/600 [00:50<02:16,  3.38it/s] 23%|██▎       | 138/600 [00:51<02:14,  3.43it/s] 23%|██▎       | 139/600 [00:51<02:13,  3.46it/s] 23%|██▎       | 140/600 [00:51<02:15,  3.40it/s] 24%|██▎       | 141/600 [00:52<02:13,  3.44it/s] 24%|██▎       | 142/600 [00:52<02:12,  3.46it/s] 24%|██▍       | 143/600 [00:52<02:10,  3.50it/s] 24%|██▍       | 144/600 [00:52<02:09,  3.53it/s] 24%|██▍       | 145/600 [00:53<02:08,  3.55it/s] 24%|██▍       | 146/600 [00:53<02:07,  3.57it/s] 24%|██▍       | 147/600 [00:53<02:06,  3.58it/s] 25%|██▍       | 148/600 [00:54<02:06,  3.58it/s] 25%|██▍       | 149/600 [00:54<02:05,  3.59it/s] 25%|██▌       | 150/600 [00:54<02:05,  3.59it/s] 25%|██▌       | 151/600 [00:54<02:04,  3.60it/s] 25%|██▌       | 152/600 [00:55<02:04,  3.60it/s] 26%|██▌       | 153/600 [00:55<02:04,  3.60it/s] 26%|██▌       | 154/600 [00:55<02:03,  3.60it/s] 26%|██▌       | 155/600 [00:55<02:03,  3.60it/s] 26%|██▌       | 156/600 [00:56<02:03,  3.61it/s] 26%|██▌       | 157/600 [00:56<02:02,  3.61it/s] 26%|██▋       | 158/600 [00:56<02:02,  3.60it/s] 26%|██▋       | 159/600 [00:57<02:02,  3.60it/s] 27%|██▋       | 160/600 [00:57<02:01,  3.61it/s] 27%|██▋       | 161/600 [00:57<02:01,  3.61it/s] 27%|██▋       | 162/600 [00:57<02:01,  3.61it/s] 27%|██▋       | 163/600 [00:58<02:01,  3.61it/s] 27%|██▋       | 164/600 [00:58<02:02,  3.55it/s] 28%|██▊       | 165/600 [00:58<02:02,  3.56it/s] 28%|██▊       | 166/600 [00:59<02:01,  3.58it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 28%|██▊       | 167/600 [00:59<02:00,  3.59it/s] 28%|██▊       | 168/600 [00:59<02:00,  3.60it/s] 28%|██▊       | 169/600 [00:59<01:59,  3.60it/s] 28%|██▊       | 170/600 [01:00<01:59,  3.60it/s] 28%|██▊       | 171/600 [01:00<01:59,  3.60it/s] 29%|██▊       | 172/600 [01:00<01:58,  3.60it/s] 29%|██▉       | 173/600 [01:00<01:58,  3.60it/s] 29%|██▉       | 174/600 [01:01<01:58,  3.61it/s] 29%|██▉       | 175/600 [01:01<01:58,  3.59it/s] 29%|██▉       | 176/600 [01:01<01:57,  3.60it/s] 30%|██▉       | 177/600 [01:02<01:57,  3.60it/s] 30%|██▉       | 178/600 [01:02<01:57,  3.60it/s] 30%|██▉       | 179/600 [01:02<01:56,  3.61it/s] 30%|███       | 180/600 [01:02<01:56,  3.60it/s] 30%|███       | 181/600 [01:03<01:56,  3.60it/s] 30%|███       | 182/600 [01:03<01:55,  3.61it/s] 30%|███       | 183/600 [01:03<01:55,  3.61it/s] 31%|███       | 184/600 [01:04<01:55,  3.60it/s] 31%|███       | 185/600 [01:04<01:55,  3.60it/s] 31%|███       | 186/600 [01:04<01:55,  3.59it/s] 31%|███       | 187/600 [01:04<01:54,  3.59it/s] 31%|███▏      | 188/600 [01:05<01:54,  3.59it/s] 32%|███▏      | 189/600 [01:05<01:54,  3.60it/s] 32%|███▏      | 190/600 [01:05<01:53,  3.60it/s] 32%|███▏      | 191/600 [01:05<01:53,  3.60it/s] 32%|███▏      | 192/600 [01:06<01:53,  3.60it/s] 32%|███▏      | 193/600 [01:06<01:52,  3.61it/s] 32%|███▏      | 194/600 [01:06<01:52,  3.61it/s] 32%|███▎      | 195/600 [01:07<01:52,  3.60it/s] 33%|███▎      | 196/600 [01:07<01:52,  3.60it/s] 33%|███▎      | 197/600 [01:07<01:52,  3.58it/s] 33%|███▎      | 198/600 [01:07<01:52,  3.58it/s] 33%|███▎      | 199/600 [01:08<01:51,  3.58it/s] 33%|███▎      | 200/600 [01:08<01:51,  3.58it/s] 34%|███▎      | 201/600 [01:08<01:51,  3.59it/s] 34%|███▎      | 202/600 [01:09<01:50,  3.59it/s] 34%|███▍      | 203/600 [01:09<01:50,  3.59it/s] 34%|███▍      | 204/600 [01:09<01:50,  3.60it/s] 34%|███▍      | 205/600 [01:09<01:49,  3.60it/s] 34%|███▍      | 206/600 [01:10<01:49,  3.60it/s] 34%|███▍      | 207/600 [01:10<01:49,  3.60it/s] 35%|███▍      | 208/600 [01:10<01:49,  3.59it/s] 35%|███▍      | 209/600 [01:10<01:48,  3.60it/s] 35%|███▌      | 210/600 [01:11<01:48,  3.60it/s] 35%|███▌      | 211/600 [01:11<01:48,  3.60it/s] 35%|███▌      | 212/600 [01:11<01:47,  3.60it/s] 36%|███▌      | 213/600 [01:12<01:47,  3.60it/s] 36%|███▌      | 214/600 [01:12<01:47,  3.60it/s] 36%|███▌      | 215/600 [01:12<01:46,  3.60it/s] 36%|███▌      | 216/600 [01:12<01:46,  3.61it/s] 36%|███▌      | 217/600 [01:13<01:46,  3.61it/s] 36%|███▋      | 218/600 [01:13<01:46,  3.60it/s] 36%|███▋      | 219/600 [01:13<01:45,  3.60it/s] 37%|███▋      | 220/600 [01:14<01:45,  3.61it/s] 37%|███▋      | 221/600 [01:14<01:45,  3.60it/s] 37%|███▋      | 222/600 [01:14<01:44,  3.60it/s] 37%|███▋      | 223/600 [01:14<01:44,  3.61it/s] 37%|███▋      | 224/600 [01:15<01:44,  3.61it/s] 38%|███▊      | 225/600 [01:15<01:44,  3.60it/s] 38%|███▊      | 226/600 [01:15<01:43,  3.60it/s] 38%|███▊      | 227/600 [01:15<01:43,  3.61it/s] 38%|███▊      | 228/600 [01:16<01:43,  3.60it/s] 38%|███▊      | 229/600 [01:16<01:42,  3.60it/s] 38%|███▊      | 230/600 [01:16<01:43,  3.57it/s] 38%|███▊      | 231/600 [01:17<01:42,  3.58it/s] 39%|███▊      | 232/600 [01:17<01:42,  3.59it/s] 39%|███▉      | 233/600 [01:17<01:42,  3.60it/s] 39%|███▉      | 234/600 [01:17<01:41,  3.60it/s] 39%|███▉      | 235/600 [01:18<01:41,  3.60it/s] 39%|███▉      | 236/600 [01:18<01:41,  3.60it/s] 40%|███▉      | 237/600 [01:18<01:40,  3.60it/s] 40%|███▉      | 238/600 [01:19<01:40,  3.60it/s] 40%|███▉      | 239/600 [01:19<01:40,  3.60it/s] 40%|████      | 240/600 [01:19<01:40,  3.60it/s][INFO|trainer.py:2140] 2023-08-29 10:11:42,361 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:11:42,361 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 10:11:42,362 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9465, 'eval_samples_per_second': 351.18, 'eval_steps_per_second': 43.935, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.79it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.53it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.58it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.49it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.68it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.43it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.19it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.03it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.14it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.27it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.39it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.47it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.26it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.06it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.96it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.88it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.86it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.07it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.24it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.34it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.36it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.14it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.00it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.98it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.92it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.89it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.09it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.25it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.33it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.18it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.18it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.00it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.97it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.97it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.97it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.08it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.00it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.12it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.33it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.11it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.97it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.97it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.89it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.98it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.10it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.22it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.21it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.27it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.16it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.03it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.93it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.88it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.94it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.15it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.17it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.10it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.17it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.08it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.07it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.04it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.93it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.03it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.11it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.20it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.04it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.09it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.17it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.12it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.00it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.81it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.04it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.18it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.21it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.09it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.12it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.17it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.09it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.01it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.95it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.08it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.18it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.01it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.09it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.10it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.13it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.06it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.01it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.01it/s][A 40%|████      | 240/600 [01:29<01:40,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:11:52,377 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-29 10:11:52,405 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:11:54,756 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:11:54,776 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:11:54,789 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-240/special_tokens_map.json
 40%|████      | 241/600 [01:32<24:34,  4.11s/it] 40%|████      | 242/600 [01:32<17:39,  2.96s/it] 40%|████      | 243/600 [01:33<12:49,  2.16s/it] 41%|████      | 244/600 [01:33<09:27,  1.59s/it] 41%|████      | 245/600 [01:33<07:05,  1.20s/it] 41%|████      | 246/600 [01:34<05:27,  1.08it/s] 41%|████      | 247/600 [01:34<04:18,  1.37it/s] 41%|████▏     | 248/600 [01:34<03:29,  1.68it/s] 42%|████▏     | 249/600 [01:34<02:56,  1.99it/s] 42%|████▏     | 250/600 [01:35<02:32,  2.29it/s] 42%|████▏     | 251/600 [01:35<02:16,  2.56it/s] 42%|████▏     | 252/600 [01:35<02:04,  2.79it/s] 42%|████▏     | 253/600 [01:36<01:56,  2.98it/s] 42%|████▏     | 254/600 [01:36<01:50,  3.13it/s] 42%|████▎     | 255/600 [01:36<01:46,  3.25it/s] 43%|████▎     | 256/600 [01:36<01:43,  3.33it/s] 43%|████▎     | 257/600 [01:37<01:41,  3.39it/s] 43%|████▎     | 258/600 [01:37<01:39,  3.44it/s] 43%|████▎     | 259/600 [01:37<01:38,  3.47it/s] 43%|████▎     | 260/600 [01:37<01:37,  3.49it/s] 44%|████▎     | 261/600 [01:38<01:36,  3.51it/s] 44%|████▎     | 262/600 [01:38<01:36,  3.50it/s] 44%|████▍     | 263/600 [01:38<01:35,  3.51it/s] 44%|████▍     | 264/600 [01:39<01:35,  3.52it/s] 44%|████▍     | 265/600 [01:39<01:34,  3.53it/s] 44%|████▍     | 266/600 [01:39<01:34,  3.54it/s] 44%|████▍     | 267/600 [01:39<01:34,  3.54it/s] 45%|████▍     | 268/600 [01:40<01:33,  3.54it/s] 45%|████▍     | 269/600 [01:40<01:33,  3.54it/s] 45%|████▌     | 270/600 [01:40<01:33,  3.55it/s] 45%|████▌     | 271/600 [01:41<01:32,  3.55it/s] 45%|████▌     | 272/600 [01:41<01:32,  3.55it/s] 46%|████▌     | 273/600 [01:41<01:32,  3.53it/s] 46%|████▌     | 274/600 [01:41<01:32,  3.54it/s] 46%|████▌     | 275/600 [01:42<01:31,  3.54it/s] 46%|████▌     | 276/600 [01:42<01:31,  3.54it/s] 46%|████▌     | 277/600 [01:42<01:31,  3.54it/s] 46%|████▋     | 278/600 [01:43<01:30,  3.54it/s] 46%|████▋     | 279/600 [01:43<01:30,  3.54it/s] 47%|████▋     | 280/600 [01:43<01:30,  3.55it/s] 47%|████▋     | 281/600 [01:43<01:29,  3.55it/s] 47%|████▋     | 282/600 [01:44<01:29,  3.55it/s] 47%|████▋     | 283/600 [01:44<01:29,  3.55it/s] 47%|████▋     | 284/600 [01:44<01:29,  3.53it/s] 48%|████▊     | 285/600 [01:45<01:28,  3.54it/s] 48%|████▊     | 286/600 [01:45<01:28,  3.54it/s] 48%|████▊     | 287/600 [01:45<01:28,  3.54it/s] 48%|████▊     | 288/600 [01:45<01:27,  3.55it/s] 48%|████▊     | 289/600 [01:46<01:27,  3.55it/s] 48%|████▊     | 290/600 [01:46<01:27,  3.55it/s] 48%|████▊     | 291/600 [01:46<01:27,  3.55it/s] 49%|████▊     | 292/600 [01:47<01:26,  3.55it/s] 49%|████▉     | 293/600 [01:47<01:26,  3.55it/s] 49%|████▉     | 294/600 [01:47<01:26,  3.55it/s] 49%|████▉     | 295/600 [01:47<01:26,  3.54it/s] 49%|████▉     | 296/600 [01:48<01:25,  3.54it/s] 50%|████▉     | 297/600 [01:48<01:25,  3.55it/s] 50%|████▉     | 298/600 [01:48<01:25,  3.55it/s] 50%|████▉     | 299/600 [01:48<01:24,  3.55it/s] 50%|█████     | 300/600 [01:49<01:24,  3.55it/s] 50%|█████     | 301/600 [01:49<01:24,  3.55it/s] 50%|█████     | 302/600 [01:49<01:23,  3.55it/s] 50%|█████     | 303/600 [01:50<01:23,  3.55it/s] 51%|█████     | 304/600 [01:50<01:23,  3.53it/s] 51%|█████     | 305/600 [01:50<01:23,  3.53it/s] 51%|█████     | 306/600 [01:50<01:24,  3.46it/s] 51%|█████     | 307/600 [01:51<01:23,  3.49it/s] 51%|█████▏    | 308/600 [01:51<01:23,  3.51it/s] 52%|█████▏    | 309/600 [01:51<01:24,  3.43it/s] 52%|█████▏    | 310/600 [01:52<01:23,  3.45it/s] 52%|█████▏    | 311/600 [01:52<01:22,  3.48it/s] 52%|█████▏    | 312/600 [01:52<01:22,  3.50it/s] 52%|█████▏    | 313/600 [01:52<01:21,  3.52it/s] 52%|█████▏    | 314/600 [01:53<01:21,  3.53it/s] 52%|█████▎    | 315/600 [01:53<01:20,  3.54it/s] 53%|█████▎    | 316/600 [01:53<01:20,  3.54it/s] 53%|█████▎    | 317/600 [01:54<01:19,  3.55it/s] 53%|█████▎    | 318/600 [01:54<01:19,  3.56it/s] 53%|█████▎    | 319/600 [01:54<01:18,  3.58it/s] 53%|█████▎    | 320/600 [01:54<01:18,  3.59it/s] 54%|█████▎    | 321/600 [01:55<01:17,  3.60it/s] 54%|█████▎    | 322/600 [01:55<01:17,  3.59it/s] 54%|█████▍    | 323/600 [01:55<01:17,  3.59it/s] 54%|█████▍    | 324/600 [01:56<01:16,  3.60it/s] 54%|█████▍    | 325/600 [01:56<01:16,  3.60it/s] 54%|█████▍    | 326/600 [01:56<01:15,  3.61it/s] 55%|█████▍    | 327/600 [01:56<01:15,  3.61it/s] 55%|█████▍    | 328/600 [01:57<01:15,  3.59it/s] 55%|█████▍    | 329/600 [01:57<01:15,  3.59it/s] 55%|█████▌    | 330/600 [01:57<01:14,  3.60it/s] 55%|█████▌    | 331/600 [01:58<01:14,  3.60it/s] 55%|█████▌    | 332/600 [01:58<01:14,  3.60it/s] 56%|█████▌    | 333/600 [01:58<01:14,  3.60it/s] 56%|█████▌    | 334/600 [01:58<01:13,  3.60it/s] 56%|█████▌    | 335/600 [01:59<01:13,  3.61it/s] 56%|█████▌    | 336/600 [01:59<01:13,  3.60it/s] 56%|█████▌    | 337/600 [01:59<01:12,  3.61it/s] 56%|█████▋    | 338/600 [01:59<01:12,  3.61it/s] 56%|█████▋    | 339/600 [02:00<01:12,  3.61it/s] 57%|█████▋    | 340/600 [02:00<01:12,  3.60it/s] 57%|█████▋    | 341/600 [02:00<01:11,  3.61it/s] 57%|█████▋    | 342/600 [02:01<01:11,  3.61it/s] 57%|█████▋    | 343/600 [02:01<01:11,  3.61it/s] 57%|█████▋    | 344/600 [02:01<01:10,  3.61it/s] 57%|█████▊    | 345/600 [02:01<01:10,  3.61it/s] 58%|█████▊    | 346/600 [02:02<01:10,  3.60it/s] 58%|█████▊    | 347/600 [02:02<01:10,  3.60it/s] 58%|█████▊    | 348/600 [02:02<01:10,  3.59it/s] 58%|█████▊    | 349/600 [02:02<01:09,  3.60it/s] 58%|█████▊    | 350/600 [02:03<01:09,  3.60it/s] 58%|█████▊    | 351/600 [02:03<01:09,  3.60it/s] 59%|█████▊    | 352/600 [02:03<01:08,  3.60it/s] 59%|█████▉    | 353/600 [02:04<01:08,  3.60it/s] 59%|█████▉    | 354/600 [02:04<01:08,  3.60it/s] 59%|█████▉    | 355/600 [02:04<01:08,  3.60it/s] 59%|█████▉    | 356/600 [02:04<01:07,  3.60it/s] 60%|█████▉    | 357/600 [02:05<01:07,  3.60it/s] 60%|█████▉    | 358/600 [02:05<01:07,  3.61it/s] 60%|█████▉    | 359/600 [02:05<01:06,  3.61it/s] 60%|██████    | 360/600 [02:06<01:06,  3.61it/s][INFO|trainer.py:2140] 2023-08-29 10:12:28,817 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:12:28,818 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 10:12:28,818 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9537, 'eval_samples_per_second': 350.925, 'eval_steps_per_second': 43.903, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.02it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.55it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.49it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.42it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.82it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.41it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.22it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.09it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.09it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.35it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.28it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.34it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.15it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.02it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.86it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.83it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.88it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.06it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.12it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.32it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.34it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.18it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.02it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.89it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.66it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.88it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.13it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.29it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.33it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.34it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.21it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.15it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.96it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.86it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.93it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.13it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.20it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.27it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.22it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.09it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.06it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.84it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.84it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.96it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.27it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.27it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.36it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.22it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.10it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.95it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.78it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.78it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.99it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.22it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.28it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.34it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.28it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.14it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.98it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.91it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.89it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.09it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.28it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.35it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.19it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.18it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.09it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.93it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.88it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.93it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.12it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.30it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.29it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.18it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.11it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.96it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.97it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.96it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.98it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.12it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.27it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.24it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.19it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.18it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.06it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.96it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.89it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.89it/s][A 60%|██████    | 360/600 [02:16<01:06,  3.61it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:12:38,780 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-360
[INFO|configuration_utils.py:351] 2023-08-29 10:12:38,797 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-360/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:12:40,512 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-360/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:12:40,530 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:12:40,544 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-360/special_tokens_map.json
 60%|██████    | 361/600 [02:18<15:30,  3.89s/it] 60%|██████    | 362/600 [02:18<11:08,  2.81s/it] 60%|██████    | 363/600 [02:18<08:06,  2.05s/it] 61%|██████    | 364/600 [02:19<05:58,  1.52s/it] 61%|██████    | 365/600 [02:19<04:29,  1.15s/it] 61%|██████    | 366/600 [02:19<03:27,  1.13it/s] 61%|██████    | 367/600 [02:20<02:44,  1.42it/s] 61%|██████▏   | 368/600 [02:20<02:14,  1.73it/s] 62%|██████▏   | 369/600 [02:20<01:53,  2.04it/s] 62%|██████▏   | 370/600 [02:20<01:38,  2.34it/s] 62%|██████▏   | 371/600 [02:21<01:28,  2.60it/s] 62%|██████▏   | 372/600 [02:21<01:20,  2.82it/s] 62%|██████▏   | 373/600 [02:21<01:15,  3.01it/s] 62%|██████▏   | 374/600 [02:22<01:11,  3.15it/s] 62%|██████▎   | 375/600 [02:22<01:08,  3.26it/s] 63%|██████▎   | 376/600 [02:22<01:06,  3.35it/s] 63%|██████▎   | 377/600 [02:22<01:05,  3.41it/s] 63%|██████▎   | 378/600 [02:23<01:04,  3.46it/s] 63%|██████▎   | 379/600 [02:23<01:03,  3.50it/s] 63%|██████▎   | 380/600 [02:23<01:02,  3.54it/s] 64%|██████▎   | 381/600 [02:23<01:01,  3.56it/s] 64%|██████▎   | 382/600 [02:24<01:01,  3.54it/s] 64%|██████▍   | 383/600 [02:24<01:00,  3.56it/s] 64%|██████▍   | 384/600 [02:24<01:00,  3.58it/s] 64%|██████▍   | 385/600 [02:25<01:00,  3.58it/s] 64%|██████▍   | 386/600 [02:25<00:59,  3.59it/s] 64%|██████▍   | 387/600 [02:25<00:59,  3.59it/s] 65%|██████▍   | 388/600 [02:25<00:58,  3.60it/s] 65%|██████▍   | 389/600 [02:26<00:58,  3.60it/s] 65%|██████▌   | 390/600 [02:26<00:58,  3.60it/s] 65%|██████▌   | 391/600 [02:26<00:57,  3.60it/s] 65%|██████▌   | 392/600 [02:27<00:57,  3.61it/s] 66%|██████▌   | 393/600 [02:27<00:57,  3.59it/s] 66%|██████▌   | 394/600 [02:27<00:57,  3.60it/s] 66%|██████▌   | 395/600 [02:27<00:57,  3.60it/s] 66%|██████▌   | 396/600 [02:28<00:56,  3.60it/s] 66%|██████▌   | 397/600 [02:28<00:56,  3.60it/s] 66%|██████▋   | 398/600 [02:28<00:56,  3.61it/s] 66%|██████▋   | 399/600 [02:28<00:55,  3.61it/s] 67%|██████▋   | 400/600 [02:29<00:55,  3.61it/s] 67%|██████▋   | 401/600 [02:29<00:55,  3.60it/s] 67%|██████▋   | 402/600 [02:29<00:54,  3.60it/s] 67%|██████▋   | 403/600 [02:30<00:54,  3.61it/s] 67%|██████▋   | 404/600 [02:30<00:54,  3.59it/s] 68%|██████▊   | 405/600 [02:30<00:54,  3.59it/s] 68%|██████▊   | 406/600 [02:30<00:53,  3.60it/s] 68%|██████▊   | 407/600 [02:31<00:53,  3.61it/s] 68%|██████▊   | 408/600 [02:31<00:53,  3.61it/s] 68%|██████▊   | 409/600 [02:31<00:53,  3.60it/s] 68%|██████▊   | 410/600 [02:32<00:52,  3.61it/s] 68%|██████▊   | 411/600 [02:32<00:52,  3.60it/s] 69%|██████▊   | 412/600 [02:32<00:52,  3.60it/s] 69%|██████▉   | 413/600 [02:32<00:51,  3.60it/s] 69%|██████▉   | 414/600 [02:33<00:51,  3.61it/s] 69%|██████▉   | 415/600 [02:33<00:51,  3.59it/s] 69%|██████▉   | 416/600 [02:33<00:51,  3.60it/s] 70%|██████▉   | 417/600 [02:33<00:50,  3.60it/s] 70%|██████▉   | 418/600 [02:34<00:50,  3.61it/s] 70%|██████▉   | 419/600 [02:34<00:50,  3.60it/s] 70%|███████   | 420/600 [02:34<00:49,  3.60it/s] 70%|███████   | 421/600 [02:35<00:49,  3.60it/s] 70%|███████   | 422/600 [02:35<00:49,  3.61it/s] 70%|███████   | 423/600 [02:35<00:49,  3.60it/s] 71%|███████   | 424/600 [02:35<00:48,  3.61it/s] 71%|███████   | 425/600 [02:36<00:48,  3.61it/s] 71%|███████   | 426/600 [02:36<00:48,  3.60it/s] 71%|███████   | 427/600 [02:36<00:48,  3.60it/s] 71%|███████▏  | 428/600 [02:37<00:47,  3.60it/s] 72%|███████▏  | 429/600 [02:37<00:47,  3.60it/s] 72%|███████▏  | 430/600 [02:37<00:47,  3.60it/s] 72%|███████▏  | 431/600 [02:37<00:46,  3.60it/s] 72%|███████▏  | 432/600 [02:38<00:46,  3.60it/s] 72%|███████▏  | 433/600 [02:38<00:46,  3.61it/s] 72%|███████▏  | 434/600 [02:38<00:46,  3.61it/s] 72%|███████▎  | 435/600 [02:38<00:45,  3.61it/s] 73%|███████▎  | 436/600 [02:39<00:45,  3.60it/s] 73%|███████▎  | 437/600 [02:39<00:45,  3.58it/s] 73%|███████▎  | 438/600 [02:39<00:45,  3.59it/s] 73%|███████▎  | 439/600 [02:40<00:44,  3.59it/s] 73%|███████▎  | 440/600 [02:40<00:44,  3.60it/s] 74%|███████▎  | 441/600 [02:40<00:44,  3.60it/s] 74%|███████▎  | 442/600 [02:40<00:43,  3.60it/s] 74%|███████▍  | 443/600 [02:41<00:43,  3.60it/s] 74%|███████▍  | 444/600 [02:41<00:43,  3.61it/s] 74%|███████▍  | 445/600 [02:41<00:43,  3.60it/s] 74%|███████▍  | 446/600 [02:42<00:42,  3.60it/s] 74%|███████▍  | 447/600 [02:42<00:42,  3.60it/s] 75%|███████▍  | 448/600 [02:42<00:42,  3.59it/s] 75%|███████▍  | 449/600 [02:42<00:42,  3.59it/s] 75%|███████▌  | 450/600 [02:43<00:41,  3.59it/s] 75%|███████▌  | 451/600 [02:43<00:41,  3.60it/s] 75%|███████▌  | 452/600 [02:43<00:41,  3.60it/s] 76%|███████▌  | 453/600 [02:43<00:40,  3.61it/s] 76%|███████▌  | 454/600 [02:44<00:40,  3.61it/s] 76%|███████▌  | 455/600 [02:44<00:40,  3.61it/s] 76%|███████▌  | 456/600 [02:44<00:39,  3.61it/s] 76%|███████▌  | 457/600 [02:45<00:39,  3.61it/s] 76%|███████▋  | 458/600 [02:45<00:39,  3.61it/s] 76%|███████▋  | 459/600 [02:45<00:39,  3.60it/s] 77%|███████▋  | 460/600 [02:45<00:38,  3.60it/s] 77%|███████▋  | 461/600 [02:46<00:38,  3.60it/s] 77%|███████▋  | 462/600 [02:46<00:38,  3.61it/s] 77%|███████▋  | 463/600 [02:46<00:38,  3.60it/s] 77%|███████▋  | 464/600 [02:47<00:37,  3.60it/s] 78%|███████▊  | 465/600 [02:47<00:37,  3.60it/s] 78%|███████▊  | 466/600 [02:47<00:37,  3.60it/s] 78%|███████▊  | 467/600 [02:47<00:36,  3.60it/s] 78%|███████▊  | 468/600 [02:48<00:36,  3.61it/s] 78%|███████▊  | 469/600 [02:48<00:36,  3.61it/s] 78%|███████▊  | 470/600 [02:48<00:36,  3.60it/s] 78%|███████▊  | 471/600 [02:48<00:35,  3.59it/s] 79%|███████▊  | 472/600 [02:49<00:35,  3.59it/s] 79%|███████▉  | 473/600 [02:49<00:35,  3.59it/s] 79%|███████▉  | 474/600 [02:49<00:35,  3.60it/s] 79%|███████▉  | 475/600 [02:50<00:34,  3.60it/s] 79%|███████▉  | 476/600 [02:50<00:34,  3.60it/s] 80%|███████▉  | 477/600 [02:50<00:34,  3.60it/s] 80%|███████▉  | 478/600 [02:50<00:33,  3.60it/s] 80%|███████▉  | 479/600 [02:51<00:33,  3.60it/s] 80%|████████  | 480/600 [02:51<00:33,  3.60it/s][INFO|trainer.py:2140] 2023-08-29 10:13:14,261 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:13:14,261 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 10:13:14,261 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.954, 'eval_samples_per_second': 350.915, 'eval_steps_per_second': 43.902, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.80it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.46it/s][A
  4%|▍         | 17/437 [00:00<00:09, 43.19it/s][A
  5%|▌         | 22/437 [00:00<00:09, 43.71it/s][A
  6%|▌         | 27/437 [00:00<00:09, 43.75it/s][A
  7%|▋         | 32/437 [00:00<00:09, 43.80it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.76it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.76it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.97it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.15it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.08it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.11it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.18it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.16it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.05it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.89it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.98it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.15it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.98it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.15it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.02it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.26it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.28it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.16it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.95it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.01it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.16it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.22it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.15it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.14it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.27it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.27it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.05it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.94it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.00it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.10it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.19it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.14it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.98it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.17it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.17it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.05it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.99it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.03it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.05it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.24it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.19it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.20it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.24it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.11it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.94it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.97it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.08it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.03it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.29it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.26it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.24it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.23it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.99it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.93it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.14it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.19it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.08it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.26it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.22it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.19it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.17it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.02it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.97it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.09it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.12it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.13it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.13it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.19it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.20it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.16it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.05it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.98it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.10it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.22it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.22it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.27it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.20it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.10it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.07it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.05it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.05it/s][A                                                 
                                                 [A 80%|████████  | 480/600 [03:01<00:33,  3.60it/s]
100%|██████████| 437/437 [00:09<00:00, 44.05it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:13:24,244 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-480
[INFO|configuration_utils.py:351] 2023-08-29 10:13:24,260 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-480/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:13:25,833 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-480/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:13:25,848 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:13:25,856 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-480/special_tokens_map.json
 80%|████████  | 481/600 [03:03<07:38,  3.85s/it] 80%|████████  | 482/600 [03:03<05:28,  2.78s/it] 80%|████████  | 483/600 [03:04<03:57,  2.03s/it] 81%|████████  | 484/600 [03:04<02:54,  1.51s/it] 81%|████████  | 485/600 [03:04<02:11,  1.14s/it] 81%|████████  | 486/600 [03:05<01:40,  1.13it/s] 81%|████████  | 487/600 [03:05<01:19,  1.42it/s] 81%|████████▏ | 488/600 [03:05<01:04,  1.74it/s] 82%|████████▏ | 489/600 [03:05<00:54,  2.05it/s] 82%|████████▏ | 490/600 [03:06<00:46,  2.35it/s] 82%|████████▏ | 491/600 [03:06<00:41,  2.61it/s] 82%|████████▏ | 492/600 [03:06<00:38,  2.83it/s] 82%|████████▏ | 493/600 [03:07<00:35,  3.02it/s] 82%|████████▏ | 494/600 [03:07<00:33,  3.15it/s] 82%|████████▎ | 495/600 [03:07<00:32,  3.26it/s] 83%|████████▎ | 496/600 [03:07<00:31,  3.34it/s] 83%|████████▎ | 497/600 [03:08<00:30,  3.40it/s] 83%|████████▎ | 498/600 [03:08<00:29,  3.44it/s] 83%|████████▎ | 499/600 [03:08<00:29,  3.47it/s] 83%|████████▎ | 500/600 [03:09<00:28,  3.50it/s]                                                  83%|████████▎ | 500/600 [03:09<00:28,  3.50it/s] 84%|████████▎ | 501/600 [03:09<00:28,  3.51it/s] 84%|████████▎ | 502/600 [03:09<00:27,  3.52it/s] 84%|████████▍ | 503/600 [03:09<00:27,  3.53it/s] 84%|████████▍ | 504/600 [03:10<00:27,  3.53it/s] 84%|████████▍ | 505/600 [03:10<00:26,  3.53it/s] 84%|████████▍ | 506/600 [03:10<00:26,  3.53it/s] 84%|████████▍ | 507/600 [03:11<00:26,  3.54it/s] 85%|████████▍ | 508/600 [03:11<00:25,  3.54it/s] 85%|████████▍ | 509/600 [03:11<00:25,  3.54it/s] 85%|████████▌ | 510/600 [03:11<00:25,  3.55it/s] 85%|████████▌ | 511/600 [03:12<00:25,  3.55it/s] 85%|████████▌ | 512/600 [03:12<00:24,  3.55it/s] 86%|████████▌ | 513/600 [03:12<00:24,  3.55it/s] 86%|████████▌ | 514/600 [03:12<00:24,  3.55it/s] 86%|████████▌ | 515/600 [03:13<00:23,  3.55it/s] 86%|████████▌ | 516/600 [03:13<00:23,  3.53it/s] 86%|████████▌ | 517/600 [03:13<00:23,  3.54it/s] 86%|████████▋ | 518/600 [03:14<00:23,  3.54it/s] 86%|████████▋ | 519/600 [03:14<00:22,  3.55it/s] 87%|████████▋ | 520/600 [03:14<00:22,  3.55it/s] 87%|████████▋ | 521/600 [03:14<00:22,  3.55it/s] 87%|████████▋ | 522/600 [03:15<00:21,  3.55it/s] 87%|████████▋ | 523/600 [03:15<00:21,  3.55it/s] 87%|████████▋ | 524/600 [03:15<00:21,  3.55it/s] 88%|████████▊ | 525/600 [03:16<00:21,  3.55it/s] 88%|████████▊ | 526/600 [03:16<00:20,  3.55it/s] 88%|████████▊ | 527/600 [03:16<00:20,  3.54it/s] 88%|████████▊ | 528/600 [03:16<00:20,  3.54it/s] 88%|████████▊ | 529/600 [03:17<00:20,  3.54it/s] 88%|████████▊ | 530/600 [03:17<00:19,  3.55it/s] 88%|████████▊ | 531/600 [03:17<00:19,  3.55it/s] 89%|████████▊ | 532/600 [03:18<00:19,  3.55it/s] 89%|████████▉ | 533/600 [03:18<00:18,  3.55it/s] 89%|████████▉ | 534/600 [03:18<00:18,  3.55it/s] 89%|████████▉ | 535/600 [03:18<00:18,  3.55it/s] 89%|████████▉ | 536/600 [03:19<00:18,  3.54it/s] 90%|████████▉ | 537/600 [03:19<00:17,  3.55it/s] 90%|████████▉ | 538/600 [03:19<00:17,  3.53it/s] 90%|████████▉ | 539/600 [03:20<00:17,  3.54it/s] 90%|█████████ | 540/600 [03:20<00:16,  3.54it/s] 90%|█████████ | 541/600 [03:20<00:16,  3.55it/s] 90%|█████████ | 542/600 [03:20<00:16,  3.54it/s] 90%|█████████ | 543/600 [03:21<00:16,  3.55it/s] 91%|█████████ | 544/600 [03:21<00:15,  3.54it/s] 91%|█████████ | 545/600 [03:21<00:15,  3.55it/s] 91%|█████████ | 546/600 [03:22<00:15,  3.54it/s] 91%|█████████ | 547/600 [03:22<00:14,  3.55it/s] 91%|█████████▏| 548/600 [03:22<00:14,  3.55it/s] 92%|█████████▏| 549/600 [03:22<00:14,  3.54it/s] 92%|█████████▏| 550/600 [03:23<00:14,  3.54it/s] 92%|█████████▏| 551/600 [03:23<00:13,  3.54it/s] 92%|█████████▏| 552/600 [03:23<00:13,  3.54it/s] 92%|█████████▏| 553/600 [03:23<00:13,  3.54it/s] 92%|█████████▏| 554/600 [03:24<00:12,  3.54it/s] 92%|█████████▎| 555/600 [03:24<00:12,  3.54it/s] 93%|█████████▎| 556/600 [03:24<00:12,  3.55it/s] 93%|█████████▎| 557/600 [03:25<00:12,  3.55it/s] 93%|█████████▎| 558/600 [03:25<00:11,  3.55it/s] 93%|█████████▎| 559/600 [03:25<00:11,  3.55it/s] 93%|█████████▎| 560/600 [03:25<00:11,  3.53it/s] 94%|█████████▎| 561/600 [03:26<00:11,  3.54it/s] 94%|█████████▎| 562/600 [03:26<00:10,  3.54it/s] 94%|█████████▍| 563/600 [03:26<00:10,  3.54it/s] 94%|█████████▍| 564/600 [03:27<00:10,  3.54it/s] 94%|█████████▍| 565/600 [03:27<00:09,  3.55it/s] 94%|█████████▍| 566/600 [03:27<00:09,  3.55it/s] 94%|█████████▍| 567/600 [03:27<00:09,  3.55it/s] 95%|█████████▍| 568/600 [03:28<00:09,  3.55it/s] 95%|█████████▍| 569/600 [03:28<00:08,  3.55it/s] 95%|█████████▌| 570/600 [03:28<00:08,  3.55it/s] 95%|█████████▌| 571/600 [03:29<00:08,  3.53it/s] 95%|█████████▌| 572/600 [03:29<00:07,  3.54it/s] 96%|█████████▌| 573/600 [03:29<00:07,  3.54it/s] 96%|█████████▌| 574/600 [03:29<00:07,  3.54it/s] 96%|█████████▌| 575/600 [03:30<00:07,  3.55it/s] 96%|█████████▌| 576/600 [03:30<00:06,  3.55it/s] 96%|█████████▌| 577/600 [03:30<00:06,  3.55it/s] 96%|█████████▋| 578/600 [03:31<00:06,  3.55it/s] 96%|█████████▋| 579/600 [03:31<00:05,  3.55it/s] 97%|█████████▋| 580/600 [03:31<00:05,  3.55it/s] 97%|█████████▋| 581/600 [03:31<00:05,  3.55it/s] 97%|█████████▋| 582/600 [03:32<00:05,  3.55it/s] 97%|█████████▋| 583/600 [03:32<00:04,  3.55it/s] 97%|█████████▋| 584/600 [03:32<00:04,  3.55it/s] 98%|█████████▊| 585/600 [03:33<00:04,  3.55it/s] 98%|█████████▊| 586/600 [03:33<00:03,  3.55it/s] 98%|█████████▊| 587/600 [03:33<00:03,  3.55it/s] 98%|█████████▊| 588/600 [03:33<00:03,  3.55it/s] 98%|█████████▊| 589/600 [03:34<00:03,  3.55it/s] 98%|█████████▊| 590/600 [03:34<00:02,  3.55it/s] 98%|█████████▊| 591/600 [03:34<00:02,  3.55it/s] 99%|█████████▊| 592/600 [03:34<00:02,  3.54it/s] 99%|█████████▉| 593/600 [03:35<00:01,  3.54it/s] 99%|█████████▉| 594/600 [03:35<00:01,  3.54it/s] 99%|█████████▉| 595/600 [03:35<00:01,  3.55it/s] 99%|█████████▉| 596/600 [03:36<00:01,  3.55it/s]100%|█████████▉| 597/600 [03:36<00:00,  3.55it/s]100%|█████████▉| 598/600 [03:36<00:00,  3.55it/s]100%|█████████▉| 599/600 [03:36<00:00,  3.55it/s]100%|██████████| 600/600 [03:37<00:00,  3.55it/s][INFO|trainer.py:2140] 2023-08-29 10:13:59,981 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:13:59,982 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 10:13:59,982 >>   Batch size = 8
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.9632, 'eval_samples_per_second': 350.59, 'eval_steps_per_second': 43.861, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.6625e-05, 'epoch': 4.17}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.90it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.57it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.82it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.84it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.11it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.65it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.31it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.05it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.12it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.30it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.48it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.46it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.36it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.27it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.14it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.00it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.89it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.03it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.18it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.27it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.43it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.33it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.18it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.02it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.87it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.01it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.07it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.16it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.35it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.38it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.29it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.16it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.01it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.97it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.97it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.13it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.20it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.33it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.35it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.28it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.15it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.00it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.95it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.03it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.15it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.14it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.28it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.32it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.22it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.13it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.04it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.98it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.04it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.12it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.21it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.26it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.26it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.16it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.13it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.06it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.05it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.14it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.22it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.37it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.32it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.94it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.18it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.12it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.06it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.02it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.09it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.23it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.39it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.37it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.15it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.14it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.06it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.95it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.98it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.07it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.24it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.40it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.30it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.11it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.12it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.08it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.02it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.02it/s][A100%|██████████| 600/600 [03:47<00:00,  3.55it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:14:09,891 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-600
[INFO|configuration_utils.py:351] 2023-08-29 10:14:09,925 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-600/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:14:11,723 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-600/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:14:11,735 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:14:11,744 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 10:14:12,049 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 10:14:12,049 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-120 (score: 1.1039679050445557).
                                                 100%|██████████| 600/600 [03:51<00:00,  3.55it/s]100%|██████████| 600/600 [03:51<00:00,  2.60it/s]
[INFO|trainer.py:1894] 2023-08-29 10:14:13,931 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 10:14:13,951 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:14:16,017 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:14:16,036 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:14:16,046 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:14:16,281 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:16,282 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:16,282 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:16,282 >>   train_runtime            = 0:03:51.18
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:16,282 >>   train_samples            =       7700
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:16,282 >>   train_samples_per_second =     166.53
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:16,282 >>   train_steps_per_second   =      2.595
{'eval_loss': 1.1039679050445557, 'eval_runtime': 9.8909, 'eval_samples_per_second': 353.153, 'eval_steps_per_second': 44.182, 'epoch': 5.0}
{'train_runtime': 231.1891, 'train_samples_per_second': 166.53, 'train_steps_per_second': 2.595, 'train_loss': nan, 'epoch': 5.0}
08/29/2023 10:14:16 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 10:14:16,333 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:14:16,333 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 10:14:16,334 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.48it/s]  3%|▎         | 12/437 [00:00<00:08, 48.64it/s]  4%|▍         | 17/437 [00:00<00:08, 47.01it/s]  5%|▌         | 22/437 [00:00<00:08, 46.20it/s]  6%|▌         | 27/437 [00:00<00:08, 45.66it/s]  7%|▋         | 32/437 [00:00<00:08, 45.34it/s]  8%|▊         | 37/437 [00:00<00:08, 45.18it/s] 10%|▉         | 42/437 [00:00<00:08, 44.56it/s] 11%|█         | 47/437 [00:01<00:08, 43.97it/s] 12%|█▏        | 52/437 [00:01<00:08, 43.88it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.04it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.19it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.31it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.41it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.54it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.47it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.07it/s] 21%|██        | 92/437 [00:02<00:07, 43.86it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.73it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.88it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.08it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.21it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.40it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.41it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.31it/s] 30%|███       | 132/437 [00:02<00:06, 44.13it/s] 31%|███▏      | 137/437 [00:03<00:06, 43.93it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.93it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.07it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.25it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.45it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.52it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.44it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.28it/s] 41%|████      | 177/437 [00:03<00:05, 44.06it/s] 42%|████▏     | 182/437 [00:04<00:05, 43.86it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.92it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.03it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.16it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.35it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.42it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.49it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.35it/s] 51%|█████     | 222/437 [00:04<00:04, 44.00it/s] 52%|█████▏    | 227/437 [00:05<00:04, 43.98it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.01it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.06it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.14it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.41it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.52it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.53it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.31it/s] 61%|██████    | 267/437 [00:06<00:03, 44.06it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.01it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.06it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.11it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.21it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.36it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.45it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.49it/s] 70%|███████   | 307/437 [00:06<00:02, 44.28it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.08it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.11it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.14it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.16it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.25it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.37it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.45it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.45it/s] 81%|████████  | 352/437 [00:07<00:01, 44.22it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.17it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.09it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.13it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.09it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.15it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.24it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.40it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.41it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.27it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.19it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.12it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.05it/s] 95%|█████████▌| 417/437 [00:09<00:00, 43.97it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.10it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.24it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.39it/s]100%|██████████| 437/437 [00:09<00:00, 44.38it/s]100%|██████████| 437/437 [00:09<00:00, 44.32it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:14:26,212 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:26,213 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:26,213 >>   eval_loss               =      1.104
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:26,213 >>   eval_runtime            = 0:00:09.87
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:26,213 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:26,213 >>   eval_samples_per_second =    353.583
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:26,213 >>   eval_steps_per_second   =     44.236
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:14:26,213 >>   perplexity              =     3.0161
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:14:31,025 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:14:31,030 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:14:31,030 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:14:31,030 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:14:31,030 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:14:31,327 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:14:31,327 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:14:31,611 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:14:32,607 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:14:32,607 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:14:35,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:14:35,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:14:35,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:14:35,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:14:35,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:14:35,428 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:14:35,429 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:14:35,690 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:14:35,860 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:14:35,860 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-600
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-240
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-360
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-480
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/generator/iter5/model/checkpoint-120
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.75it/s]Extractor Predicting: 4it [00:02,  1.81it/s]Extractor Predicting: 5it [00:02,  1.76it/s]Extractor Predicting: 6it [00:03,  1.71it/s]Extractor Predicting: 7it [00:04,  1.75it/s]Extractor Predicting: 8it [00:04,  1.75it/s]Extractor Predicting: 9it [00:05,  1.76it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:06,  1.77it/s]Extractor Predicting: 13it [00:07,  1.78it/s]Extractor Predicting: 14it [00:07,  1.80it/s]Extractor Predicting: 15it [00:08,  1.78it/s]Extractor Predicting: 16it [00:09,  1.80it/s]Extractor Predicting: 17it [00:09,  1.81it/s]Extractor Predicting: 18it [00:10,  1.79it/s]Extractor Predicting: 19it [00:10,  1.80it/s]Extractor Predicting: 20it [00:11,  1.75it/s]Extractor Predicting: 21it [00:11,  1.73it/s]Extractor Predicting: 22it [00:12,  1.69it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.75it/s]Extractor Predicting: 26it [00:14,  1.84it/s]Extractor Predicting: 27it [00:15,  1.82it/s]Extractor Predicting: 28it [00:15,  1.86it/s]Extractor Predicting: 29it [00:16,  1.82it/s]Extractor Predicting: 30it [00:16,  1.78it/s]Extractor Predicting: 31it [00:17,  1.65it/s]Extractor Predicting: 32it [00:18,  1.66it/s]Extractor Predicting: 33it [00:18,  1.64it/s]Extractor Predicting: 34it [00:19,  1.63it/s]Extractor Predicting: 35it [00:20,  1.65it/s]Extractor Predicting: 36it [00:20,  1.65it/s]Extractor Predicting: 37it [00:21,  1.64it/s]Extractor Predicting: 38it [00:21,  1.63it/s]Extractor Predicting: 39it [00:22,  1.66it/s]Extractor Predicting: 40it [00:23,  1.67it/s]Extractor Predicting: 41it [00:23,  1.63it/s]Extractor Predicting: 42it [00:24,  1.66it/s]Extractor Predicting: 43it [00:24,  1.66it/s]Extractor Predicting: 44it [00:25,  1.69it/s]Extractor Predicting: 45it [00:26,  1.72it/s]Extractor Predicting: 46it [00:26,  1.66it/s]Extractor Predicting: 47it [00:27,  1.65it/s]Extractor Predicting: 48it [00:27,  1.65it/s]Extractor Predicting: 49it [00:28,  1.65it/s]Extractor Predicting: 50it [00:29,  1.66it/s]Extractor Predicting: 51it [00:29,  1.68it/s]Extractor Predicting: 52it [00:30,  1.65it/s]Extractor Predicting: 53it [00:30,  1.70it/s]Extractor Predicting: 54it [00:31,  1.68it/s]Extractor Predicting: 55it [00:32,  1.65it/s]Extractor Predicting: 56it [00:32,  1.64it/s]Extractor Predicting: 57it [00:33,  1.62it/s]Extractor Predicting: 58it [00:34,  1.62it/s]Extractor Predicting: 59it [00:34,  1.61it/s]Extractor Predicting: 60it [00:35,  1.60it/s]Extractor Predicting: 61it [00:35,  1.62it/s]Extractor Predicting: 62it [00:36,  1.63it/s]Extractor Predicting: 63it [00:37,  1.66it/s]Extractor Predicting: 64it [00:37,  1.70it/s]Extractor Predicting: 65it [00:38,  1.67it/s]Extractor Predicting: 66it [00:38,  1.69it/s]Extractor Predicting: 67it [00:39,  1.68it/s]Extractor Predicting: 68it [00:40,  1.67it/s]Extractor Predicting: 69it [00:40,  1.67it/s]Extractor Predicting: 70it [00:41,  1.64it/s]Extractor Predicting: 71it [00:41,  1.66it/s]Extractor Predicting: 72it [00:42,  1.68it/s]Extractor Predicting: 73it [00:43,  1.67it/s]Extractor Predicting: 74it [00:43,  1.70it/s]Extractor Predicting: 75it [00:44,  1.71it/s]Extractor Predicting: 76it [00:44,  1.68it/s]Extractor Predicting: 77it [00:45,  1.68it/s]Extractor Predicting: 78it [00:46,  1.67it/s]Extractor Predicting: 79it [00:46,  1.67it/s]Extractor Predicting: 80it [00:47,  1.65it/s]Extractor Predicting: 81it [00:47,  1.71it/s]Extractor Predicting: 82it [00:48,  1.69it/s]Extractor Predicting: 83it [00:49,  1.66it/s]Extractor Predicting: 84it [00:49,  1.68it/s]Extractor Predicting: 85it [00:50,  1.71it/s]Extractor Predicting: 86it [00:50,  1.78it/s]Extractor Predicting: 87it [00:51,  1.83it/s]Extractor Predicting: 88it [00:51,  1.82it/s]Extractor Predicting: 89it [00:52,  1.82it/s]Extractor Predicting: 90it [00:52,  1.84it/s]Extractor Predicting: 91it [00:53,  1.81it/s]Extractor Predicting: 92it [00:53,  1.77it/s]Extractor Predicting: 93it [00:54,  1.82it/s]Extractor Predicting: 94it [00:55,  1.84it/s]Extractor Predicting: 95it [00:55,  1.84it/s]Extractor Predicting: 96it [00:56,  1.85it/s]Extractor Predicting: 97it [00:56,  1.82it/s]Extractor Predicting: 98it [00:57,  1.62it/s]Extractor Predicting: 99it [00:58,  1.64it/s]Extractor Predicting: 100it [00:58,  1.68it/s]Extractor Predicting: 101it [00:59,  1.74it/s]Extractor Predicting: 102it [00:59,  1.74it/s]Extractor Predicting: 103it [01:00,  1.76it/s]Extractor Predicting: 104it [01:00,  1.76it/s]Extractor Predicting: 105it [01:01,  1.81it/s]Extractor Predicting: 106it [01:01,  1.77it/s]Extractor Predicting: 107it [01:02,  1.80it/s]Extractor Predicting: 108it [01:03,  1.79it/s]Extractor Predicting: 109it [01:03,  1.83it/s]Extractor Predicting: 110it [01:04,  1.81it/s]Extractor Predicting: 111it [01:04,  1.79it/s]Extractor Predicting: 112it [01:05,  1.77it/s]Extractor Predicting: 113it [01:05,  1.77it/s]Extractor Predicting: 114it [01:06,  1.71it/s]Extractor Predicting: 115it [01:07,  1.73it/s]Extractor Predicting: 116it [01:07,  1.72it/s]Extractor Predicting: 117it [01:08,  1.70it/s]Extractor Predicting: 118it [01:08,  1.77it/s]Extractor Predicting: 119it [01:09,  1.74it/s]Extractor Predicting: 120it [01:09,  1.76it/s]Extractor Predicting: 121it [01:10,  1.73it/s]Extractor Predicting: 122it [01:11,  1.72it/s]Extractor Predicting: 123it [01:11,  1.72it/s]Extractor Predicting: 124it [01:12,  1.68it/s]Extractor Predicting: 125it [01:12,  1.68it/s]Extractor Predicting: 126it [01:13,  1.69it/s]Extractor Predicting: 127it [01:14,  1.66it/s]Extractor Predicting: 128it [01:14,  1.66it/s]Extractor Predicting: 129it [01:15,  1.61it/s]Extractor Predicting: 130it [01:15,  1.64it/s]Extractor Predicting: 131it [01:16,  1.65it/s]Extractor Predicting: 132it [01:17,  1.70it/s]Extractor Predicting: 133it [01:17,  1.70it/s]Extractor Predicting: 134it [01:18,  1.70it/s]Extractor Predicting: 135it [01:18,  1.69it/s]Extractor Predicting: 136it [01:19,  1.69it/s]Extractor Predicting: 137it [01:20,  1.68it/s]Extractor Predicting: 138it [01:20,  1.68it/s]Extractor Predicting: 139it [01:21,  1.66it/s]Extractor Predicting: 140it [01:21,  1.73it/s]Extractor Predicting: 141it [01:22,  1.70it/s]Extractor Predicting: 142it [01:22,  1.77it/s]Extractor Predicting: 142it [01:22,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:16:06,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:16:06,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:16:06,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:16:06,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:16:06,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:16:07,391 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:16:07,392 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:16:07,948 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:16:08,953 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:16:08,953 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:16:11,783 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:16:11,789 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:16:11,789 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:16:11,789 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:16:11,790 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:16:12,435 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:16:12,435 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:16:13,008 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:16:13,170 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:16:13,171 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.78it/s]Extractor Predicting: 5it [00:02,  1.76it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 7it [00:04,  1.73it/s]Extractor Predicting: 8it [00:04,  1.77it/s]Extractor Predicting: 9it [00:05,  1.77it/s]Extractor Predicting: 10it [00:05,  1.81it/s]Extractor Predicting: 11it [00:06,  1.78it/s]Extractor Predicting: 12it [00:06,  1.79it/s]Extractor Predicting: 13it [00:07,  1.75it/s]Extractor Predicting: 14it [00:07,  1.78it/s]Extractor Predicting: 15it [00:08,  1.77it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:09,  1.73it/s]Extractor Predicting: 18it [00:10,  1.72it/s]Extractor Predicting: 19it [00:10,  1.75it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:12,  1.74it/s]Extractor Predicting: 22it [00:12,  1.76it/s]Extractor Predicting: 23it [00:13,  1.73it/s]Extractor Predicting: 24it [00:13,  1.71it/s]Extractor Predicting: 25it [00:14,  1.72it/s]Extractor Predicting: 26it [00:14,  1.73it/s]Extractor Predicting: 27it [00:15,  1.71it/s]Extractor Predicting: 28it [00:16,  1.74it/s]Extractor Predicting: 29it [00:16,  1.69it/s]Extractor Predicting: 30it [00:17,  1.63it/s]Extractor Predicting: 31it [00:17,  1.63it/s]Extractor Predicting: 32it [00:18,  1.63it/s]Extractor Predicting: 33it [00:19,  1.65it/s]Extractor Predicting: 34it [00:19,  1.67it/s]Extractor Predicting: 35it [00:20,  1.67it/s]Extractor Predicting: 36it [00:20,  1.72it/s]Extractor Predicting: 37it [00:21,  1.73it/s]Extractor Predicting: 38it [00:22,  1.75it/s]Extractor Predicting: 39it [00:22,  1.73it/s]Extractor Predicting: 40it [00:23,  1.73it/s]Extractor Predicting: 41it [00:23,  1.74it/s]Extractor Predicting: 42it [00:24,  1.71it/s]Extractor Predicting: 43it [00:24,  1.71it/s]Extractor Predicting: 44it [00:25,  1.70it/s]Extractor Predicting: 45it [00:26,  1.69it/s]Extractor Predicting: 46it [00:26,  1.71it/s]Extractor Predicting: 47it [00:27,  1.71it/s]Extractor Predicting: 48it [00:27,  1.71it/s]Extractor Predicting: 49it [00:28,  1.71it/s]Extractor Predicting: 50it [00:29,  1.68it/s]Extractor Predicting: 51it [00:29,  1.70it/s]Extractor Predicting: 52it [00:30,  1.70it/s]Extractor Predicting: 53it [00:30,  1.69it/s]Extractor Predicting: 54it [00:31,  1.70it/s]Extractor Predicting: 55it [00:32,  1.65it/s]Extractor Predicting: 56it [00:32,  1.68it/s]Extractor Predicting: 57it [00:33,  1.71it/s]Extractor Predicting: 58it [00:33,  1.75it/s]Extractor Predicting: 59it [00:34,  1.75it/s]Extractor Predicting: 60it [00:34,  1.74it/s]Extractor Predicting: 61it [00:35,  1.74it/s]Extractor Predicting: 62it [00:36,  1.75it/s]Extractor Predicting: 63it [00:36,  1.74it/s]Extractor Predicting: 64it [00:37,  1.70it/s]Extractor Predicting: 65it [00:37,  1.70it/s]Extractor Predicting: 66it [00:38,  1.69it/s]Extractor Predicting: 67it [00:38,  1.72it/s]Extractor Predicting: 68it [00:39,  1.77it/s]Extractor Predicting: 69it [00:40,  1.75it/s]Extractor Predicting: 70it [00:40,  1.75it/s]Extractor Predicting: 71it [00:41,  1.74it/s]Extractor Predicting: 72it [00:41,  1.75it/s]Extractor Predicting: 73it [00:42,  1.69it/s]Extractor Predicting: 74it [00:43,  1.69it/s]Extractor Predicting: 75it [00:43,  1.71it/s]Extractor Predicting: 76it [00:44,  1.72it/s]Extractor Predicting: 77it [00:44,  1.67it/s]Extractor Predicting: 78it [00:45,  1.65it/s]Extractor Predicting: 79it [00:46,  1.66it/s]Extractor Predicting: 80it [00:46,  1.68it/s]Extractor Predicting: 81it [00:47,  1.70it/s]Extractor Predicting: 82it [00:47,  1.66it/s]Extractor Predicting: 83it [00:48,  1.68it/s]Extractor Predicting: 84it [00:48,  1.69it/s]Extractor Predicting: 85it [00:49,  1.72it/s]Extractor Predicting: 86it [00:50,  1.68it/s]Extractor Predicting: 87it [00:50,  1.69it/s]Extractor Predicting: 88it [00:51,  1.66it/s]Extractor Predicting: 89it [00:51,  1.67it/s]Extractor Predicting: 90it [00:52,  1.71it/s]Extractor Predicting: 91it [00:53,  1.67it/s]Extractor Predicting: 92it [00:53,  1.67it/s]Extractor Predicting: 93it [00:54,  1.64it/s]Extractor Predicting: 94it [00:55,  1.63it/s]Extractor Predicting: 95it [00:55,  1.64it/s]Extractor Predicting: 96it [00:56,  1.64it/s]Extractor Predicting: 97it [00:56,  1.66it/s]Extractor Predicting: 98it [00:57,  1.65it/s]Extractor Predicting: 99it [00:58,  1.66it/s]Extractor Predicting: 100it [00:58,  1.68it/s]Extractor Predicting: 101it [00:59,  1.67it/s]Extractor Predicting: 102it [00:59,  1.66it/s]Extractor Predicting: 103it [01:00,  1.68it/s]Extractor Predicting: 104it [01:01,  1.67it/s]Extractor Predicting: 105it [01:01,  1.66it/s]Extractor Predicting: 106it [01:02,  1.67it/s]Extractor Predicting: 107it [01:03,  1.50it/s]Extractor Predicting: 108it [01:03,  1.53it/s]Extractor Predicting: 109it [01:04,  1.57it/s]Extractor Predicting: 110it [01:04,  1.56it/s]Extractor Predicting: 111it [01:05,  1.58it/s]Extractor Predicting: 112it [01:06,  1.63it/s]Extractor Predicting: 113it [01:06,  1.67it/s]Extractor Predicting: 114it [01:07,  1.68it/s]Extractor Predicting: 115it [01:07,  1.68it/s]Extractor Predicting: 116it [01:08,  1.68it/s]Extractor Predicting: 117it [01:09,  1.70it/s]Extractor Predicting: 118it [01:09,  1.72it/s]Extractor Predicting: 119it [01:10,  1.71it/s]Extractor Predicting: 120it [01:10,  1.73it/s]Extractor Predicting: 121it [01:11,  1.72it/s]Extractor Predicting: 122it [01:11,  1.78it/s]Extractor Predicting: 123it [01:12,  1.76it/s]Extractor Predicting: 124it [01:12,  1.76it/s]Extractor Predicting: 125it [01:13,  1.75it/s]Extractor Predicting: 126it [01:14,  1.72it/s]Extractor Predicting: 127it [01:14,  1.72it/s]Extractor Predicting: 128it [01:15,  1.74it/s]Extractor Predicting: 129it [01:15,  1.70it/s]Extractor Predicting: 130it [01:16,  1.72it/s]Extractor Predicting: 131it [01:17,  1.69it/s]Extractor Predicting: 132it [01:17,  1.70it/s]Extractor Predicting: 133it [01:18,  1.69it/s]Extractor Predicting: 134it [01:18,  1.70it/s]Extractor Predicting: 135it [01:19,  1.74it/s]Extractor Predicting: 136it [01:19,  1.74it/s]Extractor Predicting: 137it [01:20,  1.72it/s]Extractor Predicting: 138it [01:21,  1.73it/s]Extractor Predicting: 139it [01:21,  1.78it/s]Extractor Predicting: 140it [01:22,  1.75it/s]Extractor Predicting: 141it [01:22,  1.73it/s]Extractor Predicting: 142it [01:23,  1.77it/s]Extractor Predicting: 143it [01:23,  1.75it/s]Extractor Predicting: 144it [01:24,  1.73it/s]Extractor Predicting: 145it [01:25,  1.74it/s]Extractor Predicting: 146it [01:25,  1.74it/s]Extractor Predicting: 147it [01:26,  1.71it/s]Extractor Predicting: 148it [01:26,  1.71it/s]Extractor Predicting: 149it [01:27,  1.69it/s]Extractor Predicting: 150it [01:28,  1.72it/s]Extractor Predicting: 151it [01:28,  1.74it/s]Extractor Predicting: 152it [01:29,  1.76it/s]Extractor Predicting: 153it [01:29,  1.76it/s]Extractor Predicting: 154it [01:30,  1.73it/s]Extractor Predicting: 155it [01:30,  1.72it/s]Extractor Predicting: 156it [01:31,  1.70it/s]Extractor Predicting: 157it [01:32,  1.68it/s]Extractor Predicting: 158it [01:32,  1.68it/s]Extractor Predicting: 159it [01:33,  1.68it/s]Extractor Predicting: 160it [01:33,  1.68it/s]Extractor Predicting: 161it [01:34,  1.72it/s]Extractor Predicting: 162it [01:35,  1.73it/s]Extractor Predicting: 163it [01:35,  1.73it/s]Extractor Predicting: 164it [01:36,  1.73it/s]Extractor Predicting: 165it [01:36,  1.70it/s]Extractor Predicting: 166it [01:37,  1.72it/s]Extractor Predicting: 167it [01:37,  1.75it/s]Extractor Predicting: 168it [01:38,  1.73it/s]Extractor Predicting: 169it [01:39,  1.72it/s]Extractor Predicting: 170it [01:39,  1.69it/s]Extractor Predicting: 171it [01:40,  1.65it/s]Extractor Predicting: 172it [01:41,  1.66it/s]Extractor Predicting: 173it [01:41,  1.67it/s]Extractor Predicting: 174it [01:42,  1.62it/s]Extractor Predicting: 175it [01:42,  1.57it/s]Extractor Predicting: 176it [01:43,  1.60it/s]Extractor Predicting: 177it [01:44,  1.61it/s]Extractor Predicting: 178it [01:44,  1.66it/s]Extractor Predicting: 179it [01:45,  1.66it/s]Extractor Predicting: 180it [01:45,  1.72it/s]Extractor Predicting: 181it [01:46,  1.69it/s]Extractor Predicting: 182it [01:47,  1.72it/s]Extractor Predicting: 183it [01:47,  1.71it/s]Extractor Predicting: 184it [01:48,  1.76it/s]Extractor Predicting: 185it [01:48,  1.78it/s]Extractor Predicting: 186it [01:49,  1.78it/s]Extractor Predicting: 187it [01:49,  1.79it/s]Extractor Predicting: 188it [01:50,  1.77it/s]Extractor Predicting: 189it [01:50,  1.77it/s]Extractor Predicting: 190it [01:51,  1.74it/s]Extractor Predicting: 191it [01:52,  1.68it/s]Extractor Predicting: 192it [01:52,  1.71it/s]Extractor Predicting: 193it [01:53,  1.57it/s]Extractor Predicting: 194it [01:54,  1.60it/s]Extractor Predicting: 195it [01:54,  1.63it/s]Extractor Predicting: 196it [01:55,  1.67it/s]Extractor Predicting: 197it [01:55,  1.71it/s]Extractor Predicting: 198it [01:56,  1.69it/s]Extractor Predicting: 199it [01:56,  1.71it/s]Extractor Predicting: 200it [01:57,  1.71it/s]Extractor Predicting: 201it [01:58,  1.71it/s]Extractor Predicting: 202it [01:58,  1.75it/s]Extractor Predicting: 203it [01:59,  1.76it/s]Extractor Predicting: 204it [01:59,  1.74it/s]Extractor Predicting: 205it [02:00,  1.68it/s]Extractor Predicting: 206it [02:01,  1.69it/s]Extractor Predicting: 207it [02:01,  1.72it/s]Extractor Predicting: 208it [02:02,  1.73it/s]Extractor Predicting: 209it [02:02,  1.66it/s]Extractor Predicting: 210it [02:03,  1.65it/s]Extractor Predicting: 211it [02:04,  1.65it/s]Extractor Predicting: 212it [02:04,  1.68it/s]Extractor Predicting: 213it [02:05,  1.70it/s]Extractor Predicting: 214it [02:05,  1.65it/s]Extractor Predicting: 215it [02:06,  1.66it/s]Extractor Predicting: 216it [02:07,  1.69it/s]Extractor Predicting: 217it [02:07,  1.70it/s]Extractor Predicting: 218it [02:08,  1.63it/s]Extractor Predicting: 219it [02:08,  1.65it/s]Extractor Predicting: 220it [02:09,  1.66it/s]Extractor Predicting: 221it [02:10,  1.63it/s]Extractor Predicting: 222it [02:10,  1.63it/s]Extractor Predicting: 223it [02:11,  1.63it/s]Extractor Predicting: 224it [02:11,  1.68it/s]Extractor Predicting: 225it [02:12,  1.67it/s]Extractor Predicting: 226it [02:13,  1.73it/s]Extractor Predicting: 227it [02:13,  1.76it/s]Extractor Predicting: 228it [02:14,  1.73it/s]Extractor Predicting: 229it [02:14,  1.72it/s]Extractor Predicting: 230it [02:15,  1.69it/s]Extractor Predicting: 231it [02:15,  1.68it/s]Extractor Predicting: 232it [02:16,  1.68it/s]Extractor Predicting: 233it [02:17,  1.72it/s]Extractor Predicting: 234it [02:17,  1.70it/s]Extractor Predicting: 235it [02:18,  1.71it/s]Extractor Predicting: 236it [02:18,  1.67it/s]Extractor Predicting: 237it [02:19,  1.67it/s]Extractor Predicting: 238it [02:20,  1.70it/s]Extractor Predicting: 239it [02:20,  1.72it/s]Extractor Predicting: 240it [02:21,  1.71it/s]Extractor Predicting: 241it [02:21,  1.70it/s]Extractor Predicting: 242it [02:22,  1.67it/s]Extractor Predicting: 243it [02:23,  1.64it/s]Extractor Predicting: 244it [02:23,  1.66it/s]Extractor Predicting: 245it [02:24,  1.72it/s]Extractor Predicting: 246it [02:24,  1.69it/s]Extractor Predicting: 247it [02:25,  1.71it/s]Extractor Predicting: 248it [02:25,  1.71it/s]Extractor Predicting: 249it [02:26,  1.71it/s]Extractor Predicting: 250it [02:27,  1.70it/s]Extractor Predicting: 251it [02:27,  1.66it/s]Extractor Predicting: 252it [02:28,  1.67it/s]Extractor Predicting: 253it [02:28,  1.68it/s]Extractor Predicting: 254it [02:29,  1.71it/s]Extractor Predicting: 255it [02:30,  1.70it/s]Extractor Predicting: 256it [02:30,  1.68it/s]Extractor Predicting: 257it [02:31,  1.71it/s]Extractor Predicting: 258it [02:31,  1.69it/s]Extractor Predicting: 259it [02:32,  1.63it/s]Extractor Predicting: 260it [02:33,  1.66it/s]Extractor Predicting: 261it [02:33,  1.69it/s]Extractor Predicting: 262it [02:34,  1.67it/s]Extractor Predicting: 263it [02:34,  1.66it/s]Extractor Predicting: 264it [02:35,  1.65it/s]Extractor Predicting: 265it [02:36,  1.64it/s]Extractor Predicting: 266it [02:36,  1.62it/s]Extractor Predicting: 267it [02:37,  1.60it/s]Extractor Predicting: 268it [02:38,  1.62it/s]Extractor Predicting: 269it [02:38,  1.62it/s]Extractor Predicting: 270it [02:39,  1.61it/s]Extractor Predicting: 271it [02:39,  1.62it/s]Extractor Predicting: 272it [02:40,  1.63it/s]Extractor Predicting: 273it [02:41,  1.62it/s]Extractor Predicting: 274it [02:41,  1.62it/s]Extractor Predicting: 275it [02:42,  1.66it/s]Extractor Predicting: 276it [02:42,  1.67it/s]Extractor Predicting: 277it [02:43,  1.64it/s]Extractor Predicting: 278it [02:44,  1.65it/s]Extractor Predicting: 279it [02:44,  1.65it/s]Extractor Predicting: 280it [02:45,  1.64it/s]Extractor Predicting: 281it [02:45,  1.63it/s]Extractor Predicting: 282it [02:46,  1.65it/s]Extractor Predicting: 283it [02:47,  1.60it/s]Extractor Predicting: 284it [02:47,  1.58it/s]Extractor Predicting: 285it [02:48,  1.58it/s]Extractor Predicting: 286it [02:49,  1.57it/s]Extractor Predicting: 287it [02:49,  1.60it/s]Extractor Predicting: 288it [02:49,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:10,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:10,723 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:10,723 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:10,723 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:10,723 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:19:11,320 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:19:11,321 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:19:11,890 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:19:12,898 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:19:12,898 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:15,766 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:15,772 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:15,772 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:15,772 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:19:15,772 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:19:16,397 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:19:16,399 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:19:16,961 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:19:17,122 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:19:17,122 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:01,  1.93it/s]Extractor Predicting: 3it [00:01,  1.73it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
