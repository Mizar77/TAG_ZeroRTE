/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:14, 14.46s/it]Extractor Predicting: 2it [00:15,  6.70s/it]Extractor Predicting: 3it [00:16,  3.94s/it]Extractor Predicting: 4it [00:17,  2.66s/it]Extractor Predicting: 5it [00:17,  1.95s/it]Extractor Predicting: 6it [00:18,  1.52s/it]Extractor Predicting: 7it [00:19,  1.25s/it]Extractor Predicting: 8it [00:19,  1.06s/it]Extractor Predicting: 9it [00:20,  1.06it/s]Extractor Predicting: 10it [00:21,  1.19it/s]Extractor Predicting: 11it [00:21,  1.25it/s]Extractor Predicting: 12it [00:22,  1.29it/s]Extractor Predicting: 13it [00:23,  1.33it/s]Extractor Predicting: 14it [00:23,  1.36it/s]Extractor Predicting: 15it [00:24,  1.43it/s]Extractor Predicting: 16it [00:25,  1.42it/s]Extractor Predicting: 17it [00:25,  1.46it/s]Extractor Predicting: 18it [00:26,  1.51it/s]Extractor Predicting: 19it [00:27,  1.49it/s]Extractor Predicting: 20it [00:27,  1.50it/s]Extractor Predicting: 21it [00:28,  1.51it/s]Extractor Predicting: 22it [00:29,  1.53it/s]Extractor Predicting: 23it [00:29,  1.51it/s]Extractor Predicting: 24it [00:30,  1.48it/s]Extractor Predicting: 25it [00:31,  1.46it/s]Extractor Predicting: 26it [00:31,  1.43it/s]Extractor Predicting: 27it [00:32,  1.44it/s]Extractor Predicting: 28it [00:33,  1.48it/s]Extractor Predicting: 29it [00:33,  1.47it/s]Extractor Predicting: 30it [00:34,  1.44it/s]Extractor Predicting: 31it [00:35,  1.45it/s]Extractor Predicting: 32it [00:36,  1.46it/s]Extractor Predicting: 33it [00:36,  1.46it/s]Extractor Predicting: 34it [00:37,  1.49it/s]Extractor Predicting: 35it [00:38,  1.50it/s]Extractor Predicting: 36it [00:38,  1.51it/s]Extractor Predicting: 37it [00:39,  1.51it/s]Extractor Predicting: 38it [00:39,  1.52it/s]Extractor Predicting: 39it [00:40,  1.53it/s]Extractor Predicting: 40it [00:41,  1.51it/s]Extractor Predicting: 41it [00:42,  1.42it/s]Extractor Predicting: 42it [00:42,  1.44it/s]Extractor Predicting: 43it [00:43,  1.48it/s]Extractor Predicting: 44it [00:44,  1.51it/s]Extractor Predicting: 45it [00:44,  1.49it/s]Extractor Predicting: 46it [00:45,  1.49it/s]Extractor Predicting: 47it [00:46,  1.48it/s]Extractor Predicting: 48it [00:46,  1.50it/s]Extractor Predicting: 49it [00:47,  1.50it/s]Extractor Predicting: 50it [00:48,  1.52it/s]Extractor Predicting: 51it [00:48,  1.53it/s]Extractor Predicting: 52it [00:49,  1.53it/s]Extractor Predicting: 53it [00:49,  1.53it/s]Extractor Predicting: 54it [00:50,  1.52it/s]Extractor Predicting: 55it [00:51,  1.49it/s]Extractor Predicting: 56it [00:51,  1.52it/s]Extractor Predicting: 57it [00:52,  1.49it/s]Extractor Predicting: 58it [00:53,  1.49it/s]Extractor Predicting: 59it [00:53,  1.53it/s]Extractor Predicting: 60it [00:54,  1.57it/s]Extractor Predicting: 61it [00:55,  1.58it/s]Extractor Predicting: 62it [00:55,  1.56it/s]Extractor Predicting: 63it [00:56,  1.53it/s]Extractor Predicting: 64it [00:57,  1.53it/s]Extractor Predicting: 65it [00:57,  1.55it/s]Extractor Predicting: 66it [00:58,  1.54it/s]Extractor Predicting: 67it [00:59,  1.54it/s]Extractor Predicting: 68it [00:59,  1.54it/s]Extractor Predicting: 69it [01:00,  1.60it/s]Extractor Predicting: 70it [01:00,  1.59it/s]Extractor Predicting: 71it [01:01,  1.60it/s]Extractor Predicting: 72it [01:02,  1.55it/s]Extractor Predicting: 73it [01:02,  1.56it/s]Extractor Predicting: 74it [01:03,  1.55it/s]Extractor Predicting: 75it [01:04,  1.53it/s]Extractor Predicting: 76it [01:04,  1.51it/s]Extractor Predicting: 77it [01:05,  1.53it/s]Extractor Predicting: 78it [01:06,  1.55it/s]Extractor Predicting: 79it [01:06,  1.58it/s]Extractor Predicting: 80it [01:07,  1.57it/s]Extractor Predicting: 81it [01:08,  1.55it/s]Extractor Predicting: 82it [01:08,  1.55it/s]Extractor Predicting: 83it [01:09,  1.56it/s]Extractor Predicting: 84it [01:09,  1.57it/s]Extractor Predicting: 85it [01:10,  1.58it/s]Extractor Predicting: 86it [01:11,  1.59it/s]Extractor Predicting: 87it [01:11,  1.61it/s]Extractor Predicting: 88it [01:12,  1.58it/s]Extractor Predicting: 89it [01:13,  1.58it/s]Extractor Predicting: 90it [01:13,  1.59it/s]Extractor Predicting: 91it [01:14,  1.61it/s]Extractor Predicting: 92it [01:14,  1.66it/s]Extractor Predicting: 93it [01:15,  1.63it/s]Extractor Predicting: 94it [01:16,  1.62it/s]Extractor Predicting: 95it [01:16,  1.62it/s]Extractor Predicting: 96it [01:17,  1.61it/s]Extractor Predicting: 97it [01:18,  1.60it/s]Extractor Predicting: 98it [01:18,  1.60it/s]Extractor Predicting: 99it [01:19,  1.58it/s]Extractor Predicting: 100it [01:20,  1.53it/s]Extractor Predicting: 101it [01:20,  1.53it/s]Extractor Predicting: 102it [01:21,  1.62it/s]Extractor Predicting: 103it [01:21,  1.63it/s]Extractor Predicting: 104it [01:22,  1.61it/s]Extractor Predicting: 105it [01:23,  1.60it/s]Extractor Predicting: 106it [01:23,  1.60it/s]Extractor Predicting: 107it [01:24,  1.59it/s]Extractor Predicting: 108it [01:24,  1.60it/s]Extractor Predicting: 109it [01:25,  1.60it/s]Extractor Predicting: 110it [01:26,  1.61it/s]Extractor Predicting: 111it [01:27,  1.49it/s]Extractor Predicting: 112it [01:27,  1.54it/s]Extractor Predicting: 113it [01:28,  1.60it/s]Extractor Predicting: 114it [01:28,  1.63it/s]Extractor Predicting: 115it [01:29,  1.61it/s]Extractor Predicting: 116it [01:30,  1.59it/s]Extractor Predicting: 117it [01:30,  1.57it/s]Extractor Predicting: 118it [01:31,  1.59it/s]Extractor Predicting: 119it [01:31,  1.56it/s]Extractor Predicting: 120it [01:32,  1.57it/s]Extractor Predicting: 121it [01:33,  1.56it/s]Extractor Predicting: 122it [01:33,  1.54it/s]Extractor Predicting: 123it [01:34,  1.55it/s]Extractor Predicting: 124it [01:35,  1.56it/s]Extractor Predicting: 125it [01:35,  1.59it/s]Extractor Predicting: 126it [01:36,  1.56it/s]Extractor Predicting: 127it [01:37,  1.59it/s]Extractor Predicting: 128it [01:37,  1.59it/s]Extractor Predicting: 129it [01:38,  1.57it/s]Extractor Predicting: 130it [01:39,  1.55it/s]Extractor Predicting: 131it [01:39,  1.58it/s]Extractor Predicting: 132it [01:40,  1.56it/s]Extractor Predicting: 133it [01:40,  1.54it/s]Extractor Predicting: 134it [01:41,  1.52it/s]Extractor Predicting: 135it [01:42,  1.54it/s]Extractor Predicting: 136it [01:42,  1.53it/s]Extractor Predicting: 137it [01:43,  1.54it/s]Extractor Predicting: 138it [01:44,  1.51it/s]Extractor Predicting: 139it [01:44,  1.52it/s]Extractor Predicting: 140it [01:45,  1.50it/s]Extractor Predicting: 141it [01:46,  1.53it/s]Extractor Predicting: 142it [01:46,  1.50it/s]Extractor Predicting: 143it [01:47,  1.52it/s]Extractor Predicting: 144it [01:48,  1.58it/s]Extractor Predicting: 144it [01:48,  1.33it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3235294117647059,
  "recall": 0.012647312446105202,
  "score": 0.024343015214384506,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:14,  1.65it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:21,  1.60it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:24,  1.64it/s]Extractor Predicting: 40it [00:24,  1.60it/s]Extractor Predicting: 41it [00:25,  1.61it/s]Extractor Predicting: 42it [00:25,  1.65it/s]Extractor Predicting: 43it [00:26,  1.60it/s]Extractor Predicting: 44it [00:27,  1.59it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.57it/s]Extractor Predicting: 47it [00:29,  1.60it/s]Extractor Predicting: 48it [00:29,  1.58it/s]Extractor Predicting: 49it [00:30,  1.62it/s]Extractor Predicting: 50it [00:30,  1.60it/s]Extractor Predicting: 51it [00:31,  1.59it/s]Extractor Predicting: 52it [00:32,  1.57it/s]Extractor Predicting: 53it [00:32,  1.58it/s]Extractor Predicting: 54it [00:33,  1.57it/s]Extractor Predicting: 55it [00:34,  1.61it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:35,  1.57it/s]Extractor Predicting: 58it [00:36,  1.57it/s]Extractor Predicting: 59it [00:36,  1.57it/s]Extractor Predicting: 60it [00:37,  1.55it/s]Extractor Predicting: 61it [00:37,  1.55it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:39,  1.57it/s]Extractor Predicting: 64it [00:39,  1.61it/s]Extractor Predicting: 65it [00:40,  1.58it/s]Extractor Predicting: 66it [00:41,  1.55it/s]Extractor Predicting: 67it [00:41,  1.54it/s]Extractor Predicting: 68it [00:42,  1.54it/s]Extractor Predicting: 69it [00:43,  1.53it/s]Extractor Predicting: 70it [00:43,  1.54it/s]Extractor Predicting: 71it [00:44,  1.54it/s]Extractor Predicting: 72it [00:45,  1.52it/s]Extractor Predicting: 73it [00:45,  1.55it/s]Extractor Predicting: 74it [00:46,  1.55it/s]Extractor Predicting: 75it [00:46,  1.57it/s]Extractor Predicting: 76it [00:47,  1.53it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:48,  1.57it/s]Extractor Predicting: 79it [00:49,  1.61it/s]Extractor Predicting: 80it [00:50,  1.63it/s]Extractor Predicting: 81it [00:50,  1.63it/s]Extractor Predicting: 82it [00:51,  1.61it/s]Extractor Predicting: 83it [00:51,  1.59it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:53,  1.54it/s]Extractor Predicting: 86it [00:54,  1.50it/s]Extractor Predicting: 87it [00:54,  1.51it/s]Extractor Predicting: 88it [00:55,  1.51it/s]Extractor Predicting: 89it [00:56,  1.49it/s]Extractor Predicting: 90it [00:56,  1.49it/s]Extractor Predicting: 91it [00:57,  1.36it/s]Extractor Predicting: 92it [00:58,  1.42it/s]Extractor Predicting: 93it [00:58,  1.47it/s]Extractor Predicting: 94it [00:59,  1.50it/s]Extractor Predicting: 95it [01:00,  1.52it/s]Extractor Predicting: 96it [01:00,  1.49it/s]Extractor Predicting: 97it [01:01,  1.49it/s]Extractor Predicting: 98it [01:02,  1.48it/s]Extractor Predicting: 99it [01:02,  1.51it/s]Extractor Predicting: 100it [01:03,  1.52it/s]Extractor Predicting: 101it [01:04,  1.56it/s]Extractor Predicting: 102it [01:04,  1.53it/s]Extractor Predicting: 103it [01:05,  1.51it/s]Extractor Predicting: 104it [01:05,  1.55it/s]Extractor Predicting: 105it [01:06,  1.53it/s]Extractor Predicting: 106it [01:07,  1.54it/s]Extractor Predicting: 107it [01:07,  1.56it/s]Extractor Predicting: 108it [01:08,  1.54it/s]Extractor Predicting: 109it [01:09,  1.53it/s]Extractor Predicting: 110it [01:09,  1.52it/s]Extractor Predicting: 111it [01:10,  1.55it/s]Extractor Predicting: 112it [01:11,  1.53it/s]Extractor Predicting: 113it [01:11,  1.55it/s]Extractor Predicting: 114it [01:12,  1.55it/s]Extractor Predicting: 115it [01:13,  1.50it/s]Extractor Predicting: 116it [01:13,  1.54it/s]Extractor Predicting: 117it [01:14,  1.53it/s]Extractor Predicting: 118it [01:15,  1.53it/s]Extractor Predicting: 119it [01:15,  1.53it/s]Extractor Predicting: 120it [01:16,  1.56it/s]Extractor Predicting: 121it [01:17,  1.57it/s]Extractor Predicting: 122it [01:17,  1.58it/s]Extractor Predicting: 123it [01:18,  1.57it/s]Extractor Predicting: 124it [01:18,  1.59it/s]Extractor Predicting: 125it [01:19,  1.61it/s]Extractor Predicting: 126it [01:20,  1.61it/s]Extractor Predicting: 127it [01:20,  1.59it/s]Extractor Predicting: 128it [01:21,  1.63it/s]Extractor Predicting: 129it [01:21,  1.63it/s]Extractor Predicting: 130it [01:22,  1.59it/s]Extractor Predicting: 131it [01:23,  1.64it/s]Extractor Predicting: 132it [01:23,  1.64it/s]Extractor Predicting: 133it [01:24,  1.67it/s]Extractor Predicting: 134it [01:25,  1.61it/s]Extractor Predicting: 135it [01:25,  1.64it/s]Extractor Predicting: 136it [01:26,  1.65it/s]Extractor Predicting: 137it [01:26,  1.67it/s]Extractor Predicting: 138it [01:27,  1.65it/s]Extractor Predicting: 139it [01:28,  1.63it/s]Extractor Predicting: 140it [01:28,  1.64it/s]Extractor Predicting: 141it [01:29,  1.61it/s]Extractor Predicting: 142it [01:29,  1.60it/s]Extractor Predicting: 143it [01:30,  1.62it/s]Extractor Predicting: 144it [01:31,  1.59it/s]Extractor Predicting: 145it [01:31,  1.64it/s]Extractor Predicting: 146it [01:32,  1.66it/s]Extractor Predicting: 147it [01:32,  1.67it/s]Extractor Predicting: 148it [01:33,  1.71it/s]Extractor Predicting: 149it [01:34,  1.69it/s]Extractor Predicting: 150it [01:34,  1.70it/s]Extractor Predicting: 151it [01:35,  1.71it/s]Extractor Predicting: 152it [01:35,  1.73it/s]Extractor Predicting: 153it [01:36,  1.70it/s]Extractor Predicting: 154it [01:37,  1.70it/s]Extractor Predicting: 155it [01:37,  1.75it/s]Extractor Predicting: 156it [01:38,  1.73it/s]Extractor Predicting: 157it [01:38,  1.81it/s]Extractor Predicting: 158it [01:39,  1.83it/s]Extractor Predicting: 159it [01:39,  1.78it/s]Extractor Predicting: 160it [01:40,  1.73it/s]Extractor Predicting: 161it [01:41,  1.71it/s]Extractor Predicting: 162it [01:41,  1.73it/s]Extractor Predicting: 163it [01:42,  1.76it/s]Extractor Predicting: 164it [01:42,  1.76it/s]Extractor Predicting: 165it [01:43,  1.76it/s]Extractor Predicting: 166it [01:43,  1.72it/s]Extractor Predicting: 167it [01:44,  1.75it/s]Extractor Predicting: 168it [01:45,  1.73it/s]Extractor Predicting: 169it [01:45,  1.79it/s]Extractor Predicting: 170it [01:46,  1.77it/s]Extractor Predicting: 171it [01:46,  1.77it/s]Extractor Predicting: 172it [01:47,  1.69it/s]Extractor Predicting: 173it [01:47,  1.66it/s]Extractor Predicting: 174it [01:48,  1.62it/s]Extractor Predicting: 175it [01:49,  1.56it/s]Extractor Predicting: 176it [01:49,  1.57it/s]Extractor Predicting: 177it [01:50,  1.57it/s]Extractor Predicting: 178it [01:51,  1.55it/s]Extractor Predicting: 179it [01:51,  1.55it/s]Extractor Predicting: 180it [01:52,  1.56it/s]Extractor Predicting: 181it [01:53,  1.56it/s]Extractor Predicting: 182it [01:53,  1.55it/s]Extractor Predicting: 183it [01:54,  1.55it/s]Extractor Predicting: 184it [01:55,  1.53it/s]Extractor Predicting: 185it [01:55,  1.51it/s]Extractor Predicting: 186it [01:56,  1.39it/s]Extractor Predicting: 187it [01:57,  1.43it/s]Extractor Predicting: 188it [01:57,  1.46it/s]Extractor Predicting: 189it [01:58,  1.48it/s]Extractor Predicting: 190it [01:59,  1.52it/s]Extractor Predicting: 191it [01:59,  1.49it/s]Extractor Predicting: 192it [02:00,  1.48it/s]Extractor Predicting: 193it [02:01,  1.49it/s]Extractor Predicting: 194it [02:01,  1.50it/s]Extractor Predicting: 195it [02:02,  1.51it/s]Extractor Predicting: 196it [02:03,  1.52it/s]Extractor Predicting: 197it [02:03,  1.51it/s]Extractor Predicting: 198it [02:04,  1.54it/s]Extractor Predicting: 199it [02:05,  1.56it/s]Extractor Predicting: 200it [02:05,  1.53it/s]Extractor Predicting: 201it [02:06,  1.51it/s]Extractor Predicting: 202it [02:07,  1.59it/s]Extractor Predicting: 203it [02:07,  1.56it/s]Extractor Predicting: 204it [02:08,  1.58it/s]Extractor Predicting: 205it [02:08,  1.57it/s]Extractor Predicting: 206it [02:09,  1.56it/s]Extractor Predicting: 207it [02:10,  1.54it/s]Extractor Predicting: 208it [02:10,  1.54it/s]Extractor Predicting: 209it [02:11,  1.48it/s]Extractor Predicting: 210it [02:12,  1.50it/s]Extractor Predicting: 211it [02:13,  1.50it/s]Extractor Predicting: 212it [02:13,  1.50it/s]Extractor Predicting: 213it [02:14,  1.50it/s]Extractor Predicting: 214it [02:15,  1.49it/s]Extractor Predicting: 215it [02:15,  1.52it/s]Extractor Predicting: 216it [02:16,  1.49it/s]Extractor Predicting: 217it [02:16,  1.51it/s]Extractor Predicting: 218it [02:17,  1.52it/s]Extractor Predicting: 219it [02:18,  1.50it/s]Extractor Predicting: 220it [02:18,  1.51it/s]Extractor Predicting: 221it [02:19,  1.48it/s]Extractor Predicting: 222it [02:20,  1.48it/s]Extractor Predicting: 223it [02:20,  1.51it/s]Extractor Predicting: 224it [02:21,  1.51it/s]Extractor Predicting: 225it [02:22,  1.52it/s]Extractor Predicting: 226it [02:22,  1.51it/s]Extractor Predicting: 227it [02:23,  1.56it/s]Extractor Predicting: 228it [02:24,  1.55it/s]Extractor Predicting: 229it [02:24,  1.54it/s]Extractor Predicting: 230it [02:25,  1.59it/s]Extractor Predicting: 231it [02:26,  1.59it/s]Extractor Predicting: 232it [02:26,  1.62it/s]Extractor Predicting: 233it [02:27,  1.59it/s]Extractor Predicting: 234it [02:27,  1.60it/s]Extractor Predicting: 235it [02:28,  1.58it/s]Extractor Predicting: 236it [02:29,  1.56it/s]Extractor Predicting: 237it [02:29,  1.54it/s]Extractor Predicting: 238it [02:30,  1.56it/s]Extractor Predicting: 239it [02:31,  1.54it/s]Extractor Predicting: 240it [02:31,  1.54it/s]Extractor Predicting: 241it [02:32,  1.49it/s]Extractor Predicting: 242it [02:33,  1.53it/s]Extractor Predicting: 243it [02:33,  1.52it/s]Extractor Predicting: 244it [02:34,  1.54it/s]Extractor Predicting: 245it [02:35,  1.56it/s]Extractor Predicting: 246it [02:35,  1.55it/s]Extractor Predicting: 247it [02:36,  1.56it/s]Extractor Predicting: 248it [02:37,  1.56it/s]Extractor Predicting: 249it [02:37,  1.54it/s]Extractor Predicting: 250it [02:38,  1.56it/s]Extractor Predicting: 251it [02:38,  1.57it/s]Extractor Predicting: 252it [02:39,  1.57it/s]Extractor Predicting: 253it [02:40,  1.55it/s]Extractor Predicting: 254it [02:40,  1.57it/s]Extractor Predicting: 255it [02:41,  1.52it/s]Extractor Predicting: 256it [02:42,  1.48it/s]Extractor Predicting: 257it [02:42,  1.49it/s]Extractor Predicting: 258it [02:43,  1.48it/s]Extractor Predicting: 259it [02:44,  1.50it/s]Extractor Predicting: 260it [02:44,  1.48it/s]Extractor Predicting: 261it [02:45,  1.52it/s]Extractor Predicting: 262it [02:46,  1.51it/s]Extractor Predicting: 263it [02:46,  1.49it/s]Extractor Predicting: 264it [02:47,  1.49it/s]Extractor Predicting: 265it [02:48,  1.44it/s]Extractor Predicting: 266it [02:49,  1.45it/s]Extractor Predicting: 267it [02:49,  1.46it/s]Extractor Predicting: 268it [02:50,  1.43it/s]Extractor Predicting: 269it [02:51,  1.47it/s]Extractor Predicting: 270it [02:51,  1.49it/s]Extractor Predicting: 271it [02:52,  1.46it/s]Extractor Predicting: 272it [02:53,  1.47it/s]Extractor Predicting: 273it [02:53,  1.47it/s]Extractor Predicting: 274it [02:54,  1.50it/s]Extractor Predicting: 275it [02:55,  1.52it/s]Extractor Predicting: 276it [02:55,  1.56it/s]Extractor Predicting: 277it [02:56,  1.55it/s]Extractor Predicting: 278it [02:57,  1.42it/s]Extractor Predicting: 279it [02:57,  1.45it/s]Extractor Predicting: 280it [02:58,  1.43it/s]Extractor Predicting: 281it [02:59,  1.45it/s]Extractor Predicting: 282it [02:59,  1.56it/s]Extractor Predicting: 282it [02:59,  1.57it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4372549019607843,
  "recall": 0.09896449704142012,
  "score": 0.16139927623642944,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 5it [00:03,  1.51it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5416666666666666,
  "recall": 0.05416666666666667,
  "score": 0.09848484848484848,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:20<04:49, 20.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:38<04:04, 18.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:56<03:41, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:14<03:20, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:32<03:03, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:50<02:44, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:08<02:24, 18.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:27<02:07, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:43<01:46, 17.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:59<01:25, 17.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:17<01:09, 17.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:33<00:51, 17.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:48<00:32, 16.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:04<00:16, 16.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:23<00:00, 17.08s/it]Generating: 100%|██████████| 15/15 [04:23<00:00, 17.58s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 298, 'raw': 416}
{'target': 600, 'success': 320, 'raw': 448}
{'target': 600, 'success': 342, 'raw': 480}
{'target': 600, 'success': 365, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 480, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 547, 'raw': 768}
{'target': 600, 'success': 574, 'raw': 800}
{'target': 600, 'success': 600, 'raw': 832}
{'prompt': 'Relation : country .', 'success_rate': 0.7211538461538461, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : followed by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : genre . Context : Following his work with the band The Last Man , Günther Schiller \'s " I Am Not There " was critically praised , and has been released by Sony Pictures Entertainment and Columbia Pictures . Head Entity : I Am Not There , Tail Entity : bands .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 419, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 600, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.78125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8355978260869565, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8721590909090909, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8532608695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8536931818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8478260869565217, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 13714
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13814, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:03,  3.97s/it]Extractor Estimating: 2it [00:04,  2.07s/it]Extractor Estimating: 3it [00:05,  1.48s/it]Extractor Estimating: 4it [00:07,  1.83s/it]Extractor Estimating: 5it [00:08,  1.40s/it]Extractor Estimating: 6it [00:09,  1.14s/it]Extractor Estimating: 7it [00:09,  1.03it/s]Extractor Estimating: 8it [00:10,  1.14it/s]Extractor Estimating: 9it [00:11,  1.23it/s]Extractor Estimating: 10it [00:11,  1.32it/s]Extractor Estimating: 11it [00:12,  1.36it/s]Extractor Estimating: 12it [00:13,  1.38it/s]Extractor Estimating: 13it [00:13,  1.40it/s]Extractor Estimating: 14it [00:14,  1.42it/s]Extractor Estimating: 15it [00:15,  1.48it/s]Extractor Estimating: 16it [00:15,  1.49it/s]Extractor Estimating: 17it [00:16,  1.47it/s]Extractor Estimating: 18it [00:17,  1.48it/s]Extractor Estimating: 19it [00:17,  1.37it/s]Extractor Estimating: 20it [00:18,  1.39it/s]Extractor Estimating: 21it [00:19,  1.40it/s]Extractor Estimating: 22it [00:20,  1.39it/s]Extractor Estimating: 23it [00:20,  1.43it/s]Extractor Estimating: 24it [00:21,  1.44it/s]Extractor Estimating: 25it [00:22,  1.50it/s]Extractor Estimating: 26it [00:22,  1.50it/s]Extractor Estimating: 27it [00:23,  1.40it/s]Extractor Estimating: 28it [00:24,  1.39it/s]Extractor Estimating: 29it [00:24,  1.41it/s]Extractor Estimating: 30it [00:25,  1.40it/s]Extractor Estimating: 31it [00:26,  1.40it/s]Extractor Estimating: 32it [00:27,  1.38it/s]Extractor Estimating: 33it [00:27,  1.46it/s]Extractor Estimating: 34it [00:28,  1.40it/s]Extractor Estimating: 35it [00:29,  1.43it/s]Extractor Estimating: 36it [00:29,  1.43it/s]Extractor Estimating: 37it [00:30,  1.42it/s]Extractor Estimating: 38it [00:31,  1.35it/s]Extractor Estimating: 39it [00:32,  1.39it/s]Extractor Estimating: 40it [00:32,  1.40it/s]Extractor Estimating: 41it [00:33,  1.41it/s]Extractor Estimating: 42it [00:34,  1.40it/s]Extractor Estimating: 43it [00:34,  1.41it/s]Extractor Estimating: 44it [00:35,  1.40it/s]Extractor Estimating: 45it [00:36,  1.46it/s]Extractor Estimating: 46it [00:36,  1.43it/s]Extractor Estimating: 47it [00:37,  1.41it/s]Extractor Estimating: 48it [00:38,  1.43it/s]Extractor Estimating: 49it [00:40,  1.06s/it]Extractor Estimating: 50it [00:41,  1.05it/s]Extractor Estimating: 51it [00:41,  1.14it/s]Extractor Estimating: 52it [00:42,  1.22it/s]Extractor Estimating: 53it [00:43,  1.28it/s]Extractor Estimating: 54it [00:43,  1.33it/s]Extractor Estimating: 55it [00:44,  1.37it/s]Extractor Estimating: 56it [00:45,  1.38it/s]Extractor Estimating: 57it [00:45,  1.41it/s]Extractor Estimating: 58it [00:46,  1.42it/s]Extractor Estimating: 59it [00:47,  1.44it/s]Extractor Estimating: 60it [00:47,  1.48it/s]Extractor Estimating: 61it [00:48,  1.54it/s]Extractor Estimating: 62it [00:49,  1.47it/s]Extractor Estimating: 63it [00:49,  1.45it/s]Extractor Estimating: 64it [00:50,  1.45it/s]Extractor Estimating: 65it [00:51,  1.47it/s]Extractor Estimating: 66it [00:51,  1.47it/s]Extractor Estimating: 67it [00:52,  1.44it/s]Extractor Estimating: 68it [00:53,  1.47it/s]Extractor Estimating: 69it [00:53,  1.52it/s]Extractor Estimating: 70it [00:54,  1.48it/s]Extractor Estimating: 71it [00:55,  1.47it/s]Extractor Estimating: 72it [00:56,  1.44it/s]Extractor Estimating: 73it [00:56,  1.45it/s]Extractor Estimating: 74it [00:57,  1.42it/s]Extractor Estimating: 75it [00:58,  1.48it/s]Extractor Estimating: 76it [00:58,  1.48it/s]Extractor Estimating: 77it [00:59,  1.48it/s]Extractor Estimating: 78it [01:00,  1.49it/s]Extractor Estimating: 79it [01:00,  1.51it/s]Extractor Estimating: 80it [01:01,  1.56it/s]Extractor Estimating: 81it [01:01,  1.55it/s]Extractor Estimating: 82it [01:02,  1.50it/s]Extractor Estimating: 83it [01:03,  1.44it/s]Extractor Estimating: 84it [01:04,  1.45it/s]Extractor Estimating: 85it [01:04,  1.47it/s]Extractor Estimating: 86it [01:05,  1.47it/s]Extractor Estimating: 87it [01:06,  1.48it/s]Extractor Estimating: 88it [01:06,  1.49it/s]Extractor Estimating: 89it [01:07,  1.47it/s]Extractor Estimating: 90it [01:08,  1.50it/s]Extractor Estimating: 91it [01:08,  1.50it/s]Extractor Estimating: 92it [01:09,  1.40it/s]Extractor Estimating: 93it [01:10,  1.41it/s]Extractor Estimating: 94it [01:11,  1.41it/s]Extractor Estimating: 95it [01:11,  1.49it/s]Extractor Estimating: 96it [01:12,  1.52it/s]Extractor Estimating: 97it [01:12,  1.54it/s]Extractor Estimating: 98it [01:13,  1.44it/s]Extractor Estimating: 99it [01:14,  1.44it/s]Extractor Estimating: 100it [01:14,  1.46it/s]Extractor Estimating: 101it [01:15,  1.51it/s]Extractor Estimating: 102it [01:16,  1.48it/s]Extractor Estimating: 103it [01:16,  1.49it/s]Extractor Estimating: 104it [01:17,  1.46it/s]Extractor Estimating: 105it [01:18,  1.53it/s]Extractor Estimating: 106it [01:18,  1.49it/s]Extractor Estimating: 107it [01:19,  1.49it/s]Extractor Estimating: 108it [01:20,  1.48it/s]Extractor Estimating: 109it [01:21,  1.44it/s]Extractor Estimating: 110it [01:21,  1.41it/s]Extractor Estimating: 111it [01:22,  1.41it/s]Extractor Estimating: 112it [01:23,  1.43it/s]Extractor Estimating: 113it [01:23,  1.39it/s]Extractor Estimating: 114it [01:24,  1.44it/s]Extractor Estimating: 115it [01:25,  1.46it/s]Extractor Estimating: 116it [01:25,  1.50it/s]Extractor Estimating: 117it [01:26,  1.47it/s]Extractor Estimating: 118it [01:27,  1.43it/s]Extractor Estimating: 119it [01:28,  1.45it/s]Extractor Estimating: 120it [01:28,  1.48it/s]Extractor Estimating: 121it [01:29,  1.49it/s]Extractor Estimating: 122it [01:30,  1.45it/s]Extractor Estimating: 123it [01:30,  1.38it/s]Extractor Estimating: 124it [01:31,  1.41it/s]Extractor Estimating: 125it [01:32,  1.41it/s]Extractor Estimating: 126it [01:32,  1.45it/s]Extractor Estimating: 127it [01:33,  1.46it/s]Extractor Estimating: 128it [01:34,  1.50it/s]Extractor Estimating: 129it [01:34,  1.55it/s]Extractor Estimating: 130it [01:35,  1.55it/s]Extractor Estimating: 131it [01:36,  1.54it/s]Extractor Estimating: 132it [01:36,  1.52it/s]Extractor Estimating: 133it [01:37,  1.58it/s]Extractor Estimating: 134it [01:37,  1.57it/s]Extractor Estimating: 135it [01:38,  1.61it/s]Extractor Estimating: 136it [01:39,  1.62it/s]Extractor Estimating: 137it [01:39,  1.64it/s]Extractor Estimating: 138it [01:40,  1.56it/s]Extractor Estimating: 139it [01:41,  1.60it/s]Extractor Estimating: 140it [01:41,  1.54it/s]Extractor Estimating: 141it [01:42,  1.55it/s]Extractor Estimating: 142it [01:43,  1.58it/s]Extractor Estimating: 143it [01:43,  1.64it/s]Extractor Estimating: 144it [01:44,  1.58it/s]Extractor Estimating: 145it [01:44,  1.54it/s]Extractor Estimating: 146it [01:45,  1.50it/s]Extractor Estimating: 147it [01:46,  1.51it/s]Extractor Estimating: 148it [01:46,  1.51it/s]Extractor Estimating: 149it [01:47,  1.44it/s]Extractor Estimating: 150it [01:48,  1.41it/s]Extractor Estimating: 151it [01:49,  1.47it/s]Extractor Estimating: 152it [01:49,  1.50it/s]Extractor Estimating: 153it [01:50,  1.54it/s]Extractor Estimating: 154it [01:51,  1.51it/s]Extractor Estimating: 155it [01:51,  1.50it/s]Extractor Estimating: 156it [01:52,  1.51it/s]Extractor Estimating: 157it [01:53,  1.46it/s]Extractor Estimating: 158it [01:53,  1.41it/s]Extractor Estimating: 159it [01:54,  1.43it/s]Extractor Estimating: 160it [01:55,  1.42it/s]Extractor Estimating: 161it [01:55,  1.49it/s]Extractor Estimating: 162it [01:56,  1.49it/s]Extractor Estimating: 163it [01:57,  1.44it/s]Extractor Estimating: 164it [01:57,  1.44it/s]Extractor Estimating: 165it [01:58,  1.45it/s]Extractor Estimating: 166it [01:59,  1.48it/s]Extractor Estimating: 167it [01:59,  1.45it/s]Extractor Estimating: 168it [02:00,  1.50it/s]Extractor Estimating: 169it [02:01,  1.50it/s]Extractor Estimating: 170it [02:01,  1.51it/s]Extractor Estimating: 171it [02:02,  1.54it/s]Extractor Estimating: 172it [02:03,  1.53it/s]Extractor Estimating: 173it [02:03,  1.48it/s]Extractor Estimating: 174it [02:04,  1.37it/s]Extractor Estimating: 175it [02:05,  1.44it/s]Extractor Estimating: 176it [02:06,  1.39it/s]Extractor Estimating: 177it [02:06,  1.38it/s]Extractor Estimating: 178it [02:07,  1.38it/s]Extractor Estimating: 179it [02:08,  1.36it/s]Extractor Estimating: 180it [02:09,  1.39it/s]Extractor Estimating: 181it [02:09,  1.41it/s]Extractor Estimating: 182it [02:10,  1.43it/s]Extractor Estimating: 183it [02:11,  1.43it/s]Extractor Estimating: 184it [02:11,  1.46it/s]Extractor Estimating: 185it [02:12,  1.49it/s]Extractor Estimating: 186it [02:13,  1.50it/s]Extractor Estimating: 187it [02:13,  1.48it/s]Extractor Estimating: 188it [02:14,  1.47it/s]Extractor Estimating: 189it [02:15,  1.47it/s]Extractor Estimating: 190it [02:15,  1.47it/s]Extractor Estimating: 191it [02:16,  1.44it/s]Extractor Estimating: 192it [02:17,  1.55it/s]Extractor Estimating: 193it [02:17,  1.54it/s]Extractor Estimating: 194it [02:18,  1.51it/s]Extractor Estimating: 195it [02:19,  1.46it/s]Extractor Estimating: 196it [02:19,  1.46it/s]Extractor Estimating: 197it [02:20,  1.41it/s]Extractor Estimating: 198it [02:21,  1.46it/s]Extractor Estimating: 199it [02:21,  1.47it/s]Extractor Estimating: 200it [02:22,  1.47it/s]Extractor Estimating: 201it [02:23,  1.46it/s]Extractor Estimating: 202it [02:24,  1.44it/s]Extractor Estimating: 203it [02:24,  1.43it/s]Extractor Estimating: 204it [02:25,  1.47it/s]Extractor Estimating: 205it [02:26,  1.48it/s]Extractor Estimating: 206it [02:26,  1.44it/s]Extractor Estimating: 207it [02:27,  1.46it/s]Extractor Estimating: 208it [02:28,  1.46it/s]Extractor Estimating: 209it [02:28,  1.46it/s]Extractor Estimating: 210it [02:29,  1.44it/s]Extractor Estimating: 211it [02:30,  1.46it/s]Extractor Estimating: 212it [02:30,  1.40it/s]Extractor Estimating: 213it [02:31,  1.41it/s]Extractor Estimating: 214it [02:32,  1.46it/s]Extractor Estimating: 215it [02:32,  1.49it/s]Extractor Estimating: 216it [02:33,  1.41it/s]Extractor Estimating: 217it [02:34,  1.37it/s]Extractor Estimating: 218it [02:35,  1.40it/s]Extractor Estimating: 219it [02:35,  1.40it/s]Extractor Estimating: 220it [02:36,  1.40it/s]Extractor Estimating: 221it [02:37,  1.41it/s]Extractor Estimating: 222it [02:38,  1.42it/s]Extractor Estimating: 223it [02:38,  1.42it/s]Extractor Estimating: 224it [02:39,  1.44it/s]Extractor Estimating: 225it [02:40,  1.45it/s]Extractor Estimating: 226it [02:40,  1.49it/s]Extractor Estimating: 227it [02:41,  1.53it/s]Extractor Estimating: 228it [02:41,  1.58it/s]Extractor Estimating: 229it [02:42,  1.61it/s]Extractor Estimating: 230it [02:43,  1.61it/s]Extractor Estimating: 231it [02:43,  1.59it/s]Extractor Estimating: 232it [02:44,  1.61it/s]Extractor Estimating: 233it [02:45,  1.54it/s]Extractor Estimating: 234it [02:45,  1.58it/s]Extractor Estimating: 235it [02:46,  1.57it/s]Extractor Estimating: 236it [02:47,  1.53it/s]Extractor Estimating: 237it [02:47,  1.55it/s]Extractor Estimating: 238it [02:48,  1.49it/s]Extractor Estimating: 239it [02:48,  1.52it/s]Extractor Estimating: 240it [02:49,  1.49it/s]Extractor Estimating: 241it [02:50,  1.50it/s]Extractor Estimating: 242it [02:50,  1.53it/s]Extractor Estimating: 243it [02:51,  1.58it/s]Extractor Estimating: 244it [02:52,  1.59it/s]Extractor Estimating: 245it [02:52,  1.55it/s]Extractor Estimating: 246it [02:53,  1.55it/s]Extractor Estimating: 247it [02:54,  1.52it/s]Extractor Estimating: 248it [02:54,  1.55it/s]Extractor Estimating: 249it [02:55,  1.58it/s]Extractor Estimating: 250it [02:56,  1.55it/s]Extractor Estimating: 251it [02:56,  1.54it/s]Extractor Estimating: 252it [02:57,  1.50it/s]Extractor Estimating: 253it [02:58,  1.52it/s]Extractor Estimating: 254it [02:58,  1.38it/s]Extractor Estimating: 255it [02:59,  1.38it/s]Extractor Estimating: 256it [03:00,  1.45it/s]Extractor Estimating: 257it [03:01,  1.41it/s]Extractor Estimating: 258it [03:01,  1.41it/s]Extractor Estimating: 259it [03:02,  1.39it/s]Extractor Estimating: 260it [03:03,  1.43it/s]Extractor Estimating: 261it [03:03,  1.43it/s]Extractor Estimating: 262it [03:04,  1.46it/s]Extractor Estimating: 263it [03:05,  1.46it/s]Extractor Estimating: 264it [03:05,  1.45it/s]Extractor Estimating: 265it [03:06,  1.43it/s]Extractor Estimating: 266it [03:07,  1.49it/s]Extractor Estimating: 267it [03:07,  1.43it/s]Extractor Estimating: 268it [03:08,  1.43it/s]Extractor Estimating: 269it [03:09,  1.49it/s]Extractor Estimating: 270it [03:09,  1.48it/s]Extractor Estimating: 271it [03:10,  1.47it/s]Extractor Estimating: 272it [03:11,  1.41it/s]Extractor Estimating: 273it [03:12,  1.45it/s]Extractor Estimating: 274it [03:12,  1.42it/s]Extractor Estimating: 275it [03:13,  1.48it/s]Extractor Estimating: 276it [03:14,  1.47it/s]Extractor Estimating: 277it [03:14,  1.46it/s]Extractor Estimating: 278it [03:15,  1.42it/s]Extractor Estimating: 279it [03:16,  1.47it/s]Extractor Estimating: 280it [03:16,  1.53it/s]Extractor Estimating: 281it [03:17,  1.54it/s]Extractor Estimating: 282it [03:18,  1.50it/s]Extractor Estimating: 283it [03:18,  1.54it/s]Extractor Estimating: 284it [03:19,  1.56it/s]Extractor Estimating: 285it [03:19,  1.58it/s]Extractor Estimating: 286it [03:20,  1.55it/s]Extractor Estimating: 287it [03:21,  1.54it/s]Extractor Estimating: 288it [03:21,  1.53it/s]Extractor Estimating: 289it [03:22,  1.55it/s]Extractor Estimating: 290it [03:23,  1.53it/s]Extractor Estimating: 291it [03:23,  1.51it/s]Extractor Estimating: 292it [03:24,  1.55it/s]Extractor Estimating: 293it [03:25,  1.58it/s]Extractor Estimating: 294it [03:25,  1.58it/s]Extractor Estimating: 295it [03:26,  1.52it/s]Extractor Estimating: 296it [03:27,  1.53it/s]Extractor Estimating: 297it [03:27,  1.54it/s]Extractor Estimating: 298it [03:28,  1.58it/s]Extractor Estimating: 299it [03:29,  1.56it/s]Extractor Estimating: 300it [03:29,  1.53it/s]Extractor Estimating: 301it [03:30,  1.59it/s]Extractor Estimating: 302it [03:30,  1.61it/s]Extractor Estimating: 303it [03:31,  1.58it/s]Extractor Estimating: 304it [03:32,  1.61it/s]Extractor Estimating: 305it [03:32,  1.54it/s]Extractor Estimating: 306it [03:33,  1.61it/s]Extractor Estimating: 307it [03:34,  1.61it/s]Extractor Estimating: 308it [03:34,  1.64it/s]Extractor Estimating: 309it [03:35,  1.67it/s]Extractor Estimating: 310it [03:35,  1.62it/s]Extractor Estimating: 311it [03:36,  1.69it/s]Extractor Estimating: 312it [03:37,  1.62it/s]Extractor Estimating: 313it [03:37,  1.64it/s]Extractor Estimating: 314it [03:38,  1.63it/s]Extractor Estimating: 315it [03:38,  1.65it/s]Extractor Estimating: 316it [03:39,  1.64it/s]Extractor Estimating: 317it [03:40,  1.61it/s]Extractor Estimating: 318it [03:40,  1.60it/s]Extractor Estimating: 319it [03:41,  1.59it/s]Extractor Estimating: 320it [03:42,  1.60it/s]Extractor Estimating: 321it [03:42,  1.58it/s]Extractor Estimating: 322it [03:43,  1.45it/s]Extractor Estimating: 323it [03:44,  1.47it/s]Extractor Estimating: 324it [03:44,  1.47it/s]Extractor Estimating: 325it [03:45,  1.55it/s]Extractor Estimating: 326it [03:46,  1.57it/s]Extractor Estimating: 327it [03:46,  1.47it/s]Extractor Estimating: 328it [03:47,  1.47it/s]Extractor Estimating: 329it [03:48,  1.48it/s]Extractor Estimating: 330it [03:48,  1.49it/s]Extractor Estimating: 331it [03:49,  1.51it/s]Extractor Estimating: 332it [03:50,  1.47it/s]Extractor Estimating: 333it [03:50,  1.48it/s]Extractor Estimating: 334it [03:51,  1.53it/s]Extractor Estimating: 335it [03:52,  1.52it/s]Extractor Estimating: 336it [03:52,  1.53it/s]Extractor Estimating: 337it [03:53,  1.52it/s]Extractor Estimating: 338it [03:54,  1.46it/s]Extractor Estimating: 339it [03:54,  1.47it/s]Extractor Estimating: 340it [03:55,  1.44it/s]Extractor Estimating: 341it [03:56,  1.43it/s]Extractor Estimating: 342it [03:57,  1.41it/s]Extractor Estimating: 343it [03:57,  1.44it/s]Extractor Estimating: 344it [03:58,  1.47it/s]Extractor Estimating: 345it [03:58,  1.50it/s]Extractor Estimating: 346it [03:59,  1.54it/s]Extractor Estimating: 347it [04:00,  1.49it/s]Extractor Estimating: 348it [04:00,  1.48it/s]Extractor Estimating: 349it [04:01,  1.48it/s]Extractor Estimating: 350it [04:02,  1.47it/s]Extractor Estimating: 351it [04:02,  1.52it/s]Extractor Estimating: 352it [04:03,  1.57it/s]Extractor Estimating: 353it [04:04,  1.62it/s]Extractor Estimating: 354it [04:04,  1.56it/s]Extractor Estimating: 355it [04:05,  1.53it/s]Extractor Estimating: 356it [04:06,  1.57it/s]Extractor Estimating: 357it [04:06,  1.54it/s]Extractor Estimating: 358it [04:07,  1.55it/s]Extractor Estimating: 359it [04:08,  1.55it/s]Extractor Estimating: 360it [04:08,  1.52it/s]Extractor Estimating: 361it [04:09,  1.56it/s]Extractor Estimating: 362it [04:10,  1.51it/s]Extractor Estimating: 363it [04:10,  1.53it/s]Extractor Estimating: 364it [04:11,  1.53it/s]Extractor Estimating: 365it [04:11,  1.53it/s]Extractor Estimating: 366it [04:12,  1.53it/s]Extractor Estimating: 367it [04:13,  1.56it/s]Extractor Estimating: 368it [04:13,  1.60it/s]Extractor Estimating: 369it [04:14,  1.56it/s]Extractor Estimating: 370it [04:15,  1.59it/s]Extractor Estimating: 371it [04:15,  1.56it/s]Extractor Estimating: 372it [04:16,  1.56it/s]Extractor Estimating: 373it [04:17,  1.60it/s]Extractor Estimating: 374it [04:17,  1.60it/s]Extractor Estimating: 375it [04:18,  1.55it/s]Extractor Estimating: 375it [04:18,  1.45it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7490 mean pseudo reward: 0.9209080828437701
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 25642
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25742, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25742, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.360, loss:945.3505
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.141, loss:945.0443
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.106, loss:941.3446
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.085, loss:877.7326
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.087, loss:909.4610
>> valid entity prec:0.4284, rec:0.4882, f1:0.4563
>> valid relation prec:0.0514, rec:0.0057, f1:0.0103
>> valid relation with NER prec:0.0514, rec:0.0057, f1:0.0103
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.508, loss:885.4018
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.100, loss:864.8782
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.095, loss:877.0031
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.093, loss:863.4001
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.091, loss:832.2309
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4802, rec:0.5088, f1:0.4941
>> valid relation prec:0.0760, rec:0.0126, f1:0.0217
>> valid relation with NER prec:0.0760, rec:0.0126, f1:0.0217
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.498, loss:860.3222
g_step 1200, step 261, avg_time 1.088, loss:842.2279
g_step 1300, step 48, avg_time 1.088, loss:819.8815
g_step 1400, step 148, avg_time 1.084, loss:769.8390
g_step 1500, step 248, avg_time 1.104, loss:797.4362
>> valid entity prec:0.4296, rec:0.4790, f1:0.4530
>> valid relation prec:0.0797, rec:0.0147, f1:0.0248
>> valid relation with NER prec:0.0797, rec:0.0147, f1:0.0248
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.487, loss:784.4986
g_step 1700, step 135, avg_time 1.103, loss:743.3767
g_step 1800, step 235, avg_time 1.087, loss:750.5783
g_step 1900, step 22, avg_time 1.095, loss:761.6683
g_step 2000, step 122, avg_time 1.102, loss:722.0492
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5282, rec:0.3981, f1:0.4540
>> valid relation prec:0.0481, rec:0.0063, f1:0.0112
>> valid relation with NER prec:0.0481, rec:0.0063, f1:0.0112
g_step 2100, step 222, avg_time 2.488, loss:715.1213
g_step 2200, step 9, avg_time 1.077, loss:705.7763
g_step 2300, step 109, avg_time 1.089, loss:677.4946
g_step 2400, step 209, avg_time 1.095, loss:704.0552
g_step 2500, step 309, avg_time 1.105, loss:661.3033
>> valid entity prec:0.5076, rec:0.4558, f1:0.4803
>> valid relation prec:0.0930, rec:0.0138, f1:0.0240
>> valid relation with NER prec:0.0930, rec:0.0138, f1:0.0240
g_step 2600, step 96, avg_time 2.489, loss:623.3020
g_step 2700, step 196, avg_time 1.086, loss:648.2730
g_step 2800, step 296, avg_time 1.099, loss:675.6592
g_step 2900, step 83, avg_time 1.090, loss:602.0247
g_step 3000, step 183, avg_time 1.092, loss:625.0912
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5139, rec:0.4798, f1:0.4963
>> valid relation prec:0.0596, rec:0.0124, f1:0.0205
>> valid relation with NER prec:0.0596, rec:0.0124, f1:0.0205
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 283, avg_time 2.493, loss:641.1288
g_step 3200, step 70, avg_time 1.086, loss:570.9711
g_step 3300, step 170, avg_time 1.091, loss:572.9231
g_step 3400, step 270, avg_time 1.088, loss:610.4971
g_step 3500, step 57, avg_time 1.082, loss:573.9417
>> valid entity prec:0.4946, rec:0.4588, f1:0.4760
>> valid relation prec:0.0374, rec:0.0098, f1:0.0155
>> valid relation with NER prec:0.0374, rec:0.0098, f1:0.0155
g_step 3600, step 157, avg_time 2.492, loss:563.3564
g_step 3700, step 257, avg_time 1.091, loss:579.5536
g_step 3800, step 44, avg_time 1.085, loss:543.9136
g_step 3900, step 144, avg_time 1.097, loss:523.3850
g_step 4000, step 244, avg_time 1.088, loss:546.7509
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4617, rec:0.4704, f1:0.4660
>> valid relation prec:0.0591, rec:0.0132, f1:0.0216
>> valid relation with NER prec:0.0591, rec:0.0132, f1:0.0216
g_step 4100, step 31, avg_time 2.492, loss:549.7919
g_step 4200, step 131, avg_time 1.091, loss:502.8030
g_step 4300, step 231, avg_time 1.100, loss:538.2634
g_step 4400, step 18, avg_time 1.094, loss:518.0596
g_step 4500, step 118, avg_time 1.077, loss:477.4828
>> valid entity prec:0.4664, rec:0.4535, f1:0.4598
>> valid relation prec:0.0499, rec:0.0147, f1:0.0227
>> valid relation with NER prec:0.0499, rec:0.0147, f1:0.0227
g_step 4600, step 218, avg_time 2.483, loss:496.7023
g_step 4700, step 5, avg_time 1.100, loss:527.4987
g_step 4800, step 105, avg_time 1.095, loss:472.7723
g_step 4900, step 205, avg_time 1.089, loss:480.8283
g_step 5000, step 305, avg_time 1.103, loss:500.1707
learning rate was adjusted to 0.0008
>> valid entity prec:0.5006, rec:0.3999, f1:0.4446
>> valid relation prec:0.0668, rec:0.0141, f1:0.0233
>> valid relation with NER prec:0.0668, rec:0.0141, f1:0.0233
g_step 5100, step 92, avg_time 2.479, loss:445.3689
g_step 5200, step 192, avg_time 1.098, loss:453.4900
g_step 5300, step 292, avg_time 1.095, loss:476.5532
g_step 5400, step 79, avg_time 1.089, loss:436.3488
g_step 5500, step 179, avg_time 1.100, loss:441.3704
>> valid entity prec:0.4696, rec:0.4673, f1:0.4685
>> valid relation prec:0.0446, rec:0.0149, f1:0.0224
>> valid relation with NER prec:0.0446, rec:0.0149, f1:0.0224
g_step 5600, step 279, avg_time 2.482, loss:442.9090
g_step 5700, step 66, avg_time 1.094, loss:436.1033
g_step 5800, step 166, avg_time 1.095, loss:410.0669
g_step 5900, step 266, avg_time 1.091, loss:434.4323
g_step 6000, step 53, avg_time 1.101, loss:409.9190
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4801, rec:0.4311, f1:0.4543
>> valid relation prec:0.0533, rec:0.0158, f1:0.0244
>> valid relation with NER prec:0.0533, rec:0.0158, f1:0.0244
g_step 6100, step 153, avg_time 2.493, loss:399.7365
g_step 6200, step 253, avg_time 1.095, loss:414.7834
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:24:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:24:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-24-48_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:24:49 - WARNING - datasets.builder -   Using custom data configuration default-1a0347136a98cb07
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-1a0347136a98cb07/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:24:50,070 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:24:50,071 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:24:50,072 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:24:50,073 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:24:50,084 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:24:50,089 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:24:50,089 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:24:50,089 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:24:50,089 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:24:50,089 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:24:50,089 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:24:50,214 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:24:53,280 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:24:53,284 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-1a0347136a98cb07/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 13:24:53 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1473154a1320> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.92ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.80ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.14ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.33ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.40ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.48ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.53ba/s]100%|██████████| 8/8 [00:01<00:00,  4.48ba/s]100%|██████████| 8/8 [00:01<00:00,  4.30ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.00ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.26ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.39ba/s]100%|██████████| 4/4 [00:00<00:00,  5.49ba/s]100%|██████████| 4/4 [00:00<00:00,  4.96ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.10ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.16ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.31ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.29ba/s]100%|██████████| 8/8 [00:00<00:00, 10.89ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.23ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.62ba/s]100%|██████████| 4/4 [00:00<00:00, 10.97ba/s]
[INFO|trainer.py:414] 2023-08-28 13:24:57,437 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:24:57,451 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:24:57,451 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 13:24:57,451 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:24:57,451 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:24:57,451 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:24:57,451 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:24:57,451 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.33it/s]  0%|          | 2/585 [00:00<02:56,  3.29it/s]  1%|          | 3/585 [00:00<02:52,  3.37it/s]  1%|          | 4/585 [00:01<02:50,  3.41it/s]  1%|          | 5/585 [00:01<02:48,  3.44it/s]  1%|          | 6/585 [00:01<02:48,  3.45it/s]  1%|          | 7/585 [00:02<02:47,  3.46it/s]  1%|▏         | 8/585 [00:02<02:47,  3.45it/s]  2%|▏         | 9/585 [00:02<02:46,  3.46it/s]  2%|▏         | 10/585 [00:02<02:45,  3.47it/s]  2%|▏         | 11/585 [00:03<02:45,  3.46it/s]  2%|▏         | 12/585 [00:03<02:45,  3.47it/s]  2%|▏         | 13/585 [00:03<02:44,  3.47it/s]  2%|▏         | 14/585 [00:04<02:44,  3.47it/s]  3%|▎         | 15/585 [00:04<02:44,  3.47it/s]  3%|▎         | 16/585 [00:04<02:43,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.47it/s]  3%|▎         | 18/585 [00:05<02:43,  3.48it/s]  3%|▎         | 19/585 [00:05<02:43,  3.47it/s]  3%|▎         | 20/585 [00:05<02:42,  3.47it/s]  4%|▎         | 21/585 [00:06<02:42,  3.48it/s]  4%|▍         | 22/585 [00:06<02:42,  3.47it/s]  4%|▍         | 23/585 [00:06<02:42,  3.47it/s]  4%|▍         | 24/585 [00:06<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.47it/s]  4%|▍         | 26/585 [00:07<02:41,  3.47it/s]  5%|▍         | 27/585 [00:07<02:41,  3.47it/s]  5%|▍         | 28/585 [00:08<02:40,  3.47it/s]  5%|▍         | 29/585 [00:08<02:40,  3.47it/s]  5%|▌         | 30/585 [00:08<02:39,  3.47it/s]  5%|▌         | 31/585 [00:08<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.47it/s]  6%|▌         | 33/585 [00:09<02:39,  3.47it/s]  6%|▌         | 34/585 [00:09<02:38,  3.48it/s]  6%|▌         | 35/585 [00:10<02:38,  3.47it/s]  6%|▌         | 36/585 [00:10<02:38,  3.47it/s]  6%|▋         | 37/585 [00:10<02:37,  3.47it/s]  6%|▋         | 38/585 [00:10<02:37,  3.47it/s]  7%|▋         | 39/585 [00:11<02:37,  3.47it/s]  7%|▋         | 40/585 [00:11<02:36,  3.47it/s]  7%|▋         | 41/585 [00:11<02:36,  3.47it/s]  7%|▋         | 42/585 [00:12<02:36,  3.47it/s]  7%|▋         | 43/585 [00:12<02:36,  3.47it/s]  8%|▊         | 44/585 [00:12<02:35,  3.47it/s]  8%|▊         | 45/585 [00:12<02:35,  3.47it/s]  8%|▊         | 46/585 [00:13<02:35,  3.47it/s]  8%|▊         | 47/585 [00:13<02:35,  3.46it/s]  8%|▊         | 48/585 [00:13<02:34,  3.47it/s]  8%|▊         | 49/585 [00:14<02:34,  3.47it/s]  9%|▊         | 50/585 [00:14<02:34,  3.46it/s]  9%|▊         | 51/585 [00:14<02:34,  3.46it/s]  9%|▉         | 52/585 [00:15<02:33,  3.47it/s]  9%|▉         | 53/585 [00:15<02:33,  3.47it/s]  9%|▉         | 54/585 [00:15<02:33,  3.47it/s]  9%|▉         | 55/585 [00:15<02:32,  3.47it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:31,  3.47it/s] 10%|█         | 59/585 [00:17<02:31,  3.47it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.46it/s] 11%|█         | 62/585 [00:17<02:30,  3.46it/s] 11%|█         | 63/585 [00:18<02:30,  3.47it/s] 11%|█         | 64/585 [00:18<02:30,  3.47it/s] 11%|█         | 65/585 [00:18<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.46it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 69/585 [00:19<02:28,  3.47it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 73/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 76/585 [00:21<02:26,  3.46it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.47it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.47it/s] 14%|█▎        | 79/585 [00:22<02:25,  3.47it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 83/585 [00:23<02:24,  3.47it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:25<02:23,  3.46it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.46it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.46it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.46it/s] 17%|█▋        | 97/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.46it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 104/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.46it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 111/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:32<02:15,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.46it/s] 20%|██        | 117/585 [00:33<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 13:25:31,281 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:25:31,282 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:25:31,282 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.06it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.67it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.69it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.04it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.54it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.27it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.98it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.44it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.52it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.59it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.64it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.76it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.85it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.72it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.82it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.58it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.40it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.47it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.47it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.52it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.70it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.70it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.62it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.63it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.59it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.40it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.45it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.37it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.45it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.46it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.64it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.70it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.77it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.70it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.47it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.37it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.46it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.47it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.30it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.69it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.57it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.64it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.69it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.62it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.56it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.54it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.31it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.40it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.55it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.58it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.63it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.52it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.51it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.52it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.52it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.51it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.48it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.45it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.55it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.50it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.57it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.49it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.45it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.47it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.31it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.41it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.63it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.57it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.60it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.64it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.52it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.47it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.39it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.34it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.30it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.52it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.52it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.68it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.62it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.58it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.48it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.54it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.45it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.47it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.46it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.42it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:43<02:15,  3.46it/s]
100%|██████████| 435/435 [00:09<00:00, 46.42it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:25:40,666 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 13:25:40,685 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:25:44,801 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:25:44,821 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:25:44,827 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:52<45:34,  5.86s/it] 20%|██        | 119/585 [00:52<32:30,  4.19s/it] 21%|██        | 120/585 [00:53<23:22,  3.02s/it] 21%|██        | 121/585 [00:53<16:59,  2.20s/it] 21%|██        | 122/585 [00:53<12:32,  1.62s/it] 21%|██        | 123/585 [00:54<09:25,  1.22s/it] 21%|██        | 124/585 [00:54<07:14,  1.06it/s] 21%|██▏       | 125/585 [00:54<05:43,  1.34it/s] 22%|██▏       | 126/585 [00:54<04:39,  1.64it/s] 22%|██▏       | 127/585 [00:55<03:54,  1.95it/s] 22%|██▏       | 128/585 [00:55<03:23,  2.24it/s] 22%|██▏       | 129/585 [00:55<03:01,  2.51it/s] 22%|██▏       | 130/585 [00:56<02:46,  2.74it/s] 22%|██▏       | 131/585 [00:56<02:35,  2.92it/s] 23%|██▎       | 132/585 [00:56<02:27,  3.06it/s] 23%|██▎       | 133/585 [00:56<02:22,  3.18it/s] 23%|██▎       | 134/585 [00:57<02:18,  3.26it/s] 23%|██▎       | 135/585 [00:57<02:15,  3.32it/s] 23%|██▎       | 136/585 [00:57<02:13,  3.36it/s] 23%|██▎       | 137/585 [00:58<02:12,  3.39it/s] 24%|██▎       | 138/585 [00:58<02:11,  3.41it/s] 24%|██▍       | 139/585 [00:58<02:10,  3.43it/s] 24%|██▍       | 140/585 [00:58<02:09,  3.44it/s] 24%|██▍       | 141/585 [00:59<02:08,  3.45it/s] 24%|██▍       | 142/585 [00:59<02:08,  3.45it/s] 24%|██▍       | 143/585 [00:59<02:07,  3.45it/s] 25%|██▍       | 144/585 [01:00<02:07,  3.46it/s] 25%|██▍       | 145/585 [01:00<02:07,  3.46it/s] 25%|██▍       | 146/585 [01:00<02:10,  3.37it/s] 25%|██▌       | 147/585 [01:01<02:09,  3.39it/s] 25%|██▌       | 148/585 [01:01<02:07,  3.41it/s] 25%|██▌       | 149/585 [01:01<02:07,  3.43it/s] 26%|██▌       | 150/585 [01:01<02:06,  3.44it/s] 26%|██▌       | 151/585 [01:02<02:05,  3.45it/s] 26%|██▌       | 152/585 [01:02<02:05,  3.45it/s] 26%|██▌       | 153/585 [01:02<02:04,  3.46it/s] 26%|██▋       | 154/585 [01:03<02:04,  3.46it/s] 26%|██▋       | 155/585 [01:03<02:04,  3.46it/s] 27%|██▋       | 156/585 [01:03<02:03,  3.46it/s] 27%|██▋       | 157/585 [01:03<02:03,  3.46it/s] 27%|██▋       | 158/585 [01:04<02:03,  3.47it/s] 27%|██▋       | 159/585 [01:04<02:02,  3.46it/s] 27%|██▋       | 160/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:05<02:02,  3.46it/s] 28%|██▊       | 162/585 [01:05<02:02,  3.47it/s] 28%|██▊       | 163/585 [01:05<02:01,  3.46it/s] 28%|██▊       | 164/585 [01:05<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:06<02:01,  3.46it/s] 28%|██▊       | 166/585 [01:06<02:01,  3.45it/s] 29%|██▊       | 167/585 [01:06<02:01,  3.45it/s] 29%|██▊       | 168/585 [01:07<02:00,  3.46it/s] 29%|██▉       | 169/585 [01:07<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:07<01:59,  3.46it/s] 29%|██▉       | 171/585 [01:07<01:59,  3.46it/s] 29%|██▉       | 172/585 [01:08<01:59,  3.45it/s] 30%|██▉       | 173/585 [01:08<01:59,  3.46it/s] 30%|██▉       | 174/585 [01:08<01:58,  3.46it/s] 30%|██▉       | 175/585 [01:09<01:58,  3.46it/s] 30%|███       | 176/585 [01:09<01:58,  3.46it/s] 30%|███       | 177/585 [01:09<01:57,  3.46it/s] 30%|███       | 178/585 [01:09<01:57,  3.46it/s] 31%|███       | 179/585 [01:10<01:57,  3.46it/s] 31%|███       | 180/585 [01:10<01:57,  3.46it/s] 31%|███       | 181/585 [01:10<01:56,  3.46it/s] 31%|███       | 182/585 [01:11<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:11<01:56,  3.45it/s] 31%|███▏      | 184/585 [01:11<01:56,  3.45it/s] 32%|███▏      | 185/585 [01:12<01:55,  3.46it/s] 32%|███▏      | 186/585 [01:12<01:55,  3.46it/s] 32%|███▏      | 187/585 [01:12<01:55,  3.46it/s] 32%|███▏      | 188/585 [01:12<01:54,  3.46it/s] 32%|███▏      | 189/585 [01:13<01:54,  3.46it/s] 32%|███▏      | 190/585 [01:13<01:54,  3.46it/s] 33%|███▎      | 191/585 [01:13<01:53,  3.46it/s] 33%|███▎      | 192/585 [01:14<01:53,  3.46it/s] 33%|███▎      | 193/585 [01:14<01:53,  3.46it/s] 33%|███▎      | 194/585 [01:14<01:53,  3.45it/s] 33%|███▎      | 195/585 [01:14<01:52,  3.45it/s] 34%|███▎      | 196/585 [01:15<01:52,  3.45it/s] 34%|███▎      | 197/585 [01:15<01:52,  3.45it/s] 34%|███▍      | 198/585 [01:15<01:52,  3.45it/s] 34%|███▍      | 199/585 [01:16<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:16<01:51,  3.46it/s] 34%|███▍      | 201/585 [01:16<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:16<01:50,  3.46it/s] 35%|███▍      | 203/585 [01:17<01:50,  3.46it/s] 35%|███▍      | 204/585 [01:17<01:50,  3.46it/s] 35%|███▌      | 205/585 [01:17<01:50,  3.45it/s] 35%|███▌      | 206/585 [01:18<01:50,  3.44it/s] 35%|███▌      | 207/585 [01:18<01:49,  3.44it/s] 36%|███▌      | 208/585 [01:18<01:49,  3.45it/s] 36%|███▌      | 209/585 [01:18<01:49,  3.45it/s] 36%|███▌      | 210/585 [01:19<01:48,  3.45it/s] 36%|███▌      | 211/585 [01:19<01:48,  3.45it/s] 36%|███▌      | 212/585 [01:19<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:20<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:20<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:20<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:20<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:21<01:47,  3.44it/s] 37%|███▋      | 218/585 [01:21<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:21<01:46,  3.45it/s] 38%|███▊      | 220/585 [01:22<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:22<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:22<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:23<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:23<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:23<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 227/585 [01:24<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:24<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:24<01:43,  3.43it/s] 39%|███▉      | 230/585 [01:25<01:43,  3.44it/s] 39%|███▉      | 231/585 [01:25<01:42,  3.44it/s] 40%|███▉      | 232/585 [01:25<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:25<01:42,  3.45it/s] 40%|████      | 234/585 [01:26<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:26:23,700 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:26:23,700 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:26:23,700 >>   Batch size = 8
{'eval_loss': 0.9587679505348206, 'eval_runtime': 9.3597, 'eval_samples_per_second': 371.698, 'eval_steps_per_second': 46.476, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.66it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.24it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.40it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.65it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.13it/s][A
  8%|▊         | 33/435 [00:00<00:08, 46.79it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.65it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.34it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.30it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.32it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.42it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 46.49it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.52it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.40it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.35it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.19it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.11it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.22it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.32it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.40it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.29it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.51it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.48it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.44it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.34it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.21it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.14it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.26it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.29it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.46it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.43it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.43it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.27it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.25it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.23it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.17it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.23it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.38it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.33it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 46.37it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.36it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.42it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.39it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.16it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.12it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.22it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.36it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.38it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.43it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.35it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.21it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.31it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.28it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.22it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.30it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.30it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.30it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.38it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.36it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.37it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.31it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.22it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.08it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.18it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.37it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.39it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.46it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.36it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.27it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.29it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.26it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.23it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.23it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.19it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.23it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.34it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.32it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.28it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.32it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.16it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.11it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.12it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.26it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.28it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.38it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.39it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:35<01:41,  3.45it/s]
100%|██████████| 435/435 [00:09<00:00, 46.39it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:26:33,128 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 13:26:33,151 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:26:35,673 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:26:35,690 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:26:35,699 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:43<31:43,  5.44s/it] 40%|████      | 236/585 [01:43<22:39,  3.89s/it] 41%|████      | 237/585 [01:44<16:18,  2.81s/it] 41%|████      | 238/585 [01:44<11:53,  2.06s/it] 41%|████      | 239/585 [01:44<08:47,  1.53s/it] 41%|████      | 240/585 [01:45<06:38,  1.15s/it] 41%|████      | 241/585 [01:45<05:07,  1.12it/s] 41%|████▏     | 242/585 [01:45<04:04,  1.40it/s] 42%|████▏     | 243/585 [01:45<03:20,  1.71it/s] 42%|████▏     | 244/585 [01:46<02:49,  2.01it/s] 42%|████▏     | 245/585 [01:46<02:27,  2.30it/s] 42%|████▏     | 246/585 [01:46<02:12,  2.56it/s] 42%|████▏     | 247/585 [01:47<02:02,  2.76it/s] 42%|████▏     | 248/585 [01:47<01:54,  2.94it/s] 43%|████▎     | 249/585 [01:47<01:49,  3.08it/s] 43%|████▎     | 250/585 [01:47<01:45,  3.18it/s] 43%|████▎     | 251/585 [01:48<01:42,  3.26it/s] 43%|████▎     | 252/585 [01:48<01:40,  3.32it/s] 43%|████▎     | 253/585 [01:48<01:38,  3.36it/s] 43%|████▎     | 254/585 [01:49<01:37,  3.39it/s] 44%|████▎     | 255/585 [01:49<01:36,  3.41it/s] 44%|████▍     | 256/585 [01:49<01:36,  3.42it/s] 44%|████▍     | 257/585 [01:50<01:35,  3.44it/s] 44%|████▍     | 258/585 [01:50<01:35,  3.42it/s] 44%|████▍     | 259/585 [01:50<01:34,  3.43it/s] 44%|████▍     | 260/585 [01:50<01:34,  3.44it/s] 45%|████▍     | 261/585 [01:51<01:34,  3.45it/s] 45%|████▍     | 262/585 [01:51<01:33,  3.45it/s] 45%|████▍     | 263/585 [01:51<01:33,  3.45it/s] 45%|████▌     | 264/585 [01:52<01:32,  3.45it/s] 45%|████▌     | 265/585 [01:52<01:32,  3.45it/s] 45%|████▌     | 266/585 [01:52<01:32,  3.45it/s] 46%|████▌     | 267/585 [01:52<01:31,  3.46it/s] 46%|████▌     | 268/585 [01:53<01:31,  3.46it/s] 46%|████▌     | 269/585 [01:53<01:31,  3.44it/s] 46%|████▌     | 270/585 [01:53<01:31,  3.44it/s] 46%|████▋     | 271/585 [01:54<01:31,  3.45it/s] 46%|████▋     | 272/585 [01:54<01:30,  3.45it/s] 47%|████▋     | 273/585 [01:54<01:30,  3.45it/s] 47%|████▋     | 274/585 [01:54<01:30,  3.45it/s] 47%|████▋     | 275/585 [01:55<01:29,  3.45it/s] 47%|████▋     | 276/585 [01:55<01:29,  3.45it/s] 47%|████▋     | 277/585 [01:55<01:29,  3.45it/s] 48%|████▊     | 278/585 [01:56<01:28,  3.45it/s] 48%|████▊     | 279/585 [01:56<01:28,  3.46it/s] 48%|████▊     | 280/585 [01:56<01:28,  3.44it/s] 48%|████▊     | 281/585 [01:56<01:28,  3.44it/s] 48%|████▊     | 282/585 [01:57<01:27,  3.44it/s] 48%|████▊     | 283/585 [01:57<01:27,  3.45it/s] 49%|████▊     | 284/585 [01:57<01:27,  3.45it/s] 49%|████▊     | 285/585 [01:58<01:26,  3.46it/s] 49%|████▉     | 286/585 [01:58<01:26,  3.46it/s] 49%|████▉     | 287/585 [01:58<01:26,  3.46it/s] 49%|████▉     | 288/585 [01:59<01:26,  3.45it/s] 49%|████▉     | 289/585 [01:59<01:25,  3.46it/s] 50%|████▉     | 290/585 [01:59<01:25,  3.46it/s] 50%|████▉     | 291/585 [01:59<01:25,  3.44it/s] 50%|████▉     | 292/585 [02:00<01:25,  3.45it/s] 50%|█████     | 293/585 [02:00<01:24,  3.45it/s] 50%|█████     | 294/585 [02:00<01:25,  3.39it/s] 50%|█████     | 295/585 [02:01<01:25,  3.41it/s] 51%|█████     | 296/585 [02:01<01:24,  3.43it/s] 51%|█████     | 297/585 [02:01<01:23,  3.44it/s] 51%|█████     | 298/585 [02:01<01:23,  3.44it/s] 51%|█████     | 299/585 [02:02<01:23,  3.45it/s] 51%|█████▏    | 300/585 [02:02<01:22,  3.45it/s] 51%|█████▏    | 301/585 [02:02<01:22,  3.44it/s] 52%|█████▏    | 302/585 [02:03<01:22,  3.44it/s] 52%|█████▏    | 303/585 [02:03<01:21,  3.44it/s] 52%|█████▏    | 304/585 [02:03<01:21,  3.45it/s] 52%|█████▏    | 305/585 [02:03<01:21,  3.45it/s] 52%|█████▏    | 306/585 [02:04<01:20,  3.45it/s] 52%|█████▏    | 307/585 [02:04<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:04<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:05<01:19,  3.45it/s] 53%|█████▎    | 310/585 [02:05<01:19,  3.45it/s] 53%|█████▎    | 311/585 [02:05<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:05<01:19,  3.45it/s] 54%|█████▎    | 313/585 [02:06<01:18,  3.45it/s] 54%|█████▎    | 314/585 [02:06<01:18,  3.44it/s] 54%|█████▍    | 315/585 [02:06<01:18,  3.45it/s] 54%|█████▍    | 316/585 [02:07<01:18,  3.45it/s] 54%|█████▍    | 317/585 [02:07<01:17,  3.45it/s] 54%|█████▍    | 318/585 [02:07<01:17,  3.45it/s] 55%|█████▍    | 319/585 [02:08<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:08<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:08<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:08<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:09<01:16,  3.44it/s] 55%|█████▌    | 324/585 [02:09<01:15,  3.44it/s] 56%|█████▌    | 325/585 [02:09<01:15,  3.45it/s] 56%|█████▌    | 326/585 [02:10<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:10<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:10<01:14,  3.45it/s] 56%|█████▌    | 329/585 [02:10<01:14,  3.44it/s] 56%|█████▋    | 330/585 [02:11<01:14,  3.44it/s] 57%|█████▋    | 331/585 [02:11<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:11<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:12<01:13,  3.45it/s] 57%|█████▋    | 334/585 [02:12<01:13,  3.43it/s] 57%|█████▋    | 335/585 [02:12<01:12,  3.43it/s] 57%|█████▋    | 336/585 [02:12<01:12,  3.44it/s] 58%|█████▊    | 337/585 [02:13<01:12,  3.44it/s] 58%|█████▊    | 338/585 [02:13<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:13<01:11,  3.45it/s] 58%|█████▊    | 340/585 [02:14<01:11,  3.45it/s] 58%|█████▊    | 341/585 [02:14<01:10,  3.45it/s] 58%|█████▊    | 342/585 [02:14<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:14<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:15<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:15<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:15<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:16<01:09,  3.45it/s] 59%|█████▉    | 348/585 [02:16<01:08,  3.45it/s] 60%|█████▉    | 349/585 [02:16<01:08,  3.45it/s] 60%|█████▉    | 350/585 [02:17<01:08,  3.45it/s] 60%|██████    | 351/585 [02:17<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:27:14,791 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:27:14,791 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:27:14,792 >>   Batch size = 8
{'eval_loss': 0.964785099029541, 'eval_runtime': 9.4083, 'eval_samples_per_second': 369.781, 'eval_steps_per_second': 46.236, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.85it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.29it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.49it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.71it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.26it/s][A
  8%|▊         | 33/435 [00:00<00:08, 46.76it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.53it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.28it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.37it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.37it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.43it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.55it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.52it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.46it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.38it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.14it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.08it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.09it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.29it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.34it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.34it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.30it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.39it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.35it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.21it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.17it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.16it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.11it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.25it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.28it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.34it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.21it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.21it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.25it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.07it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.15it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.18it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.27it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.30it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 46.33it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.16it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.16it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.18it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.16it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.09it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.17it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.27it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.24it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.33it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.24it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.25it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.14it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.23it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.05it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.15it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.29it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.32it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.30it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.21it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.08it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.16it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.10it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.18it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.18it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.31it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.31it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.29it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.26it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.18it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.19it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.08it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.19it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.14it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.16it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.30it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.33it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.29it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.27it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.13it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.14it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.17it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.25it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.30it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.22it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.34it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.17it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:26<01:07,  3.45it/s]
100%|██████████| 435/435 [00:09<00:00, 46.17it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:27:24,239 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 13:27:24,253 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:27:26,462 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:27:26,482 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:27:26,489 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:33<20:03,  5.16s/it] 60%|██████    | 353/585 [02:34<14:19,  3.70s/it] 61%|██████    | 354/585 [02:34<10:18,  2.68s/it] 61%|██████    | 355/585 [02:34<07:31,  1.96s/it] 61%|██████    | 356/585 [02:34<05:34,  1.46s/it] 61%|██████    | 357/585 [02:35<04:12,  1.11s/it] 61%|██████    | 358/585 [02:35<03:15,  1.16it/s] 61%|██████▏   | 359/585 [02:35<02:36,  1.45it/s] 62%|██████▏   | 360/585 [02:36<02:08,  1.75it/s] 62%|██████▏   | 361/585 [02:36<01:48,  2.06it/s] 62%|██████▏   | 362/585 [02:36<01:35,  2.34it/s] 62%|██████▏   | 363/585 [02:37<01:25,  2.59it/s] 62%|██████▏   | 364/585 [02:37<01:19,  2.80it/s] 62%|██████▏   | 365/585 [02:37<01:14,  2.97it/s] 63%|██████▎   | 366/585 [02:37<01:10,  3.10it/s] 63%|██████▎   | 367/585 [02:38<01:08,  3.20it/s] 63%|██████▎   | 368/585 [02:38<01:06,  3.27it/s] 63%|██████▎   | 369/585 [02:38<01:04,  3.33it/s] 63%|██████▎   | 370/585 [02:39<01:03,  3.37it/s] 63%|██████▎   | 371/585 [02:39<01:03,  3.40it/s] 64%|██████▎   | 372/585 [02:39<01:02,  3.42it/s] 64%|██████▍   | 373/585 [02:39<01:01,  3.43it/s] 64%|██████▍   | 374/585 [02:40<01:01,  3.44it/s] 64%|██████▍   | 375/585 [02:40<01:01,  3.44it/s] 64%|██████▍   | 376/585 [02:40<01:00,  3.44it/s] 64%|██████▍   | 377/585 [02:41<01:00,  3.44it/s] 65%|██████▍   | 378/585 [02:41<00:59,  3.45it/s] 65%|██████▍   | 379/585 [02:41<00:59,  3.45it/s] 65%|██████▍   | 380/585 [02:41<00:59,  3.45it/s] 65%|██████▌   | 381/585 [02:42<00:58,  3.46it/s] 65%|██████▌   | 382/585 [02:42<00:58,  3.46it/s] 65%|██████▌   | 383/585 [02:42<00:58,  3.46it/s] 66%|██████▌   | 384/585 [02:43<00:58,  3.46it/s] 66%|██████▌   | 385/585 [02:43<00:57,  3.46it/s] 66%|██████▌   | 386/585 [02:43<00:57,  3.45it/s] 66%|██████▌   | 387/585 [02:43<00:57,  3.45it/s] 66%|██████▋   | 388/585 [02:44<00:57,  3.45it/s] 66%|██████▋   | 389/585 [02:44<00:56,  3.46it/s] 67%|██████▋   | 390/585 [02:44<00:56,  3.46it/s] 67%|██████▋   | 391/585 [02:45<00:56,  3.46it/s] 67%|██████▋   | 392/585 [02:45<00:55,  3.46it/s] 67%|██████▋   | 393/585 [02:45<00:55,  3.46it/s] 67%|██████▋   | 394/585 [02:45<00:55,  3.46it/s] 68%|██████▊   | 395/585 [02:46<00:55,  3.45it/s] 68%|██████▊   | 396/585 [02:46<00:54,  3.45it/s] 68%|██████▊   | 397/585 [02:46<00:54,  3.44it/s] 68%|██████▊   | 398/585 [02:47<00:54,  3.44it/s] 68%|██████▊   | 399/585 [02:47<00:53,  3.45it/s] 68%|██████▊   | 400/585 [02:47<00:53,  3.45it/s] 69%|██████▊   | 401/585 [02:48<00:53,  3.45it/s] 69%|██████▊   | 402/585 [02:48<00:53,  3.45it/s] 69%|██████▉   | 403/585 [02:48<00:52,  3.45it/s] 69%|██████▉   | 404/585 [02:48<00:52,  3.45it/s] 69%|██████▉   | 405/585 [02:49<00:52,  3.45it/s] 69%|██████▉   | 406/585 [02:49<00:51,  3.45it/s] 70%|██████▉   | 407/585 [02:49<00:51,  3.45it/s] 70%|██████▉   | 408/585 [02:50<00:51,  3.45it/s] 70%|██████▉   | 409/585 [02:50<00:51,  3.45it/s] 70%|███████   | 410/585 [02:50<00:50,  3.45it/s] 70%|███████   | 411/585 [02:50<00:50,  3.45it/s] 70%|███████   | 412/585 [02:51<00:50,  3.45it/s] 71%|███████   | 413/585 [02:51<00:49,  3.45it/s] 71%|███████   | 414/585 [02:51<00:49,  3.45it/s] 71%|███████   | 415/585 [02:52<00:49,  3.45it/s] 71%|███████   | 416/585 [02:52<00:48,  3.45it/s] 71%|███████▏  | 417/585 [02:52<00:48,  3.45it/s] 71%|███████▏  | 418/585 [02:52<00:48,  3.46it/s] 72%|███████▏  | 419/585 [02:53<00:48,  3.44it/s] 72%|███████▏  | 420/585 [02:53<00:47,  3.45it/s] 72%|███████▏  | 421/585 [02:53<00:47,  3.45it/s] 72%|███████▏  | 422/585 [02:54<00:47,  3.45it/s] 72%|███████▏  | 423/585 [02:54<00:46,  3.45it/s] 72%|███████▏  | 424/585 [02:54<00:46,  3.45it/s] 73%|███████▎  | 425/585 [02:54<00:46,  3.45it/s] 73%|███████▎  | 426/585 [02:55<00:46,  3.45it/s] 73%|███████▎  | 427/585 [02:55<00:45,  3.45it/s] 73%|███████▎  | 428/585 [02:55<00:45,  3.45it/s] 73%|███████▎  | 429/585 [02:56<00:45,  3.45it/s] 74%|███████▎  | 430/585 [02:56<00:45,  3.44it/s] 74%|███████▎  | 431/585 [02:56<00:44,  3.45it/s] 74%|███████▍  | 432/585 [02:57<00:44,  3.45it/s] 74%|███████▍  | 433/585 [02:57<00:44,  3.45it/s] 74%|███████▍  | 434/585 [02:57<00:43,  3.45it/s] 74%|███████▍  | 435/585 [02:57<00:43,  3.45it/s] 75%|███████▍  | 436/585 [02:58<00:43,  3.45it/s] 75%|███████▍  | 437/585 [02:58<00:42,  3.45it/s] 75%|███████▍  | 438/585 [02:58<00:42,  3.45it/s] 75%|███████▌  | 439/585 [02:59<00:42,  3.45it/s] 75%|███████▌  | 440/585 [02:59<00:42,  3.45it/s] 75%|███████▌  | 441/585 [02:59<00:41,  3.45it/s] 76%|███████▌  | 442/585 [02:59<00:41,  3.45it/s] 76%|███████▌  | 443/585 [03:00<00:41,  3.45it/s] 76%|███████▌  | 444/585 [03:00<00:40,  3.44it/s] 76%|███████▌  | 445/585 [03:00<00:41,  3.39it/s] 76%|███████▌  | 446/585 [03:01<00:40,  3.40it/s] 76%|███████▋  | 447/585 [03:01<00:40,  3.42it/s] 77%|███████▋  | 448/585 [03:01<00:39,  3.43it/s] 77%|███████▋  | 449/585 [03:01<00:39,  3.43it/s] 77%|███████▋  | 450/585 [03:02<00:39,  3.44it/s] 77%|███████▋  | 451/585 [03:02<00:38,  3.44it/s] 77%|███████▋  | 452/585 [03:02<00:38,  3.45it/s] 77%|███████▋  | 453/585 [03:03<00:38,  3.45it/s] 78%|███████▊  | 454/585 [03:03<00:37,  3.45it/s] 78%|███████▊  | 455/585 [03:03<00:37,  3.44it/s] 78%|███████▊  | 456/585 [03:03<00:37,  3.44it/s] 78%|███████▊  | 457/585 [03:04<00:37,  3.44it/s] 78%|███████▊  | 458/585 [03:04<00:36,  3.45it/s] 78%|███████▊  | 459/585 [03:04<00:36,  3.45it/s] 79%|███████▊  | 460/585 [03:05<00:36,  3.45it/s] 79%|███████▉  | 461/585 [03:05<00:35,  3.45it/s] 79%|███████▉  | 462/585 [03:05<00:35,  3.45it/s] 79%|███████▉  | 463/585 [03:06<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:06<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:06<00:34,  3.44it/s] 80%|███████▉  | 466/585 [03:06<00:34,  3.44it/s] 80%|███████▉  | 467/585 [03:07<00:34,  3.44it/s] 80%|████████  | 468/585 [03:07<00:33,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 13:28:04,957 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:28:04,957 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:28:04,957 >>   Batch size = 8
{'eval_loss': 0.9701401591300964, 'eval_runtime': 9.4204, 'eval_samples_per_second': 369.305, 'eval_steps_per_second': 46.176, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.82it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.22it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.46it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.71it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.14it/s][A
  8%|▊         | 33/435 [00:00<00:08, 46.67it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.50it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.17it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.19it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.33it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.31it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 46.41it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.49it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.52it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.05it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.08it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.00it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.14it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.24it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.35it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.33it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.37it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.37it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.30it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.19it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.15it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.14it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.25it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.27it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.24it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.33it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.38it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.34it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.17it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.15it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.18it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.23it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.32it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.33it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 46.09it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.31it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.23it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.28it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.23it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.19it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.15it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.28it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.28it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.34it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.31it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.32it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.22it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.13it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.07it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.22it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.28it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.24it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.35it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.27it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.22it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.25it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.20it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.17it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.23it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.26it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.07it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.18it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.23it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.20it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.10it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.07it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.15it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.16it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.25it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.31it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.28it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.26it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.22it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.21it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.25it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.14it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.23it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.19it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.18it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.24it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.24it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:16<00:33,  3.44it/s]
100%|██████████| 435/435 [00:09<00:00, 46.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:28:14,413 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 13:28:14,433 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:28:16,643 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:28:16,656 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:28:16,664 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:24<10:20,  5.35s/it] 80%|████████  | 470/585 [03:24<07:20,  3.83s/it] 81%|████████  | 471/585 [03:25<05:15,  2.77s/it] 81%|████████  | 472/585 [03:25<03:48,  2.03s/it] 81%|████████  | 473/585 [03:25<02:48,  1.50s/it] 81%|████████  | 474/585 [03:26<02:06,  1.14s/it] 81%|████████  | 475/585 [03:26<01:37,  1.13it/s] 81%|████████▏ | 476/585 [03:26<01:16,  1.42it/s] 82%|████████▏ | 477/585 [03:26<01:02,  1.72it/s] 82%|████████▏ | 478/585 [03:27<00:52,  2.03it/s] 82%|████████▏ | 479/585 [03:27<00:45,  2.32it/s] 82%|████████▏ | 480/585 [03:27<00:40,  2.57it/s] 82%|████████▏ | 481/585 [03:28<00:37,  2.78it/s] 82%|████████▏ | 482/585 [03:28<00:34,  2.95it/s] 83%|████████▎ | 483/585 [03:28<00:32,  3.09it/s] 83%|████████▎ | 484/585 [03:28<00:31,  3.19it/s] 83%|████████▎ | 485/585 [03:29<00:30,  3.27it/s] 83%|████████▎ | 486/585 [03:29<00:29,  3.33it/s] 83%|████████▎ | 487/585 [03:29<00:29,  3.36it/s] 83%|████████▎ | 488/585 [03:30<00:28,  3.39it/s] 84%|████████▎ | 489/585 [03:30<00:28,  3.41it/s] 84%|████████▍ | 490/585 [03:30<00:27,  3.43it/s] 84%|████████▍ | 491/585 [03:30<00:27,  3.44it/s] 84%|████████▍ | 492/585 [03:31<00:27,  3.43it/s] 84%|████████▍ | 493/585 [03:31<00:26,  3.44it/s] 84%|████████▍ | 494/585 [03:31<00:26,  3.45it/s] 85%|████████▍ | 495/585 [03:32<00:26,  3.45it/s] 85%|████████▍ | 496/585 [03:32<00:25,  3.45it/s] 85%|████████▍ | 497/585 [03:32<00:25,  3.46it/s] 85%|████████▌ | 498/585 [03:33<00:25,  3.46it/s] 85%|████████▌ | 499/585 [03:33<00:24,  3.46it/s] 85%|████████▌ | 500/585 [03:33<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [03:33<00:24,  3.46it/s] 86%|████████▌ | 501/585 [03:33<00:24,  3.46it/s] 86%|████████▌ | 502/585 [03:34<00:24,  3.46it/s] 86%|████████▌ | 503/585 [03:34<00:23,  3.45it/s] 86%|████████▌ | 504/585 [03:34<00:23,  3.46it/s] 86%|████████▋ | 505/585 [03:35<00:23,  3.46it/s] 86%|████████▋ | 506/585 [03:35<00:22,  3.46it/s] 87%|████████▋ | 507/585 [03:35<00:22,  3.46it/s] 87%|████████▋ | 508/585 [03:35<00:22,  3.46it/s] 87%|████████▋ | 509/585 [03:36<00:21,  3.46it/s] 87%|████████▋ | 510/585 [03:36<00:21,  3.46it/s] 87%|████████▋ | 511/585 [03:36<00:21,  3.46it/s] 88%|████████▊ | 512/585 [03:37<00:21,  3.46it/s] 88%|████████▊ | 513/585 [03:37<00:20,  3.46it/s] 88%|████████▊ | 514/585 [03:37<00:20,  3.44it/s] 88%|████████▊ | 515/585 [03:37<00:20,  3.45it/s] 88%|████████▊ | 516/585 [03:38<00:20,  3.45it/s] 88%|████████▊ | 517/585 [03:38<00:19,  3.45it/s] 89%|████████▊ | 518/585 [03:38<00:19,  3.45it/s] 89%|████████▊ | 519/585 [03:39<00:19,  3.46it/s] 89%|████████▉ | 520/585 [03:39<00:18,  3.45it/s] 89%|████████▉ | 521/585 [03:39<00:18,  3.45it/s] 89%|████████▉ | 522/585 [03:39<00:18,  3.45it/s] 89%|████████▉ | 523/585 [03:40<00:17,  3.45it/s] 90%|████████▉ | 524/585 [03:40<00:17,  3.46it/s] 90%|████████▉ | 525/585 [03:40<00:17,  3.44it/s] 90%|████████▉ | 526/585 [03:41<00:17,  3.45it/s] 90%|█████████ | 527/585 [03:41<00:16,  3.45it/s] 90%|█████████ | 528/585 [03:41<00:16,  3.45it/s] 90%|█████████ | 529/585 [03:41<00:16,  3.45it/s] 91%|█████████ | 530/585 [03:42<00:15,  3.45it/s] 91%|█████████ | 531/585 [03:42<00:15,  3.45it/s] 91%|█████████ | 532/585 [03:42<00:15,  3.45it/s] 91%|█████████ | 533/585 [03:43<00:15,  3.45it/s] 91%|█████████▏| 534/585 [03:43<00:14,  3.45it/s] 91%|█████████▏| 535/585 [03:43<00:14,  3.45it/s] 92%|█████████▏| 536/585 [03:44<00:14,  3.44it/s] 92%|█████████▏| 537/585 [03:44<00:13,  3.45it/s] 92%|█████████▏| 538/585 [03:44<00:13,  3.45it/s] 92%|█████████▏| 539/585 [03:44<00:13,  3.45it/s] 92%|█████████▏| 540/585 [03:45<00:13,  3.45it/s] 92%|█████████▏| 541/585 [03:45<00:12,  3.45it/s] 93%|█████████▎| 542/585 [03:45<00:12,  3.45it/s] 93%|█████████▎| 543/585 [03:46<00:12,  3.45it/s] 93%|█████████▎| 544/585 [03:46<00:11,  3.45it/s] 93%|█████████▎| 545/585 [03:46<00:11,  3.45it/s] 93%|█████████▎| 546/585 [03:46<00:11,  3.45it/s] 94%|█████████▎| 547/585 [03:47<00:11,  3.44it/s] 94%|█████████▎| 548/585 [03:47<00:10,  3.44it/s] 94%|█████████▍| 549/585 [03:47<00:10,  3.45it/s] 94%|█████████▍| 550/585 [03:48<00:10,  3.45it/s] 94%|█████████▍| 551/585 [03:48<00:09,  3.45it/s] 94%|█████████▍| 552/585 [03:48<00:09,  3.45it/s] 95%|█████████▍| 553/585 [03:48<00:09,  3.45it/s] 95%|█████████▍| 554/585 [03:49<00:08,  3.45it/s] 95%|█████████▍| 555/585 [03:49<00:08,  3.45it/s] 95%|█████████▌| 556/585 [03:49<00:08,  3.45it/s] 95%|█████████▌| 557/585 [03:50<00:08,  3.45it/s] 95%|█████████▌| 558/585 [03:50<00:07,  3.45it/s] 96%|█████████▌| 559/585 [03:50<00:07,  3.45it/s] 96%|█████████▌| 560/585 [03:50<00:07,  3.45it/s] 96%|█████████▌| 561/585 [03:51<00:06,  3.45it/s] 96%|█████████▌| 562/585 [03:51<00:06,  3.42it/s] 96%|█████████▌| 563/585 [03:51<00:06,  3.43it/s] 96%|█████████▋| 564/585 [03:52<00:06,  3.44it/s] 97%|█████████▋| 565/585 [03:52<00:05,  3.44it/s] 97%|█████████▋| 566/585 [03:52<00:05,  3.44it/s] 97%|█████████▋| 567/585 [03:53<00:05,  3.44it/s] 97%|█████████▋| 568/585 [03:53<00:04,  3.45it/s] 97%|█████████▋| 569/585 [03:53<00:04,  3.45it/s] 97%|█████████▋| 570/585 [03:53<00:04,  3.45it/s] 98%|█████████▊| 571/585 [03:54<00:04,  3.45it/s] 98%|█████████▊| 572/585 [03:54<00:03,  3.45it/s] 98%|█████████▊| 573/585 [03:54<00:03,  3.43it/s] 98%|█████████▊| 574/585 [03:55<00:03,  3.44it/s] 98%|█████████▊| 575/585 [03:55<00:02,  3.44it/s] 98%|█████████▊| 576/585 [03:55<00:02,  3.45it/s] 99%|█████████▊| 577/585 [03:55<00:02,  3.44it/s] 99%|█████████▉| 578/585 [03:56<00:02,  3.45it/s] 99%|█████████▉| 579/585 [03:56<00:01,  3.45it/s] 99%|█████████▉| 580/585 [03:56<00:01,  3.45it/s] 99%|█████████▉| 581/585 [03:57<00:01,  3.45it/s] 99%|█████████▉| 582/585 [03:57<00:00,  3.45it/s]100%|█████████▉| 583/585 [03:57<00:00,  3.45it/s]100%|█████████▉| 584/585 [03:57<00:00,  3.43it/s]100%|██████████| 585/585 [03:58<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 13:28:55,690 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:28:55,690 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:28:55,690 >>   Batch size = 8
{'eval_loss': 0.975568950176239, 'eval_runtime': 9.4212, 'eval_samples_per_second': 369.275, 'eval_steps_per_second': 46.173, 'epoch': 4.0}
{'loss': 0.7484, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.99it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.35it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.55it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.71it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.28it/s][A
  8%|▊         | 33/435 [00:00<00:08, 46.82it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.49it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.36it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.32it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.36it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.41it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 46.47it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.43it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.39it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.26it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.18it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.03it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.07it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.15it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.31it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.42it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.46it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 42.89it/s][A
 28%|██▊       | 123/435 [00:02<00:07, 44.40it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 45.05it/s][A
 31%|███       | 133/435 [00:02<00:06, 45.40it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 45.64it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 45.81it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 45.97it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.21it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.19it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 45.83it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 45.94it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.16it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.16it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.23it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.25it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.22it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.35it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 46.32it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.14it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.08it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.08it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.23it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.20it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.33it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.35it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.28it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.22it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.21it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.20it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.11it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.19it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.14it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 46.21it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.35it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.35it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.24it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.21it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.10it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.16it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.13it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.28it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.23it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.31it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.34it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.14it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.07it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.11it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.20it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.26it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.18it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.28it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.28it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.35it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.30it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.21it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.11it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.05it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.18it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.20it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.23it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.19it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.25it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.15it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.17it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:07<00:00,  3.43it/s]
100%|██████████| 435/435 [00:09<00:00, 46.17it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:29:05,139 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 13:29:05,156 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:29:07,637 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:29:07,662 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:29:07,672 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:29:12,691 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:29:12,695 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117 (score: 0.9587679505348206).
                                                 100%|██████████| 585/585 [04:17<00:00,  3.43it/s]100%|██████████| 585/585 [04:17<00:00,  2.28it/s]
[INFO|trainer.py:1894] 2023-08-28 13:29:14,514 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 13:29:14,530 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:29:17,340 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:29:17,359 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:29:17,365 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:29:17,556 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:17,557 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:17,557 >>   train_loss               =     0.7439
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:17,557 >>   train_runtime            = 0:04:17.03
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:17,557 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:17,557 >>   train_samples_per_second =    145.893
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:17,557 >>   train_steps_per_second   =      2.276
{'eval_loss': 0.9776016473770142, 'eval_runtime': 9.432, 'eval_samples_per_second': 368.852, 'eval_steps_per_second': 46.12, 'epoch': 5.0}
{'train_runtime': 257.0371, 'train_samples_per_second': 145.893, 'train_steps_per_second': 2.276, 'train_loss': 0.7438609457423544, 'epoch': 5.0}
08/28/2023 13:29:17 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:29:17,606 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:29:17,606 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:29:17,607 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 58.32it/s]  3%|▎         | 12/435 [00:00<00:08, 51.05it/s]  4%|▍         | 18/435 [00:00<00:08, 48.98it/s]  5%|▌         | 23/435 [00:00<00:08, 48.36it/s]  6%|▋         | 28/435 [00:00<00:08, 47.86it/s]  8%|▊         | 33/435 [00:00<00:08, 47.50it/s]  9%|▊         | 38/435 [00:00<00:08, 47.32it/s] 10%|▉         | 43/435 [00:00<00:08, 46.90it/s] 11%|█         | 48/435 [00:01<00:08, 46.65it/s] 12%|█▏        | 53/435 [00:01<00:08, 46.58it/s] 13%|█▎        | 58/435 [00:01<00:08, 46.60it/s] 14%|█▍        | 63/435 [00:01<00:07, 46.76it/s] 16%|█▌        | 68/435 [00:01<00:07, 46.82it/s] 17%|█▋        | 73/435 [00:01<00:07, 46.87it/s] 18%|█▊        | 78/435 [00:01<00:07, 46.77it/s] 19%|█▉        | 83/435 [00:01<00:07, 46.63it/s] 20%|██        | 88/435 [00:01<00:07, 46.55it/s] 21%|██▏       | 93/435 [00:01<00:07, 46.48it/s] 23%|██▎       | 98/435 [00:02<00:07, 46.50it/s] 24%|██▎       | 103/435 [00:02<00:07, 46.59it/s] 25%|██▍       | 108/435 [00:02<00:07, 46.57it/s] 26%|██▌       | 113/435 [00:02<00:06, 46.66it/s] 27%|██▋       | 118/435 [00:02<00:06, 46.73it/s] 28%|██▊       | 123/435 [00:02<00:06, 46.82it/s] 29%|██▉       | 128/435 [00:02<00:06, 46.84it/s] 31%|███       | 133/435 [00:02<00:06, 46.62it/s] 32%|███▏      | 138/435 [00:02<00:06, 46.44it/s] 33%|███▎      | 143/435 [00:03<00:06, 46.34it/s] 34%|███▍      | 148/435 [00:03<00:06, 46.56it/s] 35%|███▌      | 153/435 [00:03<00:06, 46.61it/s] 36%|███▋      | 158/435 [00:03<00:05, 46.57it/s] 37%|███▋      | 163/435 [00:03<00:05, 46.64it/s] 39%|███▊      | 168/435 [00:03<00:05, 46.56it/s] 40%|███▉      | 173/435 [00:03<00:05, 46.66it/s] 41%|████      | 178/435 [00:03<00:05, 46.67it/s] 42%|████▏     | 183/435 [00:03<00:05, 46.48it/s] 43%|████▎     | 188/435 [00:04<00:05, 46.39it/s] 44%|████▍     | 193/435 [00:04<00:05, 46.38it/s] 46%|████▌     | 198/435 [00:04<00:05, 46.43it/s] 47%|████▋     | 203/435 [00:04<00:04, 46.60it/s] 48%|████▊     | 208/435 [00:04<00:04, 46.56it/s] 49%|████▉     | 213/435 [00:04<00:04, 46.58it/s] 50%|█████     | 218/435 [00:04<00:04, 46.70it/s] 51%|█████▏    | 223/435 [00:04<00:04, 46.73it/s] 52%|█████▏    | 228/435 [00:04<00:04, 46.52it/s] 54%|█████▎    | 233/435 [00:04<00:04, 46.40it/s] 55%|█████▍    | 238/435 [00:05<00:04, 46.45it/s] 56%|█████▌    | 243/435 [00:05<00:04, 46.58it/s] 57%|█████▋    | 248/435 [00:05<00:04, 46.63it/s] 58%|█████▊    | 253/435 [00:05<00:03, 46.62it/s] 59%|█████▉    | 258/435 [00:05<00:03, 46.60it/s] 60%|██████    | 263/435 [00:05<00:03, 46.56it/s] 62%|██████▏   | 268/435 [00:05<00:03, 46.61it/s] 63%|██████▎   | 273/435 [00:05<00:03, 46.66it/s] 64%|██████▍   | 278/435 [00:05<00:03, 46.60it/s] 65%|██████▌   | 283/435 [00:06<00:03, 46.54it/s] 66%|██████▌   | 288/435 [00:06<00:03, 46.53it/s] 67%|██████▋   | 293/435 [00:06<00:03, 46.48it/s] 69%|██████▊   | 298/435 [00:06<00:02, 46.60it/s] 70%|██████▉   | 303/435 [00:06<00:02, 46.60it/s] 71%|███████   | 308/435 [00:06<00:02, 46.67it/s] 72%|███████▏  | 313/435 [00:06<00:02, 46.74it/s] 73%|███████▎  | 318/435 [00:06<00:02, 46.73it/s] 74%|███████▍  | 323/435 [00:06<00:02, 46.59it/s] 75%|███████▌  | 328/435 [00:07<00:02, 46.63it/s] 77%|███████▋  | 333/435 [00:07<00:02, 46.46it/s] 78%|███████▊  | 338/435 [00:07<00:02, 46.43it/s] 79%|███████▉  | 343/435 [00:07<00:01, 46.48it/s] 80%|████████  | 348/435 [00:07<00:01, 46.58it/s] 81%|████████  | 353/435 [00:07<00:01, 46.55it/s] 82%|████████▏ | 358/435 [00:07<00:01, 46.68it/s] 83%|████████▎ | 363/435 [00:07<00:01, 46.70it/s] 85%|████████▍ | 368/435 [00:07<00:01, 46.68it/s] 86%|████████▌ | 373/435 [00:07<00:01, 46.67it/s] 87%|████████▋ | 378/435 [00:08<00:01, 46.68it/s] 88%|████████▊ | 383/435 [00:08<00:01, 46.49it/s] 89%|████████▉ | 388/435 [00:08<00:01, 46.36it/s] 90%|█████████ | 393/435 [00:08<00:00, 46.50it/s] 91%|█████████▏| 398/435 [00:08<00:00, 46.52it/s] 93%|█████████▎| 403/435 [00:08<00:00, 46.64it/s] 94%|█████████▍| 408/435 [00:08<00:00, 46.67it/s] 95%|█████████▍| 413/435 [00:08<00:00, 46.60it/s] 96%|█████████▌| 418/435 [00:08<00:00, 46.59it/s] 97%|█████████▋| 423/435 [00:09<00:00, 46.57it/s] 98%|█████████▊| 428/435 [00:09<00:00, 46.52it/s]100%|█████████▉| 433/435 [00:09<00:00, 46.39it/s]100%|██████████| 435/435 [00:09<00:00, 46.68it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:29:26,948 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:26,949 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:26,949 >>   eval_loss               =     0.9588
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:26,949 >>   eval_runtime            = 0:00:09.34
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:26,949 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:26,949 >>   eval_samples_per_second =    372.414
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:26,949 >>   eval_steps_per_second   =     46.565
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:29:26,949 >>   perplexity              =     2.6085
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:29:32,010 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:29:32,015 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:29:32,015 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:29:32,015 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:29:32,015 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:29:32,315 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:29:32,316 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:29:32,579 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:29:33,600 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:29:33,600 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:29:35,373 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:29:35,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:29:35,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:29:35,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:29:35,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:29:35,705 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:29:35,706 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:29:35,983 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:29:36,143 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:29:36,143 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.42it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.41it/s]Extractor Predicting: 6it [00:04,  1.40it/s]Extractor Predicting: 7it [00:04,  1.40it/s]Extractor Predicting: 8it [00:05,  1.41it/s]Extractor Predicting: 9it [00:06,  1.34it/s]Extractor Predicting: 10it [00:07,  1.40it/s]Extractor Predicting: 11it [00:07,  1.40it/s]Extractor Predicting: 12it [00:08,  1.38it/s]Extractor Predicting: 13it [00:09,  1.38it/s]Extractor Predicting: 14it [00:10,  1.38it/s]Extractor Predicting: 15it [00:10,  1.42it/s]Extractor Predicting: 16it [00:11,  1.40it/s]Extractor Predicting: 17it [00:12,  1.43it/s]Extractor Predicting: 18it [00:12,  1.47it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:14,  1.44it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.47it/s]Extractor Predicting: 23it [00:16,  1.45it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:17,  1.41it/s]Extractor Predicting: 26it [00:18,  1.39it/s]Extractor Predicting: 27it [00:19,  1.39it/s]Extractor Predicting: 28it [00:19,  1.42it/s]Extractor Predicting: 29it [00:20,  1.41it/s]Extractor Predicting: 30it [00:21,  1.38it/s]Extractor Predicting: 31it [00:21,  1.39it/s]Extractor Predicting: 32it [00:22,  1.40it/s]Extractor Predicting: 33it [00:23,  1.40it/s]Extractor Predicting: 34it [00:24,  1.43it/s]Extractor Predicting: 35it [00:24,  1.44it/s]Extractor Predicting: 36it [00:25,  1.43it/s]Extractor Predicting: 37it [00:26,  1.44it/s]Extractor Predicting: 38it [00:26,  1.45it/s]Extractor Predicting: 39it [00:27,  1.46it/s]Extractor Predicting: 40it [00:28,  1.44it/s]Extractor Predicting: 41it [00:28,  1.45it/s]Extractor Predicting: 42it [00:29,  1.45it/s]Extractor Predicting: 43it [00:30,  1.47it/s]Extractor Predicting: 44it [00:30,  1.48it/s]Extractor Predicting: 45it [00:31,  1.46it/s]Extractor Predicting: 46it [00:32,  1.45it/s]Extractor Predicting: 47it [00:33,  1.44it/s]Extractor Predicting: 48it [00:33,  1.45it/s]Extractor Predicting: 49it [00:34,  1.45it/s]Extractor Predicting: 50it [00:35,  1.45it/s]Extractor Predicting: 51it [00:35,  1.47it/s]Extractor Predicting: 52it [00:36,  1.46it/s]Extractor Predicting: 53it [00:37,  1.47it/s]Extractor Predicting: 54it [00:37,  1.46it/s]Extractor Predicting: 55it [00:38,  1.43it/s]Extractor Predicting: 56it [00:39,  1.45it/s]Extractor Predicting: 57it [00:39,  1.43it/s]Extractor Predicting: 58it [00:40,  1.43it/s]Extractor Predicting: 59it [00:41,  1.47it/s]Extractor Predicting: 60it [00:41,  1.50it/s]Extractor Predicting: 61it [00:42,  1.52it/s]Extractor Predicting: 62it [00:43,  1.49it/s]Extractor Predicting: 63it [00:43,  1.46it/s]Extractor Predicting: 64it [00:44,  1.47it/s]Extractor Predicting: 65it [00:45,  1.48it/s]Extractor Predicting: 66it [00:45,  1.48it/s]Extractor Predicting: 67it [00:46,  1.48it/s]Extractor Predicting: 68it [00:47,  1.48it/s]Extractor Predicting: 69it [00:47,  1.52it/s]Extractor Predicting: 70it [00:48,  1.52it/s]Extractor Predicting: 71it [00:49,  1.53it/s]Extractor Predicting: 72it [00:49,  1.48it/s]Extractor Predicting: 73it [00:50,  1.50it/s]Extractor Predicting: 74it [00:51,  1.48it/s]Extractor Predicting: 75it [00:52,  1.46it/s]Extractor Predicting: 76it [00:52,  1.44it/s]Extractor Predicting: 77it [00:53,  1.47it/s]Extractor Predicting: 78it [00:54,  1.49it/s]Extractor Predicting: 79it [00:54,  1.51it/s]Extractor Predicting: 80it [00:55,  1.50it/s]Extractor Predicting: 81it [00:56,  1.48it/s]Extractor Predicting: 82it [00:56,  1.48it/s]Extractor Predicting: 83it [00:57,  1.50it/s]Extractor Predicting: 84it [00:58,  1.50it/s]Extractor Predicting: 85it [00:58,  1.51it/s]Extractor Predicting: 86it [00:59,  1.52it/s]Extractor Predicting: 87it [00:59,  1.54it/s]Extractor Predicting: 88it [01:00,  1.41it/s]Extractor Predicting: 89it [01:01,  1.45it/s]Extractor Predicting: 90it [01:02,  1.48it/s]Extractor Predicting: 91it [01:02,  1.52it/s]Extractor Predicting: 92it [01:03,  1.57it/s]Extractor Predicting: 93it [01:03,  1.55it/s]Extractor Predicting: 94it [01:04,  1.54it/s]Extractor Predicting: 95it [01:05,  1.55it/s]Extractor Predicting: 96it [01:05,  1.55it/s]Extractor Predicting: 97it [01:06,  1.54it/s]Extractor Predicting: 98it [01:07,  1.54it/s]Extractor Predicting: 99it [01:07,  1.52it/s]Extractor Predicting: 100it [01:08,  1.48it/s]Extractor Predicting: 101it [01:09,  1.48it/s]Extractor Predicting: 102it [01:09,  1.56it/s]Extractor Predicting: 103it [01:10,  1.57it/s]Extractor Predicting: 104it [01:11,  1.55it/s]Extractor Predicting: 105it [01:11,  1.55it/s]Extractor Predicting: 106it [01:12,  1.54it/s]Extractor Predicting: 107it [01:13,  1.54it/s]Extractor Predicting: 108it [01:13,  1.54it/s]Extractor Predicting: 109it [01:14,  1.54it/s]Extractor Predicting: 110it [01:15,  1.54it/s]Extractor Predicting: 111it [01:15,  1.54it/s]Extractor Predicting: 112it [01:16,  1.55it/s]Extractor Predicting: 113it [01:16,  1.59it/s]Extractor Predicting: 114it [01:17,  1.59it/s]Extractor Predicting: 115it [01:18,  1.60it/s]Extractor Predicting: 116it [01:18,  1.56it/s]Extractor Predicting: 117it [01:19,  1.52it/s]Extractor Predicting: 118it [01:20,  1.52it/s]Extractor Predicting: 119it [01:20,  1.49it/s]Extractor Predicting: 120it [01:21,  1.50it/s]Extractor Predicting: 121it [01:22,  1.49it/s]Extractor Predicting: 122it [01:22,  1.47it/s]Extractor Predicting: 123it [01:23,  1.48it/s]Extractor Predicting: 124it [01:24,  1.49it/s]Extractor Predicting: 125it [01:24,  1.51it/s]Extractor Predicting: 126it [01:25,  1.48it/s]Extractor Predicting: 127it [01:26,  1.50it/s]Extractor Predicting: 128it [01:26,  1.51it/s]Extractor Predicting: 129it [01:27,  1.49it/s]Extractor Predicting: 130it [01:28,  1.47it/s]Extractor Predicting: 131it [01:28,  1.49it/s]Extractor Predicting: 132it [01:29,  1.48it/s]Extractor Predicting: 133it [01:30,  1.46it/s]Extractor Predicting: 134it [01:31,  1.45it/s]Extractor Predicting: 135it [01:31,  1.47it/s]Extractor Predicting: 136it [01:32,  1.45it/s]Extractor Predicting: 137it [01:33,  1.45it/s]Extractor Predicting: 138it [01:33,  1.43it/s]Extractor Predicting: 139it [01:34,  1.44it/s]Extractor Predicting: 140it [01:35,  1.43it/s]Extractor Predicting: 141it [01:35,  1.45it/s]Extractor Predicting: 142it [01:36,  1.44it/s]Extractor Predicting: 143it [01:37,  1.45it/s]Extractor Predicting: 144it [01:37,  1.78it/s]Extractor Predicting: 144it [01:37,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:22,022 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:22,025 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:22,026 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:22,026 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:22,026 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:31:22,634 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:31:22,635 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:31:23,231 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:31:24,252 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:31:24,252 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:27,124 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:27,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:27,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:27,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:27,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:31:27,769 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:31:27,770 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:31:28,326 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:31:28,475 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:31:28,475 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.24696356275303644,
  "recall": 0.017533774073009486,
  "score": 0.03274288781535158,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.62it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.58it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:17,  1.54it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:21,  1.57it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:24,  1.59it/s]Extractor Predicting: 39it [00:25,  1.57it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:29,  1.51it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:30,  1.52it/s]Extractor Predicting: 49it [00:31,  1.56it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:32,  1.52it/s]Extractor Predicting: 52it [00:33,  1.51it/s]Extractor Predicting: 53it [00:34,  1.52it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.55it/s]Extractor Predicting: 56it [00:36,  1.54it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:37,  1.51it/s]Extractor Predicting: 59it [00:38,  1.51it/s]Extractor Predicting: 60it [00:38,  1.49it/s]Extractor Predicting: 61it [00:39,  1.48it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:40,  1.52it/s]Extractor Predicting: 64it [00:41,  1.54it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:42,  1.49it/s]Extractor Predicting: 67it [00:43,  1.49it/s]Extractor Predicting: 68it [00:44,  1.49it/s]Extractor Predicting: 69it [00:44,  1.47it/s]Extractor Predicting: 70it [00:45,  1.48it/s]Extractor Predicting: 71it [00:46,  1.48it/s]Extractor Predicting: 72it [00:46,  1.47it/s]Extractor Predicting: 73it [00:47,  1.49it/s]Extractor Predicting: 74it [00:48,  1.49it/s]Extractor Predicting: 75it [00:48,  1.51it/s]Extractor Predicting: 76it [00:49,  1.48it/s]Extractor Predicting: 77it [00:50,  1.53it/s]Extractor Predicting: 78it [00:50,  1.52it/s]Extractor Predicting: 79it [00:51,  1.56it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:52,  1.57it/s]Extractor Predicting: 82it [00:53,  1.56it/s]Extractor Predicting: 83it [00:53,  1.54it/s]Extractor Predicting: 84it [00:54,  1.52it/s]Extractor Predicting: 85it [00:55,  1.48it/s]Extractor Predicting: 86it [00:56,  1.45it/s]Extractor Predicting: 87it [00:56,  1.47it/s]Extractor Predicting: 88it [00:57,  1.47it/s]Extractor Predicting: 89it [00:58,  1.44it/s]Extractor Predicting: 90it [00:58,  1.44it/s]Extractor Predicting: 91it [00:59,  1.43it/s]Extractor Predicting: 92it [01:00,  1.45it/s]Extractor Predicting: 93it [01:01,  1.36it/s]Extractor Predicting: 94it [01:01,  1.40it/s]Extractor Predicting: 95it [01:02,  1.43it/s]Extractor Predicting: 96it [01:03,  1.42it/s]Extractor Predicting: 97it [01:03,  1.41it/s]Extractor Predicting: 98it [01:04,  1.41it/s]Extractor Predicting: 99it [01:05,  1.44it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:06,  1.50it/s]Extractor Predicting: 102it [01:07,  1.47it/s]Extractor Predicting: 103it [01:07,  1.45it/s]Extractor Predicting: 104it [01:08,  1.48it/s]Extractor Predicting: 105it [01:09,  1.47it/s]Extractor Predicting: 106it [01:09,  1.47it/s]Extractor Predicting: 107it [01:10,  1.50it/s]Extractor Predicting: 108it [01:11,  1.47it/s]Extractor Predicting: 109it [01:11,  1.47it/s]Extractor Predicting: 110it [01:12,  1.46it/s]Extractor Predicting: 111it [01:13,  1.48it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:14,  1.48it/s]Extractor Predicting: 114it [01:15,  1.48it/s]Extractor Predicting: 115it [01:16,  1.44it/s]Extractor Predicting: 116it [01:16,  1.47it/s]Extractor Predicting: 117it [01:17,  1.46it/s]Extractor Predicting: 118it [01:18,  1.46it/s]Extractor Predicting: 119it [01:18,  1.46it/s]Extractor Predicting: 120it [01:19,  1.49it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:20,  1.51it/s]Extractor Predicting: 123it [01:21,  1.50it/s]Extractor Predicting: 124it [01:22,  1.52it/s]Extractor Predicting: 125it [01:22,  1.54it/s]Extractor Predicting: 126it [01:23,  1.53it/s]Extractor Predicting: 127it [01:24,  1.52it/s]Extractor Predicting: 128it [01:24,  1.55it/s]Extractor Predicting: 129it [01:25,  1.55it/s]Extractor Predicting: 130it [01:25,  1.52it/s]Extractor Predicting: 131it [01:26,  1.56it/s]Extractor Predicting: 132it [01:27,  1.56it/s]Extractor Predicting: 133it [01:27,  1.58it/s]Extractor Predicting: 134it [01:28,  1.53it/s]Extractor Predicting: 135it [01:29,  1.56it/s]Extractor Predicting: 136it [01:29,  1.57it/s]Extractor Predicting: 137it [01:30,  1.58it/s]Extractor Predicting: 138it [01:31,  1.57it/s]Extractor Predicting: 139it [01:31,  1.56it/s]Extractor Predicting: 140it [01:32,  1.57it/s]Extractor Predicting: 141it [01:33,  1.53it/s]Extractor Predicting: 142it [01:33,  1.53it/s]Extractor Predicting: 143it [01:34,  1.54it/s]Extractor Predicting: 144it [01:34,  1.53it/s]Extractor Predicting: 145it [01:35,  1.58it/s]Extractor Predicting: 146it [01:36,  1.59it/s]Extractor Predicting: 147it [01:36,  1.60it/s]Extractor Predicting: 148it [01:37,  1.63it/s]Extractor Predicting: 149it [01:38,  1.62it/s]Extractor Predicting: 150it [01:38,  1.63it/s]Extractor Predicting: 151it [01:39,  1.64it/s]Extractor Predicting: 152it [01:39,  1.65it/s]Extractor Predicting: 153it [01:40,  1.62it/s]Extractor Predicting: 154it [01:41,  1.62it/s]Extractor Predicting: 155it [01:41,  1.67it/s]Extractor Predicting: 156it [01:42,  1.65it/s]Extractor Predicting: 157it [01:42,  1.73it/s]Extractor Predicting: 158it [01:43,  1.75it/s]Extractor Predicting: 159it [01:43,  1.70it/s]Extractor Predicting: 160it [01:44,  1.65it/s]Extractor Predicting: 161it [01:45,  1.63it/s]Extractor Predicting: 162it [01:45,  1.65it/s]Extractor Predicting: 163it [01:46,  1.67it/s]Extractor Predicting: 164it [01:46,  1.67it/s]Extractor Predicting: 165it [01:47,  1.67it/s]Extractor Predicting: 166it [01:48,  1.64it/s]Extractor Predicting: 167it [01:48,  1.67it/s]Extractor Predicting: 168it [01:49,  1.65it/s]Extractor Predicting: 169it [01:49,  1.71it/s]Extractor Predicting: 170it [01:50,  1.69it/s]Extractor Predicting: 171it [01:51,  1.69it/s]Extractor Predicting: 172it [01:51,  1.61it/s]Extractor Predicting: 173it [01:52,  1.59it/s]Extractor Predicting: 174it [01:53,  1.55it/s]Extractor Predicting: 175it [01:53,  1.50it/s]Extractor Predicting: 176it [01:54,  1.50it/s]Extractor Predicting: 177it [01:55,  1.51it/s]Extractor Predicting: 178it [01:55,  1.48it/s]Extractor Predicting: 179it [01:56,  1.48it/s]Extractor Predicting: 180it [01:57,  1.49it/s]Extractor Predicting: 181it [01:57,  1.49it/s]Extractor Predicting: 182it [01:58,  1.48it/s]Extractor Predicting: 183it [01:59,  1.48it/s]Extractor Predicting: 184it [01:59,  1.47it/s]Extractor Predicting: 185it [02:00,  1.45it/s]Extractor Predicting: 186it [02:01,  1.45it/s]Extractor Predicting: 187it [02:02,  1.33it/s]Extractor Predicting: 188it [02:02,  1.37it/s]Extractor Predicting: 189it [02:03,  1.39it/s]Extractor Predicting: 190it [02:04,  1.43it/s]Extractor Predicting: 191it [02:05,  1.41it/s]Extractor Predicting: 192it [02:05,  1.41it/s]Extractor Predicting: 193it [02:06,  1.42it/s]Extractor Predicting: 194it [02:07,  1.42it/s]Extractor Predicting: 195it [02:07,  1.44it/s]Extractor Predicting: 196it [02:08,  1.45it/s]Extractor Predicting: 197it [02:09,  1.44it/s]Extractor Predicting: 198it [02:09,  1.47it/s]Extractor Predicting: 199it [02:10,  1.49it/s]Extractor Predicting: 200it [02:11,  1.46it/s]Extractor Predicting: 201it [02:11,  1.44it/s]Extractor Predicting: 202it [02:12,  1.51it/s]Extractor Predicting: 203it [02:13,  1.49it/s]Extractor Predicting: 204it [02:13,  1.51it/s]Extractor Predicting: 205it [02:14,  1.50it/s]Extractor Predicting: 206it [02:15,  1.49it/s]Extractor Predicting: 207it [02:15,  1.47it/s]Extractor Predicting: 208it [02:16,  1.47it/s]Extractor Predicting: 209it [02:17,  1.42it/s]Extractor Predicting: 210it [02:18,  1.43it/s]Extractor Predicting: 211it [02:18,  1.43it/s]Extractor Predicting: 212it [02:19,  1.43it/s]Extractor Predicting: 213it [02:20,  1.43it/s]Extractor Predicting: 214it [02:20,  1.42it/s]Extractor Predicting: 215it [02:21,  1.45it/s]Extractor Predicting: 216it [02:22,  1.43it/s]Extractor Predicting: 217it [02:22,  1.44it/s]Extractor Predicting: 218it [02:23,  1.45it/s]Extractor Predicting: 219it [02:24,  1.43it/s]Extractor Predicting: 220it [02:24,  1.45it/s]Extractor Predicting: 221it [02:25,  1.42it/s]Extractor Predicting: 222it [02:26,  1.41it/s]Extractor Predicting: 223it [02:27,  1.45it/s]Extractor Predicting: 224it [02:27,  1.45it/s]Extractor Predicting: 225it [02:28,  1.45it/s]Extractor Predicting: 226it [02:29,  1.44it/s]Extractor Predicting: 227it [02:29,  1.49it/s]Extractor Predicting: 228it [02:30,  1.48it/s]Extractor Predicting: 229it [02:31,  1.47it/s]Extractor Predicting: 230it [02:31,  1.52it/s]Extractor Predicting: 231it [02:32,  1.53it/s]Extractor Predicting: 232it [02:33,  1.56it/s]Extractor Predicting: 233it [02:33,  1.53it/s]Extractor Predicting: 234it [02:34,  1.53it/s]Extractor Predicting: 235it [02:35,  1.52it/s]Extractor Predicting: 236it [02:35,  1.50it/s]Extractor Predicting: 237it [02:36,  1.48it/s]Extractor Predicting: 238it [02:37,  1.49it/s]Extractor Predicting: 239it [02:37,  1.47it/s]Extractor Predicting: 240it [02:38,  1.48it/s]Extractor Predicting: 241it [02:39,  1.43it/s]Extractor Predicting: 242it [02:39,  1.46it/s]Extractor Predicting: 243it [02:40,  1.45it/s]Extractor Predicting: 244it [02:41,  1.47it/s]Extractor Predicting: 245it [02:41,  1.49it/s]Extractor Predicting: 246it [02:42,  1.48it/s]Extractor Predicting: 247it [02:43,  1.49it/s]Extractor Predicting: 248it [02:43,  1.49it/s]Extractor Predicting: 249it [02:44,  1.47it/s]Extractor Predicting: 250it [02:45,  1.49it/s]Extractor Predicting: 251it [02:45,  1.50it/s]Extractor Predicting: 252it [02:46,  1.50it/s]Extractor Predicting: 253it [02:47,  1.47it/s]Extractor Predicting: 254it [02:47,  1.49it/s]Extractor Predicting: 255it [02:48,  1.45it/s]Extractor Predicting: 256it [02:49,  1.41it/s]Extractor Predicting: 257it [02:50,  1.41it/s]Extractor Predicting: 258it [02:50,  1.41it/s]Extractor Predicting: 259it [02:51,  1.43it/s]Extractor Predicting: 260it [02:52,  1.42it/s]Extractor Predicting: 261it [02:52,  1.45it/s]Extractor Predicting: 262it [02:53,  1.44it/s]Extractor Predicting: 263it [02:54,  1.42it/s]Extractor Predicting: 264it [02:55,  1.42it/s]Extractor Predicting: 265it [02:55,  1.38it/s]Extractor Predicting: 266it [02:56,  1.38it/s]Extractor Predicting: 267it [02:57,  1.39it/s]Extractor Predicting: 268it [02:57,  1.37it/s]Extractor Predicting: 269it [02:58,  1.28it/s]Extractor Predicting: 270it [02:59,  1.34it/s]Extractor Predicting: 271it [03:00,  1.34it/s]Extractor Predicting: 272it [03:00,  1.37it/s]Extractor Predicting: 273it [03:01,  1.39it/s]Extractor Predicting: 274it [03:02,  1.42it/s]Extractor Predicting: 275it [03:02,  1.44it/s]Extractor Predicting: 276it [03:03,  1.48it/s]Extractor Predicting: 277it [03:04,  1.48it/s]Extractor Predicting: 278it [03:04,  1.48it/s]Extractor Predicting: 279it [03:05,  1.48it/s]Extractor Predicting: 280it [03:06,  1.43it/s]Extractor Predicting: 281it [03:07,  1.43it/s]Extractor Predicting: 282it [03:07,  1.52it/s]Extractor Predicting: 282it [03:07,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:43,885 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:43,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:43,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:43,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:43,890 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:34:44,523 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:34:44,524 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:34:45,110 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:34:46,175 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:34:46,176 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:49,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:49,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:49,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:49,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:49,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:34:49,648 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:34:49,649 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:34:50,220 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:34:50,373 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:34:50,373 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.40507152145643693,
  "recall": 0.09215976331360946,
  "score": 0.15015666425644733,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.40it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 5it [00:03,  1.43it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:34:54,243 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:34:54,244 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:34:54,247 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:34:54,248 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:34:54,251 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:34:57,488 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:34:57,496 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:34:57,506 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:34:57,507 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:34:57,517 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:34:57,521 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:34:57,521 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:34:57,521 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:34:57,521 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:34:57,521 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:34:57,521 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.42857142857142855,
  "recall": 0.0375,
  "score": 0.06896551724137931,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:34:57,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:58,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:59,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:00,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:00,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:01,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:02,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:03,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:04,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:04,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:05,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:06,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:07,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:07,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:08,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:09,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:10,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:11,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:11,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:12,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:13,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:14,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:14,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:15,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:18<04:21, 18.66s/it][WARNING|generation_utils.py:914] 2023-08-28 13:35:16,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:17,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:18,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:19,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:20,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:21,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:21,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:22,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:23,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:24,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:25,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:25,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:26,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:27,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:28,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:28,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:29,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:30,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:31,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:31,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:32,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:33,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:36<03:55, 18.09s/it][WARNING|generation_utils.py:914] 2023-08-28 13:35:34,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:34,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:35,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:36,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:37,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:37,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:38,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:39,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:39,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:40,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:41,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:42,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:43,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:43,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:44,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:45,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:46,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:46,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:47,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:48,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:48,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:49,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:50,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:53<03:31, 17.66s/it][WARNING|generation_utils.py:914] 2023-08-28 13:35:51,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:51,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:52,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:53,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:54,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:54,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:55,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:56,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:57,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:57,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:58,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:35:59,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:00,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:01,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:01,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:02,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:03,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:04,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:04,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:05,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:06,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:07,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:07,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:10<03:13, 17.57s/it][WARNING|generation_utils.py:914] 2023-08-28 13:36:08,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:09,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:10,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:11,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:11,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:12,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:13,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:14,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:14,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:15,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:16,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:17,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:18,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:18,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:19,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:20,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:21,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:22,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:22,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:23,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:24,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:27<02:50, 17.06s/it][WARNING|generation_utils.py:914] 2023-08-28 13:36:24,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:25,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:26,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:27,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:28,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:28,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:29,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:30,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:31,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:31,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:32,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:33,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:34,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:35,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:35,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:36,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:37,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:38,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:38,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:39,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:40,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:41,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:42,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:45<02:38, 17.58s/it][WARNING|generation_utils.py:914] 2023-08-28 13:36:43,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:44,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:44,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:45,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:46,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:47,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:47,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:48,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:49,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:50,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:50,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:51,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:52,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:53,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:53,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:54,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:55,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:56,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:56,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:57,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:58,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:01<02:15, 16.96s/it][WARNING|generation_utils.py:914] 2023-08-28 13:36:59,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:36:59,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:00,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:01,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:02,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:03,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:04,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:04,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:05,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:06,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:07,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:08,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:08,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:09,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:10,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:11,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:12,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:13,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:14,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:14,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:15,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:16,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:19<02:00, 17.28s/it][WARNING|generation_utils.py:914] 2023-08-28 13:37:17,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:18,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:18,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:19,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:20,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:21,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:21,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:22,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:23,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:24,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:24,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:25,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:26,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:27,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:28,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:28,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:29,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:30,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:31,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:32,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:32,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:35<01:42, 17.06s/it][WARNING|generation_utils.py:914] 2023-08-28 13:37:33,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:34,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:35,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:36,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:36,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:37,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:38,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:39,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:40,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:40,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:41,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:42,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:43,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:44,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:45,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:45,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:46,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:47,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:48,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:49,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:50,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:53<01:25, 17.14s/it][WARNING|generation_utils.py:914] 2023-08-28 13:37:50,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:51,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:52,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:53,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:53,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:54,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:55,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:56,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:57,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:58,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:58,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:37:59,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:00,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:01,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:01,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:02,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:03,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:04,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:05,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:06,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:06,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:07,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:10<01:09, 17.26s/it][WARNING|generation_utils.py:914] 2023-08-28 13:38:08,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:09,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:09,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:10,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:11,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:12,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:12,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:13,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:14,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:15,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:15,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:16,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:17,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:18,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:18,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:19,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:20,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:20,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:21,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:22,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:23,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:24,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:27<00:50, 16.96s/it][WARNING|generation_utils.py:914] 2023-08-28 13:38:24,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:25,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:26,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:26,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:27,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:28,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:28,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:29,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:30,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:31,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:32,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:33,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:33,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:34,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:35,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:35,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:36,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:37,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:37,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:38,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:39,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:42<00:32, 16.41s/it][WARNING|generation_utils.py:914] 2023-08-28 13:38:39,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:40,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:41,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:42,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:43,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:43,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:44,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:45,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:46,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:46,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:47,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:48,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:49,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:50,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:50,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:51,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:52,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:53,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:53,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:54,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:55,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:58<00:16, 16.37s/it][WARNING|generation_utils.py:914] 2023-08-28 13:38:56,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:57,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:57,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:58,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:38:59,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:00,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:01,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:01,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:02,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:03,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:04,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:05,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:05,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:06,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:07,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:08,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:09,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:09,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:10,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:11,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:12,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:13,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:39:13,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:16<00:00, 16.95s/it]Generating: 100%|██████████| 15/15 [04:16<00:00, 17.12s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:21,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:21,056 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:21,056 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:21,056 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:21,056 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:39:21,648 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:39:21,649 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:39:22,273 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:39:23,336 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:39:23,336 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:26,203 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:26,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:26,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:26,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:26,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:39:26,826 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:39:26,828 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:39:27,385 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:39:27,540 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:39:27,540 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 552, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : country .', 'success_rate': 0.7838541666666666, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : followed by .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8410326086956522, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8192934782608695, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : contains administrative territorial entity . Context : The city of St. Helena is under the Administration of the State of Montana . Head Entity : City of St. Helena , Tail Entity : State of Montana .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The city of Stuttgart is under the Administration of the German Ministry of Culture at the end of 2010 , in a post - civil war state . Head Entity : Mayor of Stuttgart , Tail Entity : German Ministry of Culture .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8607954545454546, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9360119047619048, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8565340909090909, 'errors': {'', "('Joseph II , Duke of York', 'occupation', '', 'In 1763 , Joseph II , Duke of York , appointed a member of the council to be the Governor of England .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : participating team . Context : On 31 March 2014 , the Brazilian national squad played a friendly at home against the Portuguese national football team , finishing 2–0 . Head Entity : Portuguese national football team , Tail Entity : Brazilian national football team .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8778409090909091, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8464673913043478, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 11796
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11896, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.31it/s]Extractor Estimating: 2it [00:01,  1.48it/s]Extractor Estimating: 3it [00:02,  1.51it/s]Extractor Estimating: 4it [00:02,  1.55it/s]Extractor Estimating: 5it [00:03,  1.55it/s]Extractor Estimating: 6it [00:03,  1.55it/s]Extractor Estimating: 7it [00:04,  1.47it/s]Extractor Estimating: 8it [00:05,  1.49it/s]Extractor Estimating: 9it [00:06,  1.48it/s]Extractor Estimating: 10it [00:06,  1.48it/s]Extractor Estimating: 11it [00:07,  1.48it/s]Extractor Estimating: 12it [00:07,  1.54it/s]Extractor Estimating: 13it [00:08,  1.52it/s]Extractor Estimating: 14it [00:09,  1.46it/s]Extractor Estimating: 15it [00:10,  1.46it/s]Extractor Estimating: 16it [00:10,  1.36it/s]Extractor Estimating: 17it [00:11,  1.41it/s]Extractor Estimating: 18it [00:12,  1.40it/s]Extractor Estimating: 19it [00:12,  1.48it/s]Extractor Estimating: 20it [00:13,  1.47it/s]Extractor Estimating: 21it [00:14,  1.45it/s]Extractor Estimating: 22it [00:15,  1.44it/s]Extractor Estimating: 23it [00:15,  1.44it/s]Extractor Estimating: 24it [00:16,  1.49it/s]Extractor Estimating: 25it [00:17,  1.40it/s]Extractor Estimating: 26it [00:17,  1.40it/s]Extractor Estimating: 27it [00:18,  1.37it/s]Extractor Estimating: 28it [00:19,  1.41it/s]Extractor Estimating: 29it [00:19,  1.42it/s]Extractor Estimating: 30it [00:20,  1.43it/s]Extractor Estimating: 31it [00:21,  1.45it/s]Extractor Estimating: 32it [00:22,  1.42it/s]Extractor Estimating: 33it [00:22,  1.43it/s]Extractor Estimating: 34it [00:23,  1.41it/s]Extractor Estimating: 35it [00:24,  1.36it/s]Extractor Estimating: 36it [00:24,  1.38it/s]Extractor Estimating: 37it [00:25,  1.35it/s]Extractor Estimating: 38it [00:26,  1.36it/s]Extractor Estimating: 39it [00:27,  1.40it/s]Extractor Estimating: 40it [00:27,  1.39it/s]Extractor Estimating: 41it [00:28,  1.37it/s]Extractor Estimating: 42it [00:29,  1.46it/s]Extractor Estimating: 43it [00:29,  1.47it/s]Extractor Estimating: 44it [00:30,  1.44it/s]Extractor Estimating: 45it [00:31,  1.45it/s]Extractor Estimating: 46it [00:31,  1.45it/s]Extractor Estimating: 47it [00:32,  1.46it/s]Extractor Estimating: 48it [00:33,  1.42it/s]Extractor Estimating: 49it [00:34,  1.39it/s]Extractor Estimating: 50it [00:34,  1.38it/s]Extractor Estimating: 51it [00:35,  1.37it/s]Extractor Estimating: 52it [00:36,  1.41it/s]Extractor Estimating: 53it [00:36,  1.44it/s]Extractor Estimating: 54it [00:37,  1.41it/s]Extractor Estimating: 55it [00:38,  1.40it/s]Extractor Estimating: 56it [00:39,  1.44it/s]Extractor Estimating: 57it [00:39,  1.46it/s]Extractor Estimating: 58it [00:40,  1.47it/s]Extractor Estimating: 59it [00:41,  1.49it/s]Extractor Estimating: 60it [00:41,  1.51it/s]Extractor Estimating: 61it [00:42,  1.49it/s]Extractor Estimating: 62it [00:43,  1.52it/s]Extractor Estimating: 63it [00:43,  1.53it/s]Extractor Estimating: 64it [00:44,  1.52it/s]Extractor Estimating: 65it [00:44,  1.52it/s]Extractor Estimating: 66it [00:45,  1.52it/s]Extractor Estimating: 67it [00:46,  1.52it/s]Extractor Estimating: 68it [00:46,  1.52it/s]Extractor Estimating: 69it [00:47,  1.53it/s]Extractor Estimating: 70it [00:48,  1.54it/s]Extractor Estimating: 71it [00:48,  1.51it/s]Extractor Estimating: 72it [00:49,  1.47it/s]Extractor Estimating: 73it [00:50,  1.50it/s]Extractor Estimating: 74it [00:51,  1.47it/s]Extractor Estimating: 75it [00:51,  1.47it/s]Extractor Estimating: 76it [00:52,  1.50it/s]Extractor Estimating: 77it [00:52,  1.50it/s]Extractor Estimating: 78it [00:53,  1.50it/s]Extractor Estimating: 79it [00:54,  1.38it/s]Extractor Estimating: 80it [00:55,  1.41it/s]Extractor Estimating: 81it [00:55,  1.45it/s]Extractor Estimating: 82it [00:56,  1.45it/s]Extractor Estimating: 83it [00:57,  1.46it/s]Extractor Estimating: 84it [00:57,  1.47it/s]Extractor Estimating: 85it [00:58,  1.41it/s]Extractor Estimating: 86it [00:59,  1.46it/s]Extractor Estimating: 87it [00:59,  1.51it/s]Extractor Estimating: 88it [01:00,  1.51it/s]Extractor Estimating: 89it [01:01,  1.51it/s]Extractor Estimating: 90it [01:01,  1.53it/s]Extractor Estimating: 91it [01:02,  1.54it/s]Extractor Estimating: 92it [01:03,  1.48it/s]Extractor Estimating: 93it [01:03,  1.52it/s]Extractor Estimating: 94it [01:04,  1.53it/s]Extractor Estimating: 95it [01:05,  1.55it/s]Extractor Estimating: 96it [01:05,  1.54it/s]Extractor Estimating: 97it [01:06,  1.58it/s]Extractor Estimating: 98it [01:06,  1.58it/s]Extractor Estimating: 99it [01:07,  1.51it/s]Extractor Estimating: 100it [01:08,  1.51it/s]Extractor Estimating: 101it [01:09,  1.49it/s]Extractor Estimating: 102it [01:09,  1.50it/s]Extractor Estimating: 103it [01:10,  1.50it/s]Extractor Estimating: 104it [01:11,  1.53it/s]Extractor Estimating: 105it [01:11,  1.57it/s]Extractor Estimating: 106it [01:12,  1.59it/s]Extractor Estimating: 107it [01:12,  1.64it/s]Extractor Estimating: 108it [01:13,  1.62it/s]Extractor Estimating: 109it [01:14,  1.57it/s]Extractor Estimating: 110it [01:14,  1.56it/s]Extractor Estimating: 111it [01:15,  1.48it/s]Extractor Estimating: 112it [01:16,  1.54it/s]Extractor Estimating: 113it [01:16,  1.51it/s]Extractor Estimating: 114it [01:17,  1.51it/s]Extractor Estimating: 115it [01:18,  1.49it/s]Extractor Estimating: 116it [01:18,  1.52it/s]Extractor Estimating: 117it [01:19,  1.54it/s]Extractor Estimating: 118it [01:20,  1.54it/s]Extractor Estimating: 119it [01:20,  1.57it/s]Extractor Estimating: 120it [01:21,  1.54it/s]Extractor Estimating: 121it [01:22,  1.50it/s]Extractor Estimating: 122it [01:22,  1.54it/s]Extractor Estimating: 123it [01:23,  1.57it/s]Extractor Estimating: 124it [01:23,  1.59it/s]Extractor Estimating: 125it [01:24,  1.59it/s]Extractor Estimating: 126it [01:25,  1.60it/s]Extractor Estimating: 127it [01:25,  1.56it/s]Extractor Estimating: 128it [01:26,  1.58it/s]Extractor Estimating: 129it [01:27,  1.60it/s]Extractor Estimating: 130it [01:27,  1.55it/s]Extractor Estimating: 131it [01:28,  1.55it/s]Extractor Estimating: 132it [01:29,  1.52it/s]Extractor Estimating: 133it [01:29,  1.61it/s]Extractor Estimating: 134it [01:30,  1.56it/s]Extractor Estimating: 135it [01:30,  1.57it/s]Extractor Estimating: 136it [01:31,  1.65it/s]Extractor Estimating: 137it [01:32,  1.58it/s]Extractor Estimating: 138it [01:32,  1.56it/s]Extractor Estimating: 139it [01:33,  1.60it/s]Extractor Estimating: 140it [01:33,  1.62it/s]Extractor Estimating: 141it [01:34,  1.63it/s]Extractor Estimating: 142it [01:35,  1.65it/s]Extractor Estimating: 143it [01:35,  1.61it/s]Extractor Estimating: 144it [01:36,  1.62it/s]Extractor Estimating: 145it [01:37,  1.58it/s]Extractor Estimating: 146it [01:37,  1.64it/s]Extractor Estimating: 147it [01:38,  1.61it/s]Extractor Estimating: 148it [01:38,  1.62it/s]Extractor Estimating: 149it [01:39,  1.54it/s]Extractor Estimating: 150it [01:40,  1.59it/s]Extractor Estimating: 151it [01:40,  1.60it/s]Extractor Estimating: 152it [01:41,  1.63it/s]Extractor Estimating: 153it [01:42,  1.61it/s]Extractor Estimating: 154it [01:42,  1.59it/s]Extractor Estimating: 155it [01:43,  1.55it/s]Extractor Estimating: 156it [01:43,  1.60it/s]Extractor Estimating: 157it [01:44,  1.56it/s]Extractor Estimating: 158it [01:45,  1.59it/s]Extractor Estimating: 159it [01:45,  1.55it/s]Extractor Estimating: 160it [01:46,  1.57it/s]Extractor Estimating: 161it [01:47,  1.51it/s]Extractor Estimating: 162it [01:47,  1.51it/s]Extractor Estimating: 163it [01:48,  1.45it/s]Extractor Estimating: 164it [01:49,  1.46it/s]Extractor Estimating: 165it [01:49,  1.49it/s]Extractor Estimating: 166it [01:50,  1.55it/s]Extractor Estimating: 167it [01:51,  1.57it/s]Extractor Estimating: 168it [01:51,  1.47it/s]Extractor Estimating: 169it [01:52,  1.50it/s]Extractor Estimating: 170it [01:53,  1.46it/s]Extractor Estimating: 171it [01:53,  1.52it/s]Extractor Estimating: 172it [01:54,  1.54it/s]Extractor Estimating: 173it [01:55,  1.57it/s]Extractor Estimating: 174it [01:55,  1.53it/s]Extractor Estimating: 175it [01:56,  1.54it/s]Extractor Estimating: 176it [01:57,  1.54it/s]Extractor Estimating: 177it [01:57,  1.46it/s]Extractor Estimating: 178it [01:58,  1.49it/s]Extractor Estimating: 179it [01:59,  1.51it/s]Extractor Estimating: 180it [01:59,  1.52it/s]Extractor Estimating: 181it [02:00,  1.53it/s]Extractor Estimating: 182it [02:01,  1.53it/s]Extractor Estimating: 183it [02:01,  1.54it/s]Extractor Estimating: 184it [02:02,  1.50it/s]Extractor Estimating: 185it [02:03,  1.47it/s]Extractor Estimating: 186it [02:03,  1.51it/s]Extractor Estimating: 187it [02:04,  1.48it/s]Extractor Estimating: 188it [02:05,  1.40it/s]Extractor Estimating: 189it [02:05,  1.45it/s]Extractor Estimating: 190it [02:06,  1.43it/s]Extractor Estimating: 191it [02:07,  1.43it/s]Extractor Estimating: 192it [02:08,  1.44it/s]Extractor Estimating: 193it [02:08,  1.46it/s]Extractor Estimating: 194it [02:09,  1.42it/s]Extractor Estimating: 195it [02:10,  1.42it/s]Extractor Estimating: 196it [02:10,  1.44it/s]Extractor Estimating: 197it [02:11,  1.37it/s]Extractor Estimating: 198it [02:12,  1.41it/s]Extractor Estimating: 199it [02:13,  1.40it/s]Extractor Estimating: 200it [02:13,  1.41it/s]Extractor Estimating: 201it [02:14,  1.42it/s]Extractor Estimating: 202it [02:15,  1.45it/s]Extractor Estimating: 203it [02:15,  1.47it/s]Extractor Estimating: 204it [02:16,  1.45it/s]Extractor Estimating: 205it [02:17,  1.45it/s]Extractor Estimating: 206it [02:17,  1.49it/s]Extractor Estimating: 207it [02:18,  1.47it/s]Extractor Estimating: 208it [02:19,  1.47it/s]Extractor Estimating: 209it [02:19,  1.49it/s]Extractor Estimating: 210it [02:20,  1.51it/s]Extractor Estimating: 211it [02:21,  1.52it/s]Extractor Estimating: 212it [02:21,  1.53it/s]Extractor Estimating: 213it [02:22,  1.56it/s]Extractor Estimating: 214it [02:23,  1.54it/s]Extractor Estimating: 215it [02:23,  1.57it/s]Extractor Estimating: 216it [02:24,  1.57it/s]Extractor Estimating: 217it [02:24,  1.54it/s]Extractor Estimating: 218it [02:25,  1.48it/s]Extractor Estimating: 219it [02:26,  1.46it/s]Extractor Estimating: 220it [02:27,  1.47it/s]Extractor Estimating: 221it [02:27,  1.46it/s]Extractor Estimating: 222it [02:28,  1.43it/s]Extractor Estimating: 223it [02:29,  1.49it/s]Extractor Estimating: 224it [02:29,  1.45it/s]Extractor Estimating: 225it [02:30,  1.46it/s]Extractor Estimating: 226it [02:31,  1.50it/s]Extractor Estimating: 227it [02:31,  1.50it/s]Extractor Estimating: 228it [02:32,  1.55it/s]Extractor Estimating: 229it [02:32,  1.60it/s]Extractor Estimating: 230it [02:33,  1.58it/s]Extractor Estimating: 231it [02:34,  1.61it/s]Extractor Estimating: 232it [02:34,  1.60it/s]Extractor Estimating: 233it [02:35,  1.56it/s]Extractor Estimating: 234it [02:36,  1.59it/s]Extractor Estimating: 235it [02:36,  1.62it/s]Extractor Estimating: 236it [02:37,  1.62it/s]Extractor Estimating: 237it [02:37,  1.58it/s]Extractor Estimating: 238it [02:38,  1.56it/s]Extractor Estimating: 239it [02:39,  1.61it/s]Extractor Estimating: 240it [02:39,  1.57it/s]Extractor Estimating: 241it [02:40,  1.55it/s]Extractor Estimating: 242it [02:41,  1.60it/s]Extractor Estimating: 243it [02:41,  1.58it/s]Extractor Estimating: 244it [02:42,  1.48it/s]Extractor Estimating: 245it [02:43,  1.47it/s]Extractor Estimating: 246it [02:43,  1.51it/s]Extractor Estimating: 247it [02:44,  1.48it/s]Extractor Estimating: 248it [02:45,  1.41it/s]Extractor Estimating: 249it [02:45,  1.47it/s]Extractor Estimating: 250it [02:46,  1.49it/s]Extractor Estimating: 251it [02:47,  1.39it/s]Extractor Estimating: 252it [02:48,  1.45it/s]Extractor Estimating: 253it [02:48,  1.49it/s]Extractor Estimating: 254it [02:49,  1.49it/s]Extractor Estimating: 255it [02:50,  1.52it/s]Extractor Estimating: 256it [02:50,  1.51it/s]Extractor Estimating: 257it [02:51,  1.57it/s]Extractor Estimating: 258it [02:51,  1.54it/s]Extractor Estimating: 259it [02:52,  1.53it/s]Extractor Estimating: 260it [02:53,  1.56it/s]Extractor Estimating: 261it [02:53,  1.55it/s]Extractor Estimating: 262it [02:54,  1.49it/s]Extractor Estimating: 263it [02:55,  1.48it/s]Extractor Estimating: 264it [02:55,  1.48it/s]Extractor Estimating: 265it [02:56,  1.45it/s]Extractor Estimating: 266it [02:57,  1.51it/s]Extractor Estimating: 267it [02:57,  1.49it/s]Extractor Estimating: 268it [02:58,  1.49it/s]Extractor Estimating: 269it [02:59,  1.46it/s]Extractor Estimating: 270it [03:00,  1.47it/s]Extractor Estimating: 271it [03:00,  1.50it/s]Extractor Estimating: 272it [03:01,  1.50it/s]Extractor Estimating: 273it [03:01,  1.54it/s]Extractor Estimating: 274it [03:02,  1.47it/s]Extractor Estimating: 275it [03:03,  1.47it/s]Extractor Estimating: 276it [03:04,  1.51it/s]Extractor Estimating: 277it [03:04,  1.52it/s]Extractor Estimating: 278it [03:05,  1.51it/s]Extractor Estimating: 279it [03:05,  1.55it/s]Extractor Estimating: 280it [03:06,  1.55it/s]Extractor Estimating: 281it [03:07,  1.56it/s]Extractor Estimating: 282it [03:07,  1.56it/s]Extractor Estimating: 283it [03:08,  1.53it/s]Extractor Estimating: 284it [03:09,  1.53it/s]Extractor Estimating: 285it [03:09,  1.50it/s]Extractor Estimating: 286it [03:10,  1.49it/s]Extractor Estimating: 287it [03:11,  1.50it/s]Extractor Estimating: 288it [03:11,  1.48it/s]Extractor Estimating: 289it [03:12,  1.50it/s]Extractor Estimating: 290it [03:13,  1.48it/s]Extractor Estimating: 291it [03:13,  1.48it/s]Extractor Estimating: 292it [03:14,  1.53it/s]Extractor Estimating: 293it [03:15,  1.50it/s]Extractor Estimating: 294it [03:15,  1.53it/s]Extractor Estimating: 295it [03:16,  1.53it/s]Extractor Estimating: 296it [03:17,  1.51it/s]Extractor Estimating: 297it [03:17,  1.52it/s]Extractor Estimating: 298it [03:18,  1.54it/s]Extractor Estimating: 299it [03:19,  1.58it/s]Extractor Estimating: 300it [03:19,  1.58it/s]Extractor Estimating: 301it [03:20,  1.66it/s]Extractor Estimating: 302it [03:20,  1.68it/s]Extractor Estimating: 303it [03:21,  1.65it/s]Extractor Estimating: 304it [03:22,  1.67it/s]Extractor Estimating: 305it [03:22,  1.65it/s]Extractor Estimating: 306it [03:23,  1.58it/s]Extractor Estimating: 307it [03:23,  1.57it/s]Extractor Estimating: 308it [03:24,  1.61it/s]Extractor Estimating: 309it [03:25,  1.60it/s]Extractor Estimating: 310it [03:25,  1.59it/s]Extractor Estimating: 311it [03:26,  1.57it/s]Extractor Estimating: 312it [03:27,  1.63it/s]Extractor Estimating: 313it [03:27,  1.61it/s]Extractor Estimating: 314it [03:28,  1.68it/s]Extractor Estimating: 315it [03:28,  1.62it/s]Extractor Estimating: 316it [03:29,  1.64it/s]Extractor Estimating: 317it [03:30,  1.66it/s]Extractor Estimating: 318it [03:30,  1.68it/s]Extractor Estimating: 319it [03:31,  1.67it/s]Extractor Estimating: 320it [03:31,  1.72it/s]Extractor Estimating: 321it [03:32,  1.71it/s]Extractor Estimating: 322it [03:32,  1.72it/s]Extractor Estimating: 323it [03:33,  1.68it/s]Extractor Estimating: 324it [03:34,  1.67it/s]Extractor Estimating: 325it [03:34,  1.63it/s]Extractor Estimating: 326it [03:35,  1.59it/s]Extractor Estimating: 327it [03:36,  1.54it/s]Extractor Estimating: 328it [03:37,  1.35it/s]Extractor Estimating: 329it [03:37,  1.43it/s]Extractor Estimating: 330it [03:38,  1.46it/s]Extractor Estimating: 331it [03:38,  1.55it/s]Extractor Estimating: 332it [03:39,  1.50it/s]Extractor Estimating: 333it [03:40,  1.37it/s]Extractor Estimating: 334it [03:41,  1.42it/s]Extractor Estimating: 335it [03:41,  1.49it/s]Extractor Estimating: 336it [03:42,  1.52it/s]Extractor Estimating: 337it [03:43,  1.51it/s]Extractor Estimating: 338it [03:43,  1.50it/s]Extractor Estimating: 339it [03:44,  1.54it/s]Extractor Estimating: 340it [03:45,  1.53it/s]Extractor Estimating: 341it [03:45,  1.52it/s]Extractor Estimating: 342it [03:46,  1.51it/s]Extractor Estimating: 343it [03:47,  1.50it/s]Extractor Estimating: 344it [03:47,  1.48it/s]Extractor Estimating: 345it [03:48,  1.48it/s]Extractor Estimating: 346it [03:49,  1.53it/s]Extractor Estimating: 347it [03:49,  1.50it/s]Extractor Estimating: 348it [03:50,  1.50it/s]Extractor Estimating: 349it [03:51,  1.45it/s]Extractor Estimating: 350it [03:51,  1.52it/s]Extractor Estimating: 351it [03:52,  1.56it/s]Extractor Estimating: 352it [03:52,  1.54it/s]Extractor Estimating: 353it [03:53,  1.49it/s]Extractor Estimating: 354it [03:54,  1.46it/s]Extractor Estimating: 355it [03:54,  1.53it/s]Extractor Estimating: 356it [03:55,  1.58it/s]Extractor Estimating: 357it [03:56,  1.49it/s]Extractor Estimating: 358it [03:56,  1.52it/s]Extractor Estimating: 359it [03:57,  1.52it/s]Extractor Estimating: 360it [03:58,  1.59it/s]Extractor Estimating: 361it [03:58,  1.59it/s]Extractor Estimating: 362it [03:59,  1.61it/s]Extractor Estimating: 363it [04:00,  1.58it/s]Extractor Estimating: 364it [04:00,  1.55it/s]Extractor Estimating: 365it [04:01,  1.56it/s]Extractor Estimating: 366it [04:01,  1.59it/s]Extractor Estimating: 367it [04:02,  1.57it/s]Extractor Estimating: 368it [04:03,  1.52it/s]Extractor Estimating: 369it [04:04,  1.50it/s]Extractor Estimating: 370it [04:04,  1.53it/s]Extractor Estimating: 371it [04:05,  1.53it/s]Extractor Estimating: 372it [04:05,  1.59it/s]Extractor Estimating: 373it [04:06,  1.66it/s]Extractor Estimating: 374it [04:07,  1.61it/s]Extractor Estimating: 375it [04:07,  1.69it/s]Extractor Estimating: 375it [04:07,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:47,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:47,783 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:47,784 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:47,784 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:47,784 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:43:48,414 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:43:48,415 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:43:49,008 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:43:50,119 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:43:50,119 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:53,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:53,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:53,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:53,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:53,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:43:53,830 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:43:53,831 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:43:54,397 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:43:54,559 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:43:54,559 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:06:50,817 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:06:50,822 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7492 mean pseudo reward: 0.9461123855517903
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 23323
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23423, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23423, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.096, loss:715.0620
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.146, loss:669.6577
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.099, loss:639.0095
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.111, loss:616.4971
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.104, loss:638.6391
>> valid entity prec:0.5089, rec:0.4526, f1:0.4791
>> valid relation prec:0.0598, rec:0.0112, f1:0.0189
>> valid relation with NER prec:0.0598, rec:0.0112, f1:0.0189
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.502, loss:638.5956
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.082, loss:603.5742
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.111, loss:618.8631
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.100, loss:645.5649
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.083, loss:608.7471
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4868, rec:0.4311, f1:0.4572
>> valid relation prec:0.0225, rec:0.0057, f1:0.0092
>> valid relation with NER prec:0.0225, rec:0.0057, f1:0.0092
g_step 1100, step 161, avg_time 2.506, loss:620.9756
g_step 1200, step 261, avg_time 1.104, loss:632.9291
g_step 1300, step 48, avg_time 1.093, loss:614.9099
g_step 1400, step 148, avg_time 1.101, loss:573.3425
g_step 1500, step 248, avg_time 1.106, loss:613.7865
>> valid entity prec:0.4938, rec:0.4806, f1:0.4871
>> valid relation prec:0.0392, rec:0.0101, f1:0.0160
>> valid relation with NER prec:0.0392, rec:0.0101, f1:0.0160
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.506, loss:579.0594
g_step 1700, step 135, avg_time 1.095, loss:543.4901
g_step 1800, step 235, avg_time 1.101, loss:564.8192
g_step 1900, step 22, avg_time 1.086, loss:576.7098
g_step 2000, step 122, avg_time 1.095, loss:547.7483
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4815, rec:0.4793, f1:0.4804
>> valid relation prec:0.0479, rec:0.0109, f1:0.0178
>> valid relation with NER prec:0.0479, rec:0.0109, f1:0.0178
g_step 2100, step 222, avg_time 2.503, loss:530.3276
g_step 2200, step 9, avg_time 1.113, loss:542.4960
g_step 2300, step 109, avg_time 1.105, loss:501.2439
g_step 2400, step 209, avg_time 1.114, loss:504.2564
g_step 2500, step 309, avg_time 1.096, loss:524.6189
>> valid entity prec:0.4860, rec:0.4851, f1:0.4856
>> valid relation prec:0.0534, rec:0.0175, f1:0.0264
>> valid relation with NER prec:0.0534, rec:0.0175, f1:0.0264
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.493, loss:470.1983
g_step 2700, step 196, avg_time 1.109, loss:492.2928
g_step 2800, step 296, avg_time 1.112, loss:499.9073
g_step 2900, step 83, avg_time 1.098, loss:471.6092
g_step 3000, step 183, avg_time 1.125, loss:468.5371
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5025, rec:0.4530, f1:0.4765
>> valid relation prec:0.0490, rec:0.0147, f1:0.0226
>> valid relation with NER prec:0.0490, rec:0.0147, f1:0.0226
g_step 3100, step 283, avg_time 2.477, loss:475.0098
g_step 3200, step 70, avg_time 1.104, loss:470.6450
g_step 3300, step 170, avg_time 1.090, loss:430.0736
g_step 3400, step 270, avg_time 1.112, loss:456.5618
g_step 3500, step 57, avg_time 1.098, loss:432.9680
>> valid entity prec:0.4445, rec:0.4999, f1:0.4706
>> valid relation prec:0.0366, rec:0.0155, f1:0.0218
>> valid relation with NER prec:0.0366, rec:0.0155, f1:0.0218
g_step 3600, step 157, avg_time 2.513, loss:457.6734
g_step 3700, step 257, avg_time 1.085, loss:443.2768
g_step 3800, step 44, avg_time 1.114, loss:399.7900
g_step 3900, step 144, avg_time 1.103, loss:409.3659
g_step 4000, step 244, avg_time 1.106, loss:434.5403
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4763, rec:0.4851, f1:0.4807
>> valid relation prec:0.0259, rec:0.0095, f1:0.0139
>> valid relation with NER prec:0.0259, rec:0.0095, f1:0.0139
g_step 4100, step 31, avg_time 2.505, loss:412.6504
g_step 4200, step 131, avg_time 1.105, loss:378.8475
g_step 4300, step 231, avg_time 1.091, loss:403.4137
g_step 4400, step 18, avg_time 1.094, loss:390.3907
g_step 4500, step 118, avg_time 1.108, loss:381.7892
>> valid entity prec:0.4925, rec:0.4684, f1:0.4801
>> valid relation prec:0.0521, rec:0.0181, f1:0.0269
>> valid relation with NER prec:0.0521, rec:0.0181, f1:0.0269
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 218, avg_time 2.504, loss:391.0422
g_step 4700, step 5, avg_time 1.093, loss:395.3514
g_step 4800, step 105, avg_time 1.100, loss:359.8444
g_step 4900, step 205, avg_time 1.115, loss:364.8227
g_step 5000, step 305, avg_time 1.096, loss:382.0342
learning rate was adjusted to 0.0008
>> valid entity prec:0.4813, rec:0.4395, f1:0.4594
>> valid relation prec:0.0321, rec:0.0101, f1:0.0153
>> valid relation with NER prec:0.0321, rec:0.0101, f1:0.0153
g_step 5100, step 92, avg_time 2.474, loss:349.8400
g_step 5200, step 192, avg_time 1.112, loss:357.2114
g_step 5300, step 292, avg_time 1.100, loss:368.7652
g_step 5400, step 79, avg_time 1.102, loss:324.2475
g_step 5500, step 179, avg_time 1.101, loss:338.6280
>> valid entity prec:0.4729, rec:0.4692, f1:0.4711
>> valid relation prec:0.0741, rec:0.0279, f1:0.0405
>> valid relation with NER prec:0.0741, rec:0.0279, f1:0.0405
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 279, avg_time 2.493, loss:349.6442
g_step 5700, step 66, avg_time 1.105, loss:311.2215
g_step 5800, step 166, avg_time 1.092, loss:335.1716
g_step 5900, step 266, avg_time 1.101, loss:345.9041
g_step 6000, step 53, avg_time 1.097, loss:313.5710
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4997, rec:0.4147, f1:0.4533
>> valid relation prec:0.0503, rec:0.0158, f1:0.0241
>> valid relation with NER prec:0.0503, rec:0.0158, f1:0.0241
g_step 6100, step 153, avg_time 2.491, loss:305.7903
g_step 6200, step 253, avg_time 1.103, loss:331.6145
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:06:50 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:06:50 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-06-50_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:06:51 - WARNING - datasets.builder -   Using custom data configuration default-89a47dc7bee3c2a9
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-89a47dc7bee3c2a9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:06:52,098 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:06:52,100 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:06:52,100 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:06:52,101 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:06:52,122 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:06:52,131 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:06:52,131 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:06:52,131 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:06:52,131 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:06:52,131 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:06:52,131 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:06:52,262 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:06:55,331 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:06:55,338 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-89a47dc7bee3c2a9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.19ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.01ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.33ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.47ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.55ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.60ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.62ba/s]100%|██████████| 8/8 [00:01<00:00,  5.44ba/s]100%|██████████| 8/8 [00:01<00:00,  4.72ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.05ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.30ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.42ba/s]100%|██████████| 4/4 [00:00<00:00,  5.56ba/s]100%|██████████| 4/4 [00:00<00:00,  5.02ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.89ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.63ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.05ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.17ba/s]100%|██████████| 8/8 [00:00<00:00, 10.62ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.29ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.75ba/s]100%|██████████| 4/4 [00:00<00:00, 11.09ba/s]
[INFO|trainer.py:414] 2023-08-28 16:06:59,365 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:06:59,377 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:06:59,377 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 16:06:59,377 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:06:59,377 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:06:59,377 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:06:59,377 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:06:59,377 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:52,  3.38it/s]  0%|          | 2/585 [00:00<02:49,  3.44it/s]  1%|          | 3/585 [00:00<02:47,  3.46it/s]  1%|          | 4/585 [00:01<02:47,  3.47it/s]  1%|          | 5/585 [00:01<02:46,  3.48it/s]  1%|          | 6/585 [00:01<02:46,  3.48it/s]  1%|          | 7/585 [00:02<02:45,  3.48it/s]  1%|▏         | 8/585 [00:02<02:45,  3.48it/s]  2%|▏         | 9/585 [00:02<02:45,  3.47it/s]  2%|▏         | 10/585 [00:02<02:45,  3.47it/s]  2%|▏         | 11/585 [00:03<02:45,  3.47it/s]  2%|▏         | 12/585 [00:03<02:45,  3.47it/s]  2%|▏         | 13/585 [00:03<02:44,  3.48it/s]  2%|▏         | 14/585 [00:04<02:44,  3.48it/s]  3%|▎         | 15/585 [00:04<02:43,  3.48it/s]  3%|▎         | 16/585 [00:04<02:43,  3.48it/s]  3%|▎         | 17/585 [00:04<02:43,  3.48it/s]  3%|▎         | 18/585 [00:05<02:42,  3.48it/s]  3%|▎         | 19/585 [00:05<02:42,  3.48it/s]  3%|▎         | 20/585 [00:05<02:42,  3.47it/s]  4%|▎         | 21/585 [00:06<02:42,  3.47it/s]  4%|▍         | 22/585 [00:06<02:42,  3.47it/s]  4%|▍         | 23/585 [00:06<02:41,  3.47it/s]  4%|▍         | 24/585 [00:06<02:41,  3.48it/s]  4%|▍         | 25/585 [00:07<02:41,  3.48it/s]  4%|▍         | 26/585 [00:07<02:40,  3.48it/s]  5%|▍         | 27/585 [00:07<02:40,  3.48it/s]  5%|▍         | 28/585 [00:08<02:40,  3.48it/s]  5%|▍         | 29/585 [00:08<02:39,  3.48it/s]  5%|▌         | 30/585 [00:08<02:39,  3.48it/s]  5%|▌         | 31/585 [00:08<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.48it/s]  6%|▌         | 33/585 [00:09<02:38,  3.47it/s]  6%|▌         | 34/585 [00:09<02:38,  3.48it/s]  6%|▌         | 35/585 [00:10<02:38,  3.48it/s]  6%|▌         | 36/585 [00:10<02:37,  3.48it/s]  6%|▋         | 37/585 [00:10<02:37,  3.48it/s]  6%|▋         | 38/585 [00:10<02:37,  3.47it/s]  7%|▋         | 39/585 [00:11<02:37,  3.48it/s]  7%|▋         | 40/585 [00:11<02:43,  3.33it/s]  7%|▋         | 41/585 [00:11<02:41,  3.36it/s]  7%|▋         | 42/585 [00:12<02:40,  3.39it/s]  7%|▋         | 43/585 [00:12<02:38,  3.41it/s]  8%|▊         | 44/585 [00:12<02:37,  3.43it/s]  8%|▊         | 45/585 [00:12<02:36,  3.45it/s]  8%|▊         | 46/585 [00:13<02:36,  3.45it/s]  8%|▊         | 47/585 [00:13<02:35,  3.46it/s]  8%|▊         | 48/585 [00:13<02:35,  3.46it/s]  8%|▊         | 49/585 [00:14<02:34,  3.47it/s]  9%|▊         | 50/585 [00:14<02:34,  3.47it/s]  9%|▊         | 51/585 [00:14<02:33,  3.47it/s]  9%|▉         | 52/585 [00:15<02:33,  3.48it/s]  9%|▉         | 53/585 [00:15<02:33,  3.47it/s]  9%|▉         | 54/585 [00:15<02:33,  3.47it/s]  9%|▉         | 55/585 [00:15<02:32,  3.47it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:31,  3.47it/s] 10%|█         | 59/585 [00:17<02:31,  3.48it/s] 10%|█         | 60/585 [00:17<02:31,  3.48it/s] 10%|█         | 61/585 [00:17<02:30,  3.48it/s] 11%|█         | 62/585 [00:17<02:30,  3.48it/s] 11%|█         | 63/585 [00:18<02:30,  3.48it/s] 11%|█         | 64/585 [00:18<02:30,  3.47it/s] 11%|█         | 65/585 [00:18<02:29,  3.47it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.47it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 68/585 [00:19<02:28,  3.47it/s] 12%|█▏        | 69/585 [00:19<02:28,  3.47it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.48it/s] 12%|█▏        | 71/585 [00:20<02:27,  3.47it/s] 12%|█▏        | 72/585 [00:20<02:27,  3.47it/s] 12%|█▏        | 73/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 76/585 [00:21<02:26,  3.46it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.47it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.47it/s] 14%|█▎        | 79/585 [00:22<02:25,  3.47it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 82/585 [00:23<02:24,  3.47it/s] 14%|█▍        | 83/585 [00:23<02:24,  3.47it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 86/585 [00:24<02:23,  3.47it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 90/585 [00:25<02:22,  3.46it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 92/585 [00:26<02:23,  3.43it/s] 16%|█▌        | 93/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.45it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.45it/s] 17%|█▋        | 97/585 [00:27<02:21,  3.45it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.45it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.45it/s] 17%|█▋        | 101/585 [00:29<02:20,  3.46it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.46it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:32<02:16,  3.45it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.45it/s] 20%|██        | 117/585 [00:33<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 16:07:33,203 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:07:33,203 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:07:33,203 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.12it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.12it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.63it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.99it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.61it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.39it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.13it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.66it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.68it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.68it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.62it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.69it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.71it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.64it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.71it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.63it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.57it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.55it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.49it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.51it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.49it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.58it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.67it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.71it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.59it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.46it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.47it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.54it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.58it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.58it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.63it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.63it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.61it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.65it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.58it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.51it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.51it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.54it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.39it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.50it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.52it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.52it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.53it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.58it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.55it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.56it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.51it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.57it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.52it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.60it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.60it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.64it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.56it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.50it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.54it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.55it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.60it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.59it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.59it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.42it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.44it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.48it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.56it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.42it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.49it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.42it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.43it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.65it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.56it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.53it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.56it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.43it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.52it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.58it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.58it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.63it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.57it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.57it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.55it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.54it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.53it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.56it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.61it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.58it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.56it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.55it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.55it/s][A 20%|██        | 117/585 [00:43<02:15,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:07:42,575 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 16:07:42,599 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:07:44,901 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:07:44,916 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:07:44,929 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:50<41:25,  5.32s/it] 20%|██        | 119/585 [00:51<29:37,  3.81s/it] 21%|██        | 120/585 [00:51<21:21,  2.76s/it] 21%|██        | 121/585 [00:51<15:35,  2.02s/it] 21%|██        | 122/585 [00:52<11:33,  1.50s/it] 21%|██        | 123/585 [00:52<08:44,  1.13s/it] 21%|██        | 124/585 [00:52<06:45,  1.14it/s] 21%|██▏       | 125/585 [00:52<05:23,  1.42it/s] 22%|██▏       | 126/585 [00:53<04:25,  1.73it/s] 22%|██▏       | 127/585 [00:53<03:44,  2.04it/s] 22%|██▏       | 128/585 [00:53<03:16,  2.33it/s] 22%|██▏       | 129/585 [00:54<02:56,  2.58it/s] 22%|██▏       | 130/585 [00:54<02:42,  2.79it/s] 22%|██▏       | 131/585 [00:54<02:33,  2.97it/s] 23%|██▎       | 132/585 [00:54<02:26,  3.10it/s] 23%|██▎       | 133/585 [00:55<02:21,  3.20it/s] 23%|██▎       | 134/585 [00:55<02:17,  3.28it/s] 23%|██▎       | 135/585 [00:55<02:15,  3.33it/s] 23%|██▎       | 136/585 [00:56<02:13,  3.37it/s] 23%|██▎       | 137/585 [00:56<02:11,  3.40it/s] 24%|██▎       | 138/585 [00:56<02:10,  3.42it/s] 24%|██▍       | 139/585 [00:56<02:09,  3.43it/s] 24%|██▍       | 140/585 [00:57<02:09,  3.43it/s] 24%|██▍       | 141/585 [00:57<02:09,  3.44it/s] 24%|██▍       | 142/585 [00:57<02:08,  3.45it/s] 24%|██▍       | 143/585 [00:58<02:07,  3.45it/s] 25%|██▍       | 144/585 [00:58<02:07,  3.45it/s] 25%|██▍       | 145/585 [00:58<02:07,  3.46it/s] 25%|██▍       | 146/585 [00:58<02:06,  3.46it/s] 25%|██▌       | 147/585 [00:59<02:06,  3.46it/s] 25%|██▌       | 148/585 [00:59<02:06,  3.46it/s] 25%|██▌       | 149/585 [00:59<02:05,  3.46it/s] 26%|██▌       | 150/585 [01:00<02:05,  3.47it/s] 26%|██▌       | 151/585 [01:00<02:06,  3.44it/s] 26%|██▌       | 152/585 [01:00<02:05,  3.45it/s] 26%|██▌       | 153/585 [01:00<02:05,  3.45it/s] 26%|██▋       | 154/585 [01:01<02:04,  3.46it/s] 26%|██▋       | 155/585 [01:01<02:04,  3.46it/s] 27%|██▋       | 156/585 [01:01<02:04,  3.46it/s] 27%|██▋       | 157/585 [01:02<02:03,  3.46it/s] 27%|██▋       | 158/585 [01:02<02:03,  3.46it/s] 27%|██▋       | 159/585 [01:02<02:03,  3.46it/s] 27%|██▋       | 160/585 [01:02<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:03<02:02,  3.46it/s] 28%|██▊       | 162/585 [01:03<02:02,  3.45it/s] 28%|██▊       | 163/585 [01:03<02:02,  3.45it/s] 28%|██▊       | 164/585 [01:04<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:04<02:01,  3.46it/s] 28%|██▊       | 166/585 [01:04<02:01,  3.46it/s] 29%|██▊       | 167/585 [01:04<02:00,  3.46it/s] 29%|██▊       | 168/585 [01:05<02:00,  3.46it/s] 29%|██▉       | 169/585 [01:05<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:05<01:59,  3.46it/s] 29%|██▉       | 171/585 [01:06<01:59,  3.46it/s] 29%|██▉       | 172/585 [01:06<01:59,  3.46it/s] 30%|██▉       | 173/585 [01:06<01:59,  3.44it/s] 30%|██▉       | 174/585 [01:07<01:59,  3.45it/s] 30%|██▉       | 175/585 [01:07<01:58,  3.46it/s] 30%|███       | 176/585 [01:07<01:58,  3.46it/s] 30%|███       | 177/585 [01:07<01:58,  3.46it/s] 30%|███       | 178/585 [01:08<01:57,  3.45it/s] 31%|███       | 179/585 [01:08<01:57,  3.46it/s] 31%|███       | 180/585 [01:08<01:57,  3.46it/s] 31%|███       | 181/585 [01:09<01:56,  3.46it/s] 31%|███       | 182/585 [01:09<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:09<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:09<01:56,  3.44it/s] 32%|███▏      | 185/585 [01:10<01:56,  3.44it/s] 32%|███▏      | 186/585 [01:10<01:55,  3.45it/s] 32%|███▏      | 187/585 [01:10<01:55,  3.45it/s] 32%|███▏      | 188/585 [01:11<01:54,  3.46it/s] 32%|███▏      | 189/585 [01:11<01:54,  3.46it/s] 32%|███▏      | 190/585 [01:11<02:01,  3.26it/s] 33%|███▎      | 191/585 [01:12<01:59,  3.30it/s] 33%|███▎      | 192/585 [01:12<01:57,  3.35it/s] 33%|███▎      | 193/585 [01:12<01:56,  3.38it/s] 33%|███▎      | 194/585 [01:12<01:55,  3.40it/s] 33%|███▎      | 195/585 [01:13<01:54,  3.39it/s] 34%|███▎      | 196/585 [01:13<01:53,  3.41it/s] 34%|███▎      | 197/585 [01:13<01:53,  3.42it/s] 34%|███▍      | 198/585 [01:14<01:52,  3.43it/s] 34%|███▍      | 199/585 [01:14<01:52,  3.44it/s] 34%|███▍      | 200/585 [01:14<01:51,  3.44it/s] 34%|███▍      | 201/585 [01:14<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:15<01:51,  3.45it/s] 35%|███▍      | 203/585 [01:15<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:15<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:16<01:50,  3.45it/s] 35%|███▌      | 206/585 [01:16<01:49,  3.45it/s] 35%|███▌      | 207/585 [01:16<01:49,  3.45it/s] 36%|███▌      | 208/585 [01:16<01:49,  3.45it/s] 36%|███▌      | 209/585 [01:17<01:48,  3.45it/s] 36%|███▌      | 210/585 [01:17<01:48,  3.46it/s] 36%|███▌      | 211/585 [01:17<01:48,  3.45it/s] 36%|███▌      | 212/585 [01:18<01:47,  3.45it/s] 36%|███▋      | 213/585 [01:18<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:18<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:18<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:19<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:19<01:47,  3.43it/s] 37%|███▋      | 218/585 [01:19<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:20<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:20<01:45,  3.44it/s] 38%|███▊      | 221/585 [01:20<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:20<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:21<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:21<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:21<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:22<01:44,  3.45it/s] 39%|███▉      | 227/585 [01:22<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:22<01:43,  3.44it/s] 39%|███▉      | 229/585 [01:23<01:43,  3.44it/s] 39%|███▉      | 230/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 231/585 [01:23<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:23<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:24<01:42,  3.45it/s] 40%|████      | 234/585 [01:24<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 16:08:23,901 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:08:23,901 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:08:23,901 >>   Batch size = 8
{'eval_loss': 0.9685636162757874, 'eval_runtime': 9.3544, 'eval_samples_per_second': 371.909, 'eval_steps_per_second': 46.502, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.76it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.17it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.56it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.91it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.49it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.00it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.79it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.21it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.37it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.32it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.46it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.57it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.65it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.52it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.47it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.30it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.21it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.17it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.25it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.40it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.43it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.51it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.61it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.50it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.42it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.37it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.19it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.26it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.27it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.40it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.50it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.57it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.63it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.46it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.40it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.29it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.33it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.32it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.34it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 46.33it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.50it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.53it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.54it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.49it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.51it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.45it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.31it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.35it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.39it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.35it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.50it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.52it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.40it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.37it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.35it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.40it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.42it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.41it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.32it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.33it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.40it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.40it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.43it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.41it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.35it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.42it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.42it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.41it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.43it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.45it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.50it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.39it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.33it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.35it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.38it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.43it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.46it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.43it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.37it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.43it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.51it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.47it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.39it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.48it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.34it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.40it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:33<01:41,  3.45it/s]
100%|██████████| 435/435 [00:09<00:00, 46.40it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:08:33,307 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 16:08:33,329 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:08:35,848 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:08:35,882 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:08:35,892 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:42<32:34,  5.58s/it] 40%|████      | 236/585 [01:42<23:16,  4.00s/it] 41%|████      | 237/585 [01:43<16:44,  2.89s/it] 41%|████      | 238/585 [01:43<12:11,  2.11s/it] 41%|████      | 239/585 [01:43<09:00,  1.56s/it] 41%|████      | 240/585 [01:43<06:46,  1.18s/it] 41%|████      | 241/585 [01:44<05:13,  1.10it/s] 41%|████▏     | 242/585 [01:44<04:08,  1.38it/s] 42%|████▏     | 243/585 [01:44<03:23,  1.68it/s] 42%|████▏     | 244/585 [01:45<02:51,  1.99it/s] 42%|████▏     | 245/585 [01:45<02:29,  2.28it/s] 42%|████▏     | 246/585 [01:45<02:13,  2.54it/s] 42%|████▏     | 247/585 [01:45<02:02,  2.75it/s] 42%|████▏     | 248/585 [01:46<01:54,  2.94it/s] 43%|████▎     | 249/585 [01:46<01:49,  3.08it/s] 43%|████▎     | 250/585 [01:46<01:45,  3.18it/s] 43%|████▎     | 251/585 [01:47<01:42,  3.26it/s] 43%|████▎     | 252/585 [01:47<01:40,  3.32it/s] 43%|████▎     | 253/585 [01:47<01:38,  3.36it/s] 43%|████▎     | 254/585 [01:47<01:37,  3.39it/s] 44%|████▎     | 255/585 [01:48<01:36,  3.41it/s] 44%|████▍     | 256/585 [01:48<01:35,  3.43it/s] 44%|████▍     | 257/585 [01:48<01:35,  3.44it/s] 44%|████▍     | 258/585 [01:49<01:35,  3.43it/s] 44%|████▍     | 259/585 [01:49<01:34,  3.43it/s] 44%|████▍     | 260/585 [01:49<01:34,  3.44it/s] 45%|████▍     | 261/585 [01:49<01:34,  3.45it/s] 45%|████▍     | 262/585 [01:50<01:33,  3.45it/s] 45%|████▍     | 263/585 [01:50<01:33,  3.45it/s] 45%|████▌     | 264/585 [01:50<01:32,  3.46it/s] 45%|████▌     | 265/585 [01:51<01:32,  3.46it/s] 45%|████▌     | 266/585 [01:51<01:32,  3.46it/s] 46%|████▌     | 267/585 [01:51<01:31,  3.46it/s] 46%|████▌     | 268/585 [01:51<01:31,  3.46it/s] 46%|████▌     | 269/585 [01:52<01:31,  3.45it/s] 46%|████▌     | 270/585 [01:52<01:31,  3.45it/s] 46%|████▋     | 271/585 [01:52<01:30,  3.46it/s] 46%|████▋     | 272/585 [01:53<01:30,  3.45it/s] 47%|████▋     | 273/585 [01:53<01:30,  3.45it/s] 47%|████▋     | 274/585 [01:53<01:30,  3.46it/s] 47%|████▋     | 275/585 [01:53<01:29,  3.46it/s] 47%|████▋     | 276/585 [01:54<01:29,  3.46it/s] 47%|████▋     | 277/585 [01:54<01:29,  3.46it/s] 48%|████▊     | 278/585 [01:54<01:28,  3.46it/s] 48%|████▊     | 279/585 [01:55<01:28,  3.46it/s] 48%|████▊     | 280/585 [01:55<01:28,  3.46it/s] 48%|████▊     | 281/585 [01:55<01:27,  3.46it/s] 48%|████▊     | 282/585 [01:56<01:27,  3.46it/s] 48%|████▊     | 283/585 [01:56<01:27,  3.46it/s] 49%|████▊     | 284/585 [01:56<01:27,  3.46it/s] 49%|████▊     | 285/585 [01:56<01:26,  3.46it/s] 49%|████▉     | 286/585 [01:57<01:26,  3.46it/s] 49%|████▉     | 287/585 [01:57<01:26,  3.46it/s] 49%|████▉     | 288/585 [01:57<01:25,  3.46it/s] 49%|████▉     | 289/585 [01:58<01:25,  3.46it/s] 50%|████▉     | 290/585 [01:58<01:25,  3.46it/s] 50%|████▉     | 291/585 [01:58<01:25,  3.45it/s] 50%|████▉     | 292/585 [01:58<01:24,  3.45it/s] 50%|█████     | 293/585 [01:59<01:24,  3.45it/s] 50%|█████     | 294/585 [01:59<01:24,  3.45it/s] 50%|█████     | 295/585 [01:59<01:23,  3.46it/s] 51%|█████     | 296/585 [02:00<01:23,  3.46it/s] 51%|█████     | 297/585 [02:00<01:23,  3.46it/s] 51%|█████     | 298/585 [02:00<01:23,  3.46it/s] 51%|█████     | 299/585 [02:00<01:22,  3.46it/s] 51%|█████▏    | 300/585 [02:01<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:01<01:22,  3.46it/s] 52%|█████▏    | 302/585 [02:01<01:22,  3.42it/s] 52%|█████▏    | 303/585 [02:02<01:22,  3.43it/s] 52%|█████▏    | 304/585 [02:02<01:21,  3.43it/s] 52%|█████▏    | 305/585 [02:02<01:21,  3.44it/s] 52%|█████▏    | 306/585 [02:02<01:20,  3.45it/s] 52%|█████▏    | 307/585 [02:03<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:03<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:03<01:19,  3.45it/s] 53%|█████▎    | 310/585 [02:04<01:19,  3.45it/s] 53%|█████▎    | 311/585 [02:04<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:04<01:19,  3.45it/s] 54%|█████▎    | 313/585 [02:05<01:19,  3.42it/s] 54%|█████▎    | 314/585 [02:05<01:18,  3.43it/s] 54%|█████▍    | 315/585 [02:05<01:18,  3.44it/s] 54%|█████▍    | 316/585 [02:05<01:18,  3.44it/s] 54%|█████▍    | 317/585 [02:06<01:17,  3.45it/s] 54%|█████▍    | 318/585 [02:06<01:17,  3.45it/s] 55%|█████▍    | 319/585 [02:06<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:07<01:16,  3.46it/s] 55%|█████▍    | 321/585 [02:07<01:16,  3.46it/s] 55%|█████▌    | 322/585 [02:07<01:16,  3.46it/s] 55%|█████▌    | 323/585 [02:07<01:15,  3.45it/s] 55%|█████▌    | 324/585 [02:08<01:15,  3.44it/s] 56%|█████▌    | 325/585 [02:08<01:15,  3.45it/s] 56%|█████▌    | 326/585 [02:08<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:09<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:09<01:14,  3.45it/s] 56%|█████▌    | 329/585 [02:09<01:14,  3.46it/s] 56%|█████▋    | 330/585 [02:09<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:10<01:13,  3.46it/s] 57%|█████▋    | 332/585 [02:10<01:13,  3.46it/s] 57%|█████▋    | 333/585 [02:10<01:12,  3.46it/s] 57%|█████▋    | 334/585 [02:11<01:12,  3.46it/s] 57%|█████▋    | 335/585 [02:11<01:12,  3.43it/s] 57%|█████▋    | 336/585 [02:11<01:13,  3.37it/s] 58%|█████▊    | 337/585 [02:11<01:13,  3.37it/s] 58%|█████▊    | 338/585 [02:12<01:12,  3.39it/s] 58%|█████▊    | 339/585 [02:12<01:12,  3.41it/s] 58%|█████▊    | 340/585 [02:12<01:11,  3.42it/s] 58%|█████▊    | 341/585 [02:13<01:11,  3.43it/s] 58%|█████▊    | 342/585 [02:13<01:10,  3.43it/s] 59%|█████▊    | 343/585 [02:13<01:10,  3.44it/s] 59%|█████▉    | 344/585 [02:14<01:10,  3.44it/s] 59%|█████▉    | 345/585 [02:14<01:09,  3.44it/s] 59%|█████▉    | 346/585 [02:14<01:10,  3.40it/s] 59%|█████▉    | 347/585 [02:14<01:09,  3.41it/s] 59%|█████▉    | 348/585 [02:15<01:09,  3.43it/s] 60%|█████▉    | 349/585 [02:15<01:08,  3.43it/s] 60%|█████▉    | 350/585 [02:15<01:08,  3.44it/s] 60%|██████    | 351/585 [02:16<01:07,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 16:09:15,486 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:09:15,486 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:09:15,486 >>   Batch size = 8
{'eval_loss': 0.9756200313568115, 'eval_runtime': 9.3848, 'eval_samples_per_second': 370.707, 'eval_steps_per_second': 46.352, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.77it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.35it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.56it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.88it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.33it/s][A
  8%|▊         | 33/435 [00:00<00:08, 46.97it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.65it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.22it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.34it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.32it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.33it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 46.47it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.45it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.49it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.44it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.30it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.17it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.13it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.26it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.40it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.51it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.48it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.50it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.33it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.23it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.27it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.26it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.21it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.25it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.29it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.42it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.42it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.42it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.45it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.34it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.21it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.35it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.20it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.37it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.50it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.46it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.46it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.43it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.33it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.32it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.31it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.37it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.29it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.35it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.41it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.44it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.47it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.44it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.37it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.22it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.20it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.32it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.29it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.32it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.48it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.39it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.44it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.32it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.27it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.35it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.32it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.33it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.38it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.40it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.43it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.45it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.45it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.24it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.21it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.40it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.38it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.44it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.47it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.36it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.42it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.40it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.33it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.37it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.39it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.40it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.34it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:25<01:07,  3.44it/s]
100%|██████████| 435/435 [00:09<00:00, 46.34it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:09:24,900 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 16:09:24,930 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:09:27,141 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:09:27,165 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:09:27,172 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:33<20:49,  5.36s/it] 60%|██████    | 353/585 [02:33<14:51,  3.84s/it] 61%|██████    | 354/585 [02:33<10:41,  2.78s/it] 61%|██████    | 355/585 [02:34<07:46,  2.03s/it] 61%|██████    | 356/585 [02:34<05:45,  1.51s/it] 61%|██████    | 357/585 [02:34<04:20,  1.14s/it] 61%|██████    | 358/585 [02:35<03:21,  1.13it/s] 61%|██████▏   | 359/585 [02:35<02:39,  1.41it/s] 62%|██████▏   | 360/585 [02:35<02:10,  1.72it/s] 62%|██████▏   | 361/585 [02:35<01:50,  2.03it/s] 62%|██████▏   | 362/585 [02:36<01:36,  2.31it/s] 62%|██████▏   | 363/585 [02:36<01:26,  2.57it/s] 62%|██████▏   | 364/585 [02:36<01:19,  2.78it/s] 62%|██████▏   | 365/585 [02:37<01:14,  2.95it/s] 63%|██████▎   | 366/585 [02:37<01:10,  3.09it/s] 63%|██████▎   | 367/585 [02:37<01:08,  3.19it/s] 63%|██████▎   | 368/585 [02:37<01:06,  3.27it/s] 63%|██████▎   | 369/585 [02:38<01:04,  3.33it/s] 63%|██████▎   | 370/585 [02:38<01:03,  3.36it/s] 63%|██████▎   | 371/585 [02:38<01:03,  3.39it/s] 64%|██████▎   | 372/585 [02:39<01:02,  3.41it/s] 64%|██████▍   | 373/585 [02:39<01:01,  3.43it/s] 64%|██████▍   | 374/585 [02:39<01:01,  3.44it/s] 64%|██████▍   | 375/585 [02:39<01:01,  3.43it/s] 64%|██████▍   | 376/585 [02:40<01:00,  3.44it/s] 64%|██████▍   | 377/585 [02:40<01:00,  3.44it/s] 65%|██████▍   | 378/585 [02:40<00:59,  3.45it/s] 65%|██████▍   | 379/585 [02:41<00:59,  3.45it/s] 65%|██████▍   | 380/585 [02:41<00:59,  3.46it/s] 65%|██████▌   | 381/585 [02:41<00:58,  3.46it/s] 65%|██████▌   | 382/585 [02:41<00:58,  3.46it/s] 65%|██████▌   | 383/585 [02:42<00:58,  3.46it/s] 66%|██████▌   | 384/585 [02:42<00:58,  3.46it/s] 66%|██████▌   | 385/585 [02:42<00:57,  3.46it/s] 66%|██████▌   | 386/585 [02:43<00:57,  3.45it/s] 66%|██████▌   | 387/585 [02:43<00:57,  3.45it/s] 66%|██████▋   | 388/585 [02:43<00:57,  3.45it/s] 66%|██████▋   | 389/585 [02:43<00:56,  3.46it/s] 67%|██████▋   | 390/585 [02:44<00:56,  3.46it/s] 67%|██████▋   | 391/585 [02:44<00:56,  3.46it/s] 67%|██████▋   | 392/585 [02:44<00:55,  3.46it/s] 67%|██████▋   | 393/585 [02:45<00:55,  3.46it/s] 67%|██████▋   | 394/585 [02:45<00:55,  3.46it/s] 68%|██████▊   | 395/585 [02:45<00:54,  3.46it/s] 68%|██████▊   | 396/585 [02:45<00:54,  3.46it/s] 68%|██████▊   | 397/585 [02:46<00:54,  3.44it/s] 68%|██████▊   | 398/585 [02:46<00:54,  3.44it/s] 68%|██████▊   | 399/585 [02:46<00:53,  3.45it/s] 68%|██████▊   | 400/585 [02:47<00:53,  3.45it/s] 69%|██████▊   | 401/585 [02:47<00:53,  3.45it/s] 69%|██████▊   | 402/585 [02:47<00:52,  3.46it/s] 69%|██████▉   | 403/585 [02:48<00:52,  3.46it/s] 69%|██████▉   | 404/585 [02:48<00:52,  3.46it/s] 69%|██████▉   | 405/585 [02:48<00:52,  3.45it/s] 69%|██████▉   | 406/585 [02:48<00:51,  3.46it/s] 70%|██████▉   | 407/585 [02:49<00:51,  3.46it/s] 70%|██████▉   | 408/585 [02:49<00:51,  3.45it/s] 70%|██████▉   | 409/585 [02:49<00:50,  3.45it/s] 70%|███████   | 410/585 [02:50<00:50,  3.45it/s] 70%|███████   | 411/585 [02:50<00:50,  3.46it/s] 70%|███████   | 412/585 [02:50<00:50,  3.46it/s] 71%|███████   | 413/585 [02:50<00:49,  3.46it/s] 71%|███████   | 414/585 [02:51<00:49,  3.46it/s] 71%|███████   | 415/585 [02:51<00:49,  3.46it/s] 71%|███████   | 416/585 [02:51<00:48,  3.46it/s] 71%|███████▏  | 417/585 [02:52<00:48,  3.46it/s] 71%|███████▏  | 418/585 [02:52<00:48,  3.46it/s] 72%|███████▏  | 419/585 [02:52<00:48,  3.43it/s] 72%|███████▏  | 420/585 [02:52<00:47,  3.44it/s] 72%|███████▏  | 421/585 [02:53<00:47,  3.44it/s] 72%|███████▏  | 422/585 [02:53<00:47,  3.45it/s] 72%|███████▏  | 423/585 [02:53<00:46,  3.45it/s] 72%|███████▏  | 424/585 [02:54<00:46,  3.45it/s] 73%|███████▎  | 425/585 [02:54<00:46,  3.45it/s] 73%|███████▎  | 426/585 [02:54<00:46,  3.46it/s] 73%|███████▎  | 427/585 [02:54<00:45,  3.46it/s] 73%|███████▎  | 428/585 [02:55<00:45,  3.46it/s] 73%|███████▎  | 429/585 [02:55<00:45,  3.46it/s] 74%|███████▎  | 430/585 [02:55<00:45,  3.44it/s] 74%|███████▎  | 431/585 [02:56<00:44,  3.45it/s] 74%|███████▍  | 432/585 [02:56<00:44,  3.45it/s] 74%|███████▍  | 433/585 [02:56<00:44,  3.45it/s] 74%|███████▍  | 434/585 [02:57<00:43,  3.46it/s] 74%|███████▍  | 435/585 [02:57<00:43,  3.45it/s] 75%|███████▍  | 436/585 [02:57<00:43,  3.45it/s] 75%|███████▍  | 437/585 [02:57<00:42,  3.45it/s] 75%|███████▍  | 438/585 [02:58<00:42,  3.45it/s] 75%|███████▌  | 439/585 [02:58<00:42,  3.45it/s] 75%|███████▌  | 440/585 [02:58<00:41,  3.45it/s] 75%|███████▌  | 441/585 [02:59<00:41,  3.46it/s] 76%|███████▌  | 442/585 [02:59<00:41,  3.46it/s] 76%|███████▌  | 443/585 [02:59<00:41,  3.46it/s] 76%|███████▌  | 444/585 [02:59<00:41,  3.44it/s] 76%|███████▌  | 445/585 [03:00<00:40,  3.44it/s] 76%|███████▌  | 446/585 [03:00<00:40,  3.45it/s] 76%|███████▋  | 447/585 [03:00<00:39,  3.45it/s] 77%|███████▋  | 448/585 [03:01<00:39,  3.45it/s] 77%|███████▋  | 449/585 [03:01<00:39,  3.45it/s] 77%|███████▋  | 450/585 [03:01<00:39,  3.45it/s] 77%|███████▋  | 451/585 [03:01<00:38,  3.45it/s] 77%|███████▋  | 452/585 [03:02<00:38,  3.45it/s] 77%|███████▋  | 453/585 [03:02<00:38,  3.46it/s] 78%|███████▊  | 454/585 [03:02<00:37,  3.45it/s] 78%|███████▊  | 455/585 [03:03<00:37,  3.42it/s] 78%|███████▊  | 456/585 [03:03<00:37,  3.43it/s] 78%|███████▊  | 457/585 [03:03<00:37,  3.44it/s] 78%|███████▊  | 458/585 [03:03<00:36,  3.44it/s] 78%|███████▊  | 459/585 [03:04<00:36,  3.45it/s] 79%|███████▊  | 460/585 [03:04<00:36,  3.45it/s] 79%|███████▉  | 461/585 [03:04<00:35,  3.45it/s] 79%|███████▉  | 462/585 [03:05<00:35,  3.45it/s] 79%|███████▉  | 463/585 [03:05<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:05<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:05<00:34,  3.45it/s] 80%|███████▉  | 466/585 [03:06<00:34,  3.43it/s] 80%|███████▉  | 467/585 [03:06<00:34,  3.44it/s] 80%|████████  | 468/585 [03:06<00:33,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 16:10:06,284 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:10:06,284 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:10:06,284 >>   Batch size = 8
{'eval_loss': 0.9876888394355774, 'eval_runtime': 9.3956, 'eval_samples_per_second': 370.279, 'eval_steps_per_second': 46.298, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.23it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.16it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.47it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.76it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.37it/s][A
  8%|▊         | 33/435 [00:00<00:08, 46.96it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.62it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.19it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.27it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.36it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.42it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 46.43it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.52it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.56it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.39it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.37it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.27it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.27it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.32it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.37it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.43it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.34it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.44it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.47it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.41it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.31it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.24it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.25it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.39it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.41it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.41it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.44it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.39it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.37it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.34it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.30it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.24it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.40it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.36it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 46.31it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.31it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.30it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.39it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 44.77it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 45.26it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.56it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 45.68it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.91it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.10it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.19it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.27it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.20it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.12it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.21it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.28it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.32it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.32it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.26it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.34it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.37it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.44it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.36it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.25it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.29it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.38it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.29it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.29it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.35it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.31it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.29it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.26it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.28it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.33it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.40it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.35it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.38it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.42it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.38it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.36it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.34it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.31it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.27it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.21it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.22it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.25it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.37it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.37it/s][A 80%|████████  | 468/585 [03:16<00:33,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:10:15,717 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 16:10:15,751 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:10:18,238 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:10:18,255 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:10:18,263 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:24<10:20,  5.35s/it] 80%|████████  | 470/585 [03:24<07:20,  3.83s/it] 81%|████████  | 471/585 [03:24<05:15,  2.77s/it] 81%|████████  | 472/585 [03:24<03:48,  2.02s/it] 81%|████████  | 473/585 [03:25<02:48,  1.50s/it] 81%|████████  | 474/585 [03:25<02:06,  1.14s/it] 81%|████████  | 475/585 [03:25<01:37,  1.13it/s] 81%|████████▏ | 476/585 [03:26<01:16,  1.42it/s] 82%|████████▏ | 477/585 [03:26<01:02,  1.72it/s] 82%|████████▏ | 478/585 [03:26<00:52,  2.03it/s] 82%|████████▏ | 479/585 [03:26<00:45,  2.32it/s] 82%|████████▏ | 480/585 [03:27<00:40,  2.57it/s] 82%|████████▏ | 481/585 [03:27<00:37,  2.78it/s] 82%|████████▏ | 482/585 [03:27<00:34,  2.95it/s] 83%|████████▎ | 483/585 [03:28<00:33,  3.09it/s] 83%|████████▎ | 484/585 [03:28<00:31,  3.19it/s] 83%|████████▎ | 485/585 [03:28<00:30,  3.27it/s] 83%|████████▎ | 486/585 [03:28<00:29,  3.32it/s] 83%|████████▎ | 487/585 [03:29<00:29,  3.36it/s] 83%|████████▎ | 488/585 [03:29<00:28,  3.39it/s] 84%|████████▎ | 489/585 [03:29<00:28,  3.41it/s] 84%|████████▍ | 490/585 [03:30<00:27,  3.43it/s] 84%|████████▍ | 491/585 [03:30<00:27,  3.44it/s] 84%|████████▍ | 492/585 [03:30<00:27,  3.43it/s] 84%|████████▍ | 493/585 [03:30<00:26,  3.44it/s] 84%|████████▍ | 494/585 [03:31<00:26,  3.44it/s] 85%|████████▍ | 495/585 [03:31<00:26,  3.45it/s] 85%|████████▍ | 496/585 [03:31<00:25,  3.45it/s] 85%|████████▍ | 497/585 [03:32<00:25,  3.45it/s] 85%|████████▌ | 498/585 [03:32<00:25,  3.46it/s] 85%|████████▌ | 499/585 [03:32<00:24,  3.46it/s] 85%|████████▌ | 500/585 [03:32<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [03:32<00:24,  3.46it/s] 86%|████████▌ | 501/585 [03:33<00:24,  3.46it/s] 86%|████████▌ | 502/585 [03:33<00:23,  3.46it/s] 86%|████████▌ | 503/585 [03:33<00:23,  3.45it/s] 86%|████████▌ | 504/585 [03:34<00:23,  3.46it/s] 86%|████████▋ | 505/585 [03:34<00:23,  3.45it/s] 86%|████████▋ | 506/585 [03:34<00:22,  3.45it/s] 87%|████████▋ | 507/585 [03:35<00:22,  3.45it/s] 87%|████████▋ | 508/585 [03:35<00:22,  3.46it/s] 87%|████████▋ | 509/585 [03:35<00:21,  3.46it/s] 87%|████████▋ | 510/585 [03:35<00:21,  3.46it/s] 87%|████████▋ | 511/585 [03:36<00:21,  3.46it/s] 88%|████████▊ | 512/585 [03:36<00:21,  3.46it/s] 88%|████████▊ | 513/585 [03:36<00:20,  3.46it/s] 88%|████████▊ | 514/585 [03:37<00:20,  3.45it/s] 88%|████████▊ | 515/585 [03:37<00:20,  3.46it/s] 88%|████████▊ | 516/585 [03:37<00:19,  3.45it/s] 88%|████████▊ | 517/585 [03:37<00:19,  3.46it/s] 89%|████████▊ | 518/585 [03:38<00:19,  3.46it/s] 89%|████████▊ | 519/585 [03:38<00:19,  3.46it/s] 89%|████████▉ | 520/585 [03:38<00:18,  3.46it/s] 89%|████████▉ | 521/585 [03:39<00:18,  3.46it/s] 89%|████████▉ | 522/585 [03:39<00:18,  3.46it/s] 89%|████████▉ | 523/585 [03:39<00:17,  3.46it/s] 90%|████████▉ | 524/585 [03:39<00:17,  3.46it/s] 90%|████████▉ | 525/585 [03:40<00:17,  3.43it/s] 90%|████████▉ | 526/585 [03:40<00:17,  3.44it/s] 90%|█████████ | 527/585 [03:40<00:16,  3.44it/s] 90%|█████████ | 528/585 [03:41<00:16,  3.45it/s] 90%|█████████ | 529/585 [03:41<00:16,  3.45it/s] 91%|█████████ | 530/585 [03:41<00:15,  3.45it/s] 91%|█████████ | 531/585 [03:41<00:15,  3.45it/s] 91%|█████████ | 532/585 [03:42<00:15,  3.45it/s] 91%|█████████ | 533/585 [03:42<00:15,  3.46it/s] 91%|█████████▏| 534/585 [03:42<00:14,  3.46it/s] 91%|█████████▏| 535/585 [03:43<00:14,  3.46it/s] 92%|█████████▏| 536/585 [03:43<00:14,  3.46it/s] 92%|█████████▏| 537/585 [03:43<00:13,  3.46it/s] 92%|█████████▏| 538/585 [03:43<00:13,  3.45it/s] 92%|█████████▏| 539/585 [03:44<00:13,  3.45it/s] 92%|█████████▏| 540/585 [03:44<00:13,  3.45it/s] 92%|█████████▏| 541/585 [03:44<00:12,  3.45it/s] 93%|█████████▎| 542/585 [03:45<00:12,  3.46it/s] 93%|█████████▎| 543/585 [03:45<00:12,  3.46it/s] 93%|█████████▎| 544/585 [03:45<00:11,  3.46it/s] 93%|█████████▎| 545/585 [03:46<00:11,  3.44it/s] 93%|█████████▎| 546/585 [03:46<00:11,  3.45it/s] 94%|█████████▎| 547/585 [03:46<00:11,  3.45it/s] 94%|█████████▎| 548/585 [03:46<00:10,  3.45it/s] 94%|█████████▍| 549/585 [03:47<00:10,  3.45it/s] 94%|█████████▍| 550/585 [03:47<00:10,  3.45it/s] 94%|█████████▍| 551/585 [03:47<00:09,  3.45it/s] 94%|█████████▍| 552/585 [03:48<00:09,  3.45it/s] 95%|█████████▍| 553/585 [03:48<00:09,  3.46it/s] 95%|█████████▍| 554/585 [03:48<00:08,  3.46it/s] 95%|█████████▍| 555/585 [03:48<00:08,  3.45it/s] 95%|█████████▌| 556/585 [03:49<00:08,  3.44it/s] 95%|█████████▌| 557/585 [03:49<00:08,  3.45it/s] 95%|█████████▌| 558/585 [03:49<00:07,  3.45it/s] 96%|█████████▌| 559/585 [03:50<00:07,  3.45it/s] 96%|█████████▌| 560/585 [03:50<00:07,  3.45it/s] 96%|█████████▌| 561/585 [03:50<00:06,  3.45it/s] 96%|█████████▌| 562/585 [03:50<00:06,  3.45it/s] 96%|█████████▌| 563/585 [03:51<00:06,  3.45it/s] 96%|█████████▋| 564/585 [03:51<00:06,  3.46it/s] 97%|█████████▋| 565/585 [03:51<00:05,  3.45it/s] 97%|█████████▋| 566/585 [03:52<00:05,  3.45it/s] 97%|█████████▋| 567/585 [03:52<00:05,  3.44it/s] 97%|█████████▋| 568/585 [03:52<00:04,  3.44it/s] 97%|█████████▋| 569/585 [03:52<00:04,  3.45it/s] 97%|█████████▋| 570/585 [03:53<00:04,  3.45it/s] 98%|█████████▊| 571/585 [03:53<00:04,  3.45it/s] 98%|█████████▊| 572/585 [03:53<00:03,  3.45it/s] 98%|█████████▊| 573/585 [03:54<00:03,  3.45it/s] 98%|█████████▊| 574/585 [03:54<00:03,  3.46it/s] 98%|█████████▊| 575/585 [03:54<00:02,  3.45it/s] 98%|█████████▊| 576/585 [03:54<00:02,  3.46it/s] 99%|█████████▊| 577/585 [03:55<00:02,  3.45it/s] 99%|█████████▉| 578/585 [03:55<00:02,  3.43it/s] 99%|█████████▉| 579/585 [03:55<00:01,  3.44it/s] 99%|█████████▉| 580/585 [03:56<00:01,  3.44it/s] 99%|█████████▉| 581/585 [03:56<00:01,  3.45it/s] 99%|█████████▉| 582/585 [03:56<00:00,  3.45it/s]100%|█████████▉| 583/585 [03:57<00:00,  3.45it/s]100%|█████████▉| 584/585 [03:57<00:00,  3.45it/s]100%|██████████| 585/585 [03:57<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 16:10:56,979 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:10:56,979 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:10:56,979 >>   Batch size = 8
{'eval_loss': 0.993770182132721, 'eval_runtime': 9.4121, 'eval_samples_per_second': 369.63, 'eval_steps_per_second': 46.217, 'epoch': 4.0}
{'loss': 0.6501, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.40it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.06it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.51it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.87it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.49it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.02it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.61it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.32it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.39it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.48it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.39it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 46.38it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.47it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.50it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.50it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.46it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.36it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.28it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.35it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.23it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.36it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.32it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.42it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.46it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.43it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.41it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.42it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.41it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.32it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.25it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.36it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.34it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.37it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.37it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.38it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.37it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.35it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.29it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.19it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 46.16it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.30it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.22it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.38it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.44it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.24it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.32it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.27it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.33it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.26it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.36it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.26it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.28it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.39it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.49it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.45it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.39it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.35it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.38it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.31it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.28it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.34it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.38it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 45.85it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.05it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.18it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.21it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.31it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.38it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.23it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.29it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.32it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.29it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.38it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.41it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.36it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.44it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.37it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.34it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.27it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.23it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.28it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.32it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.42it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.36it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.33it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.27it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.27it/s][A100%|██████████| 585/585 [04:06<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:11:06,392 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 16:11:06,415 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:11:08,895 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:11:08,910 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:11:08,923 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 16:11:14,600 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 16:11:14,601 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117 (score: 0.9685636162757874).
                                                 100%|██████████| 585/585 [04:17<00:00,  3.46it/s]100%|██████████| 585/585 [04:17<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-28 16:11:16,546 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 16:11:16,566 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:11:19,062 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:11:19,077 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:11:19,086 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:11:19,275 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:19,276 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:19,276 >>   train_loss               =     0.6457
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:19,276 >>   train_runtime            = 0:04:17.14
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:19,276 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:19,276 >>   train_samples_per_second =    145.834
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:19,276 >>   train_steps_per_second   =      2.275
{'eval_loss': 0.9968103766441345, 'eval_runtime': 9.3882, 'eval_samples_per_second': 370.572, 'eval_steps_per_second': 46.335, 'epoch': 5.0}
{'train_runtime': 257.1413, 'train_samples_per_second': 145.834, 'train_steps_per_second': 2.275, 'train_loss': 0.6457113869169838, 'epoch': 5.0}
08/28/2023 16:11:19 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 16:11:19,314 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:11:19,314 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:11:19,314 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 58.26it/s]  3%|▎         | 12/435 [00:00<00:08, 51.23it/s]  4%|▍         | 18/435 [00:00<00:08, 49.21it/s]  5%|▌         | 23/435 [00:00<00:08, 48.48it/s]  6%|▋         | 28/435 [00:00<00:08, 47.92it/s]  8%|▊         | 33/435 [00:00<00:08, 47.78it/s]  9%|▊         | 38/435 [00:00<00:08, 47.57it/s] 10%|▉         | 43/435 [00:00<00:08, 47.15it/s] 11%|█         | 48/435 [00:00<00:08, 46.75it/s] 12%|█▏        | 53/435 [00:01<00:08, 46.60it/s] 13%|█▎        | 58/435 [00:01<00:08, 46.74it/s] 14%|█▍        | 63/435 [00:01<00:07, 46.77it/s] 16%|█▌        | 68/435 [00:01<00:07, 46.87it/s] 17%|█▋        | 73/435 [00:01<00:07, 46.78it/s] 18%|█▊        | 78/435 [00:01<00:07, 46.86it/s] 19%|█▉        | 83/435 [00:01<00:07, 46.87it/s] 20%|██        | 88/435 [00:01<00:07, 46.94it/s] 21%|██▏       | 93/435 [00:01<00:07, 46.81it/s] 23%|██▎       | 98/435 [00:02<00:07, 46.60it/s] 24%|██▎       | 103/435 [00:02<00:07, 46.65it/s] 25%|██▍       | 108/435 [00:02<00:07, 46.63it/s] 26%|██▌       | 113/435 [00:02<00:06, 46.71it/s] 27%|██▋       | 118/435 [00:02<00:06, 46.80it/s] 28%|██▊       | 123/435 [00:02<00:06, 46.71it/s] 29%|██▉       | 128/435 [00:02<00:06, 46.70it/s] 31%|███       | 133/435 [00:02<00:06, 46.76it/s] 32%|███▏      | 138/435 [00:02<00:06, 46.55it/s] 33%|███▎      | 143/435 [00:03<00:06, 46.54it/s] 34%|███▍      | 148/435 [00:03<00:06, 46.46it/s] 35%|███▌      | 153/435 [00:03<00:06, 46.59it/s] 36%|███▋      | 158/435 [00:03<00:06, 45.40it/s] 37%|███▋      | 163/435 [00:03<00:05, 45.81it/s] 39%|███▊      | 168/435 [00:03<00:05, 46.02it/s] 40%|███▉      | 173/435 [00:03<00:05, 46.33it/s] 41%|████      | 178/435 [00:03<00:05, 46.52it/s] 42%|████▏     | 183/435 [00:03<00:05, 46.66it/s] 43%|████▎     | 188/435 [00:04<00:05, 46.62it/s] 44%|████▍     | 193/435 [00:04<00:05, 46.53it/s] 46%|████▌     | 198/435 [00:04<00:05, 46.36it/s] 47%|████▋     | 203/435 [00:04<00:04, 46.41it/s] 48%|████▊     | 208/435 [00:04<00:04, 46.47it/s] 49%|████▉     | 213/435 [00:04<00:04, 46.53it/s] 50%|█████     | 218/435 [00:04<00:04, 46.58it/s] 51%|█████▏    | 223/435 [00:04<00:04, 46.67it/s] 52%|█████▏    | 228/435 [00:04<00:04, 46.67it/s] 54%|█████▎    | 233/435 [00:04<00:04, 46.59it/s] 55%|█████▍    | 238/435 [00:05<00:04, 46.52it/s] 56%|█████▌    | 243/435 [00:05<00:04, 46.51it/s] 57%|█████▋    | 248/435 [00:05<00:04, 46.46it/s] 58%|█████▊    | 253/435 [00:05<00:03, 46.50it/s] 59%|█████▉    | 258/435 [00:05<00:03, 46.57it/s] 60%|██████    | 263/435 [00:05<00:03, 46.50it/s] 62%|██████▏   | 268/435 [00:05<00:03, 46.57it/s] 63%|██████▎   | 273/435 [00:05<00:03, 46.64it/s] 64%|██████▍   | 278/435 [00:05<00:03, 46.67it/s] 65%|██████▌   | 283/435 [00:06<00:03, 46.62it/s] 66%|██████▌   | 288/435 [00:06<00:03, 46.59it/s] 67%|██████▋   | 293/435 [00:06<00:03, 46.46it/s] 69%|██████▊   | 298/435 [00:06<00:02, 46.46it/s] 70%|██████▉   | 303/435 [00:06<00:02, 46.56it/s] 71%|███████   | 308/435 [00:06<00:02, 46.51it/s] 72%|███████▏  | 313/435 [00:06<00:02, 46.58it/s] 73%|███████▎  | 318/435 [00:06<00:02, 46.63it/s] 74%|███████▍  | 323/435 [00:06<00:02, 46.51it/s] 75%|███████▌  | 328/435 [00:07<00:02, 46.50it/s] 77%|███████▋  | 333/435 [00:07<00:02, 46.50it/s] 78%|███████▊  | 338/435 [00:07<00:02, 46.48it/s] 79%|███████▉  | 343/435 [00:07<00:01, 46.46it/s] 80%|████████  | 348/435 [00:07<00:01, 46.55it/s] 81%|████████  | 353/435 [00:07<00:01, 46.50it/s] 82%|████████▏ | 358/435 [00:07<00:01, 46.47it/s] 83%|████████▎ | 363/435 [00:07<00:01, 46.50it/s] 85%|████████▍ | 368/435 [00:07<00:01, 46.55it/s] 86%|████████▌ | 373/435 [00:07<00:01, 46.53it/s] 87%|████████▋ | 378/435 [00:08<00:01, 46.58it/s] 88%|████████▊ | 383/435 [00:08<00:01, 46.45it/s] 89%|████████▉ | 388/435 [00:08<00:01, 46.46it/s] 90%|█████████ | 393/435 [00:08<00:00, 46.48it/s] 91%|█████████▏| 398/435 [00:08<00:00, 46.60it/s] 93%|█████████▎| 403/435 [00:08<00:00, 46.54it/s] 94%|█████████▍| 408/435 [00:08<00:00, 46.62it/s] 95%|█████████▍| 413/435 [00:08<00:00, 46.45it/s] 96%|█████████▌| 418/435 [00:08<00:00, 46.51it/s] 97%|█████████▋| 423/435 [00:09<00:00, 46.49it/s] 98%|█████████▊| 428/435 [00:09<00:00, 46.49it/s]100%|█████████▉| 433/435 [00:09<00:00, 46.55it/s]100%|██████████| 435/435 [00:09<00:00, 46.66it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:11:28,658 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:28,659 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:28,659 >>   eval_loss               =     0.9686
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:28,659 >>   eval_runtime            = 0:00:09.34
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:28,659 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:28,659 >>   eval_samples_per_second =      372.3
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:28,659 >>   eval_steps_per_second   =     46.551
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:11:28,659 >>   perplexity              =     2.6342
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:11:33,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:11:33,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:11:33,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:11:33,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:11:33,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:11:34,203 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:11:34,204 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:11:34,487 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:11:35,543 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:11:35,543 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:11:36,862 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:11:36,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:11:36,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:11:36,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:11:36,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:11:37,215 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:11:37,216 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:11:37,482 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:11:37,643 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:11:37,643 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.42it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.41it/s]Extractor Predicting: 6it [00:04,  1.40it/s]Extractor Predicting: 7it [00:04,  1.40it/s]Extractor Predicting: 8it [00:05,  1.41it/s]Extractor Predicting: 9it [00:06,  1.41it/s]Extractor Predicting: 10it [00:07,  1.45it/s]Extractor Predicting: 11it [00:07,  1.43it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:09,  1.39it/s]Extractor Predicting: 14it [00:09,  1.39it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.41it/s]Extractor Predicting: 17it [00:12,  1.43it/s]Extractor Predicting: 18it [00:12,  1.47it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:14,  1.45it/s]Extractor Predicting: 21it [00:14,  1.45it/s]Extractor Predicting: 22it [00:15,  1.47it/s]Extractor Predicting: 23it [00:16,  1.46it/s]Extractor Predicting: 24it [00:16,  1.44it/s]Extractor Predicting: 25it [00:17,  1.41it/s]Extractor Predicting: 26it [00:18,  1.39it/s]Extractor Predicting: 27it [00:19,  1.39it/s]Extractor Predicting: 28it [00:19,  1.43it/s]Extractor Predicting: 29it [00:20,  1.41it/s]Extractor Predicting: 30it [00:21,  1.38it/s]Extractor Predicting: 31it [00:21,  1.39it/s]Extractor Predicting: 32it [00:22,  1.38it/s]Extractor Predicting: 33it [00:23,  1.39it/s]Extractor Predicting: 34it [00:23,  1.43it/s]Extractor Predicting: 35it [00:24,  1.44it/s]Extractor Predicting: 36it [00:25,  1.44it/s]Extractor Predicting: 37it [00:26,  1.44it/s]Extractor Predicting: 38it [00:26,  1.46it/s]Extractor Predicting: 39it [00:27,  1.47it/s]Extractor Predicting: 40it [00:28,  1.45it/s]Extractor Predicting: 41it [00:28,  1.46it/s]Extractor Predicting: 42it [00:29,  1.45it/s]Extractor Predicting: 43it [00:30,  1.48it/s]Extractor Predicting: 44it [00:30,  1.49it/s]Extractor Predicting: 45it [00:31,  1.47it/s]Extractor Predicting: 46it [00:32,  1.45it/s]Extractor Predicting: 47it [00:32,  1.44it/s]Extractor Predicting: 48it [00:33,  1.45it/s]Extractor Predicting: 49it [00:34,  1.45it/s]Extractor Predicting: 50it [00:34,  1.46it/s]Extractor Predicting: 51it [00:35,  1.47it/s]Extractor Predicting: 52it [00:36,  1.46it/s]Extractor Predicting: 53it [00:36,  1.47it/s]Extractor Predicting: 54it [00:37,  1.45it/s]Extractor Predicting: 55it [00:38,  1.43it/s]Extractor Predicting: 56it [00:39,  1.32it/s]Extractor Predicting: 57it [00:40,  1.34it/s]Extractor Predicting: 58it [00:40,  1.37it/s]Extractor Predicting: 59it [00:41,  1.42it/s]Extractor Predicting: 60it [00:41,  1.46it/s]Extractor Predicting: 61it [00:42,  1.49it/s]Extractor Predicting: 62it [00:43,  1.47it/s]Extractor Predicting: 63it [00:44,  1.45it/s]Extractor Predicting: 64it [00:44,  1.46it/s]Extractor Predicting: 65it [00:45,  1.47it/s]Extractor Predicting: 66it [00:46,  1.47it/s]Extractor Predicting: 67it [00:46,  1.48it/s]Extractor Predicting: 68it [00:47,  1.47it/s]Extractor Predicting: 69it [00:48,  1.52it/s]Extractor Predicting: 70it [00:48,  1.52it/s]Extractor Predicting: 71it [00:49,  1.52it/s]Extractor Predicting: 72it [00:50,  1.49it/s]Extractor Predicting: 73it [00:50,  1.49it/s]Extractor Predicting: 74it [00:51,  1.49it/s]Extractor Predicting: 75it [00:52,  1.46it/s]Extractor Predicting: 76it [00:52,  1.44it/s]Extractor Predicting: 77it [00:53,  1.47it/s]Extractor Predicting: 78it [00:54,  1.49it/s]Extractor Predicting: 79it [00:54,  1.52it/s]Extractor Predicting: 80it [00:55,  1.50it/s]Extractor Predicting: 81it [00:56,  1.48it/s]Extractor Predicting: 82it [00:56,  1.48it/s]Extractor Predicting: 83it [00:57,  1.50it/s]Extractor Predicting: 84it [00:58,  1.50it/s]Extractor Predicting: 85it [00:58,  1.51it/s]Extractor Predicting: 86it [00:59,  1.52it/s]Extractor Predicting: 87it [01:00,  1.54it/s]Extractor Predicting: 88it [01:00,  1.53it/s]Extractor Predicting: 89it [01:01,  1.53it/s]Extractor Predicting: 90it [01:02,  1.54it/s]Extractor Predicting: 91it [01:02,  1.56it/s]Extractor Predicting: 92it [01:03,  1.59it/s]Extractor Predicting: 93it [01:03,  1.56it/s]Extractor Predicting: 94it [01:04,  1.55it/s]Extractor Predicting: 95it [01:05,  1.56it/s]Extractor Predicting: 96it [01:05,  1.54it/s]Extractor Predicting: 97it [01:06,  1.54it/s]Extractor Predicting: 98it [01:07,  1.53it/s]Extractor Predicting: 99it [01:07,  1.52it/s]Extractor Predicting: 100it [01:08,  1.48it/s]Extractor Predicting: 101it [01:09,  1.48it/s]Extractor Predicting: 102it [01:09,  1.55it/s]Extractor Predicting: 103it [01:10,  1.56it/s]Extractor Predicting: 104it [01:11,  1.55it/s]Extractor Predicting: 105it [01:11,  1.54it/s]Extractor Predicting: 106it [01:12,  1.54it/s]Extractor Predicting: 107it [01:13,  1.53it/s]Extractor Predicting: 108it [01:13,  1.53it/s]Extractor Predicting: 109it [01:14,  1.54it/s]Extractor Predicting: 110it [01:14,  1.54it/s]Extractor Predicting: 111it [01:15,  1.54it/s]Extractor Predicting: 112it [01:16,  1.55it/s]Extractor Predicting: 113it [01:16,  1.59it/s]Extractor Predicting: 114it [01:17,  1.59it/s]Extractor Predicting: 115it [01:18,  1.60it/s]Extractor Predicting: 116it [01:18,  1.56it/s]Extractor Predicting: 117it [01:19,  1.52it/s]Extractor Predicting: 118it [01:20,  1.53it/s]Extractor Predicting: 119it [01:20,  1.49it/s]Extractor Predicting: 120it [01:21,  1.51it/s]Extractor Predicting: 121it [01:22,  1.49it/s]Extractor Predicting: 122it [01:22,  1.47it/s]Extractor Predicting: 123it [01:23,  1.48it/s]Extractor Predicting: 124it [01:24,  1.49it/s]Extractor Predicting: 125it [01:24,  1.51it/s]Extractor Predicting: 126it [01:25,  1.48it/s]Extractor Predicting: 127it [01:26,  1.51it/s]Extractor Predicting: 128it [01:26,  1.51it/s]Extractor Predicting: 129it [01:27,  1.38it/s]Extractor Predicting: 130it [01:28,  1.39it/s]Extractor Predicting: 131it [01:29,  1.44it/s]Extractor Predicting: 132it [01:29,  1.44it/s]Extractor Predicting: 133it [01:30,  1.43it/s]Extractor Predicting: 134it [01:31,  1.43it/s]Extractor Predicting: 135it [01:31,  1.45it/s]Extractor Predicting: 136it [01:32,  1.44it/s]Extractor Predicting: 137it [01:33,  1.45it/s]Extractor Predicting: 138it [01:33,  1.42it/s]Extractor Predicting: 139it [01:34,  1.43it/s]Extractor Predicting: 140it [01:35,  1.42it/s]Extractor Predicting: 141it [01:36,  1.45it/s]Extractor Predicting: 142it [01:36,  1.43it/s]Extractor Predicting: 143it [01:37,  1.44it/s]Extractor Predicting: 144it [01:37,  1.78it/s]Extractor Predicting: 144it [01:37,  1.47it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:23,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:23,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:23,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:23,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:23,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:13:24,123 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:13:24,124 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:13:24,803 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:13:25,844 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:13:25,844 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:28,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:28,661 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:28,662 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:28,662 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:28,662 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:13:29,287 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:13:29,288 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:13:29,860 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:13:30,010 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:13:30,010 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.22477064220183487,
  "recall": 0.028169014084507043,
  "score": 0.05006385696040869,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.61it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.58it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:21,  1.57it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:22,  1.43it/s]Extractor Predicting: 36it [00:23,  1.47it/s]Extractor Predicting: 37it [00:23,  1.51it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:25,  1.53it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:27,  1.58it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:29,  1.51it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:31,  1.52it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:33,  1.52it/s]Extractor Predicting: 52it [00:33,  1.51it/s]Extractor Predicting: 53it [00:34,  1.51it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:35,  1.54it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:37,  1.50it/s]Extractor Predicting: 59it [00:38,  1.50it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:39,  1.47it/s]Extractor Predicting: 62it [00:40,  1.50it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:41,  1.54it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.48it/s]Extractor Predicting: 67it [00:43,  1.48it/s]Extractor Predicting: 68it [00:44,  1.48it/s]Extractor Predicting: 69it [00:45,  1.47it/s]Extractor Predicting: 70it [00:45,  1.48it/s]Extractor Predicting: 71it [00:46,  1.48it/s]Extractor Predicting: 72it [00:47,  1.46it/s]Extractor Predicting: 73it [00:47,  1.49it/s]Extractor Predicting: 74it [00:48,  1.49it/s]Extractor Predicting: 75it [00:49,  1.50it/s]Extractor Predicting: 76it [00:49,  1.48it/s]Extractor Predicting: 77it [00:50,  1.52it/s]Extractor Predicting: 78it [00:51,  1.52it/s]Extractor Predicting: 79it [00:51,  1.55it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:52,  1.56it/s]Extractor Predicting: 82it [00:53,  1.56it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:54,  1.51it/s]Extractor Predicting: 85it [00:55,  1.48it/s]Extractor Predicting: 86it [00:56,  1.45it/s]Extractor Predicting: 87it [00:57,  1.46it/s]Extractor Predicting: 88it [00:57,  1.46it/s]Extractor Predicting: 89it [00:58,  1.44it/s]Extractor Predicting: 90it [00:59,  1.44it/s]Extractor Predicting: 91it [00:59,  1.42it/s]Extractor Predicting: 92it [01:00,  1.46it/s]Extractor Predicting: 93it [01:01,  1.47it/s]Extractor Predicting: 94it [01:01,  1.48it/s]Extractor Predicting: 95it [01:02,  1.49it/s]Extractor Predicting: 96it [01:03,  1.45it/s]Extractor Predicting: 97it [01:03,  1.44it/s]Extractor Predicting: 98it [01:04,  1.43it/s]Extractor Predicting: 99it [01:05,  1.46it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:06,  1.50it/s]Extractor Predicting: 102it [01:07,  1.46it/s]Extractor Predicting: 103it [01:08,  1.45it/s]Extractor Predicting: 104it [01:08,  1.48it/s]Extractor Predicting: 105it [01:09,  1.47it/s]Extractor Predicting: 106it [01:10,  1.47it/s]Extractor Predicting: 107it [01:10,  1.50it/s]Extractor Predicting: 108it [01:11,  1.47it/s]Extractor Predicting: 109it [01:12,  1.47it/s]Extractor Predicting: 110it [01:12,  1.46it/s]Extractor Predicting: 111it [01:13,  1.48it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:14,  1.48it/s]Extractor Predicting: 114it [01:15,  1.48it/s]Extractor Predicting: 115it [01:16,  1.44it/s]Extractor Predicting: 116it [01:16,  1.46it/s]Extractor Predicting: 117it [01:17,  1.46it/s]Extractor Predicting: 118it [01:18,  1.46it/s]Extractor Predicting: 119it [01:18,  1.46it/s]Extractor Predicting: 120it [01:19,  1.48it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:20,  1.51it/s]Extractor Predicting: 123it [01:21,  1.50it/s]Extractor Predicting: 124it [01:22,  1.51it/s]Extractor Predicting: 125it [01:22,  1.54it/s]Extractor Predicting: 126it [01:23,  1.53it/s]Extractor Predicting: 127it [01:24,  1.52it/s]Extractor Predicting: 128it [01:24,  1.54it/s]Extractor Predicting: 129it [01:25,  1.54it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:26,  1.55it/s]Extractor Predicting: 132it [01:27,  1.56it/s]Extractor Predicting: 133it [01:27,  1.58it/s]Extractor Predicting: 134it [01:28,  1.52it/s]Extractor Predicting: 135it [01:29,  1.56it/s]Extractor Predicting: 136it [01:29,  1.57it/s]Extractor Predicting: 137it [01:30,  1.42it/s]Extractor Predicting: 138it [01:31,  1.45it/s]Extractor Predicting: 139it [01:32,  1.47it/s]Extractor Predicting: 140it [01:32,  1.50it/s]Extractor Predicting: 141it [01:33,  1.48it/s]Extractor Predicting: 142it [01:34,  1.49it/s]Extractor Predicting: 143it [01:34,  1.51it/s]Extractor Predicting: 144it [01:35,  1.51it/s]Extractor Predicting: 145it [01:35,  1.56it/s]Extractor Predicting: 146it [01:36,  1.57it/s]Extractor Predicting: 147it [01:37,  1.58it/s]Extractor Predicting: 148it [01:37,  1.61it/s]Extractor Predicting: 149it [01:38,  1.60it/s]Extractor Predicting: 150it [01:39,  1.62it/s]Extractor Predicting: 151it [01:39,  1.62it/s]Extractor Predicting: 152it [01:40,  1.64it/s]Extractor Predicting: 153it [01:40,  1.62it/s]Extractor Predicting: 154it [01:41,  1.61it/s]Extractor Predicting: 155it [01:42,  1.66it/s]Extractor Predicting: 156it [01:42,  1.64it/s]Extractor Predicting: 157it [01:43,  1.72it/s]Extractor Predicting: 158it [01:43,  1.74it/s]Extractor Predicting: 159it [01:44,  1.69it/s]Extractor Predicting: 160it [01:45,  1.64it/s]Extractor Predicting: 161it [01:45,  1.63it/s]Extractor Predicting: 162it [01:46,  1.64it/s]Extractor Predicting: 163it [01:46,  1.67it/s]Extractor Predicting: 164it [01:47,  1.67it/s]Extractor Predicting: 165it [01:48,  1.67it/s]Extractor Predicting: 166it [01:48,  1.64it/s]Extractor Predicting: 167it [01:49,  1.66it/s]Extractor Predicting: 168it [01:49,  1.65it/s]Extractor Predicting: 169it [01:50,  1.70it/s]Extractor Predicting: 170it [01:51,  1.68it/s]Extractor Predicting: 171it [01:51,  1.68it/s]Extractor Predicting: 172it [01:52,  1.61it/s]Extractor Predicting: 173it [01:52,  1.59it/s]Extractor Predicting: 174it [01:53,  1.55it/s]Extractor Predicting: 175it [01:54,  1.49it/s]Extractor Predicting: 176it [01:55,  1.50it/s]Extractor Predicting: 177it [01:55,  1.50it/s]Extractor Predicting: 178it [01:56,  1.48it/s]Extractor Predicting: 179it [01:57,  1.48it/s]Extractor Predicting: 180it [01:57,  1.49it/s]Extractor Predicting: 181it [01:58,  1.49it/s]Extractor Predicting: 182it [01:59,  1.48it/s]Extractor Predicting: 183it [01:59,  1.48it/s]Extractor Predicting: 184it [02:00,  1.47it/s]Extractor Predicting: 185it [02:01,  1.45it/s]Extractor Predicting: 186it [02:01,  1.45it/s]Extractor Predicting: 187it [02:02,  1.46it/s]Extractor Predicting: 188it [02:03,  1.46it/s]Extractor Predicting: 189it [02:03,  1.46it/s]Extractor Predicting: 190it [02:04,  1.48it/s]Extractor Predicting: 191it [02:05,  1.44it/s]Extractor Predicting: 192it [02:06,  1.43it/s]Extractor Predicting: 193it [02:06,  1.43it/s]Extractor Predicting: 194it [02:07,  1.43it/s]Extractor Predicting: 195it [02:08,  1.45it/s]Extractor Predicting: 196it [02:08,  1.46it/s]Extractor Predicting: 197it [02:09,  1.45it/s]Extractor Predicting: 198it [02:10,  1.47it/s]Extractor Predicting: 199it [02:10,  1.49it/s]Extractor Predicting: 200it [02:11,  1.46it/s]Extractor Predicting: 201it [02:12,  1.44it/s]Extractor Predicting: 202it [02:12,  1.51it/s]Extractor Predicting: 203it [02:13,  1.50it/s]Extractor Predicting: 204it [02:14,  1.51it/s]Extractor Predicting: 205it [02:14,  1.49it/s]Extractor Predicting: 206it [02:15,  1.49it/s]Extractor Predicting: 207it [02:16,  1.47it/s]Extractor Predicting: 208it [02:16,  1.47it/s]Extractor Predicting: 209it [02:17,  1.42it/s]Extractor Predicting: 210it [02:18,  1.43it/s]Extractor Predicting: 211it [02:19,  1.43it/s]Extractor Predicting: 212it [02:19,  1.43it/s]Extractor Predicting: 213it [02:20,  1.43it/s]Extractor Predicting: 214it [02:21,  1.42it/s]Extractor Predicting: 215it [02:21,  1.45it/s]Extractor Predicting: 216it [02:22,  1.42it/s]Extractor Predicting: 217it [02:23,  1.44it/s]Extractor Predicting: 218it [02:23,  1.45it/s]Extractor Predicting: 219it [02:24,  1.43it/s]Extractor Predicting: 220it [02:25,  1.44it/s]Extractor Predicting: 221it [02:26,  1.42it/s]Extractor Predicting: 222it [02:26,  1.41it/s]Extractor Predicting: 223it [02:27,  1.45it/s]Extractor Predicting: 224it [02:28,  1.45it/s]Extractor Predicting: 225it [02:28,  1.45it/s]Extractor Predicting: 226it [02:29,  1.44it/s]Extractor Predicting: 227it [02:30,  1.49it/s]Extractor Predicting: 228it [02:30,  1.48it/s]Extractor Predicting: 229it [02:31,  1.47it/s]Extractor Predicting: 230it [02:32,  1.52it/s]Extractor Predicting: 231it [02:32,  1.53it/s]Extractor Predicting: 232it [02:33,  1.55it/s]Extractor Predicting: 233it [02:34,  1.53it/s]Extractor Predicting: 234it [02:34,  1.53it/s]Extractor Predicting: 235it [02:35,  1.52it/s]Extractor Predicting: 236it [02:36,  1.50it/s]Extractor Predicting: 237it [02:36,  1.48it/s]Extractor Predicting: 238it [02:37,  1.36it/s]Extractor Predicting: 239it [02:38,  1.38it/s]Extractor Predicting: 240it [02:38,  1.41it/s]Extractor Predicting: 241it [02:39,  1.38it/s]Extractor Predicting: 242it [02:40,  1.43it/s]Extractor Predicting: 243it [02:41,  1.43it/s]Extractor Predicting: 244it [02:41,  1.44it/s]Extractor Predicting: 245it [02:42,  1.47it/s]Extractor Predicting: 246it [02:43,  1.47it/s]Extractor Predicting: 247it [02:43,  1.48it/s]Extractor Predicting: 248it [02:44,  1.48it/s]Extractor Predicting: 249it [02:45,  1.46it/s]Extractor Predicting: 250it [02:45,  1.48it/s]Extractor Predicting: 251it [02:46,  1.49it/s]Extractor Predicting: 252it [02:47,  1.49it/s]Extractor Predicting: 253it [02:47,  1.47it/s]Extractor Predicting: 254it [02:48,  1.49it/s]Extractor Predicting: 255it [02:49,  1.44it/s]Extractor Predicting: 256it [02:49,  1.41it/s]Extractor Predicting: 257it [02:50,  1.41it/s]Extractor Predicting: 258it [02:51,  1.41it/s]Extractor Predicting: 259it [02:52,  1.43it/s]Extractor Predicting: 260it [02:52,  1.41it/s]Extractor Predicting: 261it [02:53,  1.45it/s]Extractor Predicting: 262it [02:54,  1.43it/s]Extractor Predicting: 263it [02:54,  1.42it/s]Extractor Predicting: 264it [02:55,  1.43it/s]Extractor Predicting: 265it [02:56,  1.38it/s]Extractor Predicting: 266it [02:57,  1.39it/s]Extractor Predicting: 267it [02:57,  1.40it/s]Extractor Predicting: 268it [02:58,  1.37it/s]Extractor Predicting: 269it [02:59,  1.40it/s]Extractor Predicting: 270it [02:59,  1.42it/s]Extractor Predicting: 271it [03:00,  1.40it/s]Extractor Predicting: 272it [03:01,  1.41it/s]Extractor Predicting: 273it [03:01,  1.42it/s]Extractor Predicting: 274it [03:02,  1.44it/s]Extractor Predicting: 275it [03:03,  1.46it/s]Extractor Predicting: 276it [03:03,  1.49it/s]Extractor Predicting: 277it [03:04,  1.49it/s]Extractor Predicting: 278it [03:05,  1.48it/s]Extractor Predicting: 279it [03:05,  1.48it/s]Extractor Predicting: 280it [03:06,  1.42it/s]Extractor Predicting: 281it [03:07,  1.43it/s]Extractor Predicting: 282it [03:08,  1.52it/s]Extractor Predicting: 282it [03:08,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:45,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:45,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:45,743 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:45,743 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:45,743 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:16:46,339 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:16:46,340 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:16:46,905 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:16:47,939 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:16:47,939 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:50,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:50,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:50,941 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:50,941 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:50,941 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:16:51,578 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:16:51,579 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:16:52,148 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:16:52,292 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:16:52,292 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.32408691631992603,
  "recall": 0.103698224852071,
  "score": 0.15712204415555306,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.40it/s]Extractor Predicting: 4it [00:02,  1.40it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 5it [00:03,  1.43it/s]
[INFO|configuration_utils.py:515] 2023-08-28 16:16:56,177 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:16:56,178 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:16:56,185 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:16:56,186 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 16:16:56,188 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:16:59,185 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 16:16:59,188 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 16:16:59,198 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:16:59,199 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:16:59,206 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:16:59,209 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:16:59,209 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:16:59,209 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:16:59,209 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:16:59,209 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:16:59,209 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.41935483870967744,
  "recall": 0.05416666666666667,
  "score": 0.0959409594095941,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 16:16:59,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:00,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:01,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:01,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:02,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:03,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:04,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:05,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:06,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:06,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:07,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:08,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:08,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:09,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:10,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:11,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:11,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:12,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:13,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:14,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:15,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:15,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:56, 16.90s/it][WARNING|generation_utils.py:914] 2023-08-28 16:17:16,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:17,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:17,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:18,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:19,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:20,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:21,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:21,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:22,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:23,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:24,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:24,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:25,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:26,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:27,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:27,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:28,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:29,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:30,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:30,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:31,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:32<03:32, 16.35s/it][WARNING|generation_utils.py:914] 2023-08-28 16:17:32,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:33,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:33,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:34,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:35,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:35,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:36,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:37,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:38,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:39,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:40,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:40,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:41,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:42,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:42,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:43,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:44,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:45,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:45,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:46,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:47,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:47,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:49<03:16, 16.33s/it][WARNING|generation_utils.py:914] 2023-08-28 16:17:48,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:49,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:50,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:50,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:51,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:52,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:53,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:54,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:54,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:56,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:56,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:57,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:58,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:59,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:59,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:00,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:01,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:02,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:02,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:03,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:04,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:05,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:06<03:04, 16.76s/it][WARNING|generation_utils.py:914] 2023-08-28 16:18:06,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:06,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:07,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:08,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:09,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:09,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:10,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:11,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:12,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:12,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:13,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:14,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:15,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:16,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:16,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:17,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:18,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:19,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:19,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:20,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:21,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:22<02:45, 16.55s/it][WARNING|generation_utils.py:914] 2023-08-28 16:18:22,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:23,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:23,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:24,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:25,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:25,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:26,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:27,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:27,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:28,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:29,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:30,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:31,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:31,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:32,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:33,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:33,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:34,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:35,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:36,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:37,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:38<02:26, 16.25s/it][WARNING|generation_utils.py:914] 2023-08-28 16:18:37,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:38,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:39,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:39,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:40,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:41,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:42,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:43,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:43,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:44,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:45,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:45,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:46,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:47,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:48,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:48,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:49,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:50,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:51,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:51,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:52,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:53<02:07, 15.97s/it][WARNING|generation_utils.py:914] 2023-08-28 16:18:53,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:54,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:54,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:55,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:56,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:57,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:58,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:59,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:59,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:00,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:01,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:02,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:02,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:03,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:04,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:05,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:05,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:06,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:07,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:08,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:09,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:10<01:53, 16.26s/it][WARNING|generation_utils.py:914] 2023-08-28 16:19:10,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:10,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:11,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:12,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:13,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:13,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:14,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:15,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:16,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:16,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:17,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:18,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:18,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:19,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:20,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:21,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:21,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:22,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:23,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:24,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:24,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:26<01:36, 16.05s/it][WARNING|generation_utils.py:914] 2023-08-28 16:19:25,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:26,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:27,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:28,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:28,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:29,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:30,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:31,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:31,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:32,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:33,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:34,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:34,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:35,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:36,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:36,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:37,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:38,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:39,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:40,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:41<01:19, 15.81s/it][WARNING|generation_utils.py:914] 2023-08-28 16:19:41,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:41,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:42,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:43,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:44,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:44,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:45,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:46,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:47,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:48,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:48,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:49,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:50,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:51,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:52,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:52,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:53,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:54,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:55,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:56,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:56,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:57,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:58,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:59<01:05, 16.47s/it][WARNING|generation_utils.py:914] 2023-08-28 16:19:58,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:19:59,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:00,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:01,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:01,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:02,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:03,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:04,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:04,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:05,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:06,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:06,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:07,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:08,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:08,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:09,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:10,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:11,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:11,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:12,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:13,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:14<00:48, 16.04s/it][WARNING|generation_utils.py:914] 2023-08-28 16:20:14,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:14,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:15,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:16,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:16,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:17,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:18,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:19,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:20,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:20,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:21,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:22,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:22,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:23,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:24,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:25,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:26,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:26,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:27,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:28,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:28,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:30<00:31, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-28 16:20:29,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:30,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:30,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:31,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:32,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:33,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:34,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:34,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:35,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:36,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:37,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:37,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:38,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:39,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:40,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:40,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:41,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:42,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:42,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:43,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:44<00:15, 15.53s/it][WARNING|generation_utils.py:914] 2023-08-28 16:20:44,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:45,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:45,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:46,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:47,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:48,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:49,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:50,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:50,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:51,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:52,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:53,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:54,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:54,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:55,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:56,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:57,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:57,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:58,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:20:59,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:21:00,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:21:01,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:21:02,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:03<00:00, 16.49s/it]Generating: 100%|██████████| 15/15 [04:03<00:00, 16.23s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:07,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:07,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:07,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:07,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:07,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:21:08,235 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:21:08,236 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:21:08,512 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:21:09,564 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:21:09,564 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:10,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:10,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:10,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:10,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:10,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:21:11,166 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:21:11,171 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:21:11,505 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:21:11,660 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:21:11,660 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : country .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : genre .', 'success_rate': 0.8934659090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.859375, 'errors': {'', "('Opera of Finland', 'member of', '', 'At the 1992 convention held at the Metropolitan Opera , he was honored by the Opera of Finland , after being nominated in each of the 15 categories nominated for .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9285714285714286, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9421875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8396739130434783, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9107142857142857, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : publisher .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8478260869565217, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 9975
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10075, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.40it/s]Extractor Estimating: 2it [00:01,  1.37it/s]Extractor Estimating: 3it [00:02,  1.36it/s]Extractor Estimating: 4it [00:02,  1.44it/s]Extractor Estimating: 5it [00:03,  1.50it/s]Extractor Estimating: 6it [00:04,  1.53it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:05,  1.47it/s]Extractor Estimating: 9it [00:06,  1.42it/s]Extractor Estimating: 10it [00:06,  1.42it/s]Extractor Estimating: 11it [00:07,  1.47it/s]Extractor Estimating: 12it [00:08,  1.51it/s]Extractor Estimating: 13it [00:08,  1.52it/s]Extractor Estimating: 14it [00:09,  1.55it/s]Extractor Estimating: 15it [00:10,  1.58it/s]Extractor Estimating: 16it [00:10,  1.53it/s]Extractor Estimating: 17it [00:11,  1.47it/s]Extractor Estimating: 18it [00:12,  1.46it/s]Extractor Estimating: 19it [00:13,  1.05it/s]Extractor Estimating: 20it [00:14,  1.16it/s]Extractor Estimating: 21it [00:15,  1.26it/s]Extractor Estimating: 22it [00:15,  1.35it/s]Extractor Estimating: 23it [00:16,  1.39it/s]Extractor Estimating: 24it [00:16,  1.45it/s]Extractor Estimating: 25it [00:17,  1.49it/s]Extractor Estimating: 26it [00:18,  1.45it/s]Extractor Estimating: 27it [00:18,  1.46it/s]Extractor Estimating: 28it [00:19,  1.45it/s]Extractor Estimating: 29it [00:20,  1.47it/s]Extractor Estimating: 30it [00:21,  1.48it/s]Extractor Estimating: 31it [00:21,  1.42it/s]Extractor Estimating: 32it [00:22,  1.35it/s]Extractor Estimating: 33it [00:23,  1.41it/s]Extractor Estimating: 34it [00:23,  1.42it/s]Extractor Estimating: 35it [00:24,  1.43it/s]Extractor Estimating: 36it [00:25,  1.44it/s]Extractor Estimating: 37it [00:26,  1.42it/s]Extractor Estimating: 38it [00:26,  1.48it/s]Extractor Estimating: 39it [00:27,  1.45it/s]Extractor Estimating: 40it [00:28,  1.47it/s]Extractor Estimating: 41it [00:28,  1.44it/s]Extractor Estimating: 42it [00:29,  1.43it/s]Extractor Estimating: 43it [00:30,  1.49it/s]Extractor Estimating: 44it [00:30,  1.51it/s]Extractor Estimating: 45it [00:31,  1.49it/s]Extractor Estimating: 46it [00:32,  1.45it/s]Extractor Estimating: 47it [00:32,  1.45it/s]Extractor Estimating: 48it [00:33,  1.44it/s]Extractor Estimating: 49it [00:34,  1.46it/s]Extractor Estimating: 50it [00:34,  1.50it/s]Extractor Estimating: 51it [00:35,  1.48it/s]Extractor Estimating: 52it [00:36,  1.48it/s]Extractor Estimating: 53it [00:36,  1.53it/s]Extractor Estimating: 54it [00:37,  1.53it/s]Extractor Estimating: 55it [00:38,  1.58it/s]Extractor Estimating: 56it [00:38,  1.59it/s]Extractor Estimating: 57it [00:39,  1.60it/s]Extractor Estimating: 58it [00:39,  1.65it/s]Extractor Estimating: 59it [00:40,  1.59it/s]Extractor Estimating: 60it [00:41,  1.62it/s]Extractor Estimating: 61it [00:41,  1.57it/s]Extractor Estimating: 62it [00:42,  1.60it/s]Extractor Estimating: 63it [00:43,  1.57it/s]Extractor Estimating: 64it [00:43,  1.59it/s]Extractor Estimating: 65it [00:44,  1.63it/s]Extractor Estimating: 66it [00:44,  1.58it/s]Extractor Estimating: 67it [00:45,  1.61it/s]Extractor Estimating: 68it [00:46,  1.49it/s]Extractor Estimating: 69it [00:46,  1.50it/s]Extractor Estimating: 70it [00:47,  1.53it/s]Extractor Estimating: 71it [00:48,  1.57it/s]Extractor Estimating: 72it [00:48,  1.55it/s]Extractor Estimating: 73it [00:49,  1.55it/s]Extractor Estimating: 74it [00:50,  1.56it/s]Extractor Estimating: 75it [00:50,  1.57it/s]Extractor Estimating: 76it [00:51,  1.55it/s]Extractor Estimating: 77it [00:52,  1.46it/s]Extractor Estimating: 78it [00:52,  1.51it/s]Extractor Estimating: 79it [00:53,  1.56it/s]Extractor Estimating: 80it [00:54,  1.54it/s]Extractor Estimating: 81it [00:54,  1.47it/s]Extractor Estimating: 82it [00:55,  1.51it/s]Extractor Estimating: 83it [00:56,  1.43it/s]Extractor Estimating: 84it [00:56,  1.45it/s]Extractor Estimating: 85it [00:57,  1.51it/s]Extractor Estimating: 86it [00:58,  1.54it/s]Extractor Estimating: 87it [00:58,  1.50it/s]Extractor Estimating: 88it [00:59,  1.54it/s]Extractor Estimating: 89it [01:00,  1.57it/s]Extractor Estimating: 90it [01:00,  1.52it/s]Extractor Estimating: 91it [01:01,  1.51it/s]Extractor Estimating: 92it [01:01,  1.58it/s]Extractor Estimating: 93it [01:02,  1.52it/s]Extractor Estimating: 94it [01:03,  1.54it/s]Extractor Estimating: 95it [01:03,  1.55it/s]Extractor Estimating: 96it [01:04,  1.56it/s]Extractor Estimating: 97it [01:05,  1.45it/s]Extractor Estimating: 98it [01:06,  1.42it/s]Extractor Estimating: 99it [01:06,  1.50it/s]Extractor Estimating: 100it [01:07,  1.53it/s]Extractor Estimating: 101it [01:07,  1.53it/s]Extractor Estimating: 102it [01:08,  1.53it/s]Extractor Estimating: 103it [01:09,  1.53it/s]Extractor Estimating: 104it [01:09,  1.50it/s]Extractor Estimating: 105it [01:10,  1.47it/s]Extractor Estimating: 106it [01:11,  1.52it/s]Extractor Estimating: 107it [01:11,  1.54it/s]Extractor Estimating: 108it [01:12,  1.56it/s]Extractor Estimating: 109it [01:13,  1.51it/s]Extractor Estimating: 110it [01:13,  1.51it/s]Extractor Estimating: 111it [01:14,  1.53it/s]Extractor Estimating: 112it [01:15,  1.58it/s]Extractor Estimating: 113it [01:15,  1.55it/s]Extractor Estimating: 114it [01:16,  1.51it/s]Extractor Estimating: 115it [01:17,  1.58it/s]Extractor Estimating: 116it [01:17,  1.58it/s]Extractor Estimating: 117it [01:18,  1.53it/s]Extractor Estimating: 118it [01:19,  1.52it/s]Extractor Estimating: 119it [01:19,  1.55it/s]Extractor Estimating: 120it [01:20,  1.54it/s]Extractor Estimating: 121it [01:20,  1.58it/s]Extractor Estimating: 122it [01:21,  1.57it/s]Extractor Estimating: 123it [01:22,  1.59it/s]Extractor Estimating: 124it [01:22,  1.61it/s]Extractor Estimating: 125it [01:23,  1.57it/s]Extractor Estimating: 126it [01:24,  1.56it/s]Extractor Estimating: 127it [01:24,  1.59it/s]Extractor Estimating: 128it [01:25,  1.63it/s]Extractor Estimating: 129it [01:25,  1.64it/s]Extractor Estimating: 130it [01:26,  1.63it/s]Extractor Estimating: 131it [01:27,  1.65it/s]Extractor Estimating: 132it [01:27,  1.60it/s]Extractor Estimating: 133it [01:28,  1.64it/s]Extractor Estimating: 134it [01:28,  1.69it/s]Extractor Estimating: 135it [01:29,  1.65it/s]Extractor Estimating: 136it [01:30,  1.69it/s]Extractor Estimating: 137it [01:30,  1.65it/s]Extractor Estimating: 138it [01:31,  1.60it/s]Extractor Estimating: 139it [01:32,  1.60it/s]Extractor Estimating: 140it [01:32,  1.63it/s]Extractor Estimating: 141it [01:33,  1.63it/s]Extractor Estimating: 142it [01:33,  1.67it/s]Extractor Estimating: 143it [01:34,  1.67it/s]Extractor Estimating: 144it [01:35,  1.65it/s]Extractor Estimating: 145it [01:35,  1.63it/s]Extractor Estimating: 146it [01:36,  1.60it/s]Extractor Estimating: 147it [01:36,  1.62it/s]Extractor Estimating: 148it [01:37,  1.60it/s]Extractor Estimating: 149it [01:38,  1.59it/s]Extractor Estimating: 150it [01:38,  1.63it/s]Extractor Estimating: 151it [01:39,  1.60it/s]Extractor Estimating: 152it [01:40,  1.54it/s]Extractor Estimating: 153it [01:40,  1.59it/s]Extractor Estimating: 154it [01:41,  1.61it/s]Extractor Estimating: 155it [01:41,  1.62it/s]Extractor Estimating: 156it [01:42,  1.62it/s]Extractor Estimating: 157it [01:43,  1.61it/s]Extractor Estimating: 158it [01:43,  1.58it/s]Extractor Estimating: 159it [01:44,  1.54it/s]Extractor Estimating: 160it [01:45,  1.59it/s]Extractor Estimating: 161it [01:45,  1.65it/s]Extractor Estimating: 162it [01:46,  1.65it/s]Extractor Estimating: 163it [01:46,  1.63it/s]Extractor Estimating: 164it [01:47,  1.67it/s]Extractor Estimating: 165it [01:48,  1.63it/s]Extractor Estimating: 166it [01:48,  1.57it/s]Extractor Estimating: 167it [01:49,  1.61it/s]Extractor Estimating: 168it [01:49,  1.62it/s]Extractor Estimating: 169it [01:50,  1.59it/s]Extractor Estimating: 170it [01:51,  1.58it/s]Extractor Estimating: 171it [01:52,  1.49it/s]Extractor Estimating: 172it [01:52,  1.54it/s]Extractor Estimating: 173it [01:53,  1.51it/s]Extractor Estimating: 174it [01:53,  1.53it/s]Extractor Estimating: 175it [01:54,  1.56it/s]Extractor Estimating: 176it [01:55,  1.47it/s]Extractor Estimating: 177it [01:56,  1.47it/s]Extractor Estimating: 178it [01:56,  1.50it/s]Extractor Estimating: 179it [01:57,  1.49it/s]Extractor Estimating: 180it [01:57,  1.51it/s]Extractor Estimating: 181it [01:58,  1.54it/s]Extractor Estimating: 182it [01:59,  1.54it/s]Extractor Estimating: 183it [01:59,  1.53it/s]Extractor Estimating: 184it [02:00,  1.53it/s]Extractor Estimating: 185it [02:01,  1.52it/s]Extractor Estimating: 186it [02:01,  1.53it/s]Extractor Estimating: 187it [02:02,  1.57it/s]Extractor Estimating: 188it [02:03,  1.54it/s]Extractor Estimating: 189it [02:03,  1.55it/s]Extractor Estimating: 190it [02:04,  1.49it/s]Extractor Estimating: 191it [02:05,  1.50it/s]Extractor Estimating: 192it [02:05,  1.47it/s]Extractor Estimating: 193it [02:06,  1.46it/s]Extractor Estimating: 194it [02:07,  1.50it/s]Extractor Estimating: 195it [02:07,  1.53it/s]Extractor Estimating: 196it [02:08,  1.45it/s]Extractor Estimating: 197it [02:09,  1.49it/s]Extractor Estimating: 198it [02:09,  1.49it/s]Extractor Estimating: 199it [02:10,  1.52it/s]Extractor Estimating: 200it [02:11,  1.54it/s]Extractor Estimating: 201it [02:11,  1.58it/s]Extractor Estimating: 202it [02:12,  1.54it/s]Extractor Estimating: 203it [02:13,  1.54it/s]Extractor Estimating: 204it [02:13,  1.57it/s]Extractor Estimating: 205it [02:14,  1.57it/s]Extractor Estimating: 206it [02:14,  1.55it/s]Extractor Estimating: 207it [02:15,  1.52it/s]Extractor Estimating: 208it [02:16,  1.53it/s]Extractor Estimating: 209it [02:16,  1.58it/s]Extractor Estimating: 210it [02:17,  1.62it/s]Extractor Estimating: 211it [02:18,  1.58it/s]Extractor Estimating: 212it [02:18,  1.56it/s]Extractor Estimating: 213it [02:19,  1.57it/s]Extractor Estimating: 214it [02:20,  1.57it/s]Extractor Estimating: 215it [02:20,  1.52it/s]Extractor Estimating: 216it [02:21,  1.52it/s]Extractor Estimating: 217it [02:22,  1.50it/s]Extractor Estimating: 218it [02:22,  1.50it/s]Extractor Estimating: 219it [02:23,  1.51it/s]Extractor Estimating: 220it [02:24,  1.51it/s]Extractor Estimating: 221it [02:24,  1.51it/s]Extractor Estimating: 222it [02:25,  1.47it/s]Extractor Estimating: 223it [02:26,  1.47it/s]Extractor Estimating: 224it [02:26,  1.51it/s]Extractor Estimating: 225it [02:27,  1.55it/s]Extractor Estimating: 226it [02:28,  1.58it/s]Extractor Estimating: 227it [02:28,  1.63it/s]Extractor Estimating: 228it [02:29,  1.60it/s]Extractor Estimating: 229it [02:29,  1.62it/s]Extractor Estimating: 230it [02:30,  1.63it/s]Extractor Estimating: 231it [02:31,  1.58it/s]Extractor Estimating: 232it [02:31,  1.59it/s]Extractor Estimating: 233it [02:32,  1.60it/s]Extractor Estimating: 234it [02:33,  1.55it/s]Extractor Estimating: 235it [02:33,  1.56it/s]Extractor Estimating: 236it [02:34,  1.58it/s]Extractor Estimating: 237it [02:34,  1.57it/s]Extractor Estimating: 238it [02:35,  1.55it/s]Extractor Estimating: 239it [02:36,  1.54it/s]Extractor Estimating: 240it [02:36,  1.54it/s]Extractor Estimating: 241it [02:37,  1.54it/s]Extractor Estimating: 242it [02:38,  1.55it/s]Extractor Estimating: 243it [02:38,  1.60it/s]Extractor Estimating: 244it [02:39,  1.68it/s]Extractor Estimating: 245it [02:39,  1.70it/s]Extractor Estimating: 246it [02:40,  1.51it/s]Extractor Estimating: 247it [02:41,  1.53it/s]Extractor Estimating: 248it [02:41,  1.53it/s]Extractor Estimating: 249it [02:42,  1.54it/s]Extractor Estimating: 250it [02:43,  1.58it/s]Extractor Estimating: 251it [02:43,  1.59it/s]Extractor Estimating: 252it [02:44,  1.56it/s]Extractor Estimating: 253it [02:45,  1.57it/s]Extractor Estimating: 254it [02:45,  1.59it/s]Extractor Estimating: 255it [02:46,  1.58it/s]Extractor Estimating: 256it [02:47,  1.55it/s]Extractor Estimating: 257it [02:47,  1.53it/s]Extractor Estimating: 258it [02:48,  1.56it/s]Extractor Estimating: 259it [02:48,  1.55it/s]Extractor Estimating: 260it [02:49,  1.53it/s]Extractor Estimating: 261it [02:50,  1.52it/s]Extractor Estimating: 262it [02:50,  1.52it/s]Extractor Estimating: 263it [02:51,  1.54it/s]Extractor Estimating: 264it [02:52,  1.53it/s]Extractor Estimating: 265it [02:52,  1.53it/s]Extractor Estimating: 266it [02:53,  1.56it/s]Extractor Estimating: 267it [02:54,  1.59it/s]Extractor Estimating: 268it [02:54,  1.58it/s]Extractor Estimating: 269it [02:55,  1.57it/s]Extractor Estimating: 270it [02:56,  1.60it/s]Extractor Estimating: 271it [02:56,  1.63it/s]Extractor Estimating: 272it [02:57,  1.62it/s]Extractor Estimating: 273it [02:57,  1.61it/s]Extractor Estimating: 274it [02:58,  1.63it/s]Extractor Estimating: 275it [02:59,  1.69it/s]Extractor Estimating: 276it [02:59,  1.67it/s]Extractor Estimating: 277it [03:00,  1.58it/s]Extractor Estimating: 278it [03:00,  1.59it/s]Extractor Estimating: 279it [03:01,  1.56it/s]Extractor Estimating: 280it [03:02,  1.54it/s]Extractor Estimating: 281it [03:02,  1.53it/s]Extractor Estimating: 282it [03:03,  1.51it/s]Extractor Estimating: 283it [03:04,  1.51it/s]Extractor Estimating: 284it [03:05,  1.49it/s]Extractor Estimating: 285it [03:05,  1.55it/s]Extractor Estimating: 286it [03:06,  1.57it/s]Extractor Estimating: 287it [03:06,  1.53it/s]Extractor Estimating: 288it [03:07,  1.52it/s]Extractor Estimating: 289it [03:08,  1.57it/s]Extractor Estimating: 290it [03:08,  1.58it/s]Extractor Estimating: 291it [03:09,  1.54it/s]Extractor Estimating: 292it [03:10,  1.54it/s]Extractor Estimating: 293it [03:10,  1.62it/s]Extractor Estimating: 294it [03:11,  1.62it/s]Extractor Estimating: 295it [03:11,  1.57it/s]Extractor Estimating: 296it [03:12,  1.56it/s]Extractor Estimating: 297it [03:13,  1.56it/s]Extractor Estimating: 298it [03:13,  1.58it/s]Extractor Estimating: 299it [03:14,  1.58it/s]Extractor Estimating: 300it [03:15,  1.56it/s]Extractor Estimating: 301it [03:15,  1.58it/s]Extractor Estimating: 302it [03:16,  1.66it/s]Extractor Estimating: 303it [03:16,  1.67it/s]Extractor Estimating: 304it [03:17,  1.64it/s]Extractor Estimating: 305it [03:18,  1.66it/s]Extractor Estimating: 306it [03:18,  1.77it/s]Extractor Estimating: 307it [03:19,  1.61it/s]Extractor Estimating: 308it [03:20,  1.56it/s]Extractor Estimating: 309it [03:20,  1.63it/s]Extractor Estimating: 310it [03:21,  1.66it/s]Extractor Estimating: 311it [03:21,  1.64it/s]Extractor Estimating: 312it [03:22,  1.61it/s]Extractor Estimating: 313it [03:22,  1.68it/s]Extractor Estimating: 314it [03:23,  1.64it/s]Extractor Estimating: 315it [03:24,  1.67it/s]Extractor Estimating: 316it [03:24,  1.62it/s]Extractor Estimating: 317it [03:25,  1.59it/s]Extractor Estimating: 318it [03:26,  1.59it/s]Extractor Estimating: 319it [03:26,  1.46it/s]Extractor Estimating: 320it [03:27,  1.53it/s]Extractor Estimating: 321it [03:28,  1.60it/s]Extractor Estimating: 322it [03:28,  1.67it/s]Extractor Estimating: 323it [03:29,  1.67it/s]Extractor Estimating: 324it [03:29,  1.67it/s]Extractor Estimating: 325it [03:30,  1.67it/s]Extractor Estimating: 326it [03:31,  1.64it/s]Extractor Estimating: 327it [03:31,  1.63it/s]Extractor Estimating: 328it [03:32,  1.47it/s]Extractor Estimating: 329it [03:33,  1.46it/s]Extractor Estimating: 330it [03:33,  1.49it/s]Extractor Estimating: 331it [03:34,  1.53it/s]Extractor Estimating: 332it [03:35,  1.51it/s]Extractor Estimating: 333it [03:35,  1.57it/s]Extractor Estimating: 334it [03:36,  1.61it/s]Extractor Estimating: 335it [03:36,  1.58it/s]Extractor Estimating: 336it [03:37,  1.57it/s]Extractor Estimating: 337it [03:38,  1.55it/s]Extractor Estimating: 338it [03:38,  1.57it/s]Extractor Estimating: 339it [03:39,  1.56it/s]Extractor Estimating: 340it [03:40,  1.55it/s]Extractor Estimating: 341it [03:40,  1.57it/s]Extractor Estimating: 342it [03:41,  1.55it/s]Extractor Estimating: 343it [03:42,  1.56it/s]Extractor Estimating: 344it [03:42,  1.59it/s]Extractor Estimating: 345it [03:43,  1.57it/s]Extractor Estimating: 346it [03:43,  1.59it/s]Extractor Estimating: 347it [03:44,  1.64it/s]Extractor Estimating: 348it [03:45,  1.64it/s]Extractor Estimating: 349it [03:45,  1.64it/s]Extractor Estimating: 350it [03:46,  1.65it/s]Extractor Estimating: 351it [03:46,  1.66it/s]Extractor Estimating: 352it [03:47,  1.59it/s]Extractor Estimating: 353it [03:48,  1.59it/s]Extractor Estimating: 354it [03:48,  1.56it/s]Extractor Estimating: 355it [03:49,  1.63it/s]Extractor Estimating: 356it [03:50,  1.58it/s]Extractor Estimating: 357it [03:50,  1.66it/s]Extractor Estimating: 358it [03:51,  1.72it/s]Extractor Estimating: 359it [03:51,  1.67it/s]Extractor Estimating: 360it [03:52,  1.67it/s]Extractor Estimating: 361it [03:53,  1.67it/s]Extractor Estimating: 362it [03:53,  1.56it/s]Extractor Estimating: 363it [03:54,  1.56it/s]Extractor Estimating: 364it [03:55,  1.53it/s]Extractor Estimating: 365it [03:55,  1.49it/s]Extractor Estimating: 366it [03:56,  1.54it/s]Extractor Estimating: 367it [03:57,  1.60it/s]Extractor Estimating: 368it [03:57,  1.64it/s]Extractor Estimating: 369it [03:58,  1.67it/s]Extractor Estimating: 370it [03:58,  1.65it/s]Extractor Estimating: 371it [03:59,  1.61it/s]Extractor Estimating: 372it [04:00,  1.62it/s]Extractor Estimating: 373it [04:00,  1.71it/s]Extractor Estimating: 374it [04:01,  1.66it/s]Extractor Estimating: 375it [04:01,  1.67it/s]Extractor Estimating: 375it [04:01,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:24,376 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:24,381 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:24,381 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:24,381 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:24,381 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:25:24,703 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:25:24,704 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:25:24,969 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:25:26,036 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:25:26,036 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:27,764 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:27,771 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:27,771 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:27,771 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:27,771 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:25:28,099 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:25:28,100 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:25:28,367 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:25:28,523 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:25:28,523 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 18:48:01,092 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 18:48:01,116 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7496 mean pseudo reward: 0.956423294458651
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 20264
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20364, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20364, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.101, loss:648.4000
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.091, loss:612.7502
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.103, loss:620.0901
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.094, loss:571.8649
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.107, loss:588.6520
>> valid entity prec:0.4532, rec:0.4841, f1:0.4681
>> valid relation prec:0.0379, rec:0.0101, f1:0.0159
>> valid relation with NER prec:0.0379, rec:0.0101, f1:0.0159
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.495, loss:590.0063
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.103, loss:557.1464
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.108, loss:556.9715
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.092, loss:594.4538
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.156, loss:568.0012
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4583, rec:0.5337, f1:0.4931
>> valid relation prec:0.0453, rec:0.0135, f1:0.0208
>> valid relation with NER prec:0.0453, rec:0.0135, f1:0.0208
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.496, loss:551.0823
g_step 1200, step 261, avg_time 1.142, loss:598.4024
g_step 1300, step 48, avg_time 1.099, loss:563.4886
g_step 1400, step 148, avg_time 1.094, loss:548.4369
g_step 1500, step 248, avg_time 1.101, loss:555.8029
>> valid entity prec:0.4828, rec:0.4960, f1:0.4893
>> valid relation prec:0.0534, rec:0.0135, f1:0.0216
>> valid relation with NER prec:0.0534, rec:0.0135, f1:0.0216
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.488, loss:544.4942
g_step 1700, step 135, avg_time 1.115, loss:504.6241
g_step 1800, step 235, avg_time 1.077, loss:519.9772
g_step 1900, step 22, avg_time 1.087, loss:513.4699
g_step 2000, step 122, avg_time 1.103, loss:508.1733
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4745, rec:0.4653, f1:0.4699
>> valid relation prec:0.0384, rec:0.0118, f1:0.0180
>> valid relation with NER prec:0.0384, rec:0.0118, f1:0.0180
g_step 2100, step 222, avg_time 2.494, loss:482.9529
g_step 2200, step 9, avg_time 1.081, loss:490.9525
g_step 2300, step 109, avg_time 1.099, loss:463.0875
g_step 2400, step 209, avg_time 1.085, loss:493.7098
g_step 2500, step 309, avg_time 1.103, loss:483.8100
>> valid entity prec:0.4888, rec:0.4847, f1:0.4867
>> valid relation prec:0.0333, rec:0.0118, f1:0.0174
>> valid relation with NER prec:0.0333, rec:0.0118, f1:0.0174
g_step 2600, step 96, avg_time 2.496, loss:436.6157
g_step 2700, step 196, avg_time 1.093, loss:438.9518
g_step 2800, step 296, avg_time 1.095, loss:470.7865
g_step 2900, step 83, avg_time 1.105, loss:425.8891
g_step 3000, step 183, avg_time 1.094, loss:436.5199
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4674, rec:0.4793, f1:0.4733
>> valid relation prec:0.0526, rec:0.0178, f1:0.0266
>> valid relation with NER prec:0.0526, rec:0.0178, f1:0.0266
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 283, avg_time 2.488, loss:443.1650
g_step 3200, step 70, avg_time 1.091, loss:432.9995
g_step 3300, step 170, avg_time 1.106, loss:413.7223
g_step 3400, step 270, avg_time 1.097, loss:419.3113
g_step 3500, step 57, avg_time 1.080, loss:394.4786
>> valid entity prec:0.4972, rec:0.3853, f1:0.4341
>> valid relation prec:0.0541, rec:0.0149, f1:0.0234
>> valid relation with NER prec:0.0541, rec:0.0149, f1:0.0234
g_step 3600, step 157, avg_time 2.476, loss:402.3601
g_step 3700, step 257, avg_time 1.104, loss:412.0950
g_step 3800, step 44, avg_time 1.103, loss:392.2777
g_step 3900, step 144, avg_time 1.088, loss:376.9549
g_step 4000, step 244, avg_time 1.099, loss:390.0640
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4599, rec:0.3890, f1:0.4215
>> valid relation prec:0.0399, rec:0.0129, f1:0.0195
>> valid relation with NER prec:0.0399, rec:0.0129, f1:0.0195
g_step 4100, step 31, avg_time 2.490, loss:382.0357
g_step 4200, step 131, avg_time 1.096, loss:356.0412
g_step 4300, step 231, avg_time 1.101, loss:379.5834
g_step 4400, step 18, avg_time 1.077, loss:395.4515
g_step 4500, step 118, avg_time 1.088, loss:342.1599
>> valid entity prec:0.4641, rec:0.4395, f1:0.4514
>> valid relation prec:0.0549, rec:0.0152, f1:0.0239
>> valid relation with NER prec:0.0549, rec:0.0152, f1:0.0239
g_step 4600, step 218, avg_time 2.502, loss:362.4477
g_step 4700, step 5, avg_time 1.095, loss:369.0353
g_step 4800, step 105, avg_time 1.094, loss:336.4088
g_step 4900, step 205, avg_time 1.097, loss:346.3006
g_step 5000, step 305, avg_time 1.114, loss:371.7200
learning rate was adjusted to 0.0008
>> valid entity prec:0.4784, rec:0.4010, f1:0.4363
>> valid relation prec:0.0513, rec:0.0167, f1:0.0252
>> valid relation with NER prec:0.0513, rec:0.0167, f1:0.0252
g_step 5100, step 92, avg_time 2.467, loss:324.4522
g_step 5200, step 192, avg_time 1.101, loss:318.7508
g_step 5300, step 292, avg_time 1.106, loss:354.2956
g_step 5400, step 79, avg_time 1.103, loss:312.7998
g_step 5500, step 179, avg_time 1.106, loss:324.1991
>> valid entity prec:0.4584, rec:0.4864, f1:0.4720
>> valid relation prec:0.0291, rec:0.0126, f1:0.0176
>> valid relation with NER prec:0.0291, rec:0.0126, f1:0.0176
g_step 5600, step 279, avg_time 2.488, loss:340.6325
g_step 5700, step 66, avg_time 1.095, loss:308.1081
g_step 5800, step 166, avg_time 1.096, loss:308.2073
g_step 5900, step 266, avg_time 1.100, loss:307.3027
g_step 6000, step 53, avg_time 1.088, loss:292.7659
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4564, rec:0.4996, f1:0.4770
>> valid relation prec:0.0532, rec:0.0204, f1:0.0295
>> valid relation with NER prec:0.0532, rec:0.0204, f1:0.0295
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6100, step 153, avg_time 2.494, loss:295.0407
g_step 6200, step 253, avg_time 1.115, loss:311.0520
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:48:01 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:48:01 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-48-01_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:48:02 - WARNING - datasets.builder -   Using custom data configuration default-a5fbd47606ddc83c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a5fbd47606ddc83c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:48:03,251 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:48:03,252 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:48:03,252 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:48:03,253 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:48:03,268 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:03,280 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:03,280 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:03,280 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:03,280 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:03,280 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:03,280 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:48:03,497 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:48:06,613 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:48:06,615 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a5fbd47606ddc83c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.68ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.66ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.19ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.47ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.61ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.69ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.75ba/s]100%|██████████| 8/8 [00:01<00:00,  5.65ba/s]100%|██████████| 8/8 [00:01<00:00,  4.71ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.89ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.24ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.38ba/s]100%|██████████| 4/4 [00:00<00:00,  5.54ba/s]100%|██████████| 4/4 [00:00<00:00,  4.97ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.35ba/s] 25%|██▌       | 2/8 [00:00<00:00,  7.26ba/s] 50%|█████     | 4/8 [00:00<00:00,  9.10ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.64ba/s]100%|██████████| 8/8 [00:00<00:00, 10.99ba/s]100%|██████████| 8/8 [00:00<00:00,  9.81ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.29ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.78ba/s]100%|██████████| 4/4 [00:00<00:00, 11.13ba/s]
[INFO|trainer.py:414] 2023-08-28 18:48:11,692 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:48:11,749 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:48:11,749 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-28 18:48:11,749 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:48:11,749 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:48:11,749 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:48:11,749 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:48:11,749 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:57,  3.30it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:49,  3.44it/s]  1%|          | 4/585 [00:01<02:48,  3.46it/s]  1%|          | 5/585 [00:01<02:47,  3.47it/s]  1%|          | 6/585 [00:01<02:46,  3.47it/s]  1%|          | 7/585 [00:02<02:46,  3.48it/s]  1%|▏         | 8/585 [00:02<02:45,  3.48it/s]  2%|▏         | 9/585 [00:02<02:45,  3.48it/s]  2%|▏         | 10/585 [00:02<02:45,  3.48it/s]  2%|▏         | 11/585 [00:03<02:44,  3.48it/s]  2%|▏         | 12/585 [00:03<02:48,  3.39it/s]  2%|▏         | 13/585 [00:03<02:47,  3.42it/s]  2%|▏         | 14/585 [00:04<02:46,  3.43it/s]  3%|▎         | 15/585 [00:04<02:45,  3.45it/s]  3%|▎         | 16/585 [00:04<02:44,  3.46it/s]  3%|▎         | 17/585 [00:04<02:43,  3.47it/s]  3%|▎         | 18/585 [00:05<02:43,  3.47it/s]  3%|▎         | 19/585 [00:05<02:42,  3.48it/s]  3%|▎         | 20/585 [00:05<02:42,  3.48it/s]  4%|▎         | 21/585 [00:06<02:42,  3.48it/s]  4%|▍         | 22/585 [00:06<02:41,  3.48it/s]  4%|▍         | 23/585 [00:06<03:09,  2.97it/s]  4%|▍         | 24/585 [00:07<03:00,  3.11it/s]  4%|▍         | 25/585 [00:07<02:54,  3.22it/s]  4%|▍         | 26/585 [00:07<02:49,  3.29it/s]  5%|▍         | 27/585 [00:07<02:46,  3.35it/s]  5%|▍         | 28/585 [00:08<02:44,  3.39it/s]  5%|▍         | 29/585 [00:08<02:42,  3.41it/s]  5%|▌         | 30/585 [00:08<02:41,  3.43it/s]  5%|▌         | 31/585 [00:09<02:40,  3.45it/s]  5%|▌         | 32/585 [00:09<02:40,  3.45it/s]  6%|▌         | 33/585 [00:09<02:41,  3.42it/s]  6%|▌         | 34/585 [00:09<02:40,  3.43it/s]  6%|▌         | 35/585 [00:10<02:39,  3.44it/s]  6%|▌         | 36/585 [00:10<02:39,  3.45it/s]  6%|▋         | 37/585 [00:10<02:38,  3.46it/s]  6%|▋         | 38/585 [00:11<02:38,  3.46it/s]  7%|▋         | 39/585 [00:11<02:37,  3.46it/s]  7%|▋         | 40/585 [00:11<02:37,  3.46it/s]  7%|▋         | 41/585 [00:12<02:36,  3.47it/s]  7%|▋         | 42/585 [00:12<02:36,  3.47it/s]  7%|▋         | 43/585 [00:12<02:36,  3.47it/s]  8%|▊         | 44/585 [00:13<03:07,  2.88it/s]  8%|▊         | 45/585 [00:13<02:57,  3.04it/s]  8%|▊         | 46/585 [00:13<02:50,  3.16it/s]  8%|▊         | 47/585 [00:13<02:45,  3.25it/s]  8%|▊         | 48/585 [00:14<02:41,  3.32it/s]  8%|▊         | 49/585 [00:14<02:39,  3.36it/s]  9%|▊         | 50/585 [00:14<02:37,  3.39it/s]  9%|▊         | 51/585 [00:15<02:36,  3.41it/s]  9%|▉         | 52/585 [00:15<02:35,  3.44it/s]  9%|▉         | 53/585 [00:15<02:34,  3.45it/s]  9%|▉         | 54/585 [00:16<03:22,  2.62it/s]  9%|▉         | 55/585 [00:16<03:07,  2.83it/s] 10%|▉         | 56/585 [00:16<02:56,  3.00it/s] 10%|▉         | 57/585 [00:17<02:48,  3.13it/s] 10%|▉         | 58/585 [00:17<02:43,  3.22it/s] 10%|█         | 59/585 [00:17<02:39,  3.29it/s] 10%|█         | 60/585 [00:17<02:36,  3.34it/s] 10%|█         | 61/585 [00:18<02:34,  3.38it/s] 11%|█         | 62/585 [00:18<02:33,  3.41it/s] 11%|█         | 63/585 [00:18<02:32,  3.43it/s] 11%|█         | 64/585 [00:19<02:35,  3.36it/s] 11%|█         | 65/585 [00:19<02:33,  3.39it/s] 11%|█▏        | 66/585 [00:19<02:32,  3.41it/s] 11%|█▏        | 67/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 68/585 [00:20<02:30,  3.45it/s] 12%|█▏        | 69/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 71/585 [00:21<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:21<02:28,  3.47it/s] 12%|█▏        | 73/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 74/585 [00:22<02:27,  3.47it/s] 13%|█▎        | 75/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.46it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.46it/s] 13%|█▎        | 78/585 [00:23<02:26,  3.47it/s] 14%|█▎        | 79/585 [00:23<02:25,  3.47it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 81/585 [00:24<02:25,  3.47it/s] 14%|█▍        | 82/585 [00:24<02:24,  3.47it/s] 14%|█▍        | 83/585 [00:24<02:24,  3.47it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 85/585 [00:25<02:24,  3.47it/s] 15%|█▍        | 86/585 [00:25<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 88/585 [00:26<02:23,  3.47it/s] 15%|█▌        | 89/585 [00:26<02:22,  3.47it/s] 15%|█▌        | 90/585 [00:26<02:22,  3.48it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.48it/s] 16%|█▌        | 92/585 [00:27<02:21,  3.48it/s] 16%|█▌        | 93/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:28<02:21,  3.47it/s] 16%|█▋        | 96/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 97/585 [00:28<02:27,  3.30it/s] 17%|█▋        | 98/585 [00:28<02:25,  3.35it/s] 17%|█▋        | 99/585 [00:29<02:23,  3.38it/s] 17%|█▋        | 100/585 [00:29<02:22,  3.41it/s] 17%|█▋        | 101/585 [00:29<02:21,  3.42it/s] 17%|█▋        | 102/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 103/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:31<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:31<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:31<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.45it/s] 19%|█▊        | 109/585 [00:32<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:32<02:17,  3.46it/s] 19%|█▉        | 111/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 112/585 [00:33<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:33<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 116/585 [00:34<02:17,  3.41it/s] 20%|██        | 117/585 [00:34<02:16,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 18:48:46,293 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:48:46,293 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:48:46,293 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.44it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.61it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.89it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.17it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.55it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.10it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.79it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.55it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.52it/s][A
 12%|█▏        | 53/435 [00:01<00:10, 37.27it/s][A
 13%|█▎        | 58/435 [00:01<00:09, 39.67it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 41.61it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 43.05it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 44.12it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 44.86it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.49it/s][A
 20%|██        | 88/435 [00:01<00:07, 45.88it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 45.65it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.90it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.17it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.36it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 41.85it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 43.15it/s][A
 28%|██▊       | 123/435 [00:02<00:07, 44.29it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 45.02it/s][A
 31%|███       | 133/435 [00:02<00:06, 45.60it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 46.04it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.23it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.41it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 45.95it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 45.96it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.17it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.44it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.50it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.60it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 46.78it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.71it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.71it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.52it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 46.31it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.38it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.50it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.53it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.62it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.70it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.67it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.71it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.65it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.45it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 43.74it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 44.57it/s][A
 60%|██████    | 263/435 [00:05<00:03, 45.20it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 45.75it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.01it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 46.23it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.42it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.52it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.12it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.10it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.37it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.43it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.55it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.58it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 46.61it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.60it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.54it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.41it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.30it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.38it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.50it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.52it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.66it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 46.66it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.63it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.64it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.54it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.42it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 45.11it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 45.63it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 45.94it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.19it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 46.37it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.45it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.52it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.49it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.26it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:44<02:16,  3.42it/s]
100%|██████████| 435/435 [00:09<00:00, 46.26it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:48:55,821 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 18:48:55,850 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:49:01,823 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:49:01,851 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:49:01,859 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:08<1:20:04, 10.29s/it] 20%|██        | 119/585 [01:08<57:13,  7.37s/it]   21%|██        | 120/585 [01:08<40:38,  5.24s/it] 21%|██        | 121/585 [01:09<29:03,  3.76s/it] 21%|██        | 122/585 [01:09<20:57,  2.72s/it] 21%|██        | 123/585 [01:09<15:17,  1.99s/it] 21%|██        | 124/585 [01:10<11:20,  1.48s/it] 21%|██▏       | 125/585 [01:10<08:35,  1.12s/it] 22%|██▏       | 126/585 [01:10<06:39,  1.15it/s] 22%|██▏       | 127/585 [01:10<05:18,  1.44it/s] 22%|██▏       | 128/585 [01:11<04:21,  1.75it/s] 22%|██▏       | 129/585 [01:11<03:49,  1.99it/s] 22%|██▏       | 130/585 [01:11<03:19,  2.28it/s] 22%|██▏       | 131/585 [01:12<02:58,  2.55it/s] 23%|██▎       | 132/585 [01:12<02:43,  2.77it/s] 23%|██▎       | 133/585 [01:12<02:33,  2.95it/s] 23%|██▎       | 134/585 [01:13<02:25,  3.09it/s] 23%|██▎       | 135/585 [01:13<02:20,  3.20it/s] 23%|██▎       | 136/585 [01:13<02:16,  3.28it/s] 23%|██▎       | 137/585 [01:13<02:14,  3.34it/s] 24%|██▎       | 138/585 [01:14<02:12,  3.38it/s] 24%|██▍       | 139/585 [01:14<02:10,  3.41it/s] 24%|██▍       | 140/585 [01:14<02:14,  3.30it/s] 24%|██▍       | 141/585 [01:15<02:12,  3.36it/s] 24%|██▍       | 142/585 [01:15<02:10,  3.39it/s] 24%|██▍       | 143/585 [01:15<02:09,  3.42it/s] 25%|██▍       | 144/585 [01:15<02:08,  3.43it/s] 25%|██▍       | 145/585 [01:16<02:07,  3.45it/s] 25%|██▍       | 146/585 [01:16<02:07,  3.46it/s] 25%|██▌       | 147/585 [01:16<02:06,  3.46it/s] 25%|██▌       | 148/585 [01:17<02:06,  3.47it/s] 25%|██▌       | 149/585 [01:17<02:05,  3.47it/s] 26%|██▌       | 150/585 [01:17<02:05,  3.47it/s] 26%|██▌       | 151/585 [01:18<02:21,  3.07it/s] 26%|██▌       | 152/585 [01:18<02:16,  3.18it/s] 26%|██▌       | 153/585 [01:18<02:12,  3.26it/s] 26%|██▋       | 154/585 [01:18<02:09,  3.32it/s] 26%|██▋       | 155/585 [01:19<02:07,  3.37it/s] 27%|██▋       | 156/585 [01:19<02:06,  3.40it/s] 27%|██▋       | 157/585 [01:19<02:05,  3.42it/s] 27%|██▋       | 158/585 [01:20<02:04,  3.44it/s] 27%|██▋       | 159/585 [01:20<02:03,  3.45it/s] 27%|██▋       | 160/585 [01:20<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:21<02:13,  3.19it/s] 28%|██▊       | 162/585 [01:21<02:09,  3.27it/s] 28%|██▊       | 163/585 [01:21<02:06,  3.33it/s] 28%|██▊       | 164/585 [01:21<02:05,  3.37it/s] 28%|██▊       | 165/585 [01:22<02:03,  3.40it/s] 28%|██▊       | 166/585 [01:22<02:02,  3.42it/s] 29%|██▊       | 167/585 [01:22<02:01,  3.44it/s] 29%|██▊       | 168/585 [01:23<02:00,  3.45it/s] 29%|██▉       | 169/585 [01:23<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:23<01:59,  3.46it/s] 29%|██▉       | 171/585 [01:23<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:24<02:03,  3.35it/s] 30%|██▉       | 173/585 [01:24<02:01,  3.39it/s] 30%|██▉       | 174/585 [01:24<02:00,  3.41it/s] 30%|██▉       | 175/585 [01:25<01:59,  3.43it/s] 30%|███       | 176/585 [01:25<01:58,  3.44it/s] 30%|███       | 177/585 [01:25<01:58,  3.45it/s] 30%|███       | 178/585 [01:26<02:09,  3.14it/s] 31%|███       | 179/585 [01:27<03:27,  1.95it/s] 31%|███       | 180/585 [01:27<03:46,  1.79it/s] 31%|███       | 181/585 [01:27<03:12,  2.10it/s] 31%|███       | 182/585 [01:28<02:49,  2.38it/s] 31%|███▏      | 183/585 [01:28<02:32,  2.63it/s] 31%|███▏      | 184/585 [01:28<02:21,  2.84it/s] 32%|███▏      | 185/585 [01:29<02:13,  3.00it/s] 32%|███▏      | 186/585 [01:29<02:07,  3.12it/s] 32%|███▏      | 187/585 [01:29<02:03,  3.22it/s] 32%|███▏      | 188/585 [01:29<02:00,  3.29it/s] 32%|███▏      | 189/585 [01:30<01:58,  3.34it/s] 32%|███▏      | 190/585 [01:30<02:03,  3.20it/s] 33%|███▎      | 191/585 [01:30<02:00,  3.27it/s] 33%|███▎      | 192/585 [01:31<01:58,  3.33it/s] 33%|███▎      | 193/585 [01:31<01:56,  3.37it/s] 33%|███▎      | 194/585 [01:31<01:55,  3.40it/s] 33%|███▎      | 195/585 [01:32<01:54,  3.42it/s] 34%|███▎      | 196/585 [01:32<01:53,  3.43it/s] 34%|███▎      | 197/585 [01:32<01:52,  3.43it/s] 34%|███▍      | 198/585 [01:32<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:33<01:52,  3.44it/s] 34%|███▍      | 200/585 [01:33<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:33<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:34<01:50,  3.46it/s] 35%|███▍      | 203/585 [01:34<01:50,  3.46it/s] 35%|███▍      | 204/585 [01:34<01:50,  3.46it/s] 35%|███▌      | 205/585 [01:34<01:49,  3.47it/s] 35%|███▌      | 206/585 [01:35<01:49,  3.47it/s] 35%|███▌      | 207/585 [01:35<01:52,  3.37it/s] 36%|███▌      | 208/585 [01:35<01:51,  3.40it/s] 36%|███▌      | 209/585 [01:36<01:50,  3.42it/s] 36%|███▌      | 210/585 [01:36<01:49,  3.43it/s] 36%|███▌      | 211/585 [01:36<01:48,  3.44it/s] 36%|███▌      | 212/585 [01:37<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:37<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:37<01:47,  3.46it/s] 37%|███▋      | 215/585 [01:37<01:47,  3.46it/s] 37%|███▋      | 216/585 [01:38<01:46,  3.46it/s] 37%|███▋      | 217/585 [01:38<01:46,  3.46it/s] 37%|███▋      | 218/585 [01:38<01:48,  3.39it/s] 37%|███▋      | 219/585 [01:39<01:47,  3.41it/s] 38%|███▊      | 220/585 [01:39<01:46,  3.43it/s] 38%|███▊      | 221/585 [01:39<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:39<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:40<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:40<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:40<01:44,  3.46it/s] 39%|███▊      | 226/585 [01:41<01:43,  3.46it/s] 39%|███▉      | 227/585 [01:41<01:43,  3.46it/s] 39%|███▉      | 228/585 [01:41<01:43,  3.46it/s] 39%|███▉      | 229/585 [01:41<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:42<01:42,  3.45it/s] 39%|███▉      | 231/585 [01:42<01:42,  3.46it/s] 40%|███▉      | 232/585 [01:42<01:42,  3.46it/s] 40%|███▉      | 233/585 [01:43<01:41,  3.46it/s] 40%|████      | 234/585 [01:43<01:41,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 18:49:55,177 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:49:55,178 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:49:55,178 >>   Batch size = 8
{'eval_loss': 0.9908778667449951, 'eval_runtime': 9.5102, 'eval_samples_per_second': 365.817, 'eval_steps_per_second': 45.74, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.17it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.73it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.75it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.06it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.49it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.05it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.85it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.53it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.57it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.64it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.66it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 42.90it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 43.96it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 44.75it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 45.35it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.75it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.11it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 46.29it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.40it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.21it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.17it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.27it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.40it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.41it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.50it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.59it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.64it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.62it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.36it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.32it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.40it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.46it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.48it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.52it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.58it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.63it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.59it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.44it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.31it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 44.41it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 45.15it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.61it/s][A
 50%|█████     | 218/435 [00:04<00:04, 45.90it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.10it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.26it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.43it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.45it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.18it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.20it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.24it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.41it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.44it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.49it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.55it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.56it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.54it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.41it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.31it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.37it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.53it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.52it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.57it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.52it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.58it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.59it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.31it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.37it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.44it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.40it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.51it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.57it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.58it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.61it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.60it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.42it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.35it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.32it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.37it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.51it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.56it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.57it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.65it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.61it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.53it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.45it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.32it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:52<01:41,  3.46it/s]
100%|██████████| 435/435 [00:09<00:00, 46.32it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:50:04,654 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 18:50:04,677 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:50:10,940 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:50:11,004 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:50:11,015 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:12<52:18,  8.97s/it] 40%|████      | 236/585 [02:12<37:06,  6.38s/it] 41%|████      | 237/585 [02:13<26:24,  4.55s/it] 41%|████      | 238/585 [02:13<18:55,  3.27s/it] 41%|████      | 239/585 [02:13<13:42,  2.38s/it] 41%|████      | 240/585 [02:14<10:03,  1.75s/it] 41%|████      | 241/585 [02:14<07:31,  1.31s/it] 41%|████▏     | 242/585 [02:14<05:44,  1.00s/it] 42%|████▏     | 243/585 [02:14<04:29,  1.27it/s] 42%|████▏     | 244/585 [02:15<03:37,  1.57it/s] 42%|████▏     | 245/585 [02:15<03:01,  1.87it/s] 42%|████▏     | 246/585 [02:15<02:35,  2.18it/s] 42%|████▏     | 247/585 [02:16<02:52,  1.96it/s] 42%|████▏     | 248/585 [02:16<02:29,  2.25it/s] 43%|████▎     | 249/585 [02:17<02:13,  2.52it/s] 43%|████▎     | 250/585 [02:17<02:02,  2.74it/s] 43%|████▎     | 251/585 [02:17<01:54,  2.93it/s] 43%|████▎     | 252/585 [02:17<01:48,  3.07it/s] 43%|████▎     | 253/585 [02:18<01:44,  3.18it/s] 43%|████▎     | 254/585 [02:18<01:41,  3.27it/s] 44%|████▎     | 255/585 [02:18<01:39,  3.33it/s] 44%|████▍     | 256/585 [02:19<01:37,  3.37it/s] 44%|████▍     | 257/585 [02:19<02:08,  2.56it/s] 44%|████▍     | 258/585 [02:19<01:57,  2.78it/s] 44%|████▍     | 259/585 [02:20<01:50,  2.95it/s] 44%|████▍     | 260/585 [02:20<01:45,  3.09it/s] 45%|████▍     | 261/585 [02:20<01:41,  3.20it/s] 45%|████▍     | 262/585 [02:21<01:38,  3.27it/s] 45%|████▍     | 263/585 [02:21<01:36,  3.33it/s] 45%|████▌     | 264/585 [02:21<01:35,  3.37it/s] 45%|████▌     | 265/585 [02:21<01:34,  3.40it/s] 45%|████▌     | 266/585 [02:22<01:33,  3.42it/s] 46%|████▌     | 267/585 [02:22<01:33,  3.41it/s] 46%|████▌     | 268/585 [02:22<01:32,  3.43it/s] 46%|████▌     | 269/585 [02:23<01:31,  3.44it/s] 46%|████▌     | 270/585 [02:23<01:31,  3.45it/s] 46%|████▋     | 271/585 [02:23<01:30,  3.45it/s] 46%|████▋     | 272/585 [02:23<01:30,  3.46it/s] 47%|████▋     | 273/585 [02:24<01:30,  3.47it/s] 47%|████▋     | 274/585 [02:24<01:29,  3.47it/s] 47%|████▋     | 275/585 [02:24<01:29,  3.47it/s] 47%|████▋     | 276/585 [02:25<01:29,  3.47it/s] 47%|████▋     | 277/585 [02:25<01:28,  3.47it/s] 48%|████▊     | 278/585 [02:25<01:30,  3.41it/s] 48%|████▊     | 279/585 [02:26<01:29,  3.43it/s] 48%|████▊     | 280/585 [02:26<01:28,  3.44it/s] 48%|████▊     | 281/585 [02:26<01:28,  3.45it/s] 48%|████▊     | 282/585 [02:26<01:27,  3.46it/s] 48%|████▊     | 283/585 [02:27<01:44,  2.89it/s] 49%|████▊     | 284/585 [02:27<01:39,  3.03it/s] 49%|████▊     | 285/585 [02:27<01:35,  3.15it/s] 49%|████▉     | 286/585 [02:28<01:32,  3.24it/s] 49%|████▉     | 287/585 [02:28<01:30,  3.30it/s] 49%|████▉     | 288/585 [02:28<01:38,  3.01it/s] 49%|████▉     | 289/585 [02:29<01:34,  3.13it/s] 50%|████▉     | 290/585 [02:29<01:31,  3.22it/s] 50%|████▉     | 291/585 [02:29<01:29,  3.29it/s] 50%|████▉     | 292/585 [02:30<01:27,  3.35it/s] 50%|█████     | 293/585 [02:30<01:26,  3.38it/s] 50%|█████     | 294/585 [02:30<01:25,  3.40it/s] 50%|█████     | 295/585 [02:30<01:24,  3.42it/s] 51%|█████     | 296/585 [02:31<01:24,  3.43it/s] 51%|█████     | 297/585 [02:31<01:23,  3.44it/s] 51%|█████     | 298/585 [02:31<01:23,  3.45it/s] 51%|█████     | 299/585 [02:32<01:26,  3.30it/s] 51%|█████▏    | 300/585 [02:32<01:25,  3.34it/s] 51%|█████▏    | 301/585 [02:32<01:24,  3.38it/s] 52%|█████▏    | 302/585 [02:33<01:23,  3.40it/s] 52%|█████▏    | 303/585 [02:33<01:22,  3.42it/s] 52%|█████▏    | 304/585 [02:33<01:21,  3.43it/s] 52%|█████▏    | 305/585 [02:33<01:21,  3.44it/s] 52%|█████▏    | 306/585 [02:34<01:21,  3.44it/s] 52%|█████▏    | 307/585 [02:34<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:34<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:35<01:19,  3.46it/s] 53%|█████▎    | 310/585 [02:35<01:19,  3.46it/s] 53%|█████▎    | 311/585 [02:35<01:19,  3.46it/s] 53%|█████▎    | 312/585 [02:35<01:18,  3.46it/s] 54%|█████▎    | 313/585 [02:36<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:36<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:36<01:18,  3.46it/s] 54%|█████▍    | 316/585 [02:37<01:18,  3.42it/s] 54%|█████▍    | 317/585 [02:37<01:18,  3.43it/s] 54%|█████▍    | 318/585 [02:37<01:17,  3.44it/s] 55%|█████▍    | 319/585 [02:37<01:17,  3.44it/s] 55%|█████▍    | 320/585 [02:38<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:38<01:16,  3.46it/s] 55%|█████▌    | 322/585 [02:38<01:16,  3.46it/s] 55%|█████▌    | 323/585 [02:39<01:15,  3.46it/s] 55%|█████▌    | 324/585 [02:39<01:15,  3.46it/s] 56%|█████▌    | 325/585 [02:39<01:15,  3.46it/s] 56%|█████▌    | 326/585 [02:39<01:14,  3.46it/s] 56%|█████▌    | 327/585 [02:40<01:15,  3.42it/s] 56%|█████▌    | 328/585 [02:40<01:14,  3.44it/s] 56%|█████▌    | 329/585 [02:40<01:14,  3.44it/s] 56%|█████▋    | 330/585 [02:41<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:41<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:41<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:41<01:12,  3.46it/s] 57%|█████▋    | 334/585 [02:42<01:12,  3.46it/s] 57%|█████▋    | 335/585 [02:42<01:12,  3.46it/s] 57%|█████▋    | 336/585 [02:42<01:11,  3.46it/s] 58%|█████▊    | 337/585 [02:43<01:11,  3.46it/s] 58%|█████▊    | 338/585 [02:43<01:12,  3.41it/s] 58%|█████▊    | 339/585 [02:43<01:11,  3.42it/s] 58%|█████▊    | 340/585 [02:44<01:11,  3.44it/s] 58%|█████▊    | 341/585 [02:44<01:10,  3.44it/s] 58%|█████▊    | 342/585 [02:44<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:44<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:45<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:45<01:09,  3.46it/s] 59%|█████▉    | 346/585 [02:45<01:09,  3.46it/s] 59%|█████▉    | 347/585 [02:46<01:08,  3.46it/s] 59%|█████▉    | 348/585 [02:46<01:08,  3.46it/s] 60%|█████▉    | 349/585 [02:46<01:08,  3.45it/s] 60%|█████▉    | 350/585 [02:46<01:08,  3.45it/s] 60%|██████    | 351/585 [02:47<01:07,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 18:50:58,998 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:50:58,998 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:50:58,998 >>   Batch size = 8
{'eval_loss': 1.0039069652557373, 'eval_runtime': 9.4094, 'eval_samples_per_second': 369.738, 'eval_steps_per_second': 46.23, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.21it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.35it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.69it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.00it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.50it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.04it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.80it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.50it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.60it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.62it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.68it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.69it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.70it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.69it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.51it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.40it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.39it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.45it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.47it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.54it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.51it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.31it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.40it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.41it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.21it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.27it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.30it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.41it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.46it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.55it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.61it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.66it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.63it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.35it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.36it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.37it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.51it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.59it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.60it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.57it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.56it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.55it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.53it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.33it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.41it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.42it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.47it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.55it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.60it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.57it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.47it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.54it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.40it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.42it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.42it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.55it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.59it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.65it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.44it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.47it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.35it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.30it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.30it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 46.32it/s][A
 75%|███████▌  | 328/435 [00:07<00:05, 18.23it/s][A
 77%|███████▋  | 333/435 [00:07<00:04, 22.33it/s][A
 78%|███████▊  | 338/435 [00:07<00:03, 26.48it/s][A
 79%|███████▉  | 343/435 [00:07<00:03, 30.43it/s][A
 80%|████████  | 348/435 [00:08<00:02, 33.97it/s][A
 81%|████████  | 353/435 [00:08<00:02, 36.99it/s][A
 82%|████████▏ | 358/435 [00:08<00:01, 39.43it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 41.39it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 42.47it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 43.52it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 44.30it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 44.98it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 45.54it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 45.87it/s][A
 91%|█████████▏| 398/435 [00:09<00:00, 46.13it/s][A
 93%|█████████▎| 403/435 [00:09<00:00, 46.24it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 46.13it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 46.10it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.06it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.23it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.31it/s][A
100%|█████████▉| 433/435 [00:10<00:00, 46.37it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:10<00:00, 46.37it/s][A 60%|██████    | 351/585 [02:57<01:07,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:51:09,595 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 18:51:09,818 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:51:14,167 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:51:14,186 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:51:14,194 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:17<36:14,  9.33s/it] 60%|██████    | 353/585 [03:17<25:36,  6.62s/it] 61%|██████    | 354/585 [03:18<18:10,  4.72s/it] 61%|██████    | 355/585 [03:18<12:59,  3.39s/it] 61%|██████    | 356/585 [03:18<09:23,  2.46s/it] 61%|██████    | 357/585 [03:19<06:52,  1.81s/it] 61%|██████    | 358/585 [03:19<05:06,  1.35s/it] 61%|██████▏   | 359/585 [03:19<03:53,  1.03s/it] 62%|██████▏   | 360/585 [03:19<03:02,  1.24it/s] 62%|██████▏   | 361/585 [03:20<02:26,  1.53it/s] 62%|██████▏   | 362/585 [03:20<02:01,  1.84it/s] 62%|██████▏   | 363/585 [03:20<01:43,  2.14it/s] 62%|██████▏   | 364/585 [03:21<01:31,  2.41it/s] 62%|██████▏   | 365/585 [03:21<01:22,  2.65it/s] 63%|██████▎   | 366/585 [03:21<01:16,  2.85it/s] 63%|██████▎   | 367/585 [03:21<01:12,  3.02it/s] 63%|██████▎   | 368/585 [03:22<01:09,  3.14it/s] 63%|██████▎   | 369/585 [03:22<01:06,  3.23it/s] 63%|██████▎   | 370/585 [03:22<01:05,  3.30it/s] 63%|██████▎   | 371/585 [03:23<01:03,  3.35it/s] 64%|██████▎   | 372/585 [03:23<01:02,  3.39it/s] 64%|██████▍   | 373/585 [03:23<01:02,  3.41it/s] 64%|██████▍   | 374/585 [03:23<01:01,  3.43it/s] 64%|██████▍   | 375/585 [03:24<01:01,  3.43it/s] 64%|██████▍   | 376/585 [03:24<01:00,  3.44it/s] 64%|██████▍   | 377/585 [03:24<01:00,  3.45it/s] 65%|██████▍   | 378/585 [03:25<01:02,  3.29it/s] 65%|██████▍   | 379/585 [03:25<01:01,  3.34it/s] 65%|██████▍   | 380/585 [03:25<01:00,  3.38it/s] 65%|██████▌   | 381/585 [03:26<01:00,  3.38it/s] 65%|██████▌   | 382/585 [03:26<00:59,  3.41it/s] 65%|██████▌   | 383/585 [03:26<00:59,  3.40it/s] 66%|██████▌   | 384/585 [03:26<00:59,  3.39it/s] 66%|██████▌   | 385/585 [03:27<00:58,  3.40it/s] 66%|██████▌   | 386/585 [03:27<00:58,  3.42it/s] 66%|██████▌   | 387/585 [03:27<00:57,  3.43it/s] 66%|██████▋   | 388/585 [03:28<00:57,  3.44it/s] 66%|██████▋   | 389/585 [03:28<00:56,  3.44it/s] 67%|██████▋   | 390/585 [03:28<00:56,  3.45it/s] 67%|██████▋   | 391/585 [03:28<00:56,  3.45it/s] 67%|██████▋   | 392/585 [03:29<00:55,  3.46it/s] 67%|██████▋   | 393/585 [03:29<00:55,  3.46it/s] 67%|██████▋   | 394/585 [03:29<00:55,  3.46it/s] 68%|██████▊   | 395/585 [03:30<00:54,  3.47it/s] 68%|██████▊   | 396/585 [03:30<00:54,  3.47it/s] 68%|██████▊   | 397/585 [03:30<00:54,  3.47it/s] 68%|██████▊   | 398/585 [03:30<00:53,  3.47it/s] 68%|██████▊   | 399/585 [03:31<00:53,  3.47it/s] 68%|██████▊   | 400/585 [03:31<00:54,  3.39it/s] 69%|██████▊   | 401/585 [03:31<00:53,  3.42it/s] 69%|██████▊   | 402/585 [03:32<00:53,  3.43it/s] 69%|██████▉   | 403/585 [03:32<00:52,  3.44it/s] 69%|██████▉   | 404/585 [03:32<00:52,  3.45it/s] 69%|██████▉   | 405/585 [03:33<00:52,  3.45it/s] 69%|██████▉   | 406/585 [03:33<00:51,  3.46it/s] 70%|██████▉   | 407/585 [03:33<00:51,  3.46it/s] 70%|██████▉   | 408/585 [03:33<00:51,  3.46it/s] 70%|██████▉   | 409/585 [03:34<00:50,  3.46it/s] 70%|███████   | 410/585 [03:34<00:50,  3.47it/s] 70%|███████   | 411/585 [03:34<00:54,  3.21it/s] 70%|███████   | 412/585 [03:35<00:52,  3.28it/s] 71%|███████   | 413/585 [03:35<00:51,  3.34it/s] 71%|███████   | 414/585 [03:35<00:50,  3.37it/s] 71%|███████   | 415/585 [03:35<00:49,  3.40it/s] 71%|███████   | 416/585 [03:36<00:49,  3.42it/s] 71%|███████▏  | 417/585 [03:36<00:48,  3.43it/s] 71%|███████▏  | 418/585 [03:36<00:48,  3.44it/s] 72%|███████▏  | 419/585 [03:37<00:48,  3.45it/s] 72%|███████▏  | 420/585 [03:37<00:47,  3.46it/s] 72%|███████▏  | 421/585 [03:37<00:47,  3.46it/s] 72%|███████▏  | 422/585 [03:38<00:47,  3.42it/s] 72%|███████▏  | 423/585 [03:38<00:47,  3.44it/s] 72%|███████▏  | 424/585 [03:38<00:46,  3.45it/s] 73%|███████▎  | 425/585 [03:38<00:46,  3.45it/s] 73%|███████▎  | 426/585 [03:39<00:46,  3.46it/s] 73%|███████▎  | 427/585 [03:39<00:45,  3.46it/s] 73%|███████▎  | 428/585 [03:39<00:45,  3.46it/s] 73%|███████▎  | 429/585 [03:40<00:45,  3.46it/s] 74%|███████▎  | 430/585 [03:40<00:44,  3.46it/s] 74%|███████▎  | 431/585 [03:40<00:44,  3.46it/s] 74%|███████▍  | 432/585 [03:40<00:44,  3.46it/s] 74%|███████▍  | 433/585 [03:41<00:44,  3.42it/s] 74%|███████▍  | 434/585 [03:41<00:43,  3.44it/s] 74%|███████▍  | 435/585 [03:41<00:43,  3.44it/s] 75%|███████▍  | 436/585 [03:42<00:43,  3.45it/s] 75%|███████▍  | 437/585 [03:42<00:42,  3.45it/s] 75%|███████▍  | 438/585 [03:42<00:42,  3.46it/s] 75%|███████▌  | 439/585 [03:42<00:42,  3.45it/s] 75%|███████▌  | 440/585 [03:43<00:41,  3.46it/s] 75%|███████▌  | 441/585 [03:43<00:41,  3.46it/s] 76%|███████▌  | 442/585 [03:43<00:41,  3.46it/s] 76%|███████▌  | 443/585 [03:44<00:41,  3.46it/s] 76%|███████▌  | 444/585 [03:44<00:43,  3.25it/s] 76%|███████▌  | 445/585 [03:44<00:42,  3.31it/s] 76%|███████▌  | 446/585 [03:45<00:41,  3.35it/s] 76%|███████▋  | 447/585 [03:45<00:40,  3.38it/s] 77%|███████▋  | 448/585 [03:45<00:40,  3.41it/s] 77%|███████▋  | 449/585 [03:45<00:39,  3.42it/s] 77%|███████▋  | 450/585 [03:46<00:39,  3.43it/s] 77%|███████▋  | 451/585 [03:46<00:38,  3.44it/s] 77%|███████▋  | 452/585 [03:46<00:38,  3.45it/s] 77%|███████▋  | 453/585 [03:47<00:38,  3.45it/s] 78%|███████▊  | 454/585 [03:47<00:37,  3.45it/s] 78%|███████▊  | 455/585 [03:47<00:40,  3.19it/s] 78%|███████▊  | 456/585 [03:47<00:39,  3.26it/s] 78%|███████▊  | 457/585 [03:48<00:38,  3.32it/s] 78%|███████▊  | 458/585 [03:48<00:37,  3.36it/s] 78%|███████▊  | 459/585 [03:48<00:37,  3.39it/s] 79%|███████▊  | 460/585 [03:49<00:36,  3.41it/s] 79%|███████▉  | 461/585 [03:49<00:36,  3.43it/s] 79%|███████▉  | 462/585 [03:49<00:35,  3.44it/s] 79%|███████▉  | 463/585 [03:50<00:35,  3.44it/s] 79%|███████▉  | 464/585 [03:50<00:35,  3.44it/s] 79%|███████▉  | 465/585 [03:50<00:34,  3.45it/s] 80%|███████▉  | 466/585 [03:50<00:36,  3.27it/s] 80%|███████▉  | 467/585 [03:51<00:35,  3.33it/s] 80%|████████  | 468/585 [03:51<00:34,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 18:52:03,307 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:52:03,307 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:52:03,307 >>   Batch size = 8
{'eval_loss': 1.013410210609436, 'eval_runtime': 10.0763, 'eval_samples_per_second': 345.265, 'eval_steps_per_second': 43.171, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.88it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.29it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.53it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.90it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.46it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.06it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.86it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.45it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.55it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.63it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.54it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.58it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.59it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.56it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.54it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.43it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.35it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.31it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.35it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.40it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.51it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.49it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.56it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.43it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.34it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.19it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.22it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.23it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.27it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.37it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.36it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.34it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.37it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.30it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.30it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.31it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.40it/s][A
 44%|████▍     | 193/435 [00:04<00:06, 36.31it/s][A
 46%|████▌     | 198/435 [00:04<00:06, 38.87it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 40.89it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 42.43it/s][A
 49%|████▉     | 213/435 [00:04<00:05, 43.65it/s][A
 50%|█████     | 218/435 [00:04<00:04, 44.58it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 45.24it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 45.67it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.28it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 45.52it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.72it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.85it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.18it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.36it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.47it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.57it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.56it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 46.42it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.26it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.24it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.34it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.37it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.41it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.52it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.59it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.59it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 46.51it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 41.12it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 42.53it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 43.74it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 44.61it/s][A
 80%|████████  | 348/435 [00:07<00:01, 45.25it/s][A
 81%|████████  | 353/435 [00:07<00:01, 45.68it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.01it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.16it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 45.81it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.84it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.06it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.19it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.37it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.48it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.50it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.52it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.55it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 46.30it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.20it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.31it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.40it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.46it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [04:01<00:34,  3.37it/s]
100%|██████████| 435/435 [00:09<00:00, 46.46it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:52:13,935 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 18:52:14,216 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:52:18,364 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:52:18,515 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:52:18,564 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:15<14:27,  7.47s/it] 80%|████████  | 470/585 [04:16<10:12,  5.32s/it] 81%|████████  | 471/585 [04:16<07:14,  3.81s/it] 81%|████████  | 472/585 [04:16<05:11,  2.75s/it] 81%|████████  | 473/585 [04:16<03:45,  2.02s/it] 81%|████████  | 474/585 [04:17<02:46,  1.50s/it] 81%|████████  | 475/585 [04:17<02:04,  1.13s/it] 81%|████████▏ | 476/585 [04:17<01:35,  1.14it/s] 82%|████████▏ | 477/585 [04:18<01:15,  1.42it/s] 82%|████████▏ | 478/585 [04:18<01:01,  1.73it/s] 82%|████████▏ | 479/585 [04:18<00:52,  2.04it/s] 82%|████████▏ | 480/585 [04:18<00:45,  2.32it/s] 82%|████████▏ | 481/585 [04:19<00:40,  2.56it/s] 82%|████████▏ | 482/585 [04:19<00:37,  2.78it/s] 83%|████████▎ | 483/585 [04:19<00:34,  2.96it/s] 83%|████████▎ | 484/585 [04:20<00:32,  3.09it/s] 83%|████████▎ | 485/585 [04:20<00:31,  3.19it/s] 83%|████████▎ | 486/585 [04:20<00:30,  3.27it/s] 83%|████████▎ | 487/585 [04:20<00:29,  3.33it/s] 83%|████████▎ | 488/585 [04:21<00:28,  3.37it/s] 84%|████████▎ | 489/585 [04:21<00:28,  3.40it/s] 84%|████████▍ | 490/585 [04:21<00:27,  3.42it/s] 84%|████████▍ | 491/585 [04:22<00:27,  3.43it/s] 84%|████████▍ | 492/585 [04:22<00:27,  3.34it/s] 84%|████████▍ | 493/585 [04:22<00:27,  3.38it/s] 84%|████████▍ | 494/585 [04:22<00:26,  3.40it/s] 85%|████████▍ | 495/585 [04:23<00:26,  3.42it/s] 85%|████████▍ | 496/585 [04:23<00:25,  3.44it/s] 85%|████████▍ | 497/585 [04:23<00:25,  3.44it/s] 85%|████████▌ | 498/585 [04:24<00:25,  3.45it/s] 85%|████████▌ | 499/585 [04:24<00:24,  3.45it/s] 85%|████████▌ | 500/585 [04:24<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [04:24<00:24,  3.46it/s] 86%|████████▌ | 501/585 [04:25<00:24,  3.46it/s] 86%|████████▌ | 502/585 [04:25<00:23,  3.46it/s] 86%|████████▌ | 503/585 [04:25<00:23,  3.43it/s] 86%|████████▌ | 504/585 [04:25<00:23,  3.44it/s] 86%|████████▋ | 505/585 [04:26<00:23,  3.41it/s] 86%|████████▋ | 506/585 [04:26<00:23,  3.43it/s] 87%|████████▋ | 507/585 [04:26<00:22,  3.42it/s] 87%|████████▋ | 508/585 [04:27<00:22,  3.38it/s] 87%|████████▋ | 509/585 [04:27<00:22,  3.40it/s] 87%|████████▋ | 510/585 [04:27<00:22,  3.29it/s] 87%|████████▋ | 511/585 [04:27<00:22,  3.34it/s] 88%|████████▊ | 512/585 [04:28<00:21,  3.37it/s] 88%|████████▊ | 513/585 [04:28<00:21,  3.40it/s] 88%|████████▊ | 514/585 [04:28<00:20,  3.42it/s] 88%|████████▊ | 515/585 [04:29<00:20,  3.43it/s] 88%|████████▊ | 516/585 [04:29<00:20,  3.44it/s] 88%|████████▊ | 517/585 [04:29<00:19,  3.43it/s] 89%|████████▊ | 518/585 [04:30<00:19,  3.44it/s] 89%|████████▊ | 519/585 [04:30<00:19,  3.44it/s] 89%|████████▉ | 520/585 [04:30<00:18,  3.45it/s] 89%|████████▉ | 521/585 [04:30<00:18,  3.46it/s] 89%|████████▉ | 522/585 [04:31<00:18,  3.46it/s] 89%|████████▉ | 523/585 [04:31<00:17,  3.46it/s] 90%|████████▉ | 524/585 [04:31<00:17,  3.46it/s] 90%|████████▉ | 525/585 [04:32<00:17,  3.47it/s] 90%|████████▉ | 526/585 [04:32<00:17,  3.47it/s] 90%|█████████ | 527/585 [04:32<00:16,  3.46it/s] 90%|█████████ | 528/585 [04:32<00:16,  3.44it/s] 90%|█████████ | 529/585 [04:33<00:16,  3.45it/s] 91%|█████████ | 530/585 [04:33<00:15,  3.45it/s] 91%|█████████ | 531/585 [04:33<00:15,  3.46it/s] 91%|█████████ | 532/585 [04:34<00:15,  3.46it/s] 91%|█████████ | 533/585 [04:34<00:15,  3.47it/s] 91%|█████████▏| 534/585 [04:34<00:14,  3.47it/s] 91%|█████████▏| 535/585 [04:34<00:14,  3.46it/s] 92%|█████████▏| 536/585 [04:35<00:14,  3.46it/s] 92%|█████████▏| 537/585 [04:35<00:13,  3.46it/s] 92%|█████████▏| 538/585 [04:35<00:13,  3.46it/s] 92%|█████████▏| 539/585 [04:36<00:13,  3.37it/s] 92%|█████████▏| 540/585 [04:36<00:13,  3.40it/s] 92%|█████████▏| 541/585 [04:36<00:12,  3.41it/s] 93%|█████████▎| 542/585 [04:36<00:12,  3.43it/s] 93%|█████████▎| 543/585 [04:37<00:12,  3.44it/s] 93%|█████████▎| 544/585 [04:37<00:11,  3.44it/s] 93%|█████████▎| 545/585 [04:37<00:11,  3.45it/s] 93%|█████████▎| 546/585 [04:38<00:11,  3.45it/s] 94%|█████████▎| 547/585 [04:38<00:11,  3.45it/s] 94%|█████████▎| 548/585 [04:38<00:10,  3.45it/s] 94%|█████████▍| 549/585 [04:38<00:10,  3.45it/s] 94%|█████████▍| 550/585 [04:39<00:12,  2.71it/s] 94%|█████████▍| 551/585 [04:39<00:11,  2.90it/s] 94%|█████████▍| 552/585 [04:40<00:10,  3.05it/s] 95%|█████████▍| 553/585 [04:40<00:10,  3.16it/s] 95%|█████████▍| 554/585 [04:40<00:09,  3.25it/s] 95%|█████████▍| 555/585 [04:40<00:09,  3.30it/s] 95%|█████████▌| 556/585 [04:41<00:08,  3.36it/s] 95%|█████████▌| 557/585 [04:41<00:08,  3.39it/s] 95%|█████████▌| 558/585 [04:41<00:07,  3.41it/s] 96%|█████████▌| 559/585 [04:42<00:07,  3.43it/s] 96%|█████████▌| 560/585 [04:42<00:07,  3.25it/s] 96%|█████████▌| 561/585 [04:42<00:07,  3.31it/s] 96%|█████████▌| 562/585 [04:43<00:06,  3.35it/s] 96%|█████████▌| 563/585 [04:43<00:06,  3.39it/s] 96%|█████████▋| 564/585 [04:43<00:06,  3.41it/s] 97%|█████████▋| 565/585 [04:43<00:05,  3.42it/s] 97%|█████████▋| 566/585 [04:44<00:05,  3.42it/s] 97%|█████████▋| 567/585 [04:44<00:05,  3.43it/s] 97%|█████████▋| 568/585 [04:44<00:04,  3.43it/s] 97%|█████████▋| 569/585 [04:45<00:04,  3.44it/s] 97%|█████████▋| 570/585 [04:45<00:04,  3.45it/s] 98%|█████████▊| 571/585 [04:45<00:04,  3.23it/s] 98%|█████████▊| 572/585 [04:46<00:03,  3.29it/s] 98%|█████████▊| 573/585 [04:46<00:03,  3.34it/s] 98%|█████████▊| 574/585 [04:46<00:03,  3.38it/s] 98%|█████████▊| 575/585 [04:46<00:02,  3.40it/s] 98%|█████████▊| 576/585 [04:47<00:02,  3.42it/s] 99%|█████████▊| 577/585 [04:47<00:02,  3.43it/s] 99%|█████████▉| 578/585 [04:47<00:02,  3.44it/s] 99%|█████████▉| 579/585 [04:48<00:01,  3.45it/s] 99%|█████████▉| 580/585 [04:48<00:01,  3.45it/s] 99%|█████████▉| 581/585 [04:48<00:01,  3.45it/s] 99%|█████████▉| 582/585 [04:49<00:00,  3.01it/s]100%|█████████▉| 583/585 [04:49<00:00,  3.13it/s]100%|█████████▉| 584/585 [04:49<00:00,  3.23it/s]100%|██████████| 585/585 [04:49<00:00,  3.29it/s][INFO|trainer.py:2140] 2023-08-28 18:53:01,682 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:53:01,682 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:53:01,682 >>   Batch size = 8
{'eval_loss': 1.0201112031936646, 'eval_runtime': 9.5262, 'eval_samples_per_second': 365.203, 'eval_steps_per_second': 45.663, 'epoch': 4.0}
{'loss': 0.5304, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.47it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.64it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.74it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.05it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.63it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.00it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.71it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.41it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.54it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.61it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.64it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.56it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.64it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.67it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.61it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.36it/s][A
 20%|██        | 88/435 [00:01<00:07, 43.38it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 44.18it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 44.96it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 45.50it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.84it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.11it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.34it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.36it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.05it/s][A
 31%|███       | 133/435 [00:02<00:06, 45.98it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.12it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.22it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.38it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.48it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.49it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.53it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.54it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.37it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.25it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.07it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.22it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.39it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.49it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.58it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.65it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.52it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.43it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.29it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 42.61it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 43.84it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 44.70it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.20it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.72it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.02it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.22it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.26it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 45.73it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 45.66it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 45.87it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.12it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.34it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.46it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.53it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.52it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.37it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 45.99it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 45.86it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 45.90it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.11it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 34.05it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 37.07it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 39.51it/s][A
 80%|████████  | 348/435 [00:07<00:02, 41.44it/s][A
 81%|████████  | 353/435 [00:07<00:01, 42.88it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 43.88it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 44.66it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 45.28it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.13it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.43it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 45.65it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 45.92it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.13it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.19it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.39it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.51it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 46.42it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.28it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.25it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.21it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.25it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.25it/s][A100%|██████████| 585/585 [04:59<00:00,  3.29it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:53:12,665 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 18:53:13,825 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:53:17,841 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:53:17,863 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:53:17,873 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:53:27,828 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:53:27,830 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117 (score: 0.9908778667449951).
                                                 100%|██████████| 585/585 [05:20<00:00,  3.29it/s]100%|██████████| 585/585 [05:20<00:00,  1.83it/s]
[INFO|trainer.py:1894] 2023-08-28 18:53:31,808 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 18:53:31,823 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:53:35,501 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:53:35,522 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:53:35,538 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:53:35,794 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:35,794 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:35,794 >>   train_loss               =     0.5267
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:35,794 >>   train_runtime            = 0:05:20.05
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:35,794 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:35,794 >>   train_samples_per_second =    117.153
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:35,794 >>   train_steps_per_second   =      1.828
{'eval_loss': 1.0262075662612915, 'eval_runtime': 9.5604, 'eval_samples_per_second': 363.895, 'eval_steps_per_second': 45.5, 'epoch': 5.0}
{'train_runtime': 320.0518, 'train_samples_per_second': 117.153, 'train_steps_per_second': 1.828, 'train_loss': 0.5267273210052751, 'epoch': 5.0}
08/28/2023 18:53:35 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:53:35,840 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:53:35,840 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:53:35,840 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 58.63it/s]  3%|▎         | 12/435 [00:00<00:08, 51.21it/s]  4%|▍         | 18/435 [00:00<00:08, 49.39it/s]  5%|▌         | 23/435 [00:00<00:08, 48.66it/s]  6%|▋         | 28/435 [00:00<00:08, 48.08it/s]  8%|▊         | 33/435 [00:00<00:08, 47.66it/s]  9%|▊         | 38/435 [00:00<00:08, 47.48it/s] 10%|▉         | 43/435 [00:00<00:08, 47.14it/s] 11%|█         | 48/435 [00:00<00:08, 46.64it/s] 12%|█▏        | 53/435 [00:01<00:08, 46.55it/s] 13%|█▎        | 58/435 [00:01<00:08, 46.61it/s] 14%|█▍        | 63/435 [00:01<00:07, 46.75it/s] 16%|█▌        | 68/435 [00:01<00:07, 46.85it/s] 17%|█▋        | 73/435 [00:01<00:07, 46.99it/s] 18%|█▊        | 78/435 [00:01<00:07, 47.03it/s] 19%|█▉        | 83/435 [00:01<00:07, 46.96it/s] 20%|██        | 88/435 [00:01<00:07, 46.84it/s] 21%|██▏       | 93/435 [00:01<00:07, 46.68it/s] 23%|██▎       | 98/435 [00:02<00:07, 46.32it/s] 24%|██▎       | 103/435 [00:02<00:07, 46.76it/s] 25%|██▍       | 108/435 [00:02<00:07, 46.68it/s] 26%|██▌       | 113/435 [00:02<00:06, 46.74it/s] 27%|██▋       | 118/435 [00:02<00:06, 46.85it/s] 28%|██▊       | 123/435 [00:02<00:06, 46.84it/s] 29%|██▉       | 128/435 [00:02<00:06, 46.77it/s] 31%|███       | 133/435 [00:02<00:06, 46.80it/s] 32%|███▏      | 138/435 [00:02<00:06, 46.87it/s] 33%|███▎      | 143/435 [00:03<00:06, 45.07it/s] 34%|███▍      | 148/435 [00:03<00:06, 45.71it/s] 35%|███▌      | 153/435 [00:03<00:06, 43.45it/s] 37%|███▋      | 159/435 [00:03<00:06, 45.45it/s] 38%|███▊      | 164/435 [00:03<00:05, 45.90it/s] 39%|███▉      | 169/435 [00:03<00:06, 42.10it/s] 40%|████      | 174/435 [00:03<00:06, 41.15it/s] 41%|████      | 179/435 [00:03<00:06, 42.66it/s] 42%|████▏     | 184/435 [00:03<00:05, 43.87it/s] 43%|████▎     | 189/435 [00:04<00:05, 44.65it/s] 45%|████▍     | 194/435 [00:04<00:05, 45.46it/s] 46%|████▌     | 199/435 [00:04<00:05, 45.84it/s] 47%|████▋     | 204/435 [00:04<00:04, 46.24it/s] 48%|████▊     | 209/435 [00:04<00:04, 46.39it/s] 49%|████▉     | 214/435 [00:04<00:04, 46.26it/s] 50%|█████     | 219/435 [00:04<00:04, 46.44it/s] 51%|█████▏    | 224/435 [00:04<00:04, 46.58it/s] 53%|█████▎    | 229/435 [00:04<00:04, 46.67it/s] 54%|█████▍    | 234/435 [00:05<00:04, 46.79it/s] 55%|█████▍    | 239/435 [00:05<00:04, 46.83it/s] 56%|█████▌    | 244/435 [00:05<00:04, 46.71it/s] 57%|█████▋    | 249/435 [00:05<00:03, 46.64it/s] 58%|█████▊    | 254/435 [00:05<00:03, 46.70it/s] 60%|█████▉    | 259/435 [00:05<00:03, 46.70it/s] 61%|██████    | 264/435 [00:05<00:03, 46.73it/s] 62%|██████▏   | 269/435 [00:05<00:03, 46.74it/s] 63%|██████▎   | 274/435 [00:05<00:03, 46.81it/s] 64%|██████▍   | 279/435 [00:06<00:03, 46.74it/s] 65%|██████▌   | 284/435 [00:06<00:03, 46.76it/s] 66%|██████▋   | 289/435 [00:06<00:03, 46.87it/s] 68%|██████▊   | 294/435 [00:06<00:03, 46.94it/s] 69%|██████▊   | 299/435 [00:06<00:02, 46.88it/s] 70%|██████▉   | 304/435 [00:06<00:02, 46.70it/s] 71%|███████   | 309/435 [00:06<00:02, 46.74it/s] 72%|███████▏  | 314/435 [00:06<00:02, 46.69it/s] 73%|███████▎  | 319/435 [00:06<00:02, 46.78it/s] 74%|███████▍  | 324/435 [00:06<00:02, 46.77it/s] 76%|███████▌  | 329/435 [00:07<00:02, 46.76it/s] 77%|███████▋  | 334/435 [00:07<00:02, 46.88it/s] 78%|███████▊  | 339/435 [00:07<00:02, 46.94it/s] 79%|███████▉  | 344/435 [00:07<00:01, 46.73it/s] 80%|████████  | 349/435 [00:07<00:01, 46.64it/s] 81%|████████▏ | 354/435 [00:07<00:01, 46.69it/s] 83%|████████▎ | 359/435 [00:07<00:01, 46.64it/s] 84%|████████▎ | 364/435 [00:07<00:01, 46.60it/s] 85%|████████▍ | 369/435 [00:07<00:01, 46.66it/s] 86%|████████▌ | 374/435 [00:08<00:01, 46.74it/s] 87%|████████▋ | 379/435 [00:08<00:01, 46.70it/s] 88%|████████▊ | 384/435 [00:08<00:01, 46.80it/s] 89%|████████▉ | 389/435 [00:08<00:00, 46.87it/s] 91%|█████████ | 394/435 [00:08<00:00, 46.86it/s] 92%|█████████▏| 399/435 [00:08<00:00, 46.62it/s] 93%|█████████▎| 404/435 [00:08<00:00, 46.62it/s] 94%|█████████▍| 409/435 [00:08<00:00, 46.54it/s] 95%|█████████▌| 414/435 [00:08<00:00, 46.63it/s] 96%|█████████▋| 419/435 [00:09<00:00, 46.75it/s] 97%|█████████▋| 424/435 [00:09<00:00, 46.70it/s] 99%|█████████▊| 429/435 [00:09<00:00, 46.77it/s]100%|█████████▉| 434/435 [00:09<00:00, 46.86it/s]100%|██████████| 435/435 [00:09<00:00, 46.46it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:53:45,226 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:45,226 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:45,226 >>   eval_loss               =     0.9909
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:45,226 >>   eval_runtime            = 0:00:09.38
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:45,226 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:45,226 >>   eval_samples_per_second =    370.671
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:45,226 >>   eval_steps_per_second   =     46.347
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:45,226 >>   perplexity              =     2.6936
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:51,525 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:51,535 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:51,535 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:51,535 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:51,535 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:53:52,222 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:53:52,223 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:53:52,480 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:53:53,509 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:53:53,509 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:56,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:56,107 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:56,107 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:56,107 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:56,107 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:53:56,448 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:53:56,449 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:53:56,718 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:53:56,876 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:53:56,876 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.39it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.38it/s]Extractor Predicting: 7it [00:05,  1.38it/s]Extractor Predicting: 8it [00:05,  1.39it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:07,  1.43it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:09,  1.38it/s]Extractor Predicting: 14it [00:10,  1.38it/s]Extractor Predicting: 15it [00:10,  1.42it/s]Extractor Predicting: 16it [00:11,  1.40it/s]Extractor Predicting: 17it [00:12,  1.43it/s]Extractor Predicting: 18it [00:12,  1.47it/s]Extractor Predicting: 19it [00:13,  1.43it/s]Extractor Predicting: 20it [00:14,  1.44it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.46it/s]Extractor Predicting: 23it [00:16,  1.45it/s]Extractor Predicting: 24it [00:17,  1.43it/s]Extractor Predicting: 25it [00:17,  1.40it/s]Extractor Predicting: 26it [00:18,  1.38it/s]Extractor Predicting: 27it [00:19,  1.38it/s]Extractor Predicting: 28it [00:19,  1.42it/s]Extractor Predicting: 29it [00:20,  1.40it/s]Extractor Predicting: 30it [00:21,  1.37it/s]Extractor Predicting: 31it [00:22,  1.38it/s]Extractor Predicting: 32it [00:22,  1.38it/s]Extractor Predicting: 33it [00:23,  1.39it/s]Extractor Predicting: 34it [00:24,  1.42it/s]Extractor Predicting: 35it [00:24,  1.43it/s]Extractor Predicting: 36it [00:25,  1.43it/s]Extractor Predicting: 37it [00:26,  1.44it/s]Extractor Predicting: 38it [00:26,  1.45it/s]Extractor Predicting: 39it [00:27,  1.46it/s]Extractor Predicting: 40it [00:28,  1.44it/s]Extractor Predicting: 41it [00:29,  1.45it/s]Extractor Predicting: 42it [00:29,  1.44it/s]Extractor Predicting: 43it [00:30,  1.47it/s]Extractor Predicting: 44it [00:31,  1.48it/s]Extractor Predicting: 45it [00:31,  1.46it/s]Extractor Predicting: 46it [00:32,  1.45it/s]Extractor Predicting: 47it [00:33,  1.43it/s]Extractor Predicting: 48it [00:33,  1.45it/s]Extractor Predicting: 49it [00:34,  1.44it/s]Extractor Predicting: 50it [00:35,  1.45it/s]Extractor Predicting: 51it [00:35,  1.46it/s]Extractor Predicting: 52it [00:36,  1.46it/s]Extractor Predicting: 53it [00:37,  1.46it/s]Extractor Predicting: 54it [00:37,  1.45it/s]Extractor Predicting: 55it [00:38,  1.42it/s]Extractor Predicting: 56it [00:39,  1.45it/s]Extractor Predicting: 57it [00:40,  1.43it/s]Extractor Predicting: 58it [00:40,  1.43it/s]Extractor Predicting: 59it [00:41,  1.46it/s]Extractor Predicting: 60it [00:42,  1.40it/s]Extractor Predicting: 61it [00:42,  1.44it/s]Extractor Predicting: 62it [00:43,  1.43it/s]Extractor Predicting: 63it [00:44,  1.42it/s]Extractor Predicting: 64it [00:44,  1.44it/s]Extractor Predicting: 65it [00:45,  1.46it/s]Extractor Predicting: 66it [00:46,  1.45it/s]Extractor Predicting: 67it [00:47,  1.46it/s]Extractor Predicting: 68it [00:47,  1.46it/s]Extractor Predicting: 69it [00:48,  1.51it/s]Extractor Predicting: 70it [00:48,  1.51it/s]Extractor Predicting: 71it [00:49,  1.51it/s]Extractor Predicting: 72it [00:50,  1.48it/s]Extractor Predicting: 73it [00:50,  1.49it/s]Extractor Predicting: 74it [00:51,  1.48it/s]Extractor Predicting: 75it [00:52,  1.44it/s]Extractor Predicting: 76it [00:53,  1.43it/s]Extractor Predicting: 77it [00:53,  1.45it/s]Extractor Predicting: 78it [00:54,  1.48it/s]Extractor Predicting: 79it [00:55,  1.51it/s]Extractor Predicting: 80it [00:55,  1.50it/s]Extractor Predicting: 81it [00:56,  1.48it/s]Extractor Predicting: 82it [00:57,  1.47it/s]Extractor Predicting: 83it [00:57,  1.49it/s]Extractor Predicting: 84it [00:58,  1.49it/s]Extractor Predicting: 85it [00:59,  1.50it/s]Extractor Predicting: 86it [00:59,  1.51it/s]Extractor Predicting: 87it [01:00,  1.53it/s]Extractor Predicting: 88it [01:01,  1.52it/s]Extractor Predicting: 89it [01:01,  1.52it/s]Extractor Predicting: 90it [01:02,  1.53it/s]Extractor Predicting: 91it [01:03,  1.53it/s]Extractor Predicting: 92it [01:03,  1.57it/s]Extractor Predicting: 93it [01:04,  1.54it/s]Extractor Predicting: 94it [01:04,  1.53it/s]Extractor Predicting: 95it [01:05,  1.54it/s]Extractor Predicting: 96it [01:06,  1.53it/s]Extractor Predicting: 97it [01:06,  1.53it/s]Extractor Predicting: 98it [01:07,  1.52it/s]Extractor Predicting: 99it [01:08,  1.50it/s]Extractor Predicting: 100it [01:08,  1.47it/s]Extractor Predicting: 101it [01:09,  1.47it/s]Extractor Predicting: 102it [01:10,  1.54it/s]Extractor Predicting: 103it [01:10,  1.55it/s]Extractor Predicting: 104it [01:11,  1.54it/s]Extractor Predicting: 105it [01:12,  1.54it/s]Extractor Predicting: 106it [01:12,  1.53it/s]Extractor Predicting: 107it [01:13,  1.52it/s]Extractor Predicting: 108it [01:14,  1.52it/s]Extractor Predicting: 109it [01:14,  1.53it/s]Extractor Predicting: 110it [01:15,  1.53it/s]Extractor Predicting: 111it [01:16,  1.53it/s]Extractor Predicting: 112it [01:16,  1.54it/s]Extractor Predicting: 113it [01:17,  1.57it/s]Extractor Predicting: 114it [01:18,  1.57it/s]Extractor Predicting: 115it [01:18,  1.59it/s]Extractor Predicting: 116it [01:19,  1.55it/s]Extractor Predicting: 117it [01:20,  1.51it/s]Extractor Predicting: 118it [01:20,  1.51it/s]Extractor Predicting: 119it [01:21,  1.48it/s]Extractor Predicting: 120it [01:22,  1.50it/s]Extractor Predicting: 121it [01:22,  1.48it/s]Extractor Predicting: 122it [01:23,  1.46it/s]Extractor Predicting: 123it [01:24,  1.47it/s]Extractor Predicting: 124it [01:24,  1.48it/s]Extractor Predicting: 125it [01:25,  1.50it/s]Extractor Predicting: 126it [01:26,  1.48it/s]Extractor Predicting: 127it [01:26,  1.50it/s]Extractor Predicting: 128it [01:27,  1.50it/s]Extractor Predicting: 129it [01:28,  1.48it/s]Extractor Predicting: 130it [01:28,  1.47it/s]Extractor Predicting: 131it [01:29,  1.49it/s]Extractor Predicting: 132it [01:30,  1.47it/s]Extractor Predicting: 133it [01:30,  1.42it/s]Extractor Predicting: 134it [01:31,  1.42it/s]Extractor Predicting: 135it [01:32,  1.44it/s]Extractor Predicting: 136it [01:33,  1.33it/s]Extractor Predicting: 137it [01:33,  1.36it/s]Extractor Predicting: 138it [01:34,  1.36it/s]Extractor Predicting: 139it [01:35,  1.38it/s]Extractor Predicting: 140it [01:36,  1.38it/s]Extractor Predicting: 141it [01:36,  1.41it/s]Extractor Predicting: 142it [01:37,  1.41it/s]Extractor Predicting: 143it [01:38,  1.42it/s]Extractor Predicting: 144it [01:38,  1.75it/s]Extractor Predicting: 144it [01:38,  1.46it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:45,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:45,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:45,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:45,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:45,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:55:45,663 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:55:45,669 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:55:46,235 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:55:47,248 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:55:47,249 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:50,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:50,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:50,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:50,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:50,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:55:51,142 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:55:51,143 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:55:51,717 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:55:51,864 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:55:51,864 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.221875,
  "recall": 0.02040816326530612,
  "score": 0.03737825743616741,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:20,  1.57it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:24,  1.58it/s]Extractor Predicting: 39it [00:25,  1.56it/s]Extractor Predicting: 40it [00:25,  1.54it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:27,  1.58it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:31,  1.51it/s]Extractor Predicting: 49it [00:31,  1.43it/s]Extractor Predicting: 50it [00:32,  1.45it/s]Extractor Predicting: 51it [00:33,  1.45it/s]Extractor Predicting: 52it [00:34,  1.46it/s]Extractor Predicting: 53it [00:34,  1.47it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:35,  1.52it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.48it/s]Extractor Predicting: 58it [00:38,  1.48it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:39,  1.47it/s]Extractor Predicting: 61it [00:40,  1.47it/s]Extractor Predicting: 62it [00:40,  1.50it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:41,  1.54it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.48it/s]Extractor Predicting: 67it [00:44,  1.48it/s]Extractor Predicting: 68it [00:44,  1.48it/s]Extractor Predicting: 69it [00:45,  1.46it/s]Extractor Predicting: 70it [00:46,  1.47it/s]Extractor Predicting: 71it [00:46,  1.48it/s]Extractor Predicting: 72it [00:47,  1.46it/s]Extractor Predicting: 73it [00:48,  1.49it/s]Extractor Predicting: 74it [00:48,  1.48it/s]Extractor Predicting: 75it [00:49,  1.50it/s]Extractor Predicting: 76it [00:50,  1.48it/s]Extractor Predicting: 77it [00:50,  1.52it/s]Extractor Predicting: 78it [00:51,  1.51it/s]Extractor Predicting: 79it [00:52,  1.55it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:53,  1.56it/s]Extractor Predicting: 82it [00:53,  1.55it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:55,  1.51it/s]Extractor Predicting: 85it [00:56,  1.48it/s]Extractor Predicting: 86it [00:56,  1.45it/s]Extractor Predicting: 87it [00:57,  1.46it/s]Extractor Predicting: 88it [00:58,  1.46it/s]Extractor Predicting: 89it [00:58,  1.44it/s]Extractor Predicting: 90it [00:59,  1.44it/s]Extractor Predicting: 91it [01:00,  1.42it/s]Extractor Predicting: 92it [01:00,  1.46it/s]Extractor Predicting: 93it [01:01,  1.47it/s]Extractor Predicting: 94it [01:02,  1.48it/s]Extractor Predicting: 95it [01:02,  1.49it/s]Extractor Predicting: 96it [01:03,  1.45it/s]Extractor Predicting: 97it [01:04,  1.43it/s]Extractor Predicting: 98it [01:05,  1.42it/s]Extractor Predicting: 99it [01:05,  1.45it/s]Extractor Predicting: 100it [01:06,  1.46it/s]Extractor Predicting: 101it [01:07,  1.50it/s]Extractor Predicting: 102it [01:07,  1.46it/s]Extractor Predicting: 103it [01:08,  1.44it/s]Extractor Predicting: 104it [01:09,  1.47it/s]Extractor Predicting: 105it [01:09,  1.46it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.49it/s]Extractor Predicting: 108it [01:11,  1.46it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:13,  1.46it/s]Extractor Predicting: 111it [01:13,  1.47it/s]Extractor Predicting: 112it [01:14,  1.45it/s]Extractor Predicting: 113it [01:15,  1.47it/s]Extractor Predicting: 114it [01:15,  1.48it/s]Extractor Predicting: 115it [01:16,  1.43it/s]Extractor Predicting: 116it [01:17,  1.47it/s]Extractor Predicting: 117it [01:17,  1.46it/s]Extractor Predicting: 118it [01:18,  1.45it/s]Extractor Predicting: 119it [01:19,  1.45it/s]Extractor Predicting: 120it [01:20,  1.48it/s]Extractor Predicting: 121it [01:20,  1.49it/s]Extractor Predicting: 122it [01:21,  1.50it/s]Extractor Predicting: 123it [01:22,  1.50it/s]Extractor Predicting: 124it [01:22,  1.51it/s]Extractor Predicting: 125it [01:23,  1.53it/s]Extractor Predicting: 126it [01:23,  1.53it/s]Extractor Predicting: 127it [01:24,  1.51it/s]Extractor Predicting: 128it [01:25,  1.54it/s]Extractor Predicting: 129it [01:25,  1.54it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:27,  1.55it/s]Extractor Predicting: 132it [01:27,  1.56it/s]Extractor Predicting: 133it [01:28,  1.58it/s]Extractor Predicting: 134it [01:29,  1.53it/s]Extractor Predicting: 135it [01:29,  1.56it/s]Extractor Predicting: 136it [01:30,  1.57it/s]Extractor Predicting: 137it [01:30,  1.58it/s]Extractor Predicting: 138it [01:31,  1.57it/s]Extractor Predicting: 139it [01:32,  1.56it/s]Extractor Predicting: 140it [01:32,  1.56it/s]Extractor Predicting: 141it [01:33,  1.53it/s]Extractor Predicting: 142it [01:34,  1.52it/s]Extractor Predicting: 143it [01:34,  1.54it/s]Extractor Predicting: 144it [01:35,  1.52it/s]Extractor Predicting: 145it [01:36,  1.57it/s]Extractor Predicting: 146it [01:36,  1.59it/s]Extractor Predicting: 147it [01:37,  1.60it/s]Extractor Predicting: 148it [01:38,  1.62it/s]Extractor Predicting: 149it [01:38,  1.61it/s]Extractor Predicting: 150it [01:39,  1.63it/s]Extractor Predicting: 151it [01:39,  1.63it/s]Extractor Predicting: 152it [01:40,  1.65it/s]Extractor Predicting: 153it [01:41,  1.47it/s]Extractor Predicting: 154it [01:41,  1.51it/s]Extractor Predicting: 155it [01:42,  1.58it/s]Extractor Predicting: 156it [01:43,  1.59it/s]Extractor Predicting: 157it [01:43,  1.67it/s]Extractor Predicting: 158it [01:44,  1.70it/s]Extractor Predicting: 159it [01:44,  1.66it/s]Extractor Predicting: 160it [01:45,  1.62it/s]Extractor Predicting: 161it [01:46,  1.61it/s]Extractor Predicting: 162it [01:46,  1.64it/s]Extractor Predicting: 163it [01:47,  1.66it/s]Extractor Predicting: 164it [01:47,  1.66it/s]Extractor Predicting: 165it [01:48,  1.66it/s]Extractor Predicting: 166it [01:49,  1.63it/s]Extractor Predicting: 167it [01:49,  1.66it/s]Extractor Predicting: 168it [01:50,  1.64it/s]Extractor Predicting: 169it [01:50,  1.70it/s]Extractor Predicting: 170it [01:51,  1.68it/s]Extractor Predicting: 171it [01:52,  1.68it/s]Extractor Predicting: 172it [01:52,  1.61it/s]Extractor Predicting: 173it [01:53,  1.59it/s]Extractor Predicting: 174it [01:54,  1.55it/s]Extractor Predicting: 175it [01:54,  1.49it/s]Extractor Predicting: 176it [01:55,  1.50it/s]Extractor Predicting: 177it [01:56,  1.50it/s]Extractor Predicting: 178it [01:56,  1.48it/s]Extractor Predicting: 179it [01:57,  1.48it/s]Extractor Predicting: 180it [01:58,  1.49it/s]Extractor Predicting: 181it [01:58,  1.49it/s]Extractor Predicting: 182it [01:59,  1.49it/s]Extractor Predicting: 183it [02:00,  1.49it/s]Extractor Predicting: 184it [02:00,  1.47it/s]Extractor Predicting: 185it [02:01,  1.45it/s]Extractor Predicting: 186it [02:02,  1.45it/s]Extractor Predicting: 187it [02:02,  1.45it/s]Extractor Predicting: 188it [02:03,  1.46it/s]Extractor Predicting: 189it [02:04,  1.46it/s]Extractor Predicting: 190it [02:04,  1.48it/s]Extractor Predicting: 191it [02:05,  1.36it/s]Extractor Predicting: 192it [02:06,  1.37it/s]Extractor Predicting: 193it [02:07,  1.39it/s]Extractor Predicting: 194it [02:07,  1.40it/s]Extractor Predicting: 195it [02:08,  1.42it/s]Extractor Predicting: 196it [02:09,  1.44it/s]Extractor Predicting: 197it [02:10,  1.43it/s]Extractor Predicting: 198it [02:10,  1.46it/s]Extractor Predicting: 199it [02:11,  1.48it/s]Extractor Predicting: 200it [02:12,  1.45it/s]Extractor Predicting: 201it [02:12,  1.43it/s]Extractor Predicting: 202it [02:13,  1.51it/s]Extractor Predicting: 203it [02:14,  1.49it/s]Extractor Predicting: 204it [02:14,  1.51it/s]Extractor Predicting: 205it [02:15,  1.50it/s]Extractor Predicting: 206it [02:16,  1.48it/s]Extractor Predicting: 207it [02:16,  1.47it/s]Extractor Predicting: 208it [02:17,  1.47it/s]Extractor Predicting: 209it [02:18,  1.42it/s]Extractor Predicting: 210it [02:18,  1.44it/s]Extractor Predicting: 211it [02:19,  1.41it/s]Extractor Predicting: 212it [02:20,  1.42it/s]Extractor Predicting: 213it [02:20,  1.43it/s]Extractor Predicting: 214it [02:21,  1.42it/s]Extractor Predicting: 215it [02:22,  1.45it/s]Extractor Predicting: 216it [02:23,  1.42it/s]Extractor Predicting: 217it [02:23,  1.43it/s]Extractor Predicting: 218it [02:24,  1.44it/s]Extractor Predicting: 219it [02:25,  1.43it/s]Extractor Predicting: 220it [02:25,  1.45it/s]Extractor Predicting: 221it [02:26,  1.41it/s]Extractor Predicting: 222it [02:27,  1.41it/s]Extractor Predicting: 223it [02:27,  1.44it/s]Extractor Predicting: 224it [02:28,  1.44it/s]Extractor Predicting: 225it [02:29,  1.45it/s]Extractor Predicting: 226it [02:30,  1.44it/s]Extractor Predicting: 227it [02:30,  1.49it/s]Extractor Predicting: 228it [02:31,  1.47it/s]Extractor Predicting: 229it [02:32,  1.47it/s]Extractor Predicting: 230it [02:32,  1.52it/s]Extractor Predicting: 231it [02:33,  1.53it/s]Extractor Predicting: 232it [02:33,  1.56it/s]Extractor Predicting: 233it [02:34,  1.53it/s]Extractor Predicting: 234it [02:35,  1.53it/s]Extractor Predicting: 235it [02:35,  1.52it/s]Extractor Predicting: 236it [02:36,  1.49it/s]Extractor Predicting: 237it [02:37,  1.47it/s]Extractor Predicting: 238it [02:37,  1.49it/s]Extractor Predicting: 239it [02:38,  1.47it/s]Extractor Predicting: 240it [02:39,  1.48it/s]Extractor Predicting: 241it [02:40,  1.43it/s]Extractor Predicting: 242it [02:40,  1.46it/s]Extractor Predicting: 243it [02:41,  1.45it/s]Extractor Predicting: 244it [02:42,  1.34it/s]Extractor Predicting: 245it [02:42,  1.39it/s]Extractor Predicting: 246it [02:43,  1.41it/s]Extractor Predicting: 247it [02:44,  1.43it/s]Extractor Predicting: 248it [02:45,  1.44it/s]Extractor Predicting: 249it [02:45,  1.44it/s]Extractor Predicting: 250it [02:46,  1.45it/s]Extractor Predicting: 251it [02:47,  1.47it/s]Extractor Predicting: 252it [02:47,  1.47it/s]Extractor Predicting: 253it [02:48,  1.46it/s]Extractor Predicting: 254it [02:49,  1.48it/s]Extractor Predicting: 255it [02:49,  1.43it/s]Extractor Predicting: 256it [02:50,  1.40it/s]Extractor Predicting: 257it [02:51,  1.41it/s]Extractor Predicting: 258it [02:52,  1.40it/s]Extractor Predicting: 259it [02:52,  1.43it/s]Extractor Predicting: 260it [02:53,  1.37it/s]Extractor Predicting: 261it [02:54,  1.42it/s]Extractor Predicting: 262it [02:54,  1.41it/s]Extractor Predicting: 263it [02:55,  1.40it/s]Extractor Predicting: 264it [02:56,  1.40it/s]Extractor Predicting: 265it [02:57,  1.36it/s]Extractor Predicting: 266it [02:57,  1.37it/s]Extractor Predicting: 267it [02:58,  1.39it/s]Extractor Predicting: 268it [02:59,  1.36it/s]Extractor Predicting: 269it [02:59,  1.39it/s]Extractor Predicting: 270it [03:00,  1.41it/s]Extractor Predicting: 271it [03:01,  1.38it/s]Extractor Predicting: 272it [03:02,  1.41it/s]Extractor Predicting: 273it [03:02,  1.41it/s]Extractor Predicting: 274it [03:03,  1.43it/s]Extractor Predicting: 275it [03:04,  1.45it/s]Extractor Predicting: 276it [03:04,  1.48it/s]Extractor Predicting: 277it [03:05,  1.48it/s]Extractor Predicting: 278it [03:06,  1.47it/s]Extractor Predicting: 279it [03:06,  1.47it/s]Extractor Predicting: 280it [03:07,  1.42it/s]Extractor Predicting: 281it [03:08,  1.42it/s]Extractor Predicting: 282it [03:08,  1.51it/s]Extractor Predicting: 282it [03:08,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:08,620 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:08,622 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:08,623 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:08,623 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:08,623 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:59:08,914 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:59:08,915 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:59:09,186 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:59:10,198 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:59:10,223 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:11,980 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:11,983 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:11,983 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:11,983 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:11,983 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:59:12,713 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:59:12,714 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:59:12,978 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:59:13,132 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:59:13,132 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3120856705762366,
  "recall": 0.09053254437869822,
  "score": 0.14035087719298245,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.39it/s]Extractor Predicting: 4it [00:02,  1.40it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 5it [00:03,  1.42it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:59:17,905 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:59:17,905 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:59:17,924 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:59:17,924 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:59:17,936 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:59:25,168 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:59:25,168 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:59:25,236 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:59:25,236 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:59:25,315 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:59:25,353 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:59:25,353 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:59:25,353 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:59:25,353 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:59:25,353 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:59:25,353 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3870967741935484,
  "recall": 0.05,
  "score": 0.0885608856088561,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:59:25,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:26,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:27,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:27,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:28,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:29,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:29,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:30,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:31,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:32,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:32,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:33,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:34,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:35,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:36,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:36,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:37,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:38,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:38,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:39,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:40,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:31, 15.08s/it][WARNING|generation_utils.py:914] 2023-08-28 18:59:40,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:41,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:42,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:43,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:44,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:44,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:45,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:46,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:47,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:47,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:48,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:49,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:50,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:50,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:51,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:52,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:53,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:53,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:54,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:55,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:30<03:17, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 18:59:56,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:56,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:57,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:58,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:58,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:59,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:00,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:00,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:01,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:02,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:03,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:03,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:04,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:05,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:05,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:06,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:07,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:07,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:08,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:09,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:09,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:44<02:58, 14.85s/it][WARNING|generation_utils.py:914] 2023-08-28 19:00:10,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:11,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:11,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:12,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:13,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:14,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:14,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:15,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:16,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:17,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:17,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:18,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:19,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:20,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:21,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:21,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:22,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:23,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:24,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:24,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:25,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:26,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:01<02:51, 15.55s/it][WARNING|generation_utils.py:914] 2023-08-28 19:00:27,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:27,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:28,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:29,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:30,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:31,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:31,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:32,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:33,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:33,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:34,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:35,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:36,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:36,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:37,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:38,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:39,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:39,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:40,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:41,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:42,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:16<02:35, 15.55s/it][WARNING|generation_utils.py:914] 2023-08-28 19:00:42,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:43,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:44,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:45,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:45,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:46,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:47,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:48,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:49,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:49,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:50,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:51,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:51,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:52,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:53,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:53,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:54,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:55,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:56,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:56,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:57,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:32<02:19, 15.53s/it][WARNING|generation_utils.py:914] 2023-08-28 19:00:58,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:58,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:59,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:00,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:00,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:01,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:02,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:02,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:03,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:04,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:04,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:05,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:06,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:06,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:07,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:08,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:09,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:10,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:10,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:11,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:12,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:47<02:02, 15.26s/it][WARNING|generation_utils.py:914] 2023-08-28 19:01:12,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:13,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:14,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:15,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:15,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:16,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:17,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:18,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:19,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:20,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:21,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:22,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:22,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:23,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:24,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:25,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:26,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:27,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:27,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:28,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:29,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:04<01:51, 15.86s/it][WARNING|generation_utils.py:914] 2023-08-28 19:01:30,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:30,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:31,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:32,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:33,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:33,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:34,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:35,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:36,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:36,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:37,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:38,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:39,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:39,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:40,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:41,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:41,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:42,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:43,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:44,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:19<01:33, 15.56s/it][WARNING|generation_utils.py:914] 2023-08-28 19:01:44,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:45,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:46,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:47,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:47,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:48,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:49,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:49,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:50,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:51,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:52,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:52,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:53,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:54,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:55,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:55,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:56,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:57,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:57,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:58,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:33<01:16, 15.27s/it][WARNING|generation_utils.py:914] 2023-08-28 19:01:59,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:00,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:01,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:01,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:02,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:03,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:03,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:04,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:05,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:06,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:06,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:07,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:08,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:08,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:09,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:10,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:11,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:11,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:12,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:13,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:13,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:49<01:00, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 19:02:14,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:15,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:16,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:16,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:17,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:18,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:18,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:19,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:20,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:21,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:22,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:23,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:23,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:24,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:25,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:25,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:26,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:27,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:28,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:28,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:03<00:45, 15.08s/it][WARNING|generation_utils.py:914] 2023-08-28 19:02:29,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:29,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:30,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:31,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:31,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:32,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:33,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:33,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:34,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:35,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:35,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:36,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:37,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:37,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:38,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:39,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:39,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:40,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:41,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:41,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:42,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:17<00:29, 14.65s/it][WARNING|generation_utils.py:914] 2023-08-28 19:02:43,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:43,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:44,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:45,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:46,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:46,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:47,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:48,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:48,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:49,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:50,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:50,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:51,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:52,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:52,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:53,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:54,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:55,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:55,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:56,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:31<00:14, 14.54s/it][WARNING|generation_utils.py:914] 2023-08-28 19:02:57,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:58,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:58,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:02:59,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:00,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:00,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:01,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:02,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:02,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:03,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:04,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:05,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:05,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:06,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:07,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:08,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:08,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:09,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:10,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:10,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:03:11,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:46<00:00, 14.69s/it]Generating: 100%|██████████| 15/15 [03:46<00:00, 15.11s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:03:18,518 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:03:18,524 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:03:18,524 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:03:18,524 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:03:18,524 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:03:19,307 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:03:19,308 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:03:19,572 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:03:20,654 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:03:20,654 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:03:22,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:03:22,404 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:03:22,404 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:03:22,404 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:03:22,404 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:03:22,781 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:03:22,783 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:03:23,079 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:03:23,259 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:03:23,259 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : country .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : followed by .', 'success_rate': 0.940625, 'errors': {'', "('Columbia Records', 'followed by', '', 'The album was released in the United States on November 12 , 2008 via Columbia Records .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : genre .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8821022727272727, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : subsidiary . Context : On 31 March 2014 , the company announced that it had sold two of its brands at a profit of € 30 billion . Head Entity : the company , Tail Entity : Valeant Banc Group .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9345238095238095, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9077380952380952, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : instrument . Context : Later in the year , the band formed the single " The Last of Us " at the end of 2010 , in which they sang the instrumental cover to a song by the Beatles , " I Want Lucy " . Head Entity : The Last of Us , Tail Entity : The Beatles .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9671875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8988095238095238, 'errors': {'', "('Frederick the Great', 'occupation', '', 'On 14 June 1779 , Frederick the Great sent his regiment to meet the first American brigade under his command , the company sent by Henry Hudson to occupy the city .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9546875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9136904761904762, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : publisher .', 'success_rate': 0.96875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9181547619047619, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 8627
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8727, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.42it/s]Extractor Estimating: 2it [00:01,  1.29it/s]Extractor Estimating: 3it [00:02,  1.35it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.41it/s]Extractor Estimating: 6it [00:04,  1.47it/s]Extractor Estimating: 7it [00:04,  1.52it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:06,  1.59it/s]Extractor Estimating: 10it [00:06,  1.61it/s]Extractor Estimating: 11it [00:07,  1.55it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:09,  1.59it/s]Extractor Estimating: 15it [00:09,  1.54it/s]Extractor Estimating: 16it [00:10,  1.52it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:11,  1.51it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:13,  1.50it/s]Extractor Estimating: 21it [00:13,  1.53it/s]Extractor Estimating: 22it [00:14,  1.55it/s]Extractor Estimating: 23it [00:15,  1.56it/s]Extractor Estimating: 24it [00:15,  1.56it/s]Extractor Estimating: 25it [00:16,  1.59it/s]Extractor Estimating: 26it [00:17,  1.52it/s]Extractor Estimating: 27it [00:17,  1.42it/s]Extractor Estimating: 28it [00:18,  1.45it/s]Extractor Estimating: 29it [00:19,  1.42it/s]Extractor Estimating: 30it [00:19,  1.42it/s]Extractor Estimating: 31it [00:20,  1.43it/s]Extractor Estimating: 32it [00:21,  1.44it/s]Extractor Estimating: 33it [00:21,  1.48it/s]Extractor Estimating: 34it [00:22,  1.43it/s]Extractor Estimating: 35it [00:23,  1.46it/s]Extractor Estimating: 36it [00:24,  1.48it/s]Extractor Estimating: 37it [00:24,  1.42it/s]Extractor Estimating: 38it [00:25,  1.46it/s]Extractor Estimating: 39it [00:26,  1.48it/s]Extractor Estimating: 40it [00:26,  1.52it/s]Extractor Estimating: 41it [00:27,  1.52it/s]Extractor Estimating: 42it [00:28,  1.51it/s]Extractor Estimating: 43it [00:28,  1.53it/s]Extractor Estimating: 44it [00:29,  1.55it/s]Extractor Estimating: 45it [00:30,  1.50it/s]Extractor Estimating: 46it [00:30,  1.47it/s]Extractor Estimating: 47it [00:31,  1.52it/s]Extractor Estimating: 48it [00:31,  1.53it/s]Extractor Estimating: 49it [00:32,  1.55it/s]Extractor Estimating: 50it [00:33,  1.53it/s]Extractor Estimating: 51it [00:33,  1.58it/s]Extractor Estimating: 52it [00:34,  1.58it/s]Extractor Estimating: 53it [00:35,  1.60it/s]Extractor Estimating: 54it [00:35,  1.64it/s]Extractor Estimating: 55it [00:36,  1.62it/s]Extractor Estimating: 56it [00:36,  1.65it/s]Extractor Estimating: 57it [00:37,  1.65it/s]Extractor Estimating: 58it [00:38,  1.64it/s]Extractor Estimating: 59it [00:38,  1.63it/s]Extractor Estimating: 60it [00:39,  1.66it/s]Extractor Estimating: 61it [00:39,  1.66it/s]Extractor Estimating: 62it [00:40,  1.67it/s]Extractor Estimating: 63it [00:41,  1.66it/s]Extractor Estimating: 64it [00:41,  1.58it/s]Extractor Estimating: 65it [00:42,  1.57it/s]Extractor Estimating: 66it [00:43,  1.61it/s]Extractor Estimating: 67it [00:43,  1.56it/s]Extractor Estimating: 68it [00:44,  1.57it/s]Extractor Estimating: 69it [00:44,  1.58it/s]Extractor Estimating: 70it [00:45,  1.57it/s]Extractor Estimating: 71it [00:46,  1.61it/s]Extractor Estimating: 72it [00:46,  1.68it/s]Extractor Estimating: 73it [00:47,  1.61it/s]Extractor Estimating: 74it [00:48,  1.64it/s]Extractor Estimating: 75it [00:48,  1.63it/s]Extractor Estimating: 76it [00:49,  1.64it/s]Extractor Estimating: 77it [00:49,  1.69it/s]Extractor Estimating: 78it [00:50,  1.69it/s]Extractor Estimating: 79it [00:50,  1.71it/s]Extractor Estimating: 80it [00:51,  1.67it/s]Extractor Estimating: 81it [00:52,  1.63it/s]Extractor Estimating: 82it [00:52,  1.64it/s]Extractor Estimating: 83it [00:53,  1.56it/s]Extractor Estimating: 84it [00:54,  1.54it/s]Extractor Estimating: 85it [00:54,  1.55it/s]Extractor Estimating: 86it [00:55,  1.54it/s]Extractor Estimating: 87it [00:56,  1.59it/s]Extractor Estimating: 88it [00:56,  1.57it/s]Extractor Estimating: 89it [00:57,  1.61it/s]Extractor Estimating: 90it [00:57,  1.59it/s]Extractor Estimating: 91it [00:58,  1.54it/s]Extractor Estimating: 92it [00:59,  1.53it/s]Extractor Estimating: 93it [00:59,  1.54it/s]Extractor Estimating: 94it [01:00,  1.59it/s]Extractor Estimating: 95it [01:01,  1.60it/s]Extractor Estimating: 96it [01:01,  1.57it/s]Extractor Estimating: 97it [01:02,  1.59it/s]Extractor Estimating: 98it [01:03,  1.60it/s]Extractor Estimating: 99it [01:03,  1.60it/s]Extractor Estimating: 100it [01:04,  1.58it/s]Extractor Estimating: 101it [01:04,  1.63it/s]Extractor Estimating: 102it [01:05,  1.59it/s]Extractor Estimating: 103it [01:06,  1.58it/s]Extractor Estimating: 104it [01:06,  1.59it/s]Extractor Estimating: 105it [01:07,  1.59it/s]Extractor Estimating: 106it [01:08,  1.62it/s]Extractor Estimating: 107it [01:08,  1.53it/s]Extractor Estimating: 108it [01:09,  1.60it/s]Extractor Estimating: 109it [01:09,  1.59it/s]Extractor Estimating: 110it [01:10,  1.62it/s]Extractor Estimating: 111it [01:11,  1.69it/s]Extractor Estimating: 112it [01:11,  1.63it/s]Extractor Estimating: 113it [01:12,  1.66it/s]Extractor Estimating: 114it [01:12,  1.64it/s]Extractor Estimating: 115it [01:13,  1.67it/s]Extractor Estimating: 116it [01:14,  1.62it/s]Extractor Estimating: 117it [01:14,  1.61it/s]Extractor Estimating: 118it [01:15,  1.68it/s]Extractor Estimating: 119it [01:15,  1.66it/s]Extractor Estimating: 120it [01:16,  1.65it/s]Extractor Estimating: 121it [01:17,  1.62it/s]Extractor Estimating: 122it [01:17,  1.66it/s]Extractor Estimating: 123it [01:18,  1.61it/s]Extractor Estimating: 124it [01:19,  1.64it/s]Extractor Estimating: 125it [01:19,  1.63it/s]Extractor Estimating: 126it [01:20,  1.62it/s]Extractor Estimating: 127it [01:20,  1.63it/s]Extractor Estimating: 128it [01:21,  1.67it/s]Extractor Estimating: 129it [01:22,  1.73it/s]Extractor Estimating: 130it [01:22,  1.67it/s]Extractor Estimating: 131it [01:23,  1.67it/s]Extractor Estimating: 132it [01:23,  1.67it/s]Extractor Estimating: 133it [01:24,  1.69it/s]Extractor Estimating: 134it [01:25,  1.55it/s]Extractor Estimating: 135it [01:25,  1.60it/s]Extractor Estimating: 136it [01:26,  1.63it/s]Extractor Estimating: 137it [01:26,  1.66it/s]Extractor Estimating: 138it [01:27,  1.67it/s]Extractor Estimating: 139it [01:28,  1.75it/s]Extractor Estimating: 140it [01:28,  1.72it/s]Extractor Estimating: 141it [01:29,  1.70it/s]Extractor Estimating: 142it [01:29,  1.64it/s]Extractor Estimating: 143it [01:30,  1.66it/s]Extractor Estimating: 144it [01:31,  1.68it/s]Extractor Estimating: 145it [01:31,  1.70it/s]Extractor Estimating: 146it [01:32,  1.75it/s]Extractor Estimating: 147it [01:32,  1.80it/s]Extractor Estimating: 148it [01:33,  1.73it/s]Extractor Estimating: 149it [01:33,  1.72it/s]Extractor Estimating: 150it [01:34,  1.73it/s]Extractor Estimating: 151it [01:35,  1.70it/s]Extractor Estimating: 152it [01:35,  1.72it/s]Extractor Estimating: 153it [01:36,  1.76it/s]Extractor Estimating: 154it [01:36,  1.72it/s]Extractor Estimating: 155it [01:37,  1.70it/s]Extractor Estimating: 156it [01:38,  1.68it/s]Extractor Estimating: 157it [01:38,  1.65it/s]Extractor Estimating: 158it [01:39,  1.68it/s]Extractor Estimating: 159it [01:39,  1.59it/s]Extractor Estimating: 160it [01:40,  1.60it/s]Extractor Estimating: 161it [01:41,  1.65it/s]Extractor Estimating: 162it [01:41,  1.63it/s]Extractor Estimating: 163it [01:42,  1.67it/s]Extractor Estimating: 164it [01:42,  1.71it/s]Extractor Estimating: 165it [01:43,  1.67it/s]Extractor Estimating: 166it [01:44,  1.71it/s]Extractor Estimating: 167it [01:44,  1.67it/s]Extractor Estimating: 168it [01:45,  1.70it/s]Extractor Estimating: 169it [01:45,  1.73it/s]Extractor Estimating: 170it [01:46,  1.52it/s]Extractor Estimating: 171it [01:47,  1.52it/s]Extractor Estimating: 172it [01:47,  1.62it/s]Extractor Estimating: 173it [01:48,  1.66it/s]Extractor Estimating: 174it [01:49,  1.64it/s]Extractor Estimating: 175it [01:49,  1.66it/s]Extractor Estimating: 176it [01:50,  1.62it/s]Extractor Estimating: 177it [01:50,  1.66it/s]Extractor Estimating: 178it [01:51,  1.67it/s]Extractor Estimating: 179it [01:52,  1.66it/s]Extractor Estimating: 180it [01:52,  1.58it/s]Extractor Estimating: 181it [01:53,  1.59it/s]Extractor Estimating: 182it [01:53,  1.66it/s]Extractor Estimating: 183it [01:54,  1.61it/s]Extractor Estimating: 184it [01:55,  1.60it/s]Extractor Estimating: 185it [01:55,  1.60it/s]Extractor Estimating: 186it [01:56,  1.63it/s]Extractor Estimating: 187it [01:57,  1.59it/s]Extractor Estimating: 188it [01:57,  1.57it/s]Extractor Estimating: 189it [01:58,  1.58it/s]Extractor Estimating: 190it [01:58,  1.58it/s]Extractor Estimating: 191it [01:59,  1.60it/s]Extractor Estimating: 192it [02:00,  1.58it/s]Extractor Estimating: 193it [02:00,  1.55it/s]Extractor Estimating: 194it [02:01,  1.53it/s]Extractor Estimating: 195it [02:02,  1.57it/s]Extractor Estimating: 196it [02:02,  1.55it/s]Extractor Estimating: 197it [02:03,  1.54it/s]Extractor Estimating: 198it [02:04,  1.57it/s]Extractor Estimating: 199it [02:04,  1.63it/s]Extractor Estimating: 200it [02:05,  1.61it/s]Extractor Estimating: 201it [02:05,  1.58it/s]Extractor Estimating: 202it [02:06,  1.56it/s]Extractor Estimating: 203it [02:07,  1.52it/s]Extractor Estimating: 204it [02:08,  1.49it/s]Extractor Estimating: 205it [02:08,  1.50it/s]Extractor Estimating: 206it [02:09,  1.51it/s]Extractor Estimating: 207it [02:09,  1.53it/s]Extractor Estimating: 208it [02:10,  1.50it/s]Extractor Estimating: 209it [02:11,  1.53it/s]Extractor Estimating: 210it [02:11,  1.54it/s]Extractor Estimating: 211it [02:12,  1.56it/s]Extractor Estimating: 212it [02:13,  1.49it/s]Extractor Estimating: 213it [02:13,  1.49it/s]Extractor Estimating: 214it [02:14,  1.50it/s]Extractor Estimating: 215it [02:15,  1.38it/s]Extractor Estimating: 216it [02:16,  1.44it/s]Extractor Estimating: 217it [02:16,  1.47it/s]Extractor Estimating: 218it [02:17,  1.52it/s]Extractor Estimating: 219it [02:18,  1.50it/s]Extractor Estimating: 220it [02:18,  1.52it/s]Extractor Estimating: 221it [02:19,  1.54it/s]Extractor Estimating: 222it [02:20,  1.46it/s]Extractor Estimating: 223it [02:20,  1.45it/s]Extractor Estimating: 224it [02:21,  1.42it/s]Extractor Estimating: 225it [02:22,  1.47it/s]Extractor Estimating: 226it [02:22,  1.54it/s]Extractor Estimating: 227it [02:23,  1.55it/s]Extractor Estimating: 228it [02:23,  1.60it/s]Extractor Estimating: 229it [02:24,  1.60it/s]Extractor Estimating: 230it [02:25,  1.62it/s]Extractor Estimating: 231it [02:25,  1.64it/s]Extractor Estimating: 232it [02:26,  1.64it/s]Extractor Estimating: 233it [02:26,  1.66it/s]Extractor Estimating: 234it [02:27,  1.61it/s]Extractor Estimating: 235it [02:28,  1.66it/s]Extractor Estimating: 236it [02:28,  1.67it/s]Extractor Estimating: 237it [02:29,  1.72it/s]Extractor Estimating: 238it [02:29,  1.70it/s]Extractor Estimating: 239it [02:30,  1.69it/s]Extractor Estimating: 240it [02:31,  1.71it/s]Extractor Estimating: 241it [02:31,  1.64it/s]Extractor Estimating: 242it [02:32,  1.61it/s]Extractor Estimating: 243it [02:32,  1.69it/s]Extractor Estimating: 244it [02:33,  1.69it/s]Extractor Estimating: 245it [02:34,  1.71it/s]Extractor Estimating: 246it [02:34,  1.68it/s]Extractor Estimating: 247it [02:35,  1.70it/s]Extractor Estimating: 248it [02:35,  1.73it/s]Extractor Estimating: 249it [02:36,  1.68it/s]Extractor Estimating: 250it [02:37,  1.68it/s]Extractor Estimating: 251it [02:37,  1.59it/s]Extractor Estimating: 252it [02:38,  1.60it/s]Extractor Estimating: 253it [02:38,  1.63it/s]Extractor Estimating: 254it [02:39,  1.68it/s]Extractor Estimating: 255it [02:40,  1.64it/s]Extractor Estimating: 256it [02:40,  1.68it/s]Extractor Estimating: 257it [02:41,  1.70it/s]Extractor Estimating: 258it [02:41,  1.64it/s]Extractor Estimating: 259it [02:42,  1.65it/s]Extractor Estimating: 260it [02:43,  1.66it/s]Extractor Estimating: 261it [02:43,  1.67it/s]Extractor Estimating: 262it [02:44,  1.78it/s]Extractor Estimating: 263it [02:44,  1.76it/s]Extractor Estimating: 264it [02:45,  1.71it/s]Extractor Estimating: 265it [02:46,  1.69it/s]Extractor Estimating: 266it [02:46,  1.65it/s]Extractor Estimating: 267it [02:47,  1.65it/s]Extractor Estimating: 268it [02:47,  1.67it/s]Extractor Estimating: 269it [02:48,  1.68it/s]Extractor Estimating: 270it [02:49,  1.69it/s]Extractor Estimating: 271it [02:49,  1.65it/s]Extractor Estimating: 272it [02:50,  1.63it/s]Extractor Estimating: 273it [02:50,  1.63it/s]Extractor Estimating: 274it [02:51,  1.59it/s]Extractor Estimating: 275it [02:52,  1.56it/s]Extractor Estimating: 276it [02:52,  1.50it/s]Extractor Estimating: 277it [02:53,  1.51it/s]Extractor Estimating: 278it [02:54,  1.56it/s]Extractor Estimating: 279it [02:54,  1.55it/s]Extractor Estimating: 280it [02:55,  1.57it/s]Extractor Estimating: 281it [02:56,  1.61it/s]Extractor Estimating: 282it [02:56,  1.57it/s]Extractor Estimating: 283it [02:57,  1.51it/s]Extractor Estimating: 284it [02:58,  1.53it/s]Extractor Estimating: 285it [02:58,  1.50it/s]Extractor Estimating: 286it [02:59,  1.56it/s]Extractor Estimating: 287it [03:00,  1.55it/s]Extractor Estimating: 288it [03:00,  1.60it/s]Extractor Estimating: 289it [03:01,  1.59it/s]Extractor Estimating: 290it [03:01,  1.60it/s]Extractor Estimating: 291it [03:02,  1.55it/s]Extractor Estimating: 292it [03:03,  1.52it/s]Extractor Estimating: 293it [03:03,  1.54it/s]Extractor Estimating: 294it [03:04,  1.56it/s]Extractor Estimating: 295it [03:05,  1.54it/s]Extractor Estimating: 296it [03:05,  1.55it/s]Extractor Estimating: 297it [03:06,  1.44it/s]Extractor Estimating: 298it [03:07,  1.49it/s]Extractor Estimating: 299it [03:07,  1.50it/s]Extractor Estimating: 300it [03:08,  1.56it/s]Extractor Estimating: 301it [03:08,  1.67it/s]Extractor Estimating: 302it [03:09,  1.70it/s]Extractor Estimating: 303it [03:10,  1.75it/s]Extractor Estimating: 304it [03:10,  1.74it/s]Extractor Estimating: 305it [03:11,  1.80it/s]Extractor Estimating: 306it [03:11,  1.77it/s]Extractor Estimating: 307it [03:12,  1.86it/s]Extractor Estimating: 308it [03:12,  1.85it/s]Extractor Estimating: 309it [03:13,  1.84it/s]Extractor Estimating: 310it [03:13,  1.86it/s]Extractor Estimating: 311it [03:14,  1.85it/s]Extractor Estimating: 312it [03:14,  1.80it/s]Extractor Estimating: 313it [03:15,  1.78it/s]Extractor Estimating: 314it [03:16,  1.76it/s]Extractor Estimating: 315it [03:16,  1.66it/s]Extractor Estimating: 316it [03:17,  1.65it/s]Extractor Estimating: 317it [03:17,  1.68it/s]Extractor Estimating: 318it [03:18,  1.68it/s]Extractor Estimating: 319it [03:19,  1.71it/s]Extractor Estimating: 320it [03:19,  1.76it/s]Extractor Estimating: 321it [03:20,  1.78it/s]Extractor Estimating: 322it [03:20,  1.73it/s]Extractor Estimating: 323it [03:21,  1.79it/s]Extractor Estimating: 324it [03:21,  1.80it/s]Extractor Estimating: 325it [03:22,  1.76it/s]Extractor Estimating: 326it [03:23,  1.73it/s]Extractor Estimating: 327it [03:23,  1.76it/s]Extractor Estimating: 328it [03:24,  1.71it/s]Extractor Estimating: 329it [03:24,  1.59it/s]Extractor Estimating: 330it [03:25,  1.60it/s]Extractor Estimating: 331it [03:26,  1.55it/s]Extractor Estimating: 332it [03:26,  1.58it/s]Extractor Estimating: 333it [03:27,  1.61it/s]Extractor Estimating: 334it [03:28,  1.55it/s]Extractor Estimating: 335it [03:28,  1.62it/s]Extractor Estimating: 336it [03:29,  1.68it/s]Extractor Estimating: 337it [03:29,  1.69it/s]Extractor Estimating: 338it [03:30,  1.74it/s]Extractor Estimating: 339it [03:31,  1.72it/s]Extractor Estimating: 340it [03:31,  1.73it/s]Extractor Estimating: 341it [03:32,  1.70it/s]Extractor Estimating: 342it [03:32,  1.71it/s]Extractor Estimating: 343it [03:33,  1.69it/s]Extractor Estimating: 344it [03:33,  1.73it/s]Extractor Estimating: 345it [03:34,  1.81it/s]Extractor Estimating: 346it [03:35,  1.55it/s]Extractor Estimating: 347it [03:35,  1.56it/s]Extractor Estimating: 348it [03:36,  1.55it/s]Extractor Estimating: 349it [03:37,  1.61it/s]Extractor Estimating: 350it [03:37,  1.57it/s]Extractor Estimating: 351it [03:38,  1.57it/s]Extractor Estimating: 352it [03:39,  1.63it/s]Extractor Estimating: 353it [03:39,  1.67it/s]Extractor Estimating: 354it [03:40,  1.71it/s]Extractor Estimating: 355it [03:40,  1.73it/s]Extractor Estimating: 356it [03:41,  1.76it/s]Extractor Estimating: 357it [03:41,  1.77it/s]Extractor Estimating: 358it [03:42,  1.80it/s]Extractor Estimating: 359it [03:42,  1.76it/s]Extractor Estimating: 360it [03:43,  1.69it/s]Extractor Estimating: 361it [03:44,  1.74it/s]Extractor Estimating: 362it [03:44,  1.64it/s]Extractor Estimating: 363it [03:45,  1.70it/s]Extractor Estimating: 364it [03:45,  1.68it/s]Extractor Estimating: 365it [03:46,  1.68it/s]Extractor Estimating: 366it [03:47,  1.70it/s]Extractor Estimating: 367it [03:47,  1.69it/s]Extractor Estimating: 368it [03:48,  1.72it/s]Extractor Estimating: 369it [03:48,  1.64it/s]Extractor Estimating: 370it [03:49,  1.73it/s]Extractor Estimating: 371it [03:50,  1.74it/s]Extractor Estimating: 372it [03:50,  1.80it/s]Extractor Estimating: 373it [03:51,  1.78it/s]Extractor Estimating: 374it [03:51,  1.70it/s]Extractor Estimating: 375it [03:52,  1.74it/s]Extractor Estimating: 375it [03:52,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:07:27,120 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:07:27,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:07:27,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:07:27,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:07:27,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:07:27,512 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:07:27,513 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:07:27,775 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:07:28,839 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:07:28,839 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:07:30,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:07:30,635 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:07:30,636 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:07:30,636 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:07:30,636 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:07:30,969 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:07:30,970 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:07:31,245 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:07:31,410 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:07:31,411 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:29:14,216 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:29:14,498 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7495 mean pseudo reward: 0.9390006697838539
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 16882
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16982, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16982, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.084, loss:639.1874
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.096, loss:616.6031
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.089, loss:592.5647
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.085, loss:581.4859
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.081, loss:556.7084
>> valid entity prec:0.4843, rec:0.4120, f1:0.4452
>> valid relation prec:0.0942, rec:0.0178, f1:0.0300
>> valid relation with NER prec:0.0942, rec:0.0178, f1:0.0300
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.495, loss:583.4480
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.101, loss:540.2902
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.093, loss:551.3042
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.087, loss:558.9979
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.085, loss:522.8715
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4850, rec:0.4899, f1:0.4874
>> valid relation prec:0.0783, rec:0.0227, f1:0.0352
>> valid relation with NER prec:0.0783, rec:0.0227, f1:0.0352
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.508, loss:522.8545
g_step 1200, step 261, avg_time 1.082, loss:567.6143
g_step 1300, step 48, avg_time 1.090, loss:525.1404
g_step 1400, step 148, avg_time 1.073, loss:509.7330
g_step 1500, step 248, avg_time 1.101, loss:506.6949
>> valid entity prec:0.4632, rec:0.4947, f1:0.4784
>> valid relation prec:0.0994, rec:0.0365, f1:0.0534
>> valid relation with NER prec:0.0994, rec:0.0365, f1:0.0534
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.494, loss:510.8290
g_step 1700, step 135, avg_time 1.094, loss:481.7372
g_step 1800, step 235, avg_time 1.095, loss:473.7578
g_step 1900, step 22, avg_time 1.081, loss:500.0536
g_step 2000, step 122, avg_time 1.083, loss:455.8560
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4664, rec:0.4376, f1:0.4515
>> valid relation prec:0.0588, rec:0.0193, f1:0.0290
>> valid relation with NER prec:0.0588, rec:0.0193, f1:0.0290
g_step 2100, step 222, avg_time 2.487, loss:481.2644
g_step 2200, step 9, avg_time 1.099, loss:475.2523
g_step 2300, step 109, avg_time 1.085, loss:429.7871
g_step 2400, step 209, avg_time 1.089, loss:452.1505
g_step 2500, step 309, avg_time 1.095, loss:461.5186
>> valid entity prec:0.4928, rec:0.3993, f1:0.4411
>> valid relation prec:0.0828, rec:0.0218, f1:0.0346
>> valid relation with NER prec:0.0828, rec:0.0218, f1:0.0346
g_step 2600, step 96, avg_time 2.477, loss:425.4513
g_step 2700, step 196, avg_time 1.084, loss:428.2351
g_step 2800, step 296, avg_time 1.093, loss:441.7729
g_step 2900, step 83, avg_time 1.091, loss:400.3811
g_step 3000, step 183, avg_time 1.093, loss:402.0593
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4658, rec:0.4592, f1:0.4625
>> valid relation prec:0.0800, rec:0.0256, f1:0.0388
>> valid relation with NER prec:0.0800, rec:0.0256, f1:0.0388
g_step 3100, step 283, avg_time 2.478, loss:429.2264
g_step 3200, step 70, avg_time 1.091, loss:410.2589
g_step 3300, step 170, avg_time 1.095, loss:400.6196
g_step 3400, step 270, avg_time 1.097, loss:398.3215
g_step 3500, step 57, avg_time 1.091, loss:394.1959
>> valid entity prec:0.4906, rec:0.4152, f1:0.4497
>> valid relation prec:0.0918, rec:0.0342, f1:0.0498
>> valid relation with NER prec:0.0918, rec:0.0342, f1:0.0498
g_step 3600, step 157, avg_time 2.484, loss:371.5426
g_step 3700, step 257, avg_time 1.083, loss:388.7010
g_step 3800, step 44, avg_time 1.087, loss:385.7209
g_step 3900, step 144, avg_time 1.095, loss:369.2293
g_step 4000, step 244, avg_time 1.085, loss:377.1120
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4691, rec:0.4866, f1:0.4777
>> valid relation prec:0.0899, rec:0.0414, f1:0.0567
>> valid relation with NER prec:0.0899, rec:0.0414, f1:0.0567
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 31, avg_time 2.490, loss:361.8812
g_step 4200, step 131, avg_time 1.101, loss:351.3205
g_step 4300, step 231, avg_time 1.083, loss:377.5769
g_step 4400, step 18, avg_time 1.083, loss:352.2469
g_step 4500, step 118, avg_time 1.099, loss:347.6790
>> valid entity prec:0.4753, rec:0.4363, f1:0.4549
>> valid relation prec:0.0855, rec:0.0305, f1:0.0449
>> valid relation with NER prec:0.0855, rec:0.0305, f1:0.0449
g_step 4600, step 218, avg_time 2.472, loss:355.2842
g_step 4700, step 5, avg_time 1.096, loss:348.8029
g_step 4800, step 105, avg_time 1.085, loss:329.5619
g_step 4900, step 205, avg_time 1.084, loss:344.6900
g_step 5000, step 305, avg_time 1.095, loss:333.2814
learning rate was adjusted to 0.0008
>> valid entity prec:0.4831, rec:0.3825, f1:0.4270
>> valid relation prec:0.0871, rec:0.0285, f1:0.0429
>> valid relation with NER prec:0.0871, rec:0.0285, f1:0.0429
g_step 5100, step 92, avg_time 2.488, loss:309.2611
g_step 5200, step 192, avg_time 1.102, loss:336.5202
g_step 5300, step 292, avg_time 1.084, loss:315.0620
g_step 5400, step 79, avg_time 1.108, loss:306.4389
g_step 5500, step 179, avg_time 1.096, loss:302.7179
>> valid entity prec:0.4546, rec:0.4624, f1:0.4585
>> valid relation prec:0.0672, rec:0.0388, f1:0.0492
>> valid relation with NER prec:0.0672, rec:0.0388, f1:0.0492
g_step 5600, step 279, avg_time 2.475, loss:330.1096
g_step 5700, step 66, avg_time 1.081, loss:305.7272
g_step 5800, step 166, avg_time 1.086, loss:298.7584
g_step 5900, step 266, avg_time 1.096, loss:305.7320
g_step 6000, step 53, avg_time 1.078, loss:319.3712
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4645, rec:0.4390, f1:0.4514
>> valid relation prec:0.0907, rec:0.0391, f1:0.0546
>> valid relation with NER prec:0.0907, rec:0.0391, f1:0.0546
g_step 6100, step 153, avg_time 2.487, loss:281.9956
g_step 6200, step 253, avg_time 1.090, loss:322.8721
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:29:14 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:29:14 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-29-14_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:29:16 - WARNING - datasets.builder -   Using custom data configuration default-406bc87d0d1523c9
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-406bc87d0d1523c9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:01,  1.46s/ tables]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:29:19,424 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:29:19,425 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:29:19,425 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:29:19,426 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:29:19,518 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:19,547 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:19,547 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:19,547 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:19,547 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:19,548 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:19,548 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:29:19,931 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:29:23,144 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:29:23,245 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-406bc87d0d1523c9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.46ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.50ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.09ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.44ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.89ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.23ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.47ba/s]100%|██████████| 8/8 [00:01<00:00,  5.41ba/s]100%|██████████| 8/8 [00:01<00:00,  4.42ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.31ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.94ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.21ba/s]100%|██████████| 4/4 [00:00<00:00,  5.37ba/s]100%|██████████| 4/4 [00:00<00:00,  4.72ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  4.59ba/s] 38%|███▊      | 3/8 [00:00<00:00,  7.71ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  8.79ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.30ba/s]100%|██████████| 8/8 [00:00<00:00,  9.27ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.28ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.16ba/s]100%|██████████| 4/4 [00:00<00:00, 10.27ba/s]
[INFO|trainer.py:414] 2023-08-28 21:29:28,003 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:29:28,026 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:29:28,026 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 21:29:28,026 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:29:28,026 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:29:28,026 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:29:28,026 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:29:28,026 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:01<09:57,  1.02s/it]  0%|          | 2/585 [00:01<05:44,  1.69it/s]  1%|          | 3/585 [00:01<04:23,  2.21it/s]  1%|          | 4/585 [00:01<03:44,  2.58it/s]  1%|          | 5/585 [00:02<03:24,  2.84it/s]  1%|          | 6/585 [00:02<03:11,  3.03it/s]  1%|          | 7/585 [00:02<03:02,  3.16it/s]  1%|▏         | 8/585 [00:03<02:57,  3.26it/s]  2%|▏         | 9/585 [00:03<02:53,  3.32it/s]  2%|▏         | 10/585 [00:03<02:50,  3.36it/s]  2%|▏         | 11/585 [00:03<02:49,  3.40it/s]  2%|▏         | 12/585 [00:04<02:47,  3.42it/s]  2%|▏         | 13/585 [00:04<02:46,  3.43it/s]  2%|▏         | 14/585 [00:04<02:52,  3.31it/s]  3%|▎         | 15/585 [00:05<02:49,  3.36it/s]  3%|▎         | 16/585 [00:05<02:47,  3.39it/s]  3%|▎         | 17/585 [00:05<02:46,  3.41it/s]  3%|▎         | 18/585 [00:05<02:45,  3.43it/s]  3%|▎         | 19/585 [00:06<02:44,  3.44it/s]  3%|▎         | 20/585 [00:06<02:43,  3.45it/s]  4%|▎         | 21/585 [00:06<02:43,  3.45it/s]  4%|▍         | 22/585 [00:07<02:42,  3.46it/s]  4%|▍         | 23/585 [00:07<02:42,  3.46it/s]  4%|▍         | 24/585 [00:07<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.47it/s]  4%|▍         | 26/585 [00:08<02:41,  3.47it/s]  5%|▍         | 27/585 [00:08<02:40,  3.47it/s]  5%|▍         | 28/585 [00:08<02:40,  3.47it/s]  5%|▍         | 29/585 [00:09<02:40,  3.47it/s]  5%|▌         | 30/585 [00:09<02:39,  3.47it/s]  5%|▌         | 31/585 [00:09<02:39,  3.47it/s]  5%|▌         | 32/585 [00:10<02:44,  3.36it/s]  6%|▌         | 33/585 [00:10<02:42,  3.39it/s]  6%|▌         | 34/585 [00:10<02:41,  3.41it/s]  6%|▌         | 35/585 [00:10<02:40,  3.43it/s]  6%|▌         | 36/585 [00:11<02:39,  3.44it/s]  6%|▋         | 37/585 [00:11<02:39,  3.44it/s]  6%|▋         | 38/585 [00:11<02:38,  3.45it/s]  7%|▋         | 39/585 [00:12<02:38,  3.45it/s]  7%|▋         | 40/585 [00:12<02:37,  3.46it/s]  7%|▋         | 41/585 [00:12<02:37,  3.46it/s]  7%|▋         | 42/585 [00:12<02:36,  3.46it/s]  7%|▋         | 43/585 [00:13<02:36,  3.46it/s]  8%|▊         | 44/585 [00:13<02:36,  3.46it/s]  8%|▊         | 45/585 [00:13<02:36,  3.46it/s]  8%|▊         | 46/585 [00:14<02:35,  3.46it/s]  8%|▊         | 47/585 [00:14<02:35,  3.46it/s]  8%|▊         | 48/585 [00:14<02:35,  3.46it/s]  8%|▊         | 49/585 [00:14<02:36,  3.42it/s]  9%|▊         | 50/585 [00:15<02:35,  3.43it/s]  9%|▊         | 51/585 [00:15<02:35,  3.44it/s]  9%|▉         | 52/585 [00:15<02:34,  3.45it/s]  9%|▉         | 53/585 [00:16<02:34,  3.45it/s]  9%|▉         | 54/585 [00:16<02:33,  3.46it/s]  9%|▉         | 55/585 [00:16<02:33,  3.46it/s] 10%|▉         | 56/585 [00:16<02:33,  3.46it/s] 10%|▉         | 57/585 [00:17<02:32,  3.46it/s] 10%|▉         | 58/585 [00:17<02:32,  3.46it/s] 10%|█         | 59/585 [00:17<02:32,  3.46it/s] 10%|█         | 60/585 [00:18<02:31,  3.46it/s] 10%|█         | 61/585 [00:18<02:31,  3.46it/s] 11%|█         | 62/585 [00:18<02:31,  3.46it/s] 11%|█         | 63/585 [00:18<02:30,  3.46it/s] 11%|█         | 64/585 [00:19<02:30,  3.46it/s] 11%|█         | 65/585 [00:19<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.46it/s] 11%|█▏        | 67/585 [00:20<02:32,  3.41it/s] 12%|█▏        | 68/585 [00:20<02:31,  3.42it/s] 12%|█▏        | 69/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 70/585 [00:21<02:29,  3.44it/s] 12%|█▏        | 71/585 [00:21<02:29,  3.45it/s] 12%|█▏        | 72/585 [00:21<02:28,  3.45it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 74/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 75/585 [00:22<02:27,  3.46it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 77/585 [00:23<02:26,  3.46it/s] 13%|█▎        | 78/585 [00:23<02:26,  3.46it/s] 14%|█▎        | 79/585 [00:23<02:26,  3.46it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.46it/s] 14%|█▍        | 81/585 [00:24<02:25,  3.46it/s] 14%|█▍        | 82/585 [00:24<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.46it/s] 14%|█▍        | 84/585 [00:25<02:30,  3.34it/s] 15%|█▍        | 85/585 [00:25<02:28,  3.38it/s] 15%|█▍        | 86/585 [00:25<02:26,  3.40it/s] 15%|█▍        | 87/585 [00:25<02:25,  3.41it/s] 15%|█▌        | 88/585 [00:26<02:25,  3.42it/s] 15%|█▌        | 89/585 [00:26<02:24,  3.44it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 91/585 [00:27<02:23,  3.45it/s] 16%|█▌        | 92/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 93/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 94/585 [00:28<02:22,  3.45it/s] 16%|█▌        | 95/585 [00:28<02:21,  3.46it/s] 16%|█▋        | 96/585 [00:28<02:21,  3.46it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.46it/s] 17%|█▋        | 98/585 [00:29<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:29<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:29<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:30<02:20,  3.45it/s] 17%|█▋        | 102/585 [00:30<02:26,  3.30it/s] 18%|█▊        | 103/585 [00:30<02:24,  3.34it/s] 18%|█▊        | 104/585 [00:30<02:22,  3.38it/s] 18%|█▊        | 105/585 [00:31<02:21,  3.40it/s] 18%|█▊        | 106/585 [00:31<02:20,  3.42it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.43it/s] 18%|█▊        | 108/585 [00:32<02:18,  3.44it/s] 19%|█▊        | 109/585 [00:32<02:18,  3.44it/s] 19%|█▉        | 110/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 112/585 [00:33<02:16,  3.45it/s] 19%|█▉        | 113/585 [00:33<02:16,  3.45it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.46it/s] 20%|█▉        | 115/585 [00:34<02:16,  3.45it/s] 20%|█▉        | 116/585 [00:34<02:16,  3.44it/s] 20%|██        | 117/585 [00:34<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 21:30:02,787 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:30:02,787 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:30:02,787 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.13it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.44it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.56it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.00it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.60it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.30it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.07it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.66it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.71it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.61it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.66it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.65it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.72it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.71it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.76it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.59it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.51it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.62it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.61it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.51it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.48it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.60it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 41.24it/s][A
 28%|██▊       | 123/435 [00:02<00:07, 42.65it/s][A
 29%|██▉       | 128/435 [00:02<00:07, 43.81it/s][A
 31%|███       | 133/435 [00:02<00:06, 44.65it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 45.29it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 45.73it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.01it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.24it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 46.10it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.22it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.38it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.34it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.49it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.63it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.63it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.76it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.75it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.47it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.47it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.51it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.47it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.57it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.61it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.62it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.72it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.71it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.63it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.54it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.56it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.44it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.55it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.54it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.64it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.69it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.64it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.38it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.54it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.58it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.54it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.42it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.55it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.64it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.69it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.71it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.67it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.62it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.64it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.52it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.50it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.48it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.54it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.64it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.67it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.62it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.58it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.61it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.56it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.55it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.51it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.40it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.49it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.58it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.60it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.43it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:44<02:16,  3.44it/s]
100%|██████████| 435/435 [00:09<00:00, 46.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:30:12,211 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 21:30:12,238 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:30:16,974 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:30:17,090 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:30:17,109 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:00<1:02:29,  8.03s/it] 20%|██        | 119/585 [01:01<44:21,  5.71s/it]   21%|██        | 120/585 [01:01<31:39,  4.08s/it] 21%|██        | 121/585 [01:01<22:46,  2.95s/it] 21%|██        | 122/585 [01:01<16:34,  2.15s/it] 21%|██        | 123/585 [01:02<12:14,  1.59s/it] 21%|██        | 124/585 [01:02<09:13,  1.20s/it] 21%|██▏       | 125/585 [01:02<07:06,  1.08it/s] 22%|██▏       | 126/585 [01:03<05:37,  1.36it/s] 22%|██▏       | 127/585 [01:03<04:35,  1.66it/s] 22%|██▏       | 128/585 [01:03<03:51,  1.97it/s] 22%|██▏       | 129/585 [01:03<03:21,  2.27it/s] 22%|██▏       | 130/585 [01:04<03:07,  2.42it/s] 22%|██▏       | 131/585 [01:04<02:50,  2.66it/s] 23%|██▎       | 132/585 [01:04<02:38,  2.86it/s] 23%|██▎       | 133/585 [01:05<02:29,  3.02it/s] 23%|██▎       | 134/585 [01:05<02:23,  3.14it/s] 23%|██▎       | 135/585 [01:05<02:19,  3.23it/s] 23%|██▎       | 136/585 [01:06<02:16,  3.30it/s] 23%|██▎       | 137/585 [01:06<02:13,  3.35it/s] 24%|██▎       | 138/585 [01:06<02:12,  3.38it/s] 24%|██▍       | 139/585 [01:06<02:11,  3.40it/s] 24%|██▍       | 140/585 [01:07<02:10,  3.42it/s] 24%|██▍       | 141/585 [01:07<02:11,  3.37it/s] 24%|██▍       | 142/585 [01:07<02:10,  3.40it/s] 24%|██▍       | 143/585 [01:08<02:09,  3.41it/s] 25%|██▍       | 144/585 [01:08<02:08,  3.43it/s] 25%|██▍       | 145/585 [01:08<02:08,  3.44it/s] 25%|██▍       | 146/585 [01:08<02:07,  3.44it/s] 25%|██▌       | 147/585 [01:09<02:07,  3.45it/s] 25%|██▌       | 148/585 [01:09<02:06,  3.45it/s] 25%|██▌       | 149/585 [01:09<02:06,  3.46it/s] 26%|██▌       | 150/585 [01:10<02:05,  3.46it/s] 26%|██▌       | 151/585 [01:10<02:05,  3.46it/s] 26%|██▌       | 152/585 [01:10<02:08,  3.37it/s] 26%|██▌       | 153/585 [01:11<02:07,  3.39it/s] 26%|██▋       | 154/585 [01:11<02:06,  3.41it/s] 26%|██▋       | 155/585 [01:11<02:05,  3.43it/s] 27%|██▋       | 156/585 [01:11<02:04,  3.44it/s] 27%|██▋       | 157/585 [01:12<02:04,  3.44it/s] 27%|██▋       | 158/585 [01:12<02:03,  3.45it/s] 27%|██▋       | 159/585 [01:12<02:03,  3.45it/s] 27%|██▋       | 160/585 [01:13<02:03,  3.45it/s] 28%|██▊       | 161/585 [01:13<02:02,  3.46it/s] 28%|██▊       | 162/585 [01:13<02:02,  3.46it/s] 28%|██▊       | 163/585 [01:13<02:02,  3.46it/s] 28%|██▊       | 164/585 [01:14<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:14<02:01,  3.46it/s] 28%|██▊       | 166/585 [01:14<02:01,  3.46it/s] 29%|██▊       | 167/585 [01:15<02:00,  3.46it/s] 29%|██▊       | 168/585 [01:15<02:00,  3.46it/s] 29%|██▉       | 169/585 [01:15<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:15<02:00,  3.46it/s] 29%|██▉       | 171/585 [01:16<01:59,  3.45it/s] 29%|██▉       | 172/585 [01:16<02:02,  3.38it/s] 30%|██▉       | 173/585 [01:16<02:01,  3.40it/s] 30%|██▉       | 174/585 [01:17<02:00,  3.41it/s] 30%|██▉       | 175/585 [01:17<01:59,  3.43it/s] 30%|███       | 176/585 [01:17<01:58,  3.44it/s] 30%|███       | 177/585 [01:17<01:58,  3.44it/s] 30%|███       | 178/585 [01:18<01:58,  3.45it/s] 31%|███       | 179/585 [01:18<01:57,  3.45it/s] 31%|███       | 180/585 [01:18<01:57,  3.45it/s] 31%|███       | 181/585 [01:19<01:57,  3.45it/s] 31%|███       | 182/585 [01:19<01:56,  3.45it/s] 31%|███▏      | 183/585 [01:19<01:58,  3.39it/s] 31%|███▏      | 184/585 [01:20<01:57,  3.41it/s] 32%|███▏      | 185/585 [01:20<01:56,  3.42it/s] 32%|███▏      | 186/585 [01:20<01:56,  3.43it/s] 32%|███▏      | 187/585 [01:20<01:55,  3.44it/s] 32%|███▏      | 188/585 [01:21<01:55,  3.44it/s] 32%|███▏      | 189/585 [01:21<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:21<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:22<01:54,  3.45it/s] 33%|███▎      | 192/585 [01:22<01:53,  3.45it/s] 33%|███▎      | 193/585 [01:22<01:53,  3.46it/s] 33%|███▎      | 194/585 [01:22<01:54,  3.41it/s] 33%|███▎      | 195/585 [01:23<01:53,  3.43it/s] 34%|███▎      | 196/585 [01:23<01:53,  3.43it/s] 34%|███▎      | 197/585 [01:23<01:52,  3.44it/s] 34%|███▍      | 198/585 [01:24<01:52,  3.45it/s] 34%|███▍      | 199/585 [01:24<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:24<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:24<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:25<01:50,  3.45it/s] 35%|███▍      | 203/585 [01:25<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:25<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:26<01:52,  3.37it/s] 35%|███▌      | 206/585 [01:26<01:51,  3.39it/s] 35%|███▌      | 207/585 [01:26<01:50,  3.41it/s] 36%|███▌      | 208/585 [01:27<01:50,  3.43it/s] 36%|███▌      | 209/585 [01:27<01:49,  3.43it/s] 36%|███▌      | 210/585 [01:27<01:48,  3.44it/s] 36%|███▌      | 211/585 [01:27<01:48,  3.44it/s] 36%|███▌      | 212/585 [01:28<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:28<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:28<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:29<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:29<01:49,  3.36it/s] 37%|███▋      | 217/585 [01:29<01:48,  3.38it/s] 37%|███▋      | 218/585 [01:29<01:47,  3.41it/s] 37%|███▋      | 219/585 [01:30<01:47,  3.42it/s] 38%|███▊      | 220/585 [01:30<01:46,  3.43it/s] 38%|███▊      | 221/585 [01:30<01:46,  3.43it/s] 38%|███▊      | 222/585 [01:31<01:45,  3.44it/s] 38%|███▊      | 223/585 [01:31<01:45,  3.44it/s] 38%|███▊      | 224/585 [01:31<01:44,  3.44it/s] 38%|███▊      | 225/585 [01:31<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:32<01:44,  3.45it/s] 39%|███▉      | 227/585 [01:32<01:47,  3.33it/s] 39%|███▉      | 228/585 [01:32<01:46,  3.36it/s] 39%|███▉      | 229/585 [01:33<01:44,  3.39it/s] 39%|███▉      | 230/585 [01:33<01:44,  3.41it/s] 39%|███▉      | 231/585 [01:33<01:43,  3.43it/s] 40%|███▉      | 232/585 [01:34<01:42,  3.43it/s] 40%|███▉      | 233/585 [01:34<01:42,  3.44it/s] 40%|████      | 234/585 [01:34<01:41,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 21:31:02,672 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:31:02,672 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:31:02,672 >>   Batch size = 8
{'eval_loss': 1.0437591075897217, 'eval_runtime': 9.395, 'eval_samples_per_second': 370.302, 'eval_steps_per_second': 46.301, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.58it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.29it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.65it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.93it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.49it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.27it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.01it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.60it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.63it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.62it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.66it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.70it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.73it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.53it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.63it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.54it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.48it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.49it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.44it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.54it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.50it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.58it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.64it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.67it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.45it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.49it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.51it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.44it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.48it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.54it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.62it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.64it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.63it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.50it/s][A
 41%|████      | 178/435 [00:03<00:07, 33.40it/s][A
 42%|████▏     | 183/435 [00:04<00:06, 36.46it/s][A
 43%|████▎     | 188/435 [00:04<00:06, 39.04it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 41.12it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 42.68it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 43.84it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 44.57it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.25it/s][A
 50%|█████     | 218/435 [00:04<00:04, 45.27it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 45.64it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 45.80it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.99it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.23it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.41it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.52it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.47it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.55it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.47it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.44it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.47it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 46.40it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.38it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.53it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.61it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.54it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.59it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.52it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.47it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.41it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 46.29it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.46it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.50it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.57it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.51it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.59it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.55it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.53it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.47it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 46.41it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.44it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.52it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.52it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.48it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.48it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.42it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.36it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.34it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.18it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.31it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.39it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.43it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.45it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:44<01:41,  3.44it/s]
100%|██████████| 435/435 [00:09<00:00, 46.45it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:31:12,225 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 21:31:12,271 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:31:16,819 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:31:16,849 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:31:16,862 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:01<47:36,  8.16s/it] 40%|████      | 236/585 [02:01<33:48,  5.81s/it] 41%|████      | 237/585 [02:01<24:06,  4.16s/it] 41%|████      | 238/585 [02:02<17:19,  3.00s/it] 41%|████      | 239/585 [02:02<12:35,  2.18s/it] 41%|████      | 240/585 [02:02<09:17,  1.61s/it] 41%|████      | 241/585 [02:02<06:58,  1.22s/it] 41%|████▏     | 242/585 [02:03<05:21,  1.07it/s] 42%|████▏     | 243/585 [02:03<04:14,  1.35it/s] 42%|████▏     | 244/585 [02:03<03:26,  1.65it/s] 42%|████▏     | 245/585 [02:04<02:53,  1.96it/s] 42%|████▏     | 246/585 [02:04<02:30,  2.25it/s] 42%|████▏     | 247/585 [02:04<02:16,  2.47it/s] 42%|████▏     | 248/585 [02:04<02:04,  2.70it/s] 43%|████▎     | 249/585 [02:05<01:56,  2.89it/s] 43%|████▎     | 250/585 [02:05<01:50,  3.04it/s] 43%|████▎     | 251/585 [02:05<01:45,  3.16it/s] 43%|████▎     | 252/585 [02:06<01:42,  3.25it/s] 43%|████▎     | 253/585 [02:06<01:40,  3.31it/s] 43%|████▎     | 254/585 [02:06<01:38,  3.35it/s] 44%|████▎     | 255/585 [02:06<01:37,  3.38it/s] 44%|████▍     | 256/585 [02:07<01:36,  3.41it/s] 44%|████▍     | 257/585 [02:07<01:35,  3.42it/s] 44%|████▍     | 258/585 [02:07<01:44,  3.12it/s] 44%|████▍     | 259/585 [02:08<01:41,  3.21it/s] 44%|████▍     | 260/585 [02:08<01:39,  3.28it/s] 45%|████▍     | 261/585 [02:08<01:37,  3.34it/s] 45%|████▍     | 262/585 [02:09<01:35,  3.37it/s] 45%|████▍     | 263/585 [02:09<01:34,  3.40it/s] 45%|████▌     | 264/585 [02:09<01:33,  3.42it/s] 45%|████▌     | 265/585 [02:09<01:33,  3.43it/s] 45%|████▌     | 266/585 [02:10<01:32,  3.44it/s] 46%|████▌     | 267/585 [02:10<01:32,  3.44it/s] 46%|████▌     | 268/585 [02:10<01:32,  3.44it/s] 46%|████▌     | 269/585 [02:11<01:34,  3.34it/s] 46%|████▌     | 270/585 [02:11<01:33,  3.37it/s] 46%|████▋     | 271/585 [02:11<01:32,  3.40it/s] 46%|████▋     | 272/585 [02:12<01:31,  3.42it/s] 47%|████▋     | 273/585 [02:12<01:31,  3.43it/s] 47%|████▋     | 274/585 [02:12<01:30,  3.44it/s] 47%|████▋     | 275/585 [02:12<01:30,  3.44it/s] 47%|████▋     | 276/585 [02:13<01:29,  3.45it/s] 47%|████▋     | 277/585 [02:13<01:29,  3.45it/s] 48%|████▊     | 278/585 [02:13<01:28,  3.45it/s] 48%|████▊     | 279/585 [02:14<01:28,  3.45it/s] 48%|████▊     | 280/585 [02:14<01:29,  3.40it/s] 48%|████▊     | 281/585 [02:14<01:29,  3.41it/s] 48%|████▊     | 282/585 [02:14<01:28,  3.41it/s] 48%|████▊     | 283/585 [02:15<01:28,  3.43it/s] 49%|████▊     | 284/585 [02:15<01:27,  3.44it/s] 49%|████▊     | 285/585 [02:15<01:27,  3.44it/s] 49%|████▉     | 286/585 [02:16<01:26,  3.44it/s] 49%|████▉     | 287/585 [02:16<01:26,  3.45it/s] 49%|████▉     | 288/585 [02:16<01:26,  3.45it/s] 49%|████▉     | 289/585 [02:16<01:25,  3.45it/s] 50%|████▉     | 290/585 [02:17<01:25,  3.45it/s] 50%|████▉     | 291/585 [02:17<01:25,  3.45it/s] 50%|████▉     | 292/585 [02:17<01:26,  3.38it/s] 50%|█████     | 293/585 [02:18<01:25,  3.40it/s] 50%|█████     | 294/585 [02:18<01:25,  3.42it/s] 50%|█████     | 295/585 [02:18<01:24,  3.43it/s] 51%|█████     | 296/585 [02:18<01:24,  3.44it/s] 51%|█████     | 297/585 [02:19<01:23,  3.44it/s] 51%|█████     | 298/585 [02:19<01:23,  3.45it/s] 51%|█████     | 299/585 [02:19<01:22,  3.45it/s] 51%|█████▏    | 300/585 [02:20<01:22,  3.45it/s] 51%|█████▏    | 301/585 [02:20<01:22,  3.45it/s] 52%|█████▏    | 302/585 [02:20<01:21,  3.45it/s] 52%|█████▏    | 303/585 [02:21<01:22,  3.43it/s] 52%|█████▏    | 304/585 [02:21<01:21,  3.44it/s] 52%|█████▏    | 305/585 [02:21<01:21,  3.45it/s] 52%|█████▏    | 306/585 [02:21<01:20,  3.45it/s] 52%|█████▏    | 307/585 [02:22<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:22<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:22<01:19,  3.45it/s] 53%|█████▎    | 310/585 [02:23<01:19,  3.45it/s] 53%|█████▎    | 311/585 [02:23<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:23<01:18,  3.46it/s] 54%|█████▎    | 313/585 [02:23<01:18,  3.45it/s] 54%|█████▎    | 314/585 [02:24<01:18,  3.44it/s] 54%|█████▍    | 315/585 [02:24<01:18,  3.44it/s] 54%|█████▍    | 316/585 [02:24<01:17,  3.45it/s] 54%|█████▍    | 317/585 [02:25<01:17,  3.45it/s] 54%|█████▍    | 318/585 [02:25<01:17,  3.46it/s] 55%|█████▍    | 319/585 [02:25<01:16,  3.46it/s] 55%|█████▍    | 320/585 [02:25<01:16,  3.46it/s] 55%|█████▍    | 321/585 [02:26<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:26<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:26<01:15,  3.46it/s] 55%|█████▌    | 324/585 [02:27<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:27<01:16,  3.41it/s] 56%|█████▌    | 326/585 [02:27<01:15,  3.42it/s] 56%|█████▌    | 327/585 [02:27<01:15,  3.43it/s] 56%|█████▌    | 328/585 [02:28<01:14,  3.44it/s] 56%|█████▌    | 329/585 [02:28<01:14,  3.44it/s] 56%|█████▋    | 330/585 [02:28<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:29<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:29<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:29<01:13,  3.45it/s] 57%|█████▋    | 334/585 [02:30<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:30<01:12,  3.45it/s] 57%|█████▋    | 336/585 [02:30<01:13,  3.41it/s] 58%|█████▊    | 337/585 [02:30<01:12,  3.42it/s] 58%|█████▊    | 338/585 [02:31<01:12,  3.43it/s] 58%|█████▊    | 339/585 [02:31<01:11,  3.44it/s] 58%|█████▊    | 340/585 [02:31<01:11,  3.44it/s] 58%|█████▊    | 341/585 [02:32<01:10,  3.45it/s] 58%|█████▊    | 342/585 [02:32<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:32<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:32<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:33<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:33<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:33<01:09,  3.44it/s] 59%|█████▉    | 348/585 [02:34<01:08,  3.45it/s] 60%|█████▉    | 349/585 [02:34<01:08,  3.45it/s] 60%|█████▉    | 350/585 [02:34<01:08,  3.45it/s] 60%|██████    | 351/585 [02:34<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 21:32:03,029 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:32:03,029 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:32:03,029 >>   Batch size = 8
{'eval_loss': 1.0538407564163208, 'eval_runtime': 9.5061, 'eval_samples_per_second': 365.976, 'eval_steps_per_second': 45.76, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.69it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.45it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.71it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.96it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.48it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.21it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.97it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.54it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.48it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.60it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.65it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.76it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.75it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.63it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.63it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.57it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.43it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.40it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.41it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.53it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.61it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.61it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.69it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.56it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.56it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.52it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.52it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.50it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.59it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.58it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.66it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.59it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.57it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.59it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.55it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.51it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.53it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.48it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.58it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.66it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.64it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.44it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.55it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.35it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.58it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.54it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.58it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.79it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.10it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.26it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.31it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.18it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.32it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.37it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.41it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.38it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.48it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.53it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.48it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.54it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.42it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.54it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.40it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.43it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.48it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.49it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.53it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.59it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.50it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.52it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.54it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.50it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.34it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.38it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.38it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 42.22it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 43.49it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 44.40it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 45.08it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 45.50it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 45.85it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.02it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.28it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.03it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.11it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.27it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:44<01:07,  3.45it/s]
100%|██████████| 435/435 [00:09<00:00, 46.27it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:32:12,492 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 21:32:12,600 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:32:16,846 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:32:16,951 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:32:17,088 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:00<30:47,  7.93s/it] 60%|██████    | 353/585 [03:01<21:48,  5.64s/it] 61%|██████    | 354/585 [03:01<15:31,  4.03s/it] 61%|██████    | 355/585 [03:01<11:09,  2.91s/it] 61%|██████    | 356/585 [03:01<08:06,  2.12s/it] 61%|██████    | 357/585 [03:02<05:58,  1.57s/it] 61%|██████    | 358/585 [03:02<04:29,  1.19s/it] 61%|██████▏   | 359/585 [03:02<03:27,  1.09it/s] 62%|██████▏   | 360/585 [03:03<02:44,  1.37it/s] 62%|██████▏   | 361/585 [03:03<02:13,  1.67it/s] 62%|██████▏   | 362/585 [03:03<01:52,  1.98it/s] 62%|██████▏   | 363/585 [03:03<01:37,  2.27it/s] 62%|██████▏   | 364/585 [03:04<01:27,  2.53it/s] 62%|██████▏   | 365/585 [03:04<01:19,  2.75it/s] 63%|██████▎   | 366/585 [03:04<01:14,  2.94it/s] 63%|██████▎   | 367/585 [03:05<01:10,  3.08it/s] 63%|██████▎   | 368/585 [03:05<01:08,  3.18it/s] 63%|██████▎   | 369/585 [03:05<01:06,  3.26it/s] 63%|██████▎   | 370/585 [03:05<01:04,  3.32it/s] 63%|██████▎   | 371/585 [03:06<01:03,  3.36it/s] 64%|██████▎   | 372/585 [03:06<01:02,  3.39it/s] 64%|██████▍   | 373/585 [03:06<01:02,  3.41it/s] 64%|██████▍   | 374/585 [03:07<01:01,  3.43it/s] 64%|██████▍   | 375/585 [03:07<01:02,  3.34it/s] 64%|██████▍   | 376/585 [03:07<01:01,  3.37it/s] 64%|██████▍   | 377/585 [03:07<01:01,  3.40it/s] 65%|██████▍   | 378/585 [03:08<01:00,  3.42it/s] 65%|██████▍   | 379/585 [03:08<01:00,  3.43it/s] 65%|██████▍   | 380/585 [03:08<00:59,  3.44it/s] 65%|██████▌   | 381/585 [03:09<00:59,  3.45it/s] 65%|██████▌   | 382/585 [03:09<00:58,  3.45it/s] 65%|██████▌   | 383/585 [03:09<00:58,  3.46it/s] 66%|██████▌   | 384/585 [03:09<00:58,  3.46it/s] 66%|██████▌   | 385/585 [03:10<00:57,  3.46it/s] 66%|██████▌   | 386/585 [03:10<00:58,  3.41it/s] 66%|██████▌   | 387/585 [03:10<00:57,  3.43it/s] 66%|██████▋   | 388/585 [03:11<00:57,  3.44it/s] 66%|██████▋   | 389/585 [03:11<00:56,  3.44it/s] 67%|██████▋   | 390/585 [03:11<00:56,  3.45it/s] 67%|██████▋   | 391/585 [03:12<00:56,  3.45it/s] 67%|██████▋   | 392/585 [03:12<00:55,  3.46it/s] 67%|██████▋   | 393/585 [03:12<00:55,  3.46it/s] 67%|██████▋   | 394/585 [03:12<00:55,  3.46it/s] 68%|██████▊   | 395/585 [03:13<00:54,  3.46it/s] 68%|██████▊   | 396/585 [03:13<00:54,  3.46it/s] 68%|██████▊   | 397/585 [03:13<00:54,  3.42it/s] 68%|██████▊   | 398/585 [03:14<00:54,  3.43it/s] 68%|██████▊   | 399/585 [03:14<00:54,  3.44it/s] 68%|██████▊   | 400/585 [03:14<00:53,  3.45it/s] 69%|██████▊   | 401/585 [03:14<00:53,  3.45it/s] 69%|██████▊   | 402/585 [03:15<00:53,  3.45it/s] 69%|██████▉   | 403/585 [03:15<00:52,  3.45it/s] 69%|██████▉   | 404/585 [03:15<00:52,  3.46it/s] 69%|██████▉   | 405/585 [03:16<00:52,  3.46it/s] 69%|██████▉   | 406/585 [03:16<00:51,  3.46it/s] 70%|██████▉   | 407/585 [03:16<00:51,  3.46it/s] 70%|██████▉   | 408/585 [03:16<00:51,  3.46it/s] 70%|██████▉   | 409/585 [03:17<00:50,  3.46it/s] 70%|███████   | 410/585 [03:17<00:50,  3.46it/s] 70%|███████   | 411/585 [03:17<00:50,  3.46it/s] 70%|███████   | 412/585 [03:18<00:50,  3.46it/s] 71%|███████   | 413/585 [03:18<00:49,  3.46it/s] 71%|███████   | 414/585 [03:18<00:49,  3.46it/s] 71%|███████   | 415/585 [03:18<00:49,  3.45it/s] 71%|███████   | 416/585 [03:19<00:49,  3.41it/s] 71%|███████▏  | 417/585 [03:19<00:49,  3.42it/s] 71%|███████▏  | 418/585 [03:19<00:48,  3.43it/s] 72%|███████▏  | 419/585 [03:20<00:48,  3.44it/s] 72%|███████▏  | 420/585 [03:20<00:47,  3.45it/s] 72%|███████▏  | 421/585 [03:20<00:47,  3.45it/s] 72%|███████▏  | 422/585 [03:21<00:47,  3.45it/s] 72%|███████▏  | 423/585 [03:21<00:46,  3.46it/s] 72%|███████▏  | 424/585 [03:21<00:46,  3.45it/s] 73%|███████▎  | 425/585 [03:21<00:46,  3.45it/s] 73%|███████▎  | 426/585 [03:22<00:46,  3.45it/s] 73%|███████▎  | 427/585 [03:22<00:46,  3.42it/s] 73%|███████▎  | 428/585 [03:22<00:45,  3.43it/s] 73%|███████▎  | 429/585 [03:23<00:45,  3.43it/s] 74%|███████▎  | 430/585 [03:23<00:45,  3.44it/s] 74%|███████▎  | 431/585 [03:23<00:44,  3.45it/s] 74%|███████▍  | 432/585 [03:23<00:44,  3.45it/s] 74%|███████▍  | 433/585 [03:24<00:44,  3.45it/s] 74%|███████▍  | 434/585 [03:24<00:43,  3.45it/s] 74%|███████▍  | 435/585 [03:24<00:43,  3.45it/s] 75%|███████▍  | 436/585 [03:25<00:43,  3.46it/s] 75%|███████▍  | 437/585 [03:25<00:42,  3.45it/s] 75%|███████▍  | 438/585 [03:25<00:48,  3.01it/s] 75%|███████▌  | 439/585 [03:26<00:46,  3.13it/s] 75%|███████▌  | 440/585 [03:26<00:44,  3.22it/s] 75%|███████▌  | 441/585 [03:26<00:43,  3.29it/s] 76%|███████▌  | 442/585 [03:26<00:42,  3.34it/s] 76%|███████▌  | 443/585 [03:27<00:42,  3.37it/s] 76%|███████▌  | 444/585 [03:27<00:41,  3.40it/s] 76%|███████▌  | 445/585 [03:27<00:41,  3.41it/s] 76%|███████▌  | 446/585 [03:28<00:40,  3.43it/s] 76%|███████▋  | 447/585 [03:28<00:40,  3.44it/s] 77%|███████▋  | 448/585 [03:28<00:39,  3.44it/s] 77%|███████▋  | 449/585 [03:28<00:39,  3.44it/s] 77%|███████▋  | 450/585 [03:29<00:39,  3.45it/s] 77%|███████▋  | 451/585 [03:29<00:38,  3.44it/s] 77%|███████▋  | 452/585 [03:29<00:38,  3.44it/s] 77%|███████▋  | 453/585 [03:30<00:38,  3.44it/s] 78%|███████▊  | 454/585 [03:30<00:37,  3.45it/s] 78%|███████▊  | 455/585 [03:30<00:37,  3.45it/s] 78%|███████▊  | 456/585 [03:31<00:37,  3.45it/s] 78%|███████▊  | 457/585 [03:31<00:37,  3.45it/s] 78%|███████▊  | 458/585 [03:31<00:40,  3.11it/s] 78%|███████▊  | 459/585 [03:31<00:39,  3.21it/s] 79%|███████▊  | 460/585 [03:32<00:38,  3.28it/s] 79%|███████▉  | 461/585 [03:32<00:37,  3.33it/s] 79%|███████▉  | 462/585 [03:32<00:36,  3.37it/s] 79%|███████▉  | 463/585 [03:33<00:35,  3.39it/s] 79%|███████▉  | 464/585 [03:33<00:35,  3.41it/s] 79%|███████▉  | 465/585 [03:33<00:35,  3.42it/s] 80%|███████▉  | 466/585 [03:34<00:34,  3.43it/s] 80%|███████▉  | 467/585 [03:34<00:34,  3.44it/s] 80%|████████  | 468/585 [03:34<00:36,  3.22it/s][INFO|trainer.py:2140] 2023-08-28 21:33:02,727 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:33:02,727 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:33:02,727 >>   Batch size = 8
{'eval_loss': 1.0777939558029175, 'eval_runtime': 9.4028, 'eval_samples_per_second': 369.995, 'eval_steps_per_second': 46.263, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.10it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.55it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.76it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.05it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.64it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.22it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.66it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.40it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.43it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.53it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.61it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.61it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.71it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.72it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.69it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.55it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.49it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.38it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.39it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.44it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.52it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.58it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.68it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.71it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.67it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.58it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.35it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.34it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.40it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.46it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.54it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.59it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.63it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.57it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.49it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.45it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.47it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.38it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.40it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.56it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.59it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.62it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.52it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.44it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.41it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.36it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.18it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.28it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.34it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.48it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.57it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.59it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.61it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.57it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.40it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.38it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.35it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.40it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.50it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.54it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.49it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.50it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.54it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.48it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.41it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.43it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.39it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.49it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.58it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.50it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.58it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.56it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.52it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.53it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.47it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.42it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.37it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.45it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.52it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.60it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.50it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.55it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.50it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.50it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.48it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.40it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:44<00:36,  3.22it/s]
100%|██████████| 435/435 [00:09<00:00, 46.40it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:33:12,133 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:33:12,230 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:33:17,490 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:33:17,517 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:33:17,525 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:59<15:03,  7.79s/it] 80%|████████  | 470/585 [04:00<10:37,  5.55s/it] 81%|████████  | 471/585 [04:00<07:32,  3.97s/it] 81%|████████  | 472/585 [04:00<05:28,  2.91s/it] 81%|████████  | 473/585 [04:01<03:57,  2.12s/it] 81%|████████  | 474/585 [04:01<02:54,  1.57s/it] 81%|████████  | 475/585 [04:01<02:10,  1.19s/it] 81%|████████▏ | 476/585 [04:02<01:40,  1.09it/s] 82%|████████▏ | 477/585 [04:02<01:18,  1.37it/s] 82%|████████▏ | 478/585 [04:02<01:03,  1.67it/s] 82%|████████▏ | 479/585 [04:02<00:53,  1.98it/s] 82%|████████▏ | 480/585 [04:03<00:47,  2.22it/s] 82%|████████▏ | 481/585 [04:03<00:41,  2.49it/s] 82%|████████▏ | 482/585 [04:03<00:37,  2.72it/s] 83%|████████▎ | 483/585 [04:04<00:35,  2.90it/s] 83%|████████▎ | 484/585 [04:04<00:33,  3.05it/s] 83%|████████▎ | 485/585 [04:04<00:31,  3.16it/s] 83%|████████▎ | 486/585 [04:05<00:30,  3.24it/s] 83%|████████▎ | 487/585 [04:05<00:29,  3.30it/s] 83%|████████▎ | 488/585 [04:05<00:28,  3.35it/s] 84%|████████▎ | 489/585 [04:05<00:28,  3.38it/s] 84%|████████▍ | 490/585 [04:06<00:27,  3.40it/s] 84%|████████▍ | 491/585 [04:06<00:27,  3.40it/s] 84%|████████▍ | 492/585 [04:06<00:27,  3.42it/s] 84%|████████▍ | 493/585 [04:07<00:26,  3.43it/s] 84%|████████▍ | 494/585 [04:07<00:26,  3.43it/s] 85%|████████▍ | 495/585 [04:07<00:26,  3.44it/s] 85%|████████▍ | 496/585 [04:07<00:25,  3.44it/s] 85%|████████▍ | 497/585 [04:08<00:25,  3.45it/s] 85%|████████▌ | 498/585 [04:08<00:25,  3.45it/s] 85%|████████▌ | 499/585 [04:08<00:24,  3.45it/s] 85%|████████▌ | 500/585 [04:09<00:24,  3.45it/s]                                                  85%|████████▌ | 500/585 [04:09<00:24,  3.45it/s] 86%|████████▌ | 501/585 [04:09<00:29,  2.87it/s] 86%|████████▌ | 502/585 [04:09<00:27,  3.02it/s] 86%|████████▌ | 503/585 [04:10<00:26,  3.14it/s] 86%|████████▌ | 504/585 [04:10<00:25,  3.23it/s] 86%|████████▋ | 505/585 [04:10<00:24,  3.30it/s] 86%|████████▋ | 506/585 [04:11<00:23,  3.34it/s] 87%|████████▋ | 507/585 [04:11<00:23,  3.38it/s] 87%|████████▋ | 508/585 [04:11<00:22,  3.40it/s] 87%|████████▋ | 509/585 [04:11<00:22,  3.42it/s] 87%|████████▋ | 510/585 [04:12<00:21,  3.43it/s] 87%|████████▋ | 511/585 [04:12<00:21,  3.44it/s] 88%|████████▊ | 512/585 [04:12<00:22,  3.31it/s] 88%|████████▊ | 513/585 [04:13<00:21,  3.36it/s] 88%|████████▊ | 514/585 [04:13<00:20,  3.39it/s] 88%|████████▊ | 515/585 [04:13<00:20,  3.41it/s] 88%|████████▊ | 516/585 [04:13<00:20,  3.43it/s] 88%|████████▊ | 517/585 [04:14<00:19,  3.43it/s] 89%|████████▊ | 518/585 [04:14<00:19,  3.44it/s] 89%|████████▊ | 519/585 [04:14<00:19,  3.45it/s] 89%|████████▉ | 520/585 [04:15<00:18,  3.46it/s] 89%|████████▉ | 521/585 [04:15<00:18,  3.46it/s] 89%|████████▉ | 522/585 [04:15<00:18,  3.46it/s] 89%|████████▉ | 523/585 [04:15<00:18,  3.42it/s] 90%|████████▉ | 524/585 [04:16<00:17,  3.43it/s] 90%|████████▉ | 525/585 [04:16<00:17,  3.44it/s] 90%|████████▉ | 526/585 [04:16<00:17,  3.45it/s] 90%|█████████ | 527/585 [04:17<00:16,  3.45it/s] 90%|█████████ | 528/585 [04:17<00:16,  3.45it/s] 90%|█████████ | 529/585 [04:17<00:16,  3.46it/s] 91%|█████████ | 530/585 [04:17<00:15,  3.46it/s] 91%|█████████ | 531/585 [04:18<00:15,  3.46it/s] 91%|█████████ | 532/585 [04:18<00:15,  3.46it/s] 91%|█████████ | 533/585 [04:18<00:15,  3.46it/s] 91%|█████████▏| 534/585 [04:19<00:15,  3.34it/s] 91%|█████████▏| 535/585 [04:19<00:14,  3.38it/s] 92%|█████████▏| 536/585 [04:19<00:14,  3.40it/s] 92%|█████████▏| 537/585 [04:20<00:14,  3.42it/s] 92%|█████████▏| 538/585 [04:20<00:13,  3.43it/s] 92%|█████████▏| 539/585 [04:20<00:13,  3.44it/s] 92%|█████████▏| 540/585 [04:20<00:13,  3.44it/s] 92%|█████████▏| 541/585 [04:21<00:12,  3.45it/s] 93%|█████████▎| 542/585 [04:21<00:12,  3.45it/s] 93%|█████████▎| 543/585 [04:21<00:12,  3.46it/s] 93%|█████████▎| 544/585 [04:22<00:11,  3.46it/s] 93%|█████████▎| 545/585 [04:22<00:11,  3.42it/s] 93%|█████████▎| 546/585 [04:22<00:11,  3.43it/s] 94%|█████████▎| 547/585 [04:22<00:11,  3.44it/s] 94%|█████████▎| 548/585 [04:23<00:10,  3.45it/s] 94%|█████████▍| 549/585 [04:23<00:10,  3.45it/s] 94%|█████████▍| 550/585 [04:23<00:10,  3.46it/s] 94%|█████████▍| 551/585 [04:24<00:09,  3.46it/s] 94%|█████████▍| 552/585 [04:24<00:09,  3.46it/s] 95%|█████████▍| 553/585 [04:24<00:09,  3.46it/s] 95%|█████████▍| 554/585 [04:24<00:08,  3.46it/s] 95%|█████████▍| 555/585 [04:25<00:08,  3.46it/s] 95%|█████████▌| 556/585 [04:25<00:08,  3.45it/s] 95%|█████████▌| 557/585 [04:25<00:08,  3.45it/s] 95%|█████████▌| 558/585 [04:26<00:07,  3.45it/s] 96%|█████████▌| 559/585 [04:26<00:07,  3.45it/s] 96%|█████████▌| 560/585 [04:26<00:07,  3.46it/s] 96%|█████████▌| 561/585 [04:26<00:06,  3.46it/s] 96%|█████████▌| 562/585 [04:27<00:06,  3.46it/s] 96%|█████████▌| 563/585 [04:27<00:06,  3.46it/s] 96%|█████████▋| 564/585 [04:27<00:06,  3.46it/s] 97%|█████████▋| 565/585 [04:28<00:05,  3.41it/s] 97%|█████████▋| 566/585 [04:28<00:05,  3.43it/s] 97%|█████████▋| 567/585 [04:28<00:05,  3.44it/s] 97%|█████████▋| 568/585 [04:29<00:04,  3.44it/s] 97%|█████████▋| 569/585 [04:29<00:04,  3.45it/s] 97%|█████████▋| 570/585 [04:29<00:04,  3.45it/s] 98%|█████████▊| 571/585 [04:29<00:04,  3.46it/s] 98%|█████████▊| 572/585 [04:30<00:03,  3.46it/s] 98%|█████████▊| 573/585 [04:30<00:03,  3.46it/s] 98%|█████████▊| 574/585 [04:30<00:03,  3.46it/s] 98%|█████████▊| 575/585 [04:31<00:02,  3.46it/s] 98%|█████████▊| 576/585 [04:31<00:02,  3.41it/s] 99%|█████████▊| 577/585 [04:31<00:02,  3.43it/s] 99%|█████████▉| 578/585 [04:31<00:02,  3.44it/s] 99%|█████████▉| 579/585 [04:32<00:01,  3.45it/s] 99%|█████████▉| 580/585 [04:32<00:01,  3.45it/s] 99%|█████████▉| 581/585 [04:32<00:01,  3.46it/s] 99%|█████████▉| 582/585 [04:33<00:00,  3.46it/s]100%|█████████▉| 583/585 [04:33<00:00,  3.46it/s]100%|█████████▉| 584/585 [04:33<00:00,  3.46it/s]100%|██████████| 585/585 [04:33<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 21:34:01,980 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:34:01,980 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:34:01,981 >>   Batch size = 8
{'eval_loss': 1.0891169309616089, 'eval_runtime': 9.3681, 'eval_samples_per_second': 371.367, 'eval_steps_per_second': 46.434, 'epoch': 4.0}
{'loss': 0.4007, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.13it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.40it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.63it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.99it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.66it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.38it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.90it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.71it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.59it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.59it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.65it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.68it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.68it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.72it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.70it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.48it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.43it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 46.44it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 43.52it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 44.46it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.13it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 45.59it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 45.94it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.20it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.30it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.36it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.24it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.30it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.43it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.50it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.55it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.65it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.64it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.68it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.60it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.41it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.42it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.45it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 45.26it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 45.71it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.03it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.23it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.38it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.54it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.44it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.48it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.38it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.33it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.45it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.56it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.55it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.63it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.69it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.59it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.58it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.45it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.37it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.41it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.46it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.50it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.55it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.60it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.58it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.51it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.40it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.46it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.39it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.48it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.50it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.54it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.61it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.59it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.60it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.52it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.38it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.42it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.39it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.48it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.58it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.57it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.55it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.61it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.49it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.44it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.45it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 41.04it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:43<00:00,  3.46it/s]
100%|██████████| 435/435 [00:09<00:00, 41.04it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:34:11,431 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 21:34:11,499 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:34:18,253 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:34:18,315 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:34:18,331 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:34:37,242 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:34:37,246 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117 (score: 1.0437591075897217).
                                                 100%|██████████| 585/585 [05:21<00:00,  3.46it/s]100%|██████████| 585/585 [05:21<00:00,  1.82it/s]
[INFO|trainer.py:1894] 2023-08-28 21:34:50,021 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 21:34:50,075 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:34:55,016 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:34:55,032 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:34:55,046 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:34:55,694 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:55,694 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:55,694 >>   train_loss               =     0.3981
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:55,694 >>   train_runtime            = 0:05:21.96
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:55,694 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:55,694 >>   train_samples_per_second =    116.472
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:34:55,694 >>   train_steps_per_second   =      1.817
{'eval_loss': 1.093494176864624, 'eval_runtime': 9.4272, 'eval_samples_per_second': 369.038, 'eval_steps_per_second': 46.143, 'epoch': 5.0}
{'train_runtime': 321.9667, 'train_samples_per_second': 116.472, 'train_steps_per_second': 1.817, 'train_loss': 0.39809115157168135, 'epoch': 5.0}
08/28/2023 21:34:55 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:34:55,817 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:34:55,817 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:34:55,817 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 58.75it/s]  3%|▎         | 12/435 [00:00<00:08, 51.42it/s]  4%|▍         | 18/435 [00:00<00:08, 49.53it/s]  5%|▌         | 23/435 [00:00<00:08, 48.72it/s]  6%|▋         | 28/435 [00:00<00:08, 48.20it/s]  8%|▊         | 33/435 [00:00<00:08, 47.90it/s]  9%|▊         | 38/435 [00:00<00:08, 47.71it/s] 10%|▉         | 43/435 [00:00<00:08, 47.45it/s] 11%|█         | 48/435 [00:00<00:08, 47.00it/s] 12%|█▏        | 53/435 [00:01<00:08, 47.03it/s] 13%|█▎        | 58/435 [00:01<00:08, 47.03it/s] 14%|█▍        | 63/435 [00:01<00:07, 47.07it/s] 16%|█▌        | 68/435 [00:01<00:07, 47.14it/s] 17%|█▋        | 73/435 [00:01<00:07, 47.18it/s] 18%|█▊        | 78/435 [00:01<00:07, 47.14it/s] 19%|█▉        | 83/435 [00:01<00:07, 47.20it/s] 20%|██        | 88/435 [00:01<00:07, 47.11it/s] 21%|██▏       | 93/435 [00:01<00:07, 46.95it/s] 23%|██▎       | 98/435 [00:02<00:07, 46.97it/s] 24%|██▎       | 103/435 [00:02<00:07, 46.99it/s] 25%|██▍       | 108/435 [00:02<00:06, 46.99it/s] 26%|██▌       | 113/435 [00:02<00:06, 47.07it/s] 27%|██▋       | 118/435 [00:02<00:06, 47.12it/s] 28%|██▊       | 123/435 [00:02<00:06, 47.15it/s] 29%|██▉       | 128/435 [00:02<00:06, 47.23it/s] 31%|███       | 133/435 [00:02<00:06, 47.26it/s] 32%|███▏      | 138/435 [00:02<00:06, 47.19it/s] 33%|███▎      | 143/435 [00:03<00:06, 47.13it/s] 34%|███▍      | 148/435 [00:03<00:06, 47.10it/s] 35%|███▌      | 153/435 [00:03<00:06, 46.93it/s] 36%|███▋      | 158/435 [00:03<00:05, 47.06it/s] 37%|███▋      | 163/435 [00:03<00:05, 47.11it/s] 39%|███▊      | 168/435 [00:03<00:05, 47.08it/s] 40%|███▉      | 173/435 [00:03<00:05, 47.08it/s] 41%|████      | 178/435 [00:03<00:05, 47.06it/s] 42%|████▏     | 183/435 [00:03<00:05, 47.08it/s] 43%|████▎     | 188/435 [00:03<00:05, 47.10it/s] 44%|████▍     | 193/435 [00:04<00:05, 47.06it/s] 46%|████▌     | 198/435 [00:04<00:05, 47.08it/s] 47%|████▋     | 203/435 [00:04<00:04, 47.06it/s] 48%|████▊     | 208/435 [00:04<00:04, 47.04it/s] 49%|████▉     | 213/435 [00:04<00:04, 47.14it/s] 50%|█████     | 218/435 [00:04<00:04, 47.16it/s] 51%|█████▏    | 223/435 [00:04<00:04, 47.08it/s] 52%|█████▏    | 228/435 [00:04<00:04, 47.14it/s] 54%|█████▎    | 233/435 [00:04<00:04, 47.09it/s] 55%|█████▍    | 238/435 [00:05<00:04, 46.98it/s] 56%|█████▌    | 243/435 [00:05<00:04, 47.06it/s] 57%|█████▋    | 248/435 [00:05<00:03, 47.04it/s] 58%|█████▊    | 253/435 [00:05<00:03, 47.01it/s] 59%|█████▉    | 258/435 [00:05<00:03, 45.11it/s] 60%|██████    | 263/435 [00:05<00:03, 45.75it/s] 62%|██████▏   | 268/435 [00:05<00:03, 46.18it/s] 63%|██████▎   | 273/435 [00:05<00:03, 46.52it/s] 64%|██████▍   | 278/435 [00:05<00:03, 46.73it/s] 65%|██████▌   | 283/435 [00:05<00:03, 46.88it/s] 66%|██████▌   | 288/435 [00:06<00:03, 46.95it/s] 67%|██████▋   | 293/435 [00:06<00:03, 46.86it/s] 69%|██████▊   | 298/435 [00:06<00:02, 46.79it/s] 70%|██████▉   | 303/435 [00:06<00:02, 46.70it/s] 71%|███████   | 308/435 [00:06<00:02, 46.82it/s] 72%|███████▏  | 313/435 [00:06<00:02, 46.86it/s] 73%|███████▎  | 318/435 [00:06<00:02, 46.93it/s] 74%|███████▍  | 323/435 [00:06<00:02, 47.04it/s] 75%|███████▌  | 328/435 [00:06<00:02, 47.10it/s] 77%|███████▋  | 333/435 [00:07<00:02, 47.12it/s] 78%|███████▊  | 338/435 [00:07<00:02, 47.05it/s] 79%|███████▉  | 343/435 [00:07<00:01, 47.03it/s] 80%|████████  | 348/435 [00:07<00:01, 46.88it/s] 81%|████████  | 353/435 [00:07<00:01, 46.80it/s] 82%|████████▏ | 358/435 [00:07<00:01, 46.97it/s] 83%|████████▎ | 363/435 [00:07<00:01, 47.10it/s] 85%|████████▍ | 368/435 [00:07<00:01, 47.03it/s] 86%|████████▌ | 373/435 [00:07<00:01, 47.12it/s] 87%|████████▋ | 378/435 [00:08<00:01, 47.18it/s] 88%|████████▊ | 383/435 [00:08<00:01, 47.08it/s] 89%|████████▉ | 388/435 [00:08<00:00, 47.05it/s] 90%|█████████ | 393/435 [00:08<00:00, 46.99it/s] 91%|█████████▏| 398/435 [00:08<00:00, 46.82it/s] 93%|█████████▎| 403/435 [00:08<00:00, 45.74it/s] 94%|█████████▍| 408/435 [00:08<00:00, 46.20it/s] 95%|█████████▍| 413/435 [00:08<00:00, 46.45it/s] 96%|█████████▌| 418/435 [00:08<00:00, 46.75it/s] 97%|█████████▋| 423/435 [00:08<00:00, 46.90it/s] 98%|█████████▊| 428/435 [00:09<00:00, 46.92it/s]100%|█████████▉| 433/435 [00:09<00:00, 46.99it/s]100%|██████████| 435/435 [00:09<00:00, 47.03it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:35:05,088 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:05,088 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:05,088 >>   eval_loss               =     1.0438
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:05,088 >>   eval_runtime            = 0:00:09.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:05,088 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:05,088 >>   eval_samples_per_second =    375.267
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:05,088 >>   eval_steps_per_second   =     46.922
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:35:05,088 >>   perplexity              =     2.8399
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:24,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:24,791 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:24,791 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:24,791 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:24,791 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:35:25,119 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:35:25,120 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:35:25,413 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:35:29,182 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:35:29,198 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:31,391 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:31,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:31,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:31,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:31,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:35:31,769 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:35:31,771 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:35:32,102 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:35:32,612 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:35:32,615 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 4it [00:02,  1.35it/s]Extractor Predicting: 5it [00:03,  1.36it/s]Extractor Predicting: 6it [00:04,  1.35it/s]Extractor Predicting: 7it [00:05,  1.36it/s]Extractor Predicting: 8it [00:05,  1.38it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:07,  1.43it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:09,  1.37it/s]Extractor Predicting: 14it [00:10,  1.37it/s]Extractor Predicting: 15it [00:10,  1.42it/s]Extractor Predicting: 16it [00:11,  1.39it/s]Extractor Predicting: 17it [00:12,  1.42it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:13,  1.42it/s]Extractor Predicting: 20it [00:14,  1.42it/s]Extractor Predicting: 21it [00:15,  1.42it/s]Extractor Predicting: 22it [00:15,  1.45it/s]Extractor Predicting: 23it [00:16,  1.44it/s]Extractor Predicting: 24it [00:17,  1.42it/s]Extractor Predicting: 25it [00:17,  1.40it/s]Extractor Predicting: 26it [00:18,  1.38it/s]Extractor Predicting: 27it [00:19,  1.39it/s]Extractor Predicting: 28it [00:20,  1.42it/s]Extractor Predicting: 29it [00:20,  1.40it/s]Extractor Predicting: 30it [00:21,  1.38it/s]Extractor Predicting: 31it [00:22,  1.39it/s]Extractor Predicting: 32it [00:22,  1.39it/s]Extractor Predicting: 33it [00:23,  1.39it/s]Extractor Predicting: 34it [00:24,  1.42it/s]Extractor Predicting: 35it [00:24,  1.43it/s]Extractor Predicting: 36it [00:25,  1.44it/s]Extractor Predicting: 37it [00:26,  1.45it/s]Extractor Predicting: 38it [00:27,  1.44it/s]Extractor Predicting: 39it [00:27,  1.46it/s]Extractor Predicting: 40it [00:28,  1.44it/s]Extractor Predicting: 41it [00:29,  1.45it/s]Extractor Predicting: 42it [00:29,  1.45it/s]Extractor Predicting: 43it [00:30,  1.46it/s]Extractor Predicting: 44it [00:31,  1.48it/s]Extractor Predicting: 45it [00:31,  1.46it/s]Extractor Predicting: 46it [00:32,  1.45it/s]Extractor Predicting: 47it [00:33,  1.44it/s]Extractor Predicting: 48it [00:33,  1.45it/s]Extractor Predicting: 49it [00:34,  1.45it/s]Extractor Predicting: 50it [00:35,  1.45it/s]Extractor Predicting: 51it [00:35,  1.47it/s]Extractor Predicting: 52it [00:36,  1.47it/s]Extractor Predicting: 53it [00:37,  1.47it/s]Extractor Predicting: 54it [00:38,  1.46it/s]Extractor Predicting: 55it [00:38,  1.42it/s]Extractor Predicting: 56it [00:39,  1.45it/s]Extractor Predicting: 57it [00:40,  1.44it/s]Extractor Predicting: 58it [00:40,  1.43it/s]Extractor Predicting: 59it [00:41,  1.47it/s]Extractor Predicting: 60it [00:42,  1.50it/s]Extractor Predicting: 61it [00:42,  1.51it/s]Extractor Predicting: 62it [00:43,  1.48it/s]Extractor Predicting: 63it [00:44,  1.46it/s]Extractor Predicting: 64it [00:44,  1.47it/s]Extractor Predicting: 65it [00:45,  1.49it/s]Extractor Predicting: 66it [00:46,  1.47it/s]Extractor Predicting: 67it [00:46,  1.48it/s]Extractor Predicting: 68it [00:47,  1.48it/s]Extractor Predicting: 69it [00:48,  1.53it/s]Extractor Predicting: 70it [00:48,  1.52it/s]Extractor Predicting: 71it [00:49,  1.53it/s]Extractor Predicting: 72it [00:50,  1.49it/s]Extractor Predicting: 73it [00:50,  1.50it/s]Extractor Predicting: 74it [00:51,  1.49it/s]Extractor Predicting: 75it [00:52,  1.46it/s]Extractor Predicting: 76it [00:52,  1.45it/s]Extractor Predicting: 77it [00:53,  1.47it/s]Extractor Predicting: 78it [00:54,  1.49it/s]Extractor Predicting: 79it [00:55,  1.41it/s]Extractor Predicting: 80it [00:55,  1.42it/s]Extractor Predicting: 81it [00:56,  1.42it/s]Extractor Predicting: 82it [00:57,  1.43it/s]Extractor Predicting: 83it [00:57,  1.46it/s]Extractor Predicting: 84it [00:58,  1.47it/s]Extractor Predicting: 85it [00:59,  1.48it/s]Extractor Predicting: 86it [00:59,  1.42it/s]Extractor Predicting: 87it [01:00,  1.47it/s]Extractor Predicting: 88it [01:01,  1.47it/s]Extractor Predicting: 89it [01:01,  1.48it/s]Extractor Predicting: 90it [01:02,  1.50it/s]Extractor Predicting: 91it [01:03,  1.48it/s]Extractor Predicting: 92it [01:03,  1.54it/s]Extractor Predicting: 93it [01:04,  1.52it/s]Extractor Predicting: 94it [01:05,  1.52it/s]Extractor Predicting: 95it [01:05,  1.53it/s]Extractor Predicting: 96it [01:06,  1.45it/s]Extractor Predicting: 97it [01:07,  1.47it/s]Extractor Predicting: 98it [01:07,  1.48it/s]Extractor Predicting: 99it [01:08,  1.48it/s]Extractor Predicting: 100it [01:09,  1.44it/s]Extractor Predicting: 101it [01:09,  1.45it/s]Extractor Predicting: 102it [01:10,  1.53it/s]Extractor Predicting: 103it [01:11,  1.54it/s]Extractor Predicting: 104it [01:11,  1.53it/s]Extractor Predicting: 105it [01:12,  1.53it/s]Extractor Predicting: 106it [01:13,  1.53it/s]Extractor Predicting: 107it [01:13,  1.52it/s]Extractor Predicting: 108it [01:14,  1.52it/s]Extractor Predicting: 109it [01:15,  1.53it/s]Extractor Predicting: 110it [01:15,  1.53it/s]Extractor Predicting: 111it [01:16,  1.54it/s]Extractor Predicting: 112it [01:17,  1.54it/s]Extractor Predicting: 113it [01:17,  1.58it/s]Extractor Predicting: 114it [01:18,  1.57it/s]Extractor Predicting: 115it [01:18,  1.59it/s]Extractor Predicting: 116it [01:19,  1.56it/s]Extractor Predicting: 117it [01:20,  1.52it/s]Extractor Predicting: 118it [01:20,  1.52it/s]Extractor Predicting: 119it [01:21,  1.49it/s]Extractor Predicting: 120it [01:22,  1.50it/s]Extractor Predicting: 121it [01:22,  1.49it/s]Extractor Predicting: 122it [01:23,  1.45it/s]Extractor Predicting: 123it [01:24,  1.47it/s]Extractor Predicting: 124it [01:24,  1.48it/s]Extractor Predicting: 125it [01:25,  1.51it/s]Extractor Predicting: 126it [01:26,  1.48it/s]Extractor Predicting: 127it [01:26,  1.50it/s]Extractor Predicting: 128it [01:27,  1.50it/s]Extractor Predicting: 129it [01:28,  1.49it/s]Extractor Predicting: 130it [01:29,  1.47it/s]Extractor Predicting: 131it [01:29,  1.50it/s]Extractor Predicting: 132it [01:30,  1.47it/s]Extractor Predicting: 133it [01:31,  1.46it/s]Extractor Predicting: 134it [01:31,  1.44it/s]Extractor Predicting: 135it [01:32,  1.47it/s]Extractor Predicting: 136it [01:33,  1.45it/s]Extractor Predicting: 137it [01:33,  1.46it/s]Extractor Predicting: 138it [01:34,  1.43it/s]Extractor Predicting: 139it [01:35,  1.44it/s]Extractor Predicting: 140it [01:35,  1.43it/s]Extractor Predicting: 141it [01:36,  1.46it/s]Extractor Predicting: 142it [01:37,  1.43it/s]Extractor Predicting: 143it [01:37,  1.45it/s]Extractor Predicting: 144it [01:38,  1.78it/s]Extractor Predicting: 144it [01:38,  1.47it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:26,523 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:26,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:26,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:26,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:26,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:37:26,879 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:37:26,880 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:37:27,158 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:28,253 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:28,253 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:30,123 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:30,235 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:30,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:30,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:30,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:37:31,592 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:37:31,593 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:37:32,113 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:32,578 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:32,578 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.19572192513368983,
  "recall": 0.052601322219028454,
  "score": 0.08291798821930221,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.49it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:24,  1.54it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:27,  1.57it/s]Extractor Predicting: 43it [00:28,  1.53it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:30,  1.47it/s]Extractor Predicting: 47it [00:30,  1.51it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:32,  1.54it/s]Extractor Predicting: 50it [00:32,  1.52it/s]Extractor Predicting: 51it [00:33,  1.51it/s]Extractor Predicting: 52it [00:34,  1.50it/s]Extractor Predicting: 53it [00:34,  1.51it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.50it/s]Extractor Predicting: 58it [00:38,  1.50it/s]Extractor Predicting: 59it [00:38,  1.49it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:40,  1.44it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:42,  1.53it/s]Extractor Predicting: 65it [00:42,  1.50it/s]Extractor Predicting: 66it [00:43,  1.48it/s]Extractor Predicting: 67it [00:44,  1.48it/s]Extractor Predicting: 68it [00:44,  1.48it/s]Extractor Predicting: 69it [00:45,  1.45it/s]Extractor Predicting: 70it [00:46,  1.47it/s]Extractor Predicting: 71it [00:46,  1.47it/s]Extractor Predicting: 72it [00:47,  1.46it/s]Extractor Predicting: 73it [00:48,  1.48it/s]Extractor Predicting: 74it [00:48,  1.48it/s]Extractor Predicting: 75it [00:49,  1.50it/s]Extractor Predicting: 76it [00:50,  1.47it/s]Extractor Predicting: 77it [00:50,  1.51it/s]Extractor Predicting: 78it [00:51,  1.51it/s]Extractor Predicting: 79it [00:52,  1.54it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:53,  1.55it/s]Extractor Predicting: 82it [00:54,  1.55it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:55,  1.35it/s]Extractor Predicting: 85it [00:56,  1.37it/s]Extractor Predicting: 86it [00:57,  1.37it/s]Extractor Predicting: 87it [00:57,  1.40it/s]Extractor Predicting: 88it [00:58,  1.40it/s]Extractor Predicting: 89it [00:59,  1.39it/s]Extractor Predicting: 90it [00:59,  1.40it/s]Extractor Predicting: 91it [01:00,  1.39it/s]Extractor Predicting: 92it [01:01,  1.43it/s]Extractor Predicting: 93it [01:02,  1.40it/s]Extractor Predicting: 94it [01:02,  1.43it/s]Extractor Predicting: 95it [01:03,  1.45it/s]Extractor Predicting: 96it [01:04,  1.42it/s]Extractor Predicting: 97it [01:04,  1.41it/s]Extractor Predicting: 98it [01:05,  1.26it/s]Extractor Predicting: 99it [01:06,  1.32it/s]Extractor Predicting: 100it [01:07,  1.37it/s]Extractor Predicting: 101it [01:07,  1.42it/s]Extractor Predicting: 102it [01:08,  1.41it/s]Extractor Predicting: 103it [01:09,  1.37it/s]Extractor Predicting: 104it [01:09,  1.42it/s]Extractor Predicting: 105it [01:10,  1.42it/s]Extractor Predicting: 106it [01:11,  1.44it/s]Extractor Predicting: 107it [01:12,  1.47it/s]Extractor Predicting: 108it [01:12,  1.44it/s]Extractor Predicting: 109it [01:13,  1.44it/s]Extractor Predicting: 110it [01:14,  1.44it/s]Extractor Predicting: 111it [01:14,  1.46it/s]Extractor Predicting: 112it [01:15,  1.45it/s]Extractor Predicting: 113it [01:16,  1.40it/s]Extractor Predicting: 114it [01:16,  1.43it/s]Extractor Predicting: 115it [01:17,  1.40it/s]Extractor Predicting: 116it [01:18,  1.44it/s]Extractor Predicting: 117it [01:19,  1.44it/s]Extractor Predicting: 118it [01:19,  1.43it/s]Extractor Predicting: 119it [01:20,  1.44it/s]Extractor Predicting: 120it [01:21,  1.47it/s]Extractor Predicting: 121it [01:21,  1.49it/s]Extractor Predicting: 122it [01:22,  1.50it/s]Extractor Predicting: 123it [01:23,  1.49it/s]Extractor Predicting: 124it [01:23,  1.51it/s]Extractor Predicting: 125it [01:24,  1.53it/s]Extractor Predicting: 126it [01:24,  1.53it/s]Extractor Predicting: 127it [01:25,  1.52it/s]Extractor Predicting: 128it [01:26,  1.54it/s]Extractor Predicting: 129it [01:26,  1.55it/s]Extractor Predicting: 130it [01:27,  1.51it/s]Extractor Predicting: 131it [01:28,  1.55it/s]Extractor Predicting: 132it [01:28,  1.55it/s]Extractor Predicting: 133it [01:29,  1.57it/s]Extractor Predicting: 134it [01:30,  1.52it/s]Extractor Predicting: 135it [01:30,  1.55it/s]Extractor Predicting: 136it [01:31,  1.56it/s]Extractor Predicting: 137it [01:32,  1.58it/s]Extractor Predicting: 138it [01:32,  1.56it/s]Extractor Predicting: 139it [01:33,  1.55it/s]Extractor Predicting: 140it [01:33,  1.56it/s]Extractor Predicting: 141it [01:34,  1.53it/s]Extractor Predicting: 142it [01:35,  1.52it/s]Extractor Predicting: 143it [01:35,  1.54it/s]Extractor Predicting: 144it [01:36,  1.52it/s]Extractor Predicting: 145it [01:37,  1.57it/s]Extractor Predicting: 146it [01:37,  1.59it/s]Extractor Predicting: 147it [01:38,  1.59it/s]Extractor Predicting: 148it [01:39,  1.62it/s]Extractor Predicting: 149it [01:39,  1.61it/s]Extractor Predicting: 150it [01:40,  1.62it/s]Extractor Predicting: 151it [01:40,  1.63it/s]Extractor Predicting: 152it [01:41,  1.65it/s]Extractor Predicting: 153it [01:42,  1.61it/s]Extractor Predicting: 154it [01:42,  1.61it/s]Extractor Predicting: 155it [01:43,  1.66it/s]Extractor Predicting: 156it [01:43,  1.65it/s]Extractor Predicting: 157it [01:44,  1.72it/s]Extractor Predicting: 158it [01:45,  1.74it/s]Extractor Predicting: 159it [01:45,  1.67it/s]Extractor Predicting: 160it [01:46,  1.63it/s]Extractor Predicting: 161it [01:46,  1.61it/s]Extractor Predicting: 162it [01:47,  1.64it/s]Extractor Predicting: 163it [01:48,  1.67it/s]Extractor Predicting: 164it [01:48,  1.66it/s]Extractor Predicting: 165it [01:49,  1.67it/s]Extractor Predicting: 166it [01:49,  1.63it/s]Extractor Predicting: 167it [01:50,  1.66it/s]Extractor Predicting: 168it [01:51,  1.64it/s]Extractor Predicting: 169it [01:51,  1.66it/s]Extractor Predicting: 170it [01:52,  1.65it/s]Extractor Predicting: 171it [01:52,  1.66it/s]Extractor Predicting: 172it [01:53,  1.60it/s]Extractor Predicting: 173it [01:54,  1.58it/s]Extractor Predicting: 174it [01:54,  1.54it/s]Extractor Predicting: 175it [01:55,  1.49it/s]Extractor Predicting: 176it [01:56,  1.50it/s]Extractor Predicting: 177it [01:57,  1.50it/s]Extractor Predicting: 178it [01:57,  1.48it/s]Extractor Predicting: 179it [01:58,  1.48it/s]Extractor Predicting: 180it [01:59,  1.49it/s]Extractor Predicting: 181it [01:59,  1.49it/s]Extractor Predicting: 182it [02:00,  1.47it/s]Extractor Predicting: 183it [02:01,  1.48it/s]Extractor Predicting: 184it [02:01,  1.46it/s]Extractor Predicting: 185it [02:02,  1.45it/s]Extractor Predicting: 186it [02:03,  1.32it/s]Extractor Predicting: 187it [02:04,  1.35it/s]Extractor Predicting: 188it [02:04,  1.38it/s]Extractor Predicting: 189it [02:05,  1.40it/s]Extractor Predicting: 190it [02:06,  1.43it/s]Extractor Predicting: 191it [02:06,  1.41it/s]Extractor Predicting: 192it [02:07,  1.39it/s]Extractor Predicting: 193it [02:08,  1.40it/s]Extractor Predicting: 194it [02:09,  1.41it/s]Extractor Predicting: 195it [02:09,  1.43it/s]Extractor Predicting: 196it [02:10,  1.44it/s]Extractor Predicting: 197it [02:11,  1.43it/s]Extractor Predicting: 198it [02:11,  1.45it/s]Extractor Predicting: 199it [02:12,  1.47it/s]Extractor Predicting: 200it [02:13,  1.44it/s]Extractor Predicting: 201it [02:13,  1.42it/s]Extractor Predicting: 202it [02:14,  1.49it/s]Extractor Predicting: 203it [02:15,  1.48it/s]Extractor Predicting: 204it [02:15,  1.50it/s]Extractor Predicting: 205it [02:16,  1.48it/s]Extractor Predicting: 206it [02:17,  1.48it/s]Extractor Predicting: 207it [02:17,  1.45it/s]Extractor Predicting: 208it [02:18,  1.46it/s]Extractor Predicting: 209it [02:19,  1.40it/s]Extractor Predicting: 210it [02:20,  1.42it/s]Extractor Predicting: 211it [02:20,  1.42it/s]Extractor Predicting: 212it [02:21,  1.41it/s]Extractor Predicting: 213it [02:22,  1.42it/s]Extractor Predicting: 214it [02:22,  1.40it/s]Extractor Predicting: 215it [02:23,  1.44it/s]Extractor Predicting: 216it [02:24,  1.41it/s]Extractor Predicting: 217it [02:24,  1.43it/s]Extractor Predicting: 218it [02:25,  1.44it/s]Extractor Predicting: 219it [02:26,  1.43it/s]Extractor Predicting: 220it [02:27,  1.44it/s]Extractor Predicting: 221it [02:27,  1.40it/s]Extractor Predicting: 222it [02:28,  1.40it/s]Extractor Predicting: 223it [02:29,  1.44it/s]Extractor Predicting: 224it [02:29,  1.44it/s]Extractor Predicting: 225it [02:30,  1.45it/s]Extractor Predicting: 226it [02:31,  1.44it/s]Extractor Predicting: 227it [02:31,  1.49it/s]Extractor Predicting: 228it [02:32,  1.47it/s]Extractor Predicting: 229it [02:33,  1.47it/s]Extractor Predicting: 230it [02:33,  1.52it/s]Extractor Predicting: 231it [02:34,  1.53it/s]Extractor Predicting: 232it [02:35,  1.55it/s]Extractor Predicting: 233it [02:35,  1.52it/s]Extractor Predicting: 234it [02:36,  1.53it/s]Extractor Predicting: 235it [02:37,  1.52it/s]Extractor Predicting: 236it [02:37,  1.49it/s]Extractor Predicting: 237it [02:38,  1.47it/s]Extractor Predicting: 238it [02:39,  1.49it/s]Extractor Predicting: 239it [02:39,  1.47it/s]Extractor Predicting: 240it [02:40,  1.48it/s]Extractor Predicting: 241it [02:41,  1.42it/s]Extractor Predicting: 242it [02:41,  1.46it/s]Extractor Predicting: 243it [02:42,  1.45it/s]Extractor Predicting: 244it [02:43,  1.45it/s]Extractor Predicting: 245it [02:44,  1.48it/s]Extractor Predicting: 246it [02:44,  1.47it/s]Extractor Predicting: 247it [02:45,  1.48it/s]Extractor Predicting: 248it [02:46,  1.48it/s]Extractor Predicting: 249it [02:46,  1.46it/s]Extractor Predicting: 250it [02:47,  1.48it/s]Extractor Predicting: 251it [02:48,  1.49it/s]Extractor Predicting: 252it [02:48,  1.49it/s]Extractor Predicting: 253it [02:49,  1.47it/s]Extractor Predicting: 254it [02:50,  1.49it/s]Extractor Predicting: 255it [02:50,  1.45it/s]Extractor Predicting: 256it [02:51,  1.41it/s]Extractor Predicting: 257it [02:52,  1.42it/s]Extractor Predicting: 258it [02:52,  1.41it/s]Extractor Predicting: 259it [02:53,  1.43it/s]Extractor Predicting: 260it [02:54,  1.42it/s]Extractor Predicting: 261it [02:55,  1.44it/s]Extractor Predicting: 262it [02:55,  1.43it/s]Extractor Predicting: 263it [02:56,  1.42it/s]Extractor Predicting: 264it [02:57,  1.42it/s]Extractor Predicting: 265it [02:57,  1.38it/s]Extractor Predicting: 266it [02:58,  1.38it/s]Extractor Predicting: 267it [02:59,  1.39it/s]Extractor Predicting: 268it [03:00,  1.37it/s]Extractor Predicting: 269it [03:00,  1.41it/s]Extractor Predicting: 270it [03:01,  1.42it/s]Extractor Predicting: 271it [03:02,  1.40it/s]Extractor Predicting: 272it [03:02,  1.41it/s]Extractor Predicting: 273it [03:03,  1.42it/s]Extractor Predicting: 274it [03:04,  1.44it/s]Extractor Predicting: 275it [03:04,  1.45it/s]Extractor Predicting: 276it [03:05,  1.49it/s]Extractor Predicting: 277it [03:06,  1.49it/s]Extractor Predicting: 278it [03:06,  1.48it/s]Extractor Predicting: 279it [03:07,  1.48it/s]Extractor Predicting: 280it [03:08,  1.42it/s]Extractor Predicting: 281it [03:09,  1.42it/s]Extractor Predicting: 282it [03:09,  1.51it/s]Extractor Predicting: 282it [03:09,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:40:50,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:40:50,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:40:50,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:40:50,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:40:50,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:40:51,747 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:40:51,748 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:40:52,435 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:40:53,585 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:40:53,585 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:40:55,785 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:40:55,787 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:40:55,787 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:40:55,787 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:40:55,788 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:40:56,140 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:40:56,142 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:40:56,415 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:40:56,644 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:40:56,644 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.1959841628959276,
  "recall": 0.10251479289940828,
  "score": 0.1346153846153846,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.40it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 5it [00:03,  1.42it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3114754098360656,
  "recall": 0.07916666666666666,
  "score": 0.12624584717607973,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:17, 17.30s/it]Extractor Predicting: 2it [00:18,  7.80s/it]Extractor Predicting: 3it [00:19,  4.53s/it]Extractor Predicting: 4it [00:19,  2.98s/it]Extractor Predicting: 5it [00:20,  2.14s/it]Extractor Predicting: 6it [00:21,  1.86s/it]Extractor Predicting: 7it [00:22,  1.46s/it]Extractor Predicting: 8it [00:22,  1.19s/it]Extractor Predicting: 9it [00:23,  1.00it/s]Extractor Predicting: 10it [00:24,  1.17it/s]Extractor Predicting: 11it [00:24,  1.27it/s]Extractor Predicting: 12it [00:25,  1.09it/s]Extractor Predicting: 13it [00:26,  1.22it/s]Extractor Predicting: 14it [00:27,  1.34it/s]Extractor Predicting: 15it [00:27,  1.41it/s]Extractor Predicting: 16it [00:28,  1.49it/s]Extractor Predicting: 17it [00:28,  1.54it/s]Extractor Predicting: 18it [00:29,  1.56it/s]Extractor Predicting: 19it [00:30,  1.63it/s]Extractor Predicting: 20it [00:30,  1.66it/s]Extractor Predicting: 21it [00:31,  1.68it/s]Extractor Predicting: 22it [00:31,  1.73it/s]Extractor Predicting: 23it [00:32,  1.74it/s]Extractor Predicting: 24it [00:32,  1.75it/s]Extractor Predicting: 25it [00:33,  1.72it/s]Extractor Predicting: 26it [00:34,  1.69it/s]Extractor Predicting: 27it [00:34,  1.62it/s]Extractor Predicting: 28it [00:35,  1.54it/s]Extractor Predicting: 29it [00:36,  1.58it/s]Extractor Predicting: 30it [00:36,  1.63it/s]Extractor Predicting: 31it [00:37,  1.66it/s]Extractor Predicting: 32it [00:37,  1.71it/s]Extractor Predicting: 33it [00:38,  1.70it/s]Extractor Predicting: 34it [00:38,  1.68it/s]Extractor Predicting: 35it [00:39,  1.69it/s]Extractor Predicting: 36it [00:40,  1.67it/s]Extractor Predicting: 37it [00:40,  1.71it/s]Extractor Predicting: 38it [00:41,  1.71it/s]Extractor Predicting: 39it [00:41,  1.68it/s]Extractor Predicting: 40it [00:42,  1.71it/s]Extractor Predicting: 41it [00:43,  1.68it/s]Extractor Predicting: 42it [00:43,  1.68it/s]Extractor Predicting: 43it [00:44,  1.68it/s]Extractor Predicting: 44it [00:44,  1.66it/s]Extractor Predicting: 45it [00:45,  1.70it/s]Extractor Predicting: 46it [00:46,  1.69it/s]Extractor Predicting: 47it [00:46,  1.69it/s]Extractor Predicting: 48it [00:47,  1.68it/s]Extractor Predicting: 49it [00:47,  1.68it/s]Extractor Predicting: 50it [00:48,  1.64it/s]Extractor Predicting: 51it [00:49,  1.65it/s]Extractor Predicting: 52it [00:49,  1.69it/s]Extractor Predicting: 53it [00:50,  1.62it/s]Extractor Predicting: 54it [00:50,  1.62it/s]Extractor Predicting: 55it [00:51,  1.57it/s]Extractor Predicting: 56it [00:52,  1.26it/s]Extractor Predicting: 57it [00:53,  1.34it/s]Extractor Predicting: 58it [00:54,  1.40it/s]Extractor Predicting: 59it [00:54,  1.47it/s]Extractor Predicting: 60it [00:55,  1.41it/s]Extractor Predicting: 61it [00:56,  1.45it/s]Extractor Predicting: 62it [00:56,  1.46it/s]Extractor Predicting: 63it [00:57,  1.49it/s]Extractor Predicting: 64it [00:58,  1.52it/s]Extractor Predicting: 65it [00:58,  1.51it/s]Extractor Predicting: 66it [00:59,  1.48it/s]Extractor Predicting: 67it [01:00,  1.50it/s]Extractor Predicting: 68it [01:00,  1.54it/s]Extractor Predicting: 69it [01:01,  1.53it/s]Extractor Predicting: 70it [01:01,  1.51it/s]Extractor Predicting: 71it [01:02,  1.53it/s]Extractor Predicting: 72it [01:03,  1.49it/s]Extractor Predicting: 73it [01:03,  1.53it/s]Extractor Predicting: 74it [01:04,  1.53it/s]Extractor Predicting: 75it [01:05,  1.54it/s]Extractor Predicting: 76it [01:05,  1.54it/s]Extractor Predicting: 77it [01:06,  1.54it/s]Extractor Predicting: 78it [01:07,  1.57it/s]Extractor Predicting: 79it [01:07,  1.59it/s]Extractor Predicting: 80it [01:08,  1.58it/s]Extractor Predicting: 81it [01:09,  1.58it/s]Extractor Predicting: 82it [01:09,  1.55it/s]Extractor Predicting: 83it [01:10,  1.56it/s]Extractor Predicting: 84it [01:11,  1.53it/s]Extractor Predicting: 85it [01:11,  1.54it/s]Extractor Predicting: 86it [01:12,  1.53it/s]Extractor Predicting: 87it [01:12,  1.53it/s]Extractor Predicting: 88it [01:13,  1.50it/s]Extractor Predicting: 89it [01:14,  1.51it/s]Extractor Predicting: 90it [01:15,  1.50it/s]Extractor Predicting: 91it [01:15,  1.51it/s]Extractor Predicting: 92it [01:16,  1.56it/s]Extractor Predicting: 93it [01:16,  1.63it/s]Extractor Predicting: 94it [01:17,  1.61it/s]Extractor Predicting: 95it [01:18,  1.61it/s]Extractor Predicting: 96it [01:18,  1.62it/s]Extractor Predicting: 97it [01:19,  1.64it/s]Extractor Predicting: 98it [01:19,  1.59it/s]Extractor Predicting: 99it [01:20,  1.52it/s]Extractor Predicting: 100it [01:21,  1.56it/s]Extractor Predicting: 101it [01:22,  1.15it/s]Extractor Predicting: 102it [01:23,  1.23it/s]Extractor Predicting: 103it [01:23,  1.31it/s]Extractor Predicting: 104it [01:24,  1.39it/s]Extractor Predicting: 105it [01:25,  1.44it/s]Extractor Predicting: 106it [01:25,  1.52it/s]Extractor Predicting: 107it [01:26,  1.54it/s]Extractor Predicting: 108it [01:27,  1.58it/s]Extractor Predicting: 109it [01:27,  1.59it/s]Extractor Predicting: 110it [01:28,  1.60it/s]Extractor Predicting: 111it [01:28,  1.63it/s]Extractor Predicting: 112it [01:29,  1.51it/s]Extractor Predicting: 113it [01:30,  1.50it/s]Extractor Predicting: 114it [01:30,  1.52it/s]Extractor Predicting: 115it [01:31,  1.54it/s]Extractor Predicting: 116it [01:32,  1.52it/s]Extractor Predicting: 117it [01:32,  1.51it/s]Extractor Predicting: 118it [01:33,  1.53it/s]Extractor Predicting: 119it [01:34,  1.51it/s]Extractor Predicting: 120it [01:34,  1.48it/s]Extractor Predicting: 121it [01:35,  1.50it/s]Extractor Predicting: 122it [01:36,  1.51it/s]Extractor Predicting: 123it [01:36,  1.53it/s]Extractor Predicting: 124it [01:37,  1.54it/s]Extractor Predicting: 125it [01:38,  1.56it/s]Extractor Predicting: 126it [01:38,  1.56it/s]Extractor Predicting: 127it [01:39,  1.55it/s]Extractor Predicting: 128it [01:40,  1.56it/s]Extractor Predicting: 129it [01:40,  1.58it/s]Extractor Predicting: 130it [01:41,  1.52it/s]Extractor Predicting: 131it [01:42,  1.51it/s]Extractor Predicting: 132it [01:42,  1.56it/s]Extractor Predicting: 133it [01:43,  1.54it/s]Extractor Predicting: 134it [01:44,  1.55it/s]Extractor Predicting: 135it [01:44,  1.54it/s]Extractor Predicting: 136it [01:45,  1.56it/s]Extractor Predicting: 137it [01:45,  1.53it/s]Extractor Predicting: 138it [01:46,  1.55it/s]Extractor Predicting: 139it [01:47,  1.51it/s]Extractor Predicting: 140it [01:47,  1.51it/s]Extractor Predicting: 141it [01:48,  1.53it/s]Extractor Predicting: 142it [01:49,  1.54it/s]Extractor Predicting: 143it [01:49,  1.51it/s]Extractor Predicting: 144it [01:50,  1.54it/s]Extractor Predicting: 145it [01:51,  1.58it/s]Extractor Predicting: 146it [01:51,  1.57it/s]Extractor Predicting: 147it [01:52,  1.54it/s]Extractor Predicting: 148it [01:53,  1.55it/s]Extractor Predicting: 149it [01:53,  1.52it/s]Extractor Predicting: 150it [01:54,  1.51it/s]Extractor Predicting: 151it [01:55,  1.51it/s]Extractor Predicting: 152it [01:55,  1.51it/s]Extractor Predicting: 153it [01:56,  1.51it/s]Extractor Predicting: 154it [01:57,  1.52it/s]Extractor Predicting: 155it [01:57,  1.52it/s]Extractor Predicting: 156it [01:58,  1.47it/s]Extractor Predicting: 157it [01:59,  1.44it/s]Extractor Predicting: 158it [01:59,  1.41it/s]Extractor Predicting: 159it [02:00,  1.45it/s]Extractor Predicting: 160it [02:01,  1.47it/s]Extractor Predicting: 161it [02:01,  1.49it/s]Extractor Predicting: 162it [02:02,  1.50it/s]Extractor Predicting: 163it [02:03,  1.50it/s]Extractor Predicting: 164it [02:03,  1.53it/s]Extractor Predicting: 165it [02:04,  1.52it/s]Extractor Predicting: 166it [02:05,  1.55it/s]Extractor Predicting: 167it [02:05,  1.54it/s]Extractor Predicting: 168it [02:06,  1.54it/s]Extractor Predicting: 169it [02:07,  1.55it/s]Extractor Predicting: 170it [02:07,  1.53it/s]Extractor Predicting: 171it [02:08,  1.56it/s]Extractor Predicting: 172it [02:08,  1.58it/s]Extractor Predicting: 173it [02:09,  1.54it/s]Extractor Predicting: 174it [02:10,  1.55it/s]Extractor Predicting: 175it [02:10,  1.53it/s]Extractor Predicting: 176it [02:11,  1.55it/s]Extractor Predicting: 177it [02:12,  1.52it/s]Extractor Predicting: 178it [02:13,  1.47it/s]Extractor Predicting: 179it [02:13,  1.49it/s]Extractor Predicting: 180it [02:14,  1.50it/s]Extractor Predicting: 181it [02:14,  1.52it/s]Extractor Predicting: 182it [02:15,  1.52it/s]Extractor Predicting: 183it [02:16,  1.48it/s]Extractor Predicting: 184it [02:17,  1.50it/s]Extractor Predicting: 185it [02:17,  1.59it/s]Extractor Predicting: 185it [02:17,  1.35it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4463336875664187,
  "recall": 0.08603031544448997,
  "score": 0.1442555383822772,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.59it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:11,  1.59it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:19,  1.71it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:20,  1.71it/s]Extractor Predicting: 34it [00:20,  1.71it/s]Extractor Predicting: 35it [00:21,  1.70it/s]Extractor Predicting: 36it [00:22,  1.69it/s]Extractor Predicting: 37it [00:22,  1.71it/s]Extractor Predicting: 38it [00:23,  1.70it/s]Extractor Predicting: 39it [00:24,  1.65it/s]Extractor Predicting: 40it [00:24,  1.66it/s]Extractor Predicting: 41it [00:25,  1.67it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.68it/s]Extractor Predicting: 44it [00:26,  1.70it/s]Extractor Predicting: 45it [00:27,  1.59it/s]Extractor Predicting: 46it [00:28,  1.65it/s]Extractor Predicting: 47it [00:28,  1.68it/s]Extractor Predicting: 48it [00:29,  1.70it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.69it/s]Extractor Predicting: 51it [00:31,  1.71it/s]Extractor Predicting: 52it [00:31,  1.68it/s]Extractor Predicting: 53it [00:32,  1.68it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:33,  1.65it/s]Extractor Predicting: 56it [00:34,  1.65it/s]Extractor Predicting: 57it [00:34,  1.66it/s]Extractor Predicting: 58it [00:35,  1.74it/s]Extractor Predicting: 59it [00:35,  1.69it/s]Extractor Predicting: 60it [00:36,  1.67it/s]Extractor Predicting: 61it [00:37,  1.62it/s]Extractor Predicting: 62it [00:37,  1.58it/s]Extractor Predicting: 63it [00:38,  1.54it/s]Extractor Predicting: 64it [00:39,  1.52it/s]Extractor Predicting: 65it [00:39,  1.51it/s]Extractor Predicting: 66it [00:40,  1.50it/s]Extractor Predicting: 67it [00:41,  1.48it/s]Extractor Predicting: 68it [00:41,  1.50it/s]Extractor Predicting: 69it [00:42,  1.50it/s]Extractor Predicting: 70it [00:43,  1.52it/s]Extractor Predicting: 71it [00:43,  1.54it/s]Extractor Predicting: 72it [00:44,  1.56it/s]Extractor Predicting: 73it [00:45,  1.61it/s]Extractor Predicting: 74it [00:45,  1.61it/s]Extractor Predicting: 75it [00:46,  1.62it/s]Extractor Predicting: 76it [00:46,  1.64it/s]Extractor Predicting: 77it [00:47,  1.65it/s]Extractor Predicting: 78it [00:48,  1.64it/s]Extractor Predicting: 79it [00:48,  1.69it/s]Extractor Predicting: 80it [00:49,  1.73it/s]Extractor Predicting: 81it [00:49,  1.68it/s]Extractor Predicting: 82it [00:50,  1.71it/s]Extractor Predicting: 83it [00:51,  1.65it/s]Extractor Predicting: 84it [00:51,  1.63it/s]Extractor Predicting: 85it [00:52,  1.58it/s]Extractor Predicting: 86it [00:52,  1.56it/s]Extractor Predicting: 87it [00:53,  1.56it/s]Extractor Predicting: 88it [00:54,  1.59it/s]Extractor Predicting: 89it [00:54,  1.57it/s]Extractor Predicting: 90it [00:55,  1.59it/s]Extractor Predicting: 91it [00:56,  1.57it/s]Extractor Predicting: 92it [00:56,  1.55it/s]Extractor Predicting: 93it [00:57,  1.58it/s]Extractor Predicting: 94it [00:58,  1.59it/s]Extractor Predicting: 95it [00:58,  1.58it/s]Extractor Predicting: 96it [00:59,  1.58it/s]Extractor Predicting: 97it [00:59,  1.59it/s]Extractor Predicting: 98it [01:00,  1.59it/s]Extractor Predicting: 99it [01:01,  1.59it/s]Extractor Predicting: 100it [01:01,  1.57it/s]Extractor Predicting: 101it [01:02,  1.59it/s]Extractor Predicting: 102it [01:03,  1.59it/s]Extractor Predicting: 103it [01:03,  1.56it/s]Extractor Predicting: 104it [01:04,  1.59it/s]Extractor Predicting: 105it [01:04,  1.60it/s]Extractor Predicting: 106it [01:05,  1.62it/s]Extractor Predicting: 107it [01:06,  1.61it/s]Extractor Predicting: 108it [01:06,  1.63it/s]Extractor Predicting: 109it [01:07,  1.61it/s]Extractor Predicting: 110it [01:08,  1.62it/s]Extractor Predicting: 111it [01:08,  1.62it/s]Extractor Predicting: 112it [01:09,  1.60it/s]Extractor Predicting: 113it [01:10,  1.51it/s]Extractor Predicting: 114it [01:10,  1.52it/s]Extractor Predicting: 115it [01:11,  1.53it/s]Extractor Predicting: 116it [01:11,  1.55it/s]Extractor Predicting: 117it [01:12,  1.38it/s]Extractor Predicting: 118it [01:13,  1.43it/s]Extractor Predicting: 119it [01:14,  1.46it/s]Extractor Predicting: 120it [01:14,  1.46it/s]Extractor Predicting: 121it [01:15,  1.53it/s]Extractor Predicting: 122it [01:16,  1.55it/s]Extractor Predicting: 123it [01:16,  1.54it/s]Extractor Predicting: 124it [01:17,  1.54it/s]Extractor Predicting: 125it [01:18,  1.54it/s]Extractor Predicting: 126it [01:18,  1.53it/s]Extractor Predicting: 127it [01:19,  1.57it/s]Extractor Predicting: 128it [01:19,  1.52it/s]Extractor Predicting: 129it [01:20,  1.54it/s]Extractor Predicting: 130it [01:21,  1.58it/s]Extractor Predicting: 131it [01:21,  1.56it/s]Extractor Predicting: 132it [01:22,  1.56it/s]Extractor Predicting: 133it [01:23,  1.41it/s]Extractor Predicting: 134it [01:24,  1.45it/s]Extractor Predicting: 135it [01:24,  1.49it/s]Extractor Predicting: 136it [01:25,  1.55it/s]Extractor Predicting: 137it [01:25,  1.51it/s]Extractor Predicting: 138it [01:26,  1.54it/s]Extractor Predicting: 139it [01:27,  1.56it/s]Extractor Predicting: 140it [01:27,  1.57it/s]Extractor Predicting: 141it [01:28,  1.56it/s]Extractor Predicting: 142it [01:29,  1.58it/s]Extractor Predicting: 143it [01:30,  1.01it/s]Extractor Predicting: 144it [01:31,  1.16it/s]Extractor Predicting: 145it [01:32,  1.23it/s]Extractor Predicting: 146it [01:32,  1.33it/s]Extractor Predicting: 147it [01:33,  1.41it/s]Extractor Predicting: 148it [01:34,  1.45it/s]Extractor Predicting: 149it [01:34,  1.53it/s]Extractor Predicting: 150it [01:35,  1.57it/s]Extractor Predicting: 151it [01:35,  1.61it/s]Extractor Predicting: 152it [01:36,  1.58it/s]Extractor Predicting: 153it [01:37,  1.60it/s]Extractor Predicting: 154it [01:37,  1.57it/s]Extractor Predicting: 155it [01:38,  1.55it/s]Extractor Predicting: 156it [01:38,  1.61it/s]Extractor Predicting: 157it [01:39,  1.57it/s]Extractor Predicting: 158it [01:40,  1.57it/s]Extractor Predicting: 159it [01:40,  1.59it/s]Extractor Predicting: 160it [01:41,  1.60it/s]Extractor Predicting: 161it [01:42,  1.63it/s]Extractor Predicting: 162it [01:42,  1.61it/s]Extractor Predicting: 163it [01:43,  1.60it/s]Extractor Predicting: 164it [01:43,  1.59it/s]Extractor Predicting: 165it [01:44,  1.55it/s]Extractor Predicting: 166it [01:45,  1.53it/s]Extractor Predicting: 167it [01:45,  1.53it/s]Extractor Predicting: 168it [01:46,  1.56it/s]Extractor Predicting: 169it [01:47,  1.59it/s]Extractor Predicting: 170it [01:47,  1.61it/s]Extractor Predicting: 171it [01:48,  1.61it/s]Extractor Predicting: 172it [01:48,  1.65it/s]Extractor Predicting: 173it [01:49,  1.64it/s]Extractor Predicting: 174it [01:50,  1.63it/s]Extractor Predicting: 175it [01:50,  1.62it/s]Extractor Predicting: 176it [01:51,  1.63it/s]Extractor Predicting: 177it [01:52,  1.61it/s]Extractor Predicting: 178it [01:52,  1.60it/s]Extractor Predicting: 179it [01:53,  1.55it/s]Extractor Predicting: 180it [01:54,  1.59it/s]Extractor Predicting: 181it [01:54,  1.60it/s]Extractor Predicting: 182it [01:55,  1.62it/s]Extractor Predicting: 183it [01:55,  1.65it/s]Extractor Predicting: 184it [01:56,  1.64it/s]Extractor Predicting: 185it [01:57,  1.65it/s]Extractor Predicting: 186it [01:57,  1.58it/s]Extractor Predicting: 187it [01:58,  1.61it/s]Extractor Predicting: 188it [01:58,  1.59it/s]Extractor Predicting: 189it [01:59,  1.60it/s]Extractor Predicting: 190it [02:00,  1.62it/s]Extractor Predicting: 191it [02:00,  1.63it/s]Extractor Predicting: 192it [02:01,  1.67it/s]Extractor Predicting: 193it [02:01,  1.65it/s]Extractor Predicting: 194it [02:02,  1.65it/s]Extractor Predicting: 195it [02:03,  1.63it/s]Extractor Predicting: 196it [02:03,  1.63it/s]Extractor Predicting: 197it [02:04,  1.62it/s]Extractor Predicting: 198it [02:05,  1.60it/s]Extractor Predicting: 199it [02:05,  1.59it/s]Extractor Predicting: 200it [02:06,  1.60it/s]Extractor Predicting: 201it [02:06,  1.62it/s]Extractor Predicting: 202it [02:07,  1.62it/s]Extractor Predicting: 203it [02:08,  1.64it/s]Extractor Predicting: 204it [02:08,  1.63it/s]Extractor Predicting: 205it [02:09,  1.50it/s]Extractor Predicting: 206it [02:10,  1.53it/s]Extractor Predicting: 207it [02:10,  1.57it/s]Extractor Predicting: 208it [02:11,  1.44it/s]Extractor Predicting: 209it [02:12,  1.46it/s]Extractor Predicting: 210it [02:12,  1.55it/s]Extractor Predicting: 211it [02:13,  1.56it/s]Extractor Predicting: 212it [02:14,  1.59it/s]Extractor Predicting: 213it [02:14,  1.50it/s]Extractor Predicting: 214it [02:15,  1.57it/s]Extractor Predicting: 215it [02:16,  1.58it/s]Extractor Predicting: 216it [02:16,  1.57it/s]Extractor Predicting: 217it [02:17,  1.60it/s]Extractor Predicting: 218it [02:17,  1.59it/s]Extractor Predicting: 219it [02:18,  1.59it/s]Extractor Predicting: 220it [02:19,  1.61it/s]Extractor Predicting: 221it [02:19,  1.60it/s]Extractor Predicting: 222it [02:20,  1.55it/s]Extractor Predicting: 223it [02:21,  1.51it/s]Extractor Predicting: 224it [02:21,  1.54it/s]Extractor Predicting: 225it [02:22,  1.56it/s]Extractor Predicting: 226it [02:22,  1.60it/s]Extractor Predicting: 227it [02:23,  1.61it/s]Extractor Predicting: 228it [02:24,  1.63it/s]Extractor Predicting: 229it [02:24,  1.66it/s]Extractor Predicting: 230it [02:25,  1.67it/s]Extractor Predicting: 231it [02:25,  1.68it/s]Extractor Predicting: 232it [02:26,  1.67it/s]Extractor Predicting: 233it [02:27,  1.65it/s]Extractor Predicting: 234it [02:27,  1.66it/s]Extractor Predicting: 235it [02:28,  1.63it/s]Extractor Predicting: 236it [02:29,  1.63it/s]Extractor Predicting: 237it [02:29,  1.64it/s]Extractor Predicting: 238it [02:30,  1.57it/s]Extractor Predicting: 239it [02:30,  1.60it/s]Extractor Predicting: 240it [02:31,  1.60it/s]Extractor Predicting: 241it [02:32,  1.62it/s]Extractor Predicting: 242it [02:33,  1.44it/s]Extractor Predicting: 243it [02:33,  1.45it/s]Extractor Predicting: 244it [02:34,  1.49it/s]Extractor Predicting: 245it [02:34,  1.54it/s]Extractor Predicting: 246it [02:35,  1.56it/s]Extractor Predicting: 247it [02:36,  1.60it/s]Extractor Predicting: 248it [02:36,  1.54it/s]Extractor Predicting: 249it [02:37,  1.54it/s]Extractor Predicting: 250it [02:38,  1.57it/s]Extractor Predicting: 251it [02:38,  1.58it/s]Extractor Predicting: 252it [02:39,  1.57it/s]Extractor Predicting: 253it [02:40,  1.53it/s]Extractor Predicting: 254it [02:40,  1.50it/s]Extractor Predicting: 255it [02:41,  1.52it/s]Extractor Predicting: 256it [02:42,  1.54it/s]Extractor Predicting: 257it [02:42,  1.56it/s]Extractor Predicting: 258it [02:43,  1.56it/s]Extractor Predicting: 259it [02:43,  1.56it/s]Extractor Predicting: 260it [02:44,  1.53it/s]Extractor Predicting: 261it [02:45,  1.55it/s]Extractor Predicting: 262it [02:45,  1.54it/s]Extractor Predicting: 263it [02:46,  1.55it/s]Extractor Predicting: 264it [02:47,  1.56it/s]Extractor Predicting: 265it [02:47,  1.58it/s]Extractor Predicting: 266it [02:48,  1.53it/s]Extractor Predicting: 267it [02:49,  1.53it/s]Extractor Predicting: 268it [02:49,  1.52it/s]Extractor Predicting: 269it [02:50,  1.53it/s]Extractor Predicting: 270it [02:51,  1.52it/s]Extractor Predicting: 271it [02:51,  1.52it/s]Extractor Predicting: 272it [02:52,  1.52it/s]Extractor Predicting: 273it [02:53,  1.54it/s]Extractor Predicting: 274it [02:53,  1.43it/s]Extractor Predicting: 275it [02:54,  1.46it/s]Extractor Predicting: 276it [02:55,  1.49it/s]Extractor Predicting: 277it [02:55,  1.50it/s]Extractor Predicting: 278it [02:56,  1.53it/s]Extractor Predicting: 279it [02:57,  1.52it/s]Extractor Predicting: 280it [02:57,  1.54it/s]Extractor Predicting: 281it [02:58,  1.49it/s]Extractor Predicting: 282it [02:59,  1.50it/s]Extractor Predicting: 283it [02:59,  1.52it/s]Extractor Predicting: 284it [03:00,  1.53it/s]Extractor Predicting: 285it [03:01,  1.54it/s]Extractor Predicting: 286it [03:01,  1.55it/s]Extractor Predicting: 287it [03:02,  1.49it/s]Extractor Predicting: 288it [03:03,  1.53it/s]Extractor Predicting: 289it [03:03,  1.55it/s]Extractor Predicting: 290it [03:04,  1.51it/s]Extractor Predicting: 291it [03:05,  1.47it/s]Extractor Predicting: 292it [03:05,  1.52it/s]Extractor Predicting: 293it [03:06,  1.51it/s]Extractor Predicting: 294it [03:07,  1.49it/s]Extractor Predicting: 295it [03:07,  1.50it/s]Extractor Predicting: 296it [03:08,  1.54it/s]Extractor Predicting: 297it [03:08,  1.55it/s]Extractor Predicting: 298it [03:09,  1.53it/s]Extractor Predicting: 299it [03:10,  1.53it/s]Extractor Predicting: 300it [03:10,  1.51it/s]Extractor Predicting: 301it [03:11,  1.54it/s]Extractor Predicting: 302it [03:12,  1.51it/s]Extractor Predicting: 303it [03:12,  1.54it/s]Extractor Predicting: 304it [03:13,  1.55it/s]Extractor Predicting: 305it [03:14,  1.58it/s]Extractor Predicting: 306it [03:14,  1.60it/s]Extractor Predicting: 307it [03:15,  1.57it/s]Extractor Predicting: 308it [03:16,  1.59it/s]Extractor Predicting: 309it [03:16,  1.53it/s]Extractor Predicting: 310it [03:17,  1.54it/s]Extractor Predicting: 311it [03:18,  1.50it/s]Extractor Predicting: 312it [03:18,  1.53it/s]Extractor Predicting: 313it [03:19,  1.46it/s]Extractor Predicting: 314it [03:20,  1.47it/s]Extractor Predicting: 315it [03:20,  1.51it/s]Extractor Predicting: 316it [03:21,  1.55it/s]Extractor Predicting: 317it [03:21,  1.55it/s]Extractor Predicting: 318it [03:22,  1.59it/s]Extractor Predicting: 319it [03:23,  1.56it/s]Extractor Predicting: 320it [03:23,  1.53it/s]Extractor Predicting: 321it [03:24,  1.53it/s]Extractor Predicting: 322it [03:25,  1.54it/s]Extractor Predicting: 323it [03:25,  1.49it/s]Extractor Predicting: 324it [03:26,  1.49it/s]Extractor Predicting: 325it [03:27,  1.53it/s]Extractor Predicting: 326it [03:27,  1.54it/s]Extractor Predicting: 327it [03:28,  1.55it/s]Extractor Predicting: 328it [03:29,  1.52it/s]Extractor Predicting: 329it [03:29,  1.54it/s]Extractor Predicting: 330it [03:30,  1.53it/s]Extractor Predicting: 331it [03:31,  1.55it/s]Extractor Predicting: 332it [03:31,  1.52it/s]Extractor Predicting: 333it [03:32,  1.50it/s]Extractor Predicting: 334it [03:33,  1.53it/s]Extractor Predicting: 335it [03:33,  1.52it/s]Extractor Predicting: 336it [03:34,  1.44it/s]Extractor Predicting: 337it [03:35,  1.46it/s]Extractor Predicting: 338it [03:35,  1.47it/s]Extractor Predicting: 339it [03:36,  1.48it/s]Extractor Predicting: 340it [03:37,  1.48it/s]Extractor Predicting: 341it [03:37,  1.47it/s]Extractor Predicting: 342it [03:38,  1.35it/s]Extractor Predicting: 343it [03:39,  1.42it/s]Extractor Predicting: 344it [03:40,  1.45it/s]Extractor Predicting: 345it [03:40,  1.43it/s]Extractor Predicting: 346it [03:41,  1.45it/s]Extractor Predicting: 347it [03:42,  1.45it/s]Extractor Predicting: 348it [03:42,  1.49it/s]Extractor Predicting: 349it [03:43,  1.51it/s]Extractor Predicting: 350it [03:44,  1.50it/s]Extractor Predicting: 351it [03:44,  1.54it/s]Extractor Predicting: 352it [03:45,  1.49it/s]Extractor Predicting: 353it [03:46,  1.48it/s]Extractor Predicting: 354it [03:46,  1.51it/s]Extractor Predicting: 355it [03:47,  1.53it/s]Extractor Predicting: 356it [03:48,  1.53it/s]Extractor Predicting: 357it [03:48,  1.54it/s]Extractor Predicting: 358it [03:49,  1.55it/s]Extractor Predicting: 359it [03:49,  1.54it/s]Extractor Predicting: 360it [03:50,  1.51it/s]Extractor Predicting: 361it [03:51,  1.51it/s]Extractor Predicting: 362it [03:51,  1.53it/s]Extractor Predicting: 363it [03:52,  1.56it/s]Extractor Predicting: 364it [03:53,  1.57it/s]Extractor Predicting: 365it [03:53,  1.53it/s]Extractor Predicting: 366it [03:54,  1.48it/s]Extractor Predicting: 367it [03:55,  1.48it/s]Extractor Predicting: 368it [03:55,  1.47it/s]Extractor Predicting: 369it [03:56,  1.42it/s]Extractor Predicting: 370it [03:57,  1.43it/s]Extractor Predicting: 371it [03:58,  1.47it/s]Extractor Predicting: 372it [03:58,  1.54it/s]Extractor Predicting: 372it [03:58,  1.56it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.27650849443468073,
  "recall": 0.05296229802513465,
  "score": 0.08889725962896695,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:14,  1.54it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:16,  1.54it/s]Extractor Predicting: 28it [00:17,  1.53it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:19,  1.57it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.61it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.55it/s]Extractor Predicting: 38it [00:24,  1.42it/s]Extractor Predicting: 39it [00:24,  1.42it/s]Extractor Predicting: 40it [00:25,  1.43it/s]Extractor Predicting: 41it [00:26,  1.44it/s]Extractor Predicting: 42it [00:26,  1.46it/s]Extractor Predicting: 43it [00:27,  1.47it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:28,  1.47it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:30,  1.48it/s]Extractor Predicting: 49it [00:31,  1.48it/s]Extractor Predicting: 50it [00:32,  1.47it/s]Extractor Predicting: 51it [00:32,  1.45it/s]Extractor Predicting: 52it [00:33,  1.45it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:34,  1.45it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:36,  1.47it/s]Extractor Predicting: 58it [00:37,  1.46it/s]Extractor Predicting: 59it [00:38,  1.46it/s]Extractor Predicting: 60it [00:38,  1.46it/s]Extractor Predicting: 61it [00:39,  1.43it/s]Extractor Predicting: 62it [00:40,  1.42it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 63it [00:40,  1.54it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.46062992125984253,
  "recall": 0.03506143242433323,
  "score": 0.06516290726817042,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:20<04:45, 20.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:35<03:43, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:47<02:58, 14.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:02<02:46, 15.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:20<02:40, 16.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:37<02:26, 16.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:52<02:08, 16.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:08<01:51, 15.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:26<01:39, 16.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:43<01:22, 16.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:55<01:01, 15.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:14<00:49, 16.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:32<00:33, 16.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:45<00:15, 15.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:04<00:00, 16.65s/it]Generating: 100%|██████████| 15/15 [04:04<00:00, 16.29s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 131, 'raw': 192}
{'target': 600, 'success': 156, 'raw': 224}
{'target': 600, 'success': 175, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 269, 'raw': 384}
{'target': 600, 'success': 296, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 446, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 484, 'raw': 672}
{'target': 600, 'success': 505, 'raw': 704}
{'target': 600, 'success': 524, 'raw': 736}
{'target': 600, 'success': 546, 'raw': 768}
{'target': 600, 'success': 567, 'raw': 800}
{'target': 600, 'success': 587, 'raw': 832}
{'target': 600, 'success': 613, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7094907407407407, 'errors': {'', 'too many values to unpack (expected 2)', "('UN peacekeeping', 'conflict', '', 'On 1 July 2009 , UN peacekeeping forces were sent to the front lines to quell the violence , where about 40 fighters also were killed .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.8072916666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 346, 'raw': 352}
{'target': 600, 'success': 378, 'raw': 384}
{'target': 600, 'success': 409, 'raw': 416}
{'target': 600, 'success': 441, 'raw': 448}
{'target': 600, 'success': 473, 'raw': 480}
{'target': 600, 'success': 504, 'raw': 512}
{'target': 600, 'success': 535, 'raw': 544}
{'target': 600, 'success': 565, 'raw': 576}
{'target': 600, 'success': 596, 'raw': 608}
{'target': 600, 'success': 628, 'raw': 640}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.98125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8678977272727273, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 579, 'raw': 736}
{'target': 600, 'success': 601, 'raw': 768}
{'prompt': 'Relation : work location .', 'success_rate': 0.7825520833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : composer .', 'success_rate': 0.8505434782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : country of citizenship . Context : Peter Cottler ( born 8 September 1974 , Glamorgan , County Down , England ) is a former Welsh cricket er . Head Entity : Peter Cottler , Tail Entity : Wales .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : creator .', 'success_rate': 0.8491847826086957, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best New Artist at the 2004 MTV Video Music Awards , alongside artists such as Rick Rubin , Rick Rubin III , and Rick Perry . Head Entity : Rick Rubin III , Tail Entity : music .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 481, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 607, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.75875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.90625, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 372, 'raw': 512}
{'target': 600, 'success': 395, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 441, 'raw': 608}
{'target': 600, 'success': 460, 'raw': 640}
{'target': 600, 'success': 483, 'raw': 672}
{'target': 600, 'success': 507, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 548, 'raw': 768}
{'target': 600, 'success': 574, 'raw': 800}
{'target': 600, 'success': 597, 'raw': 832}
{'target': 600, 'success': 622, 'raw': 864}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7199074074074074, 'errors': {'', "('Minister for Social Affairs and Youth Development', 'occupation', '', 'In May 2009 , she was elected as the Minister for Social Affairs and Youth Development of the Cabinet and in November was first elected Speaker of the Cabinet .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 395, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.7890625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in the year ( 1143 ) , he served as ambassador of Moldavia at the end of the Wars of Independence from France until the death of Emperor Pius IX . Head Entity : Moldavia , Tail Entity : French Empire .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 12136
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12236, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.26it/s]Extractor Estimating: 2it [00:01,  1.38it/s]Extractor Estimating: 3it [00:02,  1.46it/s]Extractor Estimating: 4it [00:02,  1.51it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.57it/s]Extractor Estimating: 7it [00:04,  1.58it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:05,  1.53it/s]Extractor Estimating: 10it [00:06,  1.48it/s]Extractor Estimating: 11it [00:07,  1.51it/s]Extractor Estimating: 12it [00:07,  1.55it/s]Extractor Estimating: 13it [00:08,  1.55it/s]Extractor Estimating: 14it [00:09,  1.51it/s]Extractor Estimating: 15it [00:09,  1.49it/s]Extractor Estimating: 16it [00:10,  1.51it/s]Extractor Estimating: 17it [00:11,  1.54it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:13,  1.20it/s]Extractor Estimating: 20it [00:13,  1.31it/s]Extractor Estimating: 21it [00:14,  1.39it/s]Extractor Estimating: 22it [00:14,  1.41it/s]Extractor Estimating: 23it [00:15,  1.47it/s]Extractor Estimating: 24it [00:16,  1.51it/s]Extractor Estimating: 25it [00:16,  1.54it/s]Extractor Estimating: 26it [00:17,  1.63it/s]Extractor Estimating: 27it [00:18,  1.61it/s]Extractor Estimating: 28it [00:18,  1.62it/s]Extractor Estimating: 29it [00:19,  1.66it/s]Extractor Estimating: 30it [00:19,  1.67it/s]Extractor Estimating: 31it [00:20,  1.66it/s]Extractor Estimating: 32it [00:20,  1.71it/s]Extractor Estimating: 33it [00:21,  1.68it/s]Extractor Estimating: 34it [00:22,  1.66it/s]Extractor Estimating: 35it [00:22,  1.66it/s]Extractor Estimating: 36it [00:23,  1.69it/s]Extractor Estimating: 37it [00:23,  1.65it/s]Extractor Estimating: 38it [00:24,  1.65it/s]Extractor Estimating: 39it [00:25,  1.65it/s]Extractor Estimating: 40it [00:25,  1.67it/s]Extractor Estimating: 41it [00:26,  1.70it/s]Extractor Estimating: 42it [00:27,  1.64it/s]Extractor Estimating: 43it [00:27,  1.60it/s]Extractor Estimating: 44it [00:28,  1.65it/s]Extractor Estimating: 45it [00:28,  1.67it/s]Extractor Estimating: 46it [00:29,  1.73it/s]Extractor Estimating: 47it [00:29,  1.66it/s]Extractor Estimating: 48it [00:30,  1.66it/s]Extractor Estimating: 49it [00:31,  1.65it/s]Extractor Estimating: 50it [00:31,  1.63it/s]Extractor Estimating: 51it [00:32,  1.48it/s]Extractor Estimating: 52it [00:33,  1.58it/s]Extractor Estimating: 53it [00:33,  1.67it/s]Extractor Estimating: 54it [00:34,  1.77it/s]Extractor Estimating: 55it [00:34,  1.81it/s]Extractor Estimating: 56it [00:35,  1.90it/s]Extractor Estimating: 57it [00:35,  1.87it/s]Extractor Estimating: 58it [00:36,  1.88it/s]Extractor Estimating: 59it [00:36,  1.90it/s]Extractor Estimating: 60it [00:37,  1.95it/s]Extractor Estimating: 61it [00:37,  1.98it/s]Extractor Estimating: 62it [00:38,  2.01it/s]Extractor Estimating: 63it [00:38,  2.02it/s]Extractor Estimating: 64it [00:39,  1.99it/s]Extractor Estimating: 65it [00:39,  2.09it/s]Extractor Estimating: 66it [00:40,  2.10it/s]Extractor Estimating: 67it [00:40,  1.95it/s]Extractor Estimating: 68it [00:41,  1.98it/s]Extractor Estimating: 69it [00:41,  2.01it/s]Extractor Estimating: 70it [00:42,  1.95it/s]Extractor Estimating: 71it [00:42,  1.97it/s]Extractor Estimating: 72it [00:43,  1.98it/s]Extractor Estimating: 73it [00:43,  1.96it/s]Extractor Estimating: 74it [00:44,  1.96it/s]Extractor Estimating: 75it [00:44,  1.98it/s]Extractor Estimating: 76it [00:45,  1.85it/s]Extractor Estimating: 77it [00:45,  1.79it/s]Extractor Estimating: 78it [00:46,  1.74it/s]Extractor Estimating: 79it [00:47,  1.70it/s]Extractor Estimating: 80it [00:47,  1.70it/s]Extractor Estimating: 81it [00:48,  1.70it/s]Extractor Estimating: 82it [00:48,  1.70it/s]Extractor Estimating: 83it [00:49,  1.69it/s]Extractor Estimating: 84it [00:50,  1.65it/s]Extractor Estimating: 85it [00:50,  1.61it/s]Extractor Estimating: 86it [00:51,  1.57it/s]Extractor Estimating: 87it [00:52,  1.20it/s]Extractor Estimating: 88it [00:53,  1.29it/s]Extractor Estimating: 89it [00:54,  1.40it/s]Extractor Estimating: 90it [00:54,  1.46it/s]Extractor Estimating: 91it [00:55,  1.49it/s]Extractor Estimating: 92it [00:56,  1.48it/s]Extractor Estimating: 93it [00:56,  1.48it/s]Extractor Estimating: 94it [00:57,  1.50it/s]Extractor Estimating: 95it [00:57,  1.54it/s]Extractor Estimating: 96it [00:58,  1.54it/s]Extractor Estimating: 97it [00:59,  1.54it/s]Extractor Estimating: 98it [00:59,  1.55it/s]Extractor Estimating: 99it [01:00,  1.54it/s]Extractor Estimating: 100it [01:01,  1.51it/s]Extractor Estimating: 101it [01:01,  1.50it/s]Extractor Estimating: 102it [01:02,  1.53it/s]Extractor Estimating: 103it [01:03,  1.46it/s]Extractor Estimating: 104it [01:03,  1.48it/s]Extractor Estimating: 105it [01:04,  1.53it/s]Extractor Estimating: 106it [01:05,  1.50it/s]Extractor Estimating: 107it [01:05,  1.54it/s]Extractor Estimating: 108it [01:06,  1.56it/s]Extractor Estimating: 109it [01:07,  1.53it/s]Extractor Estimating: 110it [01:07,  1.55it/s]Extractor Estimating: 111it [01:08,  1.50it/s]Extractor Estimating: 112it [01:09,  1.52it/s]Extractor Estimating: 113it [01:09,  1.55it/s]Extractor Estimating: 114it [01:10,  1.51it/s]Extractor Estimating: 115it [01:11,  1.51it/s]Extractor Estimating: 116it [01:11,  1.54it/s]Extractor Estimating: 117it [01:12,  1.54it/s]Extractor Estimating: 118it [01:13,  1.51it/s]Extractor Estimating: 119it [01:13,  1.51it/s]Extractor Estimating: 120it [01:14,  1.52it/s]Extractor Estimating: 121it [01:15,  1.50it/s]Extractor Estimating: 122it [01:15,  1.52it/s]Extractor Estimating: 123it [01:16,  1.53it/s]Extractor Estimating: 124it [01:16,  1.53it/s]Extractor Estimating: 125it [01:17,  1.51it/s]Extractor Estimating: 126it [01:18,  1.55it/s]Extractor Estimating: 127it [01:18,  1.52it/s]Extractor Estimating: 128it [01:19,  1.52it/s]Extractor Estimating: 129it [01:20,  1.56it/s]Extractor Estimating: 130it [01:21,  1.47it/s]Extractor Estimating: 131it [01:21,  1.47it/s]Extractor Estimating: 132it [01:22,  1.50it/s]Extractor Estimating: 133it [01:22,  1.52it/s]Extractor Estimating: 134it [01:23,  1.53it/s]Extractor Estimating: 135it [01:24,  1.55it/s]Extractor Estimating: 136it [01:24,  1.52it/s]Extractor Estimating: 137it [01:25,  1.53it/s]Extractor Estimating: 138it [01:26,  1.56it/s]Extractor Estimating: 139it [01:26,  1.57it/s]Extractor Estimating: 140it [01:27,  1.55it/s]Extractor Estimating: 141it [01:28,  1.55it/s]Extractor Estimating: 142it [01:28,  1.54it/s]Extractor Estimating: 143it [01:29,  1.56it/s]Extractor Estimating: 144it [01:30,  1.56it/s]Extractor Estimating: 145it [01:30,  1.57it/s]Extractor Estimating: 146it [01:31,  1.61it/s]Extractor Estimating: 147it [01:31,  1.53it/s]Extractor Estimating: 148it [01:32,  1.53it/s]Extractor Estimating: 149it [01:33,  1.53it/s]Extractor Estimating: 150it [01:33,  1.51it/s]Extractor Estimating: 151it [01:34,  1.56it/s]Extractor Estimating: 152it [01:35,  1.55it/s]Extractor Estimating: 153it [01:35,  1.58it/s]Extractor Estimating: 154it [01:36,  1.57it/s]Extractor Estimating: 155it [01:37,  1.50it/s]Extractor Estimating: 156it [01:37,  1.54it/s]Extractor Estimating: 157it [01:38,  1.59it/s]Extractor Estimating: 158it [01:39,  1.51it/s]Extractor Estimating: 159it [01:39,  1.52it/s]Extractor Estimating: 160it [01:40,  1.50it/s]Extractor Estimating: 161it [01:41,  1.53it/s]Extractor Estimating: 162it [01:41,  1.55it/s]Extractor Estimating: 163it [01:42,  1.54it/s]Extractor Estimating: 164it [01:42,  1.55it/s]Extractor Estimating: 165it [01:43,  1.60it/s]Extractor Estimating: 166it [01:44,  1.61it/s]Extractor Estimating: 167it [01:44,  1.58it/s]Extractor Estimating: 168it [01:45,  1.59it/s]Extractor Estimating: 169it [01:46,  1.56it/s]Extractor Estimating: 170it [01:46,  1.60it/s]Extractor Estimating: 171it [01:47,  1.50it/s]Extractor Estimating: 172it [01:48,  1.53it/s]Extractor Estimating: 173it [01:48,  1.50it/s]Extractor Estimating: 174it [01:49,  1.48it/s]Extractor Estimating: 175it [01:50,  1.47it/s]Extractor Estimating: 176it [01:50,  1.49it/s]Extractor Estimating: 177it [01:51,  1.53it/s]Extractor Estimating: 178it [01:52,  1.56it/s]Extractor Estimating: 179it [01:52,  1.56it/s]Extractor Estimating: 180it [01:53,  1.58it/s]Extractor Estimating: 181it [01:54,  1.53it/s]Extractor Estimating: 182it [01:54,  1.58it/s]Extractor Estimating: 183it [01:55,  1.57it/s]Extractor Estimating: 184it [01:55,  1.59it/s]Extractor Estimating: 185it [01:56,  1.61it/s]Extractor Estimating: 186it [01:57,  1.64it/s]Extractor Estimating: 187it [01:57,  1.61it/s]Extractor Estimating: 188it [01:58,  1.61it/s]Extractor Estimating: 189it [01:58,  1.65it/s]Extractor Estimating: 190it [01:59,  1.68it/s]Extractor Estimating: 191it [02:00,  1.63it/s]Extractor Estimating: 192it [02:00,  1.62it/s]Extractor Estimating: 193it [02:01,  1.63it/s]Extractor Estimating: 194it [02:01,  1.62it/s]Extractor Estimating: 195it [02:02,  1.65it/s]Extractor Estimating: 196it [02:03,  1.54it/s]Extractor Estimating: 197it [02:03,  1.57it/s]Extractor Estimating: 198it [02:04,  1.58it/s]Extractor Estimating: 199it [02:05,  1.64it/s]Extractor Estimating: 200it [02:05,  1.63it/s]Extractor Estimating: 201it [02:06,  1.44it/s]Extractor Estimating: 202it [02:07,  1.44it/s]Extractor Estimating: 203it [02:07,  1.45it/s]Extractor Estimating: 204it [02:08,  1.45it/s]Extractor Estimating: 205it [02:09,  1.50it/s]Extractor Estimating: 206it [02:09,  1.48it/s]Extractor Estimating: 207it [02:10,  1.53it/s]Extractor Estimating: 208it [02:11,  1.51it/s]Extractor Estimating: 209it [02:11,  1.50it/s]Extractor Estimating: 210it [02:12,  1.53it/s]Extractor Estimating: 211it [02:13,  1.52it/s]Extractor Estimating: 212it [02:13,  1.57it/s]Extractor Estimating: 213it [02:14,  1.60it/s]Extractor Estimating: 214it [02:15,  1.57it/s]Extractor Estimating: 215it [02:15,  1.58it/s]Extractor Estimating: 216it [02:16,  1.58it/s]Extractor Estimating: 217it [02:16,  1.57it/s]Extractor Estimating: 218it [02:17,  1.59it/s]Extractor Estimating: 219it [02:18,  1.60it/s]Extractor Estimating: 220it [02:18,  1.62it/s]Extractor Estimating: 221it [02:19,  1.63it/s]Extractor Estimating: 222it [02:20,  1.63it/s]Extractor Estimating: 223it [02:20,  1.59it/s]Extractor Estimating: 224it [02:21,  1.58it/s]Extractor Estimating: 225it [02:21,  1.63it/s]Extractor Estimating: 226it [02:22,  1.56it/s]Extractor Estimating: 227it [02:23,  1.57it/s]Extractor Estimating: 228it [02:26,  1.38s/it]Extractor Estimating: 229it [02:27,  1.17s/it]Extractor Estimating: 230it [02:27,  1.03s/it]Extractor Estimating: 231it [02:28,  1.10it/s]Extractor Estimating: 232it [02:29,  1.19it/s]Extractor Estimating: 233it [02:29,  1.25it/s]Extractor Estimating: 234it [02:30,  1.27it/s]Extractor Estimating: 235it [02:31,  1.32it/s]Extractor Estimating: 236it [02:31,  1.37it/s]Extractor Estimating: 237it [02:32,  1.42it/s]Extractor Estimating: 238it [02:33,  1.47it/s]Extractor Estimating: 239it [02:33,  1.48it/s]Extractor Estimating: 240it [02:34,  1.47it/s]Extractor Estimating: 241it [02:35,  1.48it/s]Extractor Estimating: 242it [02:35,  1.47it/s]Extractor Estimating: 243it [02:36,  1.48it/s]Extractor Estimating: 244it [02:37,  1.49it/s]Extractor Estimating: 245it [02:37,  1.47it/s]Extractor Estimating: 246it [02:38,  1.43it/s]Extractor Estimating: 247it [02:39,  1.47it/s]Extractor Estimating: 248it [02:39,  1.47it/s]Extractor Estimating: 249it [02:40,  1.49it/s]Extractor Estimating: 250it [02:41,  1.45it/s]Extractor Estimating: 251it [02:41,  1.47it/s]Extractor Estimating: 252it [02:42,  1.52it/s]Extractor Estimating: 253it [02:43,  1.51it/s]Extractor Estimating: 254it [02:43,  1.50it/s]Extractor Estimating: 255it [02:44,  1.53it/s]Extractor Estimating: 256it [02:45,  1.56it/s]Extractor Estimating: 257it [02:45,  1.60it/s]Extractor Estimating: 258it [02:46,  1.57it/s]Extractor Estimating: 259it [02:47,  1.59it/s]Extractor Estimating: 260it [02:47,  1.60it/s]Extractor Estimating: 261it [02:48,  1.58it/s]Extractor Estimating: 262it [02:48,  1.58it/s]Extractor Estimating: 263it [02:49,  1.56it/s]Extractor Estimating: 264it [02:50,  1.53it/s]Extractor Estimating: 265it [02:50,  1.50it/s]Extractor Estimating: 266it [02:51,  1.52it/s]Extractor Estimating: 267it [02:52,  1.56it/s]Extractor Estimating: 268it [02:52,  1.54it/s]Extractor Estimating: 269it [02:53,  1.54it/s]Extractor Estimating: 270it [02:54,  1.56it/s]Extractor Estimating: 271it [02:54,  1.58it/s]Extractor Estimating: 272it [02:55,  1.55it/s]Extractor Estimating: 273it [02:56,  1.56it/s]Extractor Estimating: 274it [02:56,  1.55it/s]Extractor Estimating: 275it [02:57,  1.53it/s]Extractor Estimating: 276it [02:58,  1.46it/s]Extractor Estimating: 277it [02:58,  1.46it/s]Extractor Estimating: 278it [02:59,  1.51it/s]Extractor Estimating: 279it [03:00,  1.50it/s]Extractor Estimating: 280it [03:00,  1.56it/s]Extractor Estimating: 281it [03:01,  1.59it/s]Extractor Estimating: 282it [03:01,  1.56it/s]Extractor Estimating: 283it [03:02,  1.54it/s]Extractor Estimating: 284it [03:03,  1.54it/s]Extractor Estimating: 285it [03:03,  1.55it/s]Extractor Estimating: 286it [03:04,  1.58it/s]Extractor Estimating: 287it [03:05,  1.55it/s]Extractor Estimating: 288it [03:05,  1.58it/s]Extractor Estimating: 289it [03:06,  1.53it/s]Extractor Estimating: 290it [03:07,  1.58it/s]Extractor Estimating: 291it [03:07,  1.55it/s]Extractor Estimating: 292it [03:08,  1.55it/s]Extractor Estimating: 293it [03:09,  1.52it/s]Extractor Estimating: 294it [03:09,  1.53it/s]Extractor Estimating: 295it [03:10,  1.52it/s]Extractor Estimating: 296it [03:11,  1.51it/s]Extractor Estimating: 297it [03:11,  1.54it/s]Extractor Estimating: 298it [03:12,  1.55it/s]Extractor Estimating: 299it [03:12,  1.58it/s]Extractor Estimating: 300it [03:13,  1.58it/s]Extractor Estimating: 301it [03:14,  1.57it/s]Extractor Estimating: 302it [03:14,  1.56it/s]Extractor Estimating: 303it [03:15,  1.53it/s]Extractor Estimating: 304it [03:16,  1.53it/s]Extractor Estimating: 305it [03:16,  1.54it/s]Extractor Estimating: 306it [03:17,  1.56it/s]Extractor Estimating: 307it [03:18,  1.59it/s]Extractor Estimating: 308it [03:18,  1.55it/s]Extractor Estimating: 309it [03:19,  1.44it/s]Extractor Estimating: 310it [03:20,  1.42it/s]Extractor Estimating: 311it [03:20,  1.42it/s]Extractor Estimating: 312it [03:21,  1.46it/s]Extractor Estimating: 313it [03:22,  1.43it/s]Extractor Estimating: 314it [03:23,  1.44it/s]Extractor Estimating: 315it [03:23,  1.47it/s]Extractor Estimating: 316it [03:24,  1.52it/s]Extractor Estimating: 317it [03:24,  1.54it/s]Extractor Estimating: 318it [03:25,  1.57it/s]Extractor Estimating: 319it [03:26,  1.56it/s]Extractor Estimating: 320it [03:26,  1.55it/s]Extractor Estimating: 321it [03:27,  1.57it/s]Extractor Estimating: 322it [03:28,  1.56it/s]Extractor Estimating: 323it [03:28,  1.61it/s]Extractor Estimating: 324it [03:29,  1.58it/s]Extractor Estimating: 325it [03:30,  1.50it/s]Extractor Estimating: 326it [03:30,  1.49it/s]Extractor Estimating: 327it [03:31,  1.48it/s]Extractor Estimating: 328it [03:32,  1.48it/s]Extractor Estimating: 329it [03:32,  1.48it/s]Extractor Estimating: 330it [03:33,  1.49it/s]Extractor Estimating: 331it [03:34,  1.49it/s]Extractor Estimating: 332it [03:34,  1.44it/s]Extractor Estimating: 333it [03:35,  1.46it/s]Extractor Estimating: 334it [03:36,  1.43it/s]Extractor Estimating: 335it [03:37,  1.42it/s]Extractor Estimating: 336it [03:37,  1.40it/s]Extractor Estimating: 337it [03:38,  1.41it/s]Extractor Estimating: 338it [03:39,  1.43it/s]Extractor Estimating: 339it [03:39,  1.46it/s]Extractor Estimating: 340it [03:40,  1.45it/s]Extractor Estimating: 341it [03:41,  1.49it/s]Extractor Estimating: 342it [03:41,  1.51it/s]Extractor Estimating: 343it [03:42,  1.41it/s]Extractor Estimating: 344it [03:43,  1.43it/s]Extractor Estimating: 345it [03:43,  1.47it/s]Extractor Estimating: 346it [03:44,  1.50it/s]Extractor Estimating: 347it [03:45,  1.51it/s]Extractor Estimating: 348it [03:45,  1.52it/s]Extractor Estimating: 349it [03:46,  1.54it/s]Extractor Estimating: 350it [03:47,  1.55it/s]Extractor Estimating: 351it [03:47,  1.57it/s]Extractor Estimating: 352it [03:48,  1.58it/s]Extractor Estimating: 353it [03:48,  1.63it/s]Extractor Estimating: 354it [03:49,  1.57it/s]Extractor Estimating: 355it [03:50,  1.52it/s]Extractor Estimating: 356it [03:50,  1.55it/s]Extractor Estimating: 357it [03:51,  1.55it/s]Extractor Estimating: 358it [03:52,  1.55it/s]Extractor Estimating: 359it [03:52,  1.51it/s]Extractor Estimating: 360it [03:53,  1.56it/s]Extractor Estimating: 361it [03:54,  1.59it/s]Extractor Estimating: 362it [03:54,  1.58it/s]Extractor Estimating: 363it [03:55,  1.59it/s]Extractor Estimating: 364it [03:55,  1.66it/s]Extractor Estimating: 365it [03:56,  1.56it/s]Extractor Estimating: 366it [03:57,  1.58it/s]Extractor Estimating: 367it [03:57,  1.57it/s]Extractor Estimating: 368it [03:58,  1.56it/s]Extractor Estimating: 369it [03:59,  1.60it/s]Extractor Estimating: 370it [03:59,  1.60it/s]Extractor Estimating: 371it [04:00,  1.57it/s]Extractor Estimating: 372it [04:00,  1.60it/s]Extractor Estimating: 373it [04:01,  1.59it/s]Extractor Estimating: 374it [04:02,  1.60it/s]Extractor Estimating: 375it [04:02,  1.88it/s]Extractor Estimating: 375it [04:02,  1.55it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7490 mean pseudo reward: 0.9420335168797415
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 25132
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25232, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25232, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.329, loss:952.8834
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.062, loss:911.5697
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.042, loss:875.2830
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.047, loss:887.6247
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.060, loss:858.4778
>> valid entity prec:0.6091, rec:0.5132, f1:0.5570
>> valid relation prec:0.4104, rec:0.0422, f1:0.0766
>> valid relation with NER prec:0.4104, rec:0.0422, f1:0.0766
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.820, loss:893.8375
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.048, loss:859.0803
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.060, loss:860.1302
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.055, loss:851.8036
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.040, loss:820.8787
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6514, rec:0.4701, f1:0.5461
>> valid relation prec:0.3774, rec:0.0410, f1:0.0740
>> valid relation with NER prec:0.3774, rec:0.0410, f1:0.0740
g_step 1100, step 161, avg_time 2.805, loss:836.3041
g_step 1200, step 261, avg_time 1.060, loss:808.0400
g_step 1300, step 48, avg_time 1.046, loss:829.4053
g_step 1400, step 148, avg_time 1.051, loss:790.6603
g_step 1500, step 248, avg_time 1.065, loss:791.8681
>> valid entity prec:0.5930, rec:0.6293, f1:0.6106
>> valid relation prec:0.3435, rec:0.0439, f1:0.0778
>> valid relation with NER prec:0.3435, rec:0.0439, f1:0.0778
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.819, loss:757.9522
g_step 1700, step 135, avg_time 1.066, loss:745.3035
g_step 1800, step 235, avg_time 1.052, loss:764.0177
g_step 1900, step 22, avg_time 1.046, loss:769.7027
g_step 2000, step 122, avg_time 1.051, loss:727.9304
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5379, rec:0.5234, f1:0.5305
>> valid relation prec:0.2831, rec:0.0578, f1:0.0960
>> valid relation with NER prec:0.2831, rec:0.0578, f1:0.0960
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 222, avg_time 2.815, loss:719.8866
g_step 2200, step 9, avg_time 1.046, loss:718.8901
g_step 2300, step 109, avg_time 1.060, loss:711.3254
g_step 2400, step 209, avg_time 1.059, loss:704.7521
g_step 2500, step 309, avg_time 1.050, loss:695.0136
>> valid entity prec:0.5923, rec:0.5788, f1:0.5855
>> valid relation prec:0.2988, rec:0.0596, f1:0.0994
>> valid relation with NER prec:0.2988, rec:0.0596, f1:0.0994
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.799, loss:644.0589
g_step 2700, step 196, avg_time 1.048, loss:680.4859
g_step 2800, step 296, avg_time 1.044, loss:696.7305
g_step 2900, step 83, avg_time 1.045, loss:626.9388
g_step 3000, step 183, avg_time 1.052, loss:646.3012
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5493, rec:0.5187, f1:0.5336
>> valid relation prec:0.2909, rec:0.0262, f1:0.0481
>> valid relation with NER prec:0.2909, rec:0.0262, f1:0.0481
g_step 3100, step 283, avg_time 2.817, loss:646.4212
g_step 3200, step 70, avg_time 1.042, loss:639.8507
g_step 3300, step 170, avg_time 1.061, loss:626.2754
g_step 3400, step 270, avg_time 1.060, loss:643.5249
g_step 3500, step 57, avg_time 1.053, loss:585.9948
>> valid entity prec:0.5230, rec:0.5496, f1:0.5360
>> valid relation prec:0.1765, rec:0.0283, f1:0.0488
>> valid relation with NER prec:0.1765, rec:0.0283, f1:0.0488
g_step 3600, step 157, avg_time 2.818, loss:606.7709
g_step 3700, step 257, avg_time 1.052, loss:583.0715
g_step 3800, step 44, avg_time 1.053, loss:617.7171
g_step 3900, step 144, avg_time 1.068, loss:556.5570
g_step 4000, step 244, avg_time 1.056, loss:596.1935
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5619, rec:0.4406, f1:0.4939
>> valid relation prec:0.2459, rec:0.0309, f1:0.0550
>> valid relation with NER prec:0.2459, rec:0.0309, f1:0.0550
g_step 4100, step 31, avg_time 2.791, loss:567.6668
g_step 4200, step 131, avg_time 1.057, loss:564.5719
g_step 4300, step 231, avg_time 1.061, loss:556.3335
g_step 4400, step 18, avg_time 1.046, loss:570.1318
g_step 4500, step 118, avg_time 1.064, loss:524.8185
>> valid entity prec:0.5754, rec:0.4346, f1:0.4952
>> valid relation prec:0.2597, rec:0.0535, f1:0.0887
>> valid relation with NER prec:0.2597, rec:0.0535, f1:0.0887
g_step 4600, step 218, avg_time 2.801, loss:546.3225
g_step 4700, step 5, avg_time 1.050, loss:546.2661
g_step 4800, step 105, avg_time 1.060, loss:502.9090
g_step 4900, step 205, avg_time 1.057, loss:530.3454
g_step 5000, step 305, avg_time 1.061, loss:542.6876
learning rate was adjusted to 0.0008
>> valid entity prec:0.5703, rec:0.5072, f1:0.5369
>> valid relation prec:0.2002, rec:0.0350, f1:0.0597
>> valid relation with NER prec:0.2002, rec:0.0350, f1:0.0597
g_step 5100, step 92, avg_time 2.811, loss:482.4266
g_step 5200, step 192, avg_time 1.046, loss:499.6515
g_step 5300, step 292, avg_time 1.063, loss:525.8681
g_step 5400, step 79, avg_time 1.053, loss:485.4675
g_step 5500, step 179, avg_time 1.055, loss:481.7744
>> valid entity prec:0.5594, rec:0.4062, f1:0.4707
>> valid relation prec:0.2034, rec:0.0369, f1:0.0625
>> valid relation with NER prec:0.2034, rec:0.0369, f1:0.0625
g_step 5600, step 279, avg_time 2.810, loss:490.8405
g_step 5700, step 66, avg_time 1.054, loss:482.9656
g_step 5800, step 166, avg_time 1.055, loss:470.4625
g_step 5900, step 266, avg_time 1.054, loss:454.7522
g_step 6000, step 53, avg_time 1.060, loss:473.6636
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5265, rec:0.5644, f1:0.5448
>> valid relation prec:0.1617, rec:0.0414, f1:0.0659
>> valid relation with NER prec:0.1617, rec:0.0414, f1:0.0659
g_step 6100, step 153, avg_time 2.815, loss:443.0338
g_step 6200, step 253, avg_time 1.048, loss:473.9016
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:24:59 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:24:59 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-24-59_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:25:00 - WARNING - datasets.builder -   Using custom data configuration default-33a1d13017f1c20b
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-33a1d13017f1c20b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:25:03,285 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:25:03,292 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:25:03,292 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:25:03,294 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:25:03,308 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:25:03,312 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:25:03,312 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:25:03,312 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:25:03,312 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:25:03,313 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:25:03,313 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:25:03,437 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:25:06,608 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:25:06,608 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-33a1d13017f1c20b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 00:25:06 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1544427704d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:06,  1.12ba/s] 25%|██▌       | 2/8 [00:01<00:02,  2.05ba/s] 38%|███▊      | 3/8 [00:01<00:01,  2.76ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.30ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.70ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  3.97ba/s] 88%|████████▊ | 7/8 [00:02<00:00,  4.18ba/s]100%|██████████| 8/8 [00:02<00:00,  4.92ba/s]100%|██████████| 8/8 [00:02<00:00,  3.49ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.12ba/s] 40%|████      | 2/5 [00:00<00:00,  4.38ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.41ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.46ba/s]100%|██████████| 5/5 [00:01<00:00,  4.68ba/s]100%|██████████| 5/5 [00:01<00:00,  4.54ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.87ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.83ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.18ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.46ba/s]100%|██████████| 8/8 [00:00<00:00, 10.77ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.99ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.95ba/s]100%|██████████| 5/5 [00:00<00:00, 10.53ba/s]100%|██████████| 5/5 [00:00<00:00, 10.24ba/s]
[INFO|trainer.py:414] 2023-08-29 00:25:11,680 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:25:11,701 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:25:11,701 >>   Num examples = 7570
[INFO|trainer.py:1149] 2023-08-29 00:25:11,701 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:25:11,701 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:25:11,702 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:25:11,702 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:25:11,702 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:59,  3.28it/s]  0%|          | 2/590 [00:00<02:53,  3.40it/s]  1%|          | 3/590 [00:00<02:50,  3.43it/s]  1%|          | 4/590 [00:01<02:49,  3.45it/s]  1%|          | 5/590 [00:01<02:48,  3.47it/s]  1%|          | 6/590 [00:01<02:48,  3.48it/s]  1%|          | 7/590 [00:02<02:47,  3.48it/s]  1%|▏         | 8/590 [00:02<02:47,  3.48it/s]  2%|▏         | 9/590 [00:02<02:46,  3.48it/s]  2%|▏         | 10/590 [00:02<02:46,  3.48it/s]  2%|▏         | 11/590 [00:03<02:46,  3.48it/s]  2%|▏         | 12/590 [00:03<02:45,  3.48it/s]  2%|▏         | 13/590 [00:03<02:45,  3.48it/s]  2%|▏         | 14/590 [00:04<02:45,  3.49it/s]  3%|▎         | 15/590 [00:04<02:45,  3.48it/s]  3%|▎         | 16/590 [00:04<02:44,  3.48it/s]  3%|▎         | 17/590 [00:04<02:44,  3.48it/s]  3%|▎         | 18/590 [00:05<02:44,  3.48it/s]  3%|▎         | 19/590 [00:05<02:44,  3.48it/s]  3%|▎         | 20/590 [00:05<02:43,  3.48it/s]  4%|▎         | 21/590 [00:06<02:43,  3.48it/s]  4%|▎         | 22/590 [00:06<02:43,  3.48it/s]  4%|▍         | 23/590 [00:06<02:43,  3.48it/s]  4%|▍         | 24/590 [00:06<02:42,  3.48it/s]  4%|▍         | 25/590 [00:07<02:42,  3.47it/s]  4%|▍         | 26/590 [00:07<02:42,  3.47it/s]  5%|▍         | 27/590 [00:07<02:42,  3.47it/s]  5%|▍         | 28/590 [00:08<02:41,  3.47it/s]  5%|▍         | 29/590 [00:08<02:41,  3.48it/s]  5%|▌         | 30/590 [00:08<02:41,  3.48it/s]  5%|▌         | 31/590 [00:08<02:40,  3.48it/s]  5%|▌         | 32/590 [00:09<02:40,  3.48it/s]  6%|▌         | 33/590 [00:09<02:40,  3.48it/s]  6%|▌         | 34/590 [00:09<02:39,  3.48it/s]  6%|▌         | 35/590 [00:10<02:40,  3.46it/s]  6%|▌         | 36/590 [00:10<02:39,  3.47it/s]  6%|▋         | 37/590 [00:10<02:39,  3.47it/s]  6%|▋         | 38/590 [00:10<02:38,  3.47it/s]  7%|▋         | 39/590 [00:11<02:38,  3.47it/s]  7%|▋         | 40/590 [00:11<02:38,  3.47it/s]  7%|▋         | 41/590 [00:11<02:38,  3.47it/s]  7%|▋         | 42/590 [00:12<02:37,  3.48it/s]  7%|▋         | 43/590 [00:12<02:37,  3.47it/s]  7%|▋         | 44/590 [00:12<02:37,  3.48it/s]  8%|▊         | 45/590 [00:12<02:36,  3.48it/s]  8%|▊         | 46/590 [00:13<02:36,  3.48it/s]  8%|▊         | 47/590 [00:13<02:36,  3.48it/s]  8%|▊         | 48/590 [00:13<02:35,  3.48it/s]  8%|▊         | 49/590 [00:14<02:35,  3.48it/s]  8%|▊         | 50/590 [00:14<02:35,  3.48it/s]  9%|▊         | 51/590 [00:14<02:35,  3.47it/s]  9%|▉         | 52/590 [00:14<02:34,  3.47it/s]  9%|▉         | 53/590 [00:15<02:34,  3.47it/s]  9%|▉         | 54/590 [00:15<02:34,  3.47it/s]  9%|▉         | 55/590 [00:15<02:34,  3.47it/s]  9%|▉         | 56/590 [00:16<02:33,  3.47it/s] 10%|▉         | 57/590 [00:16<02:33,  3.47it/s] 10%|▉         | 58/590 [00:16<02:33,  3.47it/s] 10%|█         | 59/590 [00:16<02:32,  3.47it/s] 10%|█         | 60/590 [00:17<02:32,  3.48it/s] 10%|█         | 61/590 [00:17<02:32,  3.47it/s] 11%|█         | 62/590 [00:17<02:31,  3.48it/s] 11%|█         | 63/590 [00:18<02:31,  3.47it/s] 11%|█         | 64/590 [00:18<02:31,  3.47it/s] 11%|█         | 65/590 [00:18<02:31,  3.47it/s] 11%|█         | 66/590 [00:19<02:30,  3.47it/s] 11%|█▏        | 67/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 68/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 69/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 70/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 71/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 72/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 73/590 [00:21<02:29,  3.47it/s] 13%|█▎        | 74/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 75/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 76/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 77/590 [00:22<02:27,  3.47it/s] 13%|█▎        | 78/590 [00:22<02:27,  3.46it/s] 13%|█▎        | 79/590 [00:22<02:27,  3.47it/s] 14%|█▎        | 80/590 [00:23<02:27,  3.47it/s] 14%|█▎        | 81/590 [00:23<02:26,  3.47it/s] 14%|█▍        | 82/590 [00:23<02:26,  3.47it/s] 14%|█▍        | 83/590 [00:23<02:26,  3.47it/s] 14%|█▍        | 84/590 [00:24<02:25,  3.47it/s] 14%|█▍        | 85/590 [00:24<02:25,  3.47it/s] 15%|█▍        | 86/590 [00:24<02:25,  3.47it/s] 15%|█▍        | 87/590 [00:25<02:25,  3.47it/s] 15%|█▍        | 88/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 89/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 90/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 91/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 92/590 [00:26<02:23,  3.46it/s] 16%|█▌        | 93/590 [00:26<02:23,  3.46it/s] 16%|█▌        | 94/590 [00:27<02:23,  3.46it/s] 16%|█▌        | 95/590 [00:27<02:22,  3.46it/s] 16%|█▋        | 96/590 [00:27<02:22,  3.47it/s] 16%|█▋        | 97/590 [00:27<02:22,  3.46it/s] 17%|█▋        | 98/590 [00:28<02:22,  3.46it/s] 17%|█▋        | 99/590 [00:28<02:21,  3.47it/s] 17%|█▋        | 100/590 [00:28<02:21,  3.47it/s] 17%|█▋        | 101/590 [00:29<02:21,  3.47it/s] 17%|█▋        | 102/590 [00:29<02:20,  3.47it/s] 17%|█▋        | 103/590 [00:29<02:20,  3.47it/s] 18%|█▊        | 104/590 [00:29<02:20,  3.47it/s] 18%|█▊        | 105/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 106/590 [00:30<02:19,  3.46it/s] 18%|█▊        | 107/590 [00:30<02:19,  3.46it/s] 18%|█▊        | 108/590 [00:31<02:19,  3.46it/s] 18%|█▊        | 109/590 [00:31<02:18,  3.46it/s] 19%|█▊        | 110/590 [00:31<02:18,  3.46it/s] 19%|█▉        | 111/590 [00:31<02:18,  3.47it/s] 19%|█▉        | 112/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 113/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 114/590 [00:32<02:17,  3.46it/s] 19%|█▉        | 115/590 [00:33<02:17,  3.46it/s] 20%|█▉        | 116/590 [00:33<02:16,  3.46it/s] 20%|█▉        | 117/590 [00:33<02:16,  3.46it/s] 20%|██        | 118/590 [00:34<02:16,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 00:25:45,754 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:25:45,754 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:25:45,754 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.85it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.48it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.75it/s][A
  4%|▍         | 23/611 [00:00<00:12, 48.13it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.71it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.22it/s][A
  6%|▌         | 38/611 [00:00<00:12, 47.02it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.61it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.68it/s][A
  9%|▊         | 53/611 [00:01<00:11, 46.71it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.77it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.72it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.83it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.85it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.83it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.67it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.68it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.47it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.53it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.59it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.59it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.72it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.71it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.71it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.68it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 44.68it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 45.25it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 45.70it/s][A
 24%|██▍       | 148/611 [00:03<00:10, 45.97it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.20it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.39it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.61it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.55it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.56it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.35it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.48it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.58it/s][A
 32%|███▏      | 193/611 [00:04<00:08, 46.58it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.59it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.75it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.65it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.75it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.69it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.51it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.57it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.57it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.53it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.69it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.77it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.73it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.78it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.69it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.54it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.44it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.47it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.55it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.59it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.64it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.63it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.68it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.67it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.63it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.61it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.57it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.38it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.38it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.54it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.59it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.66it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.67it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.41it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.55it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.45it/s][A
 61%|██████    | 373/611 [00:07<00:05, 46.45it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.55it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.47it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.52it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.67it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.64it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.58it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.62it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.54it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.45it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.56it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.44it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.50it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.51it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.62it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.58it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.60it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.57it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.63it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.66it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.47it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.49it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.52it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.54it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.59it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.69it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.65it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.49it/s][A
 84%|████████▍ | 513/611 [00:10<00:02, 46.64it/s][A
 85%|████████▍ | 518/611 [00:11<00:01, 46.61it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.59it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.63it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.58it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.54it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.61it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.55it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.59it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.65it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.58it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.53it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.63it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.59it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.52it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.58it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.49it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.46it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.54it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.53it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:47<02:16,  3.46it/s]
100%|██████████| 611/611 [00:13<00:00, 46.53it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:25:58,927 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-29 00:25:58,946 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:26:01,302 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:26:01,344 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:26:01,355 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:54<49:50,  6.35s/it] 20%|██        | 120/590 [00:54<35:29,  4.53s/it] 21%|██        | 121/590 [00:55<25:29,  3.26s/it] 21%|██        | 122/590 [00:55<18:28,  2.37s/it] 21%|██        | 123/590 [00:55<13:34,  1.74s/it] 21%|██        | 124/590 [00:55<10:09,  1.31s/it] 21%|██        | 125/590 [00:56<07:46,  1.00s/it] 21%|██▏       | 126/590 [00:56<06:05,  1.27it/s] 22%|██▏       | 127/590 [00:56<04:55,  1.57it/s] 22%|██▏       | 128/590 [00:57<04:06,  1.88it/s] 22%|██▏       | 129/590 [00:57<03:31,  2.17it/s] 22%|██▏       | 130/590 [00:57<03:07,  2.45it/s] 22%|██▏       | 131/590 [00:57<02:50,  2.68it/s] 22%|██▏       | 132/590 [00:58<02:39,  2.87it/s] 23%|██▎       | 133/590 [00:58<02:30,  3.03it/s] 23%|██▎       | 134/590 [00:58<02:24,  3.15it/s] 23%|██▎       | 135/590 [00:59<02:20,  3.24it/s] 23%|██▎       | 136/590 [00:59<02:17,  3.30it/s] 23%|██▎       | 137/590 [00:59<02:15,  3.35it/s] 23%|██▎       | 138/590 [00:59<02:13,  3.38it/s] 24%|██▎       | 139/590 [01:00<02:12,  3.40it/s] 24%|██▎       | 140/590 [01:00<02:11,  3.42it/s] 24%|██▍       | 141/590 [01:00<02:10,  3.44it/s] 24%|██▍       | 142/590 [01:01<02:10,  3.45it/s] 24%|██▍       | 143/590 [01:01<02:09,  3.45it/s] 24%|██▍       | 144/590 [01:01<02:09,  3.45it/s] 25%|██▍       | 145/590 [01:02<02:08,  3.45it/s] 25%|██▍       | 146/590 [01:02<02:08,  3.45it/s] 25%|██▍       | 147/590 [01:02<02:08,  3.45it/s] 25%|██▌       | 148/590 [01:02<02:07,  3.45it/s] 25%|██▌       | 149/590 [01:03<02:07,  3.46it/s] 25%|██▌       | 150/590 [01:03<02:07,  3.46it/s] 26%|██▌       | 151/590 [01:03<02:06,  3.46it/s] 26%|██▌       | 152/590 [01:04<02:06,  3.46it/s] 26%|██▌       | 153/590 [01:04<02:06,  3.46it/s] 26%|██▌       | 154/590 [01:04<02:06,  3.45it/s] 26%|██▋       | 155/590 [01:04<02:06,  3.44it/s] 26%|██▋       | 156/590 [01:05<02:05,  3.45it/s] 27%|██▋       | 157/590 [01:05<02:05,  3.45it/s] 27%|██▋       | 158/590 [01:05<02:05,  3.46it/s] 27%|██▋       | 159/590 [01:06<02:04,  3.46it/s] 27%|██▋       | 160/590 [01:06<02:04,  3.46it/s] 27%|██▋       | 161/590 [01:06<02:04,  3.46it/s] 27%|██▋       | 162/590 [01:06<02:03,  3.46it/s] 28%|██▊       | 163/590 [01:07<02:03,  3.46it/s] 28%|██▊       | 164/590 [01:07<02:03,  3.46it/s] 28%|██▊       | 165/590 [01:07<02:03,  3.45it/s] 28%|██▊       | 166/590 [01:08<02:02,  3.45it/s] 28%|██▊       | 167/590 [01:08<02:02,  3.45it/s] 28%|██▊       | 168/590 [01:08<02:02,  3.45it/s] 29%|██▊       | 169/590 [01:08<02:01,  3.45it/s] 29%|██▉       | 170/590 [01:09<02:01,  3.45it/s] 29%|██▉       | 171/590 [01:09<02:01,  3.45it/s] 29%|██▉       | 172/590 [01:09<02:01,  3.45it/s] 29%|██▉       | 173/590 [01:10<02:00,  3.45it/s] 29%|██▉       | 174/590 [01:10<02:00,  3.46it/s] 30%|██▉       | 175/590 [01:10<01:59,  3.46it/s] 30%|██▉       | 176/590 [01:10<02:00,  3.44it/s] 30%|███       | 177/590 [01:11<01:59,  3.45it/s] 30%|███       | 178/590 [01:11<01:59,  3.45it/s] 30%|███       | 179/590 [01:11<01:58,  3.45it/s] 31%|███       | 180/590 [01:12<01:58,  3.45it/s] 31%|███       | 181/590 [01:12<01:58,  3.46it/s] 31%|███       | 182/590 [01:12<01:57,  3.46it/s] 31%|███       | 183/590 [01:13<01:57,  3.46it/s] 31%|███       | 184/590 [01:13<01:57,  3.46it/s] 31%|███▏      | 185/590 [01:13<01:57,  3.46it/s] 32%|███▏      | 186/590 [01:13<01:56,  3.46it/s] 32%|███▏      | 187/590 [01:14<01:56,  3.45it/s] 32%|███▏      | 188/590 [01:14<01:56,  3.45it/s] 32%|███▏      | 189/590 [01:14<01:56,  3.45it/s] 32%|███▏      | 190/590 [01:15<01:55,  3.46it/s] 32%|███▏      | 191/590 [01:15<01:55,  3.46it/s] 33%|███▎      | 192/590 [01:15<01:55,  3.46it/s] 33%|███▎      | 193/590 [01:15<01:54,  3.46it/s] 33%|███▎      | 194/590 [01:16<01:54,  3.46it/s] 33%|███▎      | 195/590 [01:16<01:54,  3.46it/s] 33%|███▎      | 196/590 [01:16<01:53,  3.46it/s] 33%|███▎      | 197/590 [01:17<01:53,  3.46it/s] 34%|███▎      | 198/590 [01:17<01:54,  3.43it/s] 34%|███▎      | 199/590 [01:17<01:53,  3.44it/s] 34%|███▍      | 200/590 [01:17<01:53,  3.45it/s] 34%|███▍      | 201/590 [01:18<01:52,  3.45it/s] 34%|███▍      | 202/590 [01:18<01:52,  3.46it/s] 34%|███▍      | 203/590 [01:18<01:51,  3.46it/s] 35%|███▍      | 204/590 [01:19<01:51,  3.46it/s] 35%|███▍      | 205/590 [01:19<01:51,  3.46it/s] 35%|███▍      | 206/590 [01:19<01:50,  3.46it/s] 35%|███▌      | 207/590 [01:19<01:50,  3.46it/s] 35%|███▌      | 208/590 [01:20<01:50,  3.46it/s] 35%|███▌      | 209/590 [01:20<01:50,  3.46it/s] 36%|███▌      | 210/590 [01:20<01:49,  3.46it/s] 36%|███▌      | 211/590 [01:21<01:49,  3.46it/s] 36%|███▌      | 212/590 [01:21<01:49,  3.46it/s] 36%|███▌      | 213/590 [01:21<01:49,  3.45it/s] 36%|███▋      | 214/590 [01:21<01:48,  3.45it/s] 36%|███▋      | 215/590 [01:22<01:48,  3.46it/s] 37%|███▋      | 216/590 [01:22<01:48,  3.45it/s] 37%|███▋      | 217/590 [01:22<01:47,  3.46it/s] 37%|███▋      | 218/590 [01:23<01:47,  3.46it/s] 37%|███▋      | 219/590 [01:23<01:47,  3.46it/s] 37%|███▋      | 220/590 [01:23<01:46,  3.46it/s] 37%|███▋      | 221/590 [01:24<01:46,  3.46it/s] 38%|███▊      | 222/590 [01:24<01:46,  3.46it/s] 38%|███▊      | 223/590 [01:24<01:46,  3.45it/s] 38%|███▊      | 224/590 [01:24<01:45,  3.46it/s] 38%|███▊      | 225/590 [01:25<01:45,  3.46it/s] 38%|███▊      | 226/590 [01:25<01:45,  3.46it/s] 38%|███▊      | 227/590 [01:25<01:45,  3.45it/s] 39%|███▊      | 228/590 [01:26<01:44,  3.45it/s] 39%|███▉      | 229/590 [01:26<01:44,  3.45it/s] 39%|███▉      | 230/590 [01:26<01:44,  3.46it/s] 39%|███▉      | 231/590 [01:26<01:43,  3.46it/s] 39%|███▉      | 232/590 [01:27<01:43,  3.45it/s] 39%|███▉      | 233/590 [01:27<01:43,  3.45it/s] 40%|███▉      | 234/590 [01:27<01:43,  3.46it/s] 40%|███▉      | 235/590 [01:28<01:42,  3.46it/s] 40%|████      | 236/590 [01:28<01:42,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:26:40,096 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:26:40,096 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:26:40,096 >>   Batch size = 8
{'eval_loss': 0.9041016697883606, 'eval_runtime': 13.1533, 'eval_samples_per_second': 371.162, 'eval_steps_per_second': 46.452, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.18it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.37it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.72it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.91it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.42it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.09it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.79it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.40it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.38it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.38it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.53it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.61it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.56it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.44it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.40it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.34it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 45.94it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.15it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.18it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.36it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.52it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.60it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.56it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.45it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.42it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.32it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.24it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.38it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.32it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.42it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.53it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.61it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.57it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.43it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.30it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.27it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.27it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.33it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.41it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.36it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.45it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.39it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.41it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.38it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.31it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.30it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.36it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.41it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.52it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.47it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.41it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.39it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.35it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.36it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.27it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.32it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.33it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.37it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.42it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.37it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.37it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.21it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.25it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.36it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.31it/s][A
 55%|█████▍    | 333/611 [00:07<00:06, 46.02it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.28it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.29it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.38it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.41it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.35it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.31it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.15it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.31it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.36it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.42it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.41it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 44.54it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 45.35it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 45.64it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 45.89it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 45.99it/s][A
 68%|██████▊   | 418/611 [00:09<00:04, 46.21it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.31it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.27it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.37it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.15it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.29it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.39it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.08it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.29it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.32it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.47it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.53it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.49it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.22it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.32it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.40it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.39it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.37it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.28it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.39it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.44it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.39it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.35it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.38it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.24it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.35it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.31it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.34it/s][A
 91%|█████████▏| 558/611 [00:12<00:01, 46.34it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.40it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.29it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.35it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.28it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.31it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.29it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.34it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.23it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.30it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.41it/s][A                                                 
                                                 [A 40%|████      | 236/590 [01:41<01:42,  3.45it/s]
100%|██████████| 611/611 [00:13<00:00, 46.41it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:26:53,338 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-29 00:26:53,360 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:26:55,666 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:26:55,689 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:26:55,696 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:49<37:59,  6.46s/it] 40%|████      | 238/590 [01:49<27:02,  4.61s/it] 41%|████      | 239/590 [01:49<19:23,  3.31s/it] 41%|████      | 240/590 [01:50<14:02,  2.41s/it] 41%|████      | 241/590 [01:50<10:18,  1.77s/it] 41%|████      | 242/590 [01:50<07:41,  1.33s/it] 41%|████      | 243/590 [01:50<05:52,  1.02s/it] 41%|████▏     | 244/590 [01:51<04:35,  1.25it/s] 42%|████▏     | 245/590 [01:51<03:42,  1.55it/s] 42%|████▏     | 246/590 [01:51<03:05,  1.86it/s] 42%|████▏     | 247/590 [01:52<02:38,  2.16it/s] 42%|████▏     | 248/590 [01:52<02:20,  2.43it/s] 42%|████▏     | 249/590 [01:52<02:07,  2.66it/s] 42%|████▏     | 250/590 [01:52<01:58,  2.86it/s] 43%|████▎     | 251/590 [01:53<01:52,  3.02it/s] 43%|████▎     | 252/590 [01:53<01:47,  3.14it/s] 43%|████▎     | 253/590 [01:53<01:44,  3.23it/s] 43%|████▎     | 254/590 [01:54<01:41,  3.30it/s] 43%|████▎     | 255/590 [01:54<01:40,  3.34it/s] 43%|████▎     | 256/590 [01:54<01:38,  3.38it/s] 44%|████▎     | 257/590 [01:54<01:37,  3.40it/s] 44%|████▎     | 258/590 [01:55<01:37,  3.41it/s] 44%|████▍     | 259/590 [01:55<01:36,  3.42it/s] 44%|████▍     | 260/590 [01:55<01:36,  3.43it/s] 44%|████▍     | 261/590 [01:56<01:35,  3.43it/s] 44%|████▍     | 262/590 [01:56<01:38,  3.32it/s] 45%|████▍     | 263/590 [01:56<01:37,  3.36it/s] 45%|████▍     | 264/590 [01:57<01:36,  3.38it/s] 45%|████▍     | 265/590 [01:57<01:35,  3.40it/s] 45%|████▌     | 266/590 [01:57<01:34,  3.41it/s] 45%|████▌     | 267/590 [01:57<01:34,  3.42it/s] 45%|████▌     | 268/590 [01:58<01:33,  3.43it/s] 46%|████▌     | 269/590 [01:58<01:33,  3.43it/s] 46%|████▌     | 270/590 [01:58<01:33,  3.44it/s] 46%|████▌     | 271/590 [01:59<01:32,  3.45it/s] 46%|████▌     | 272/590 [01:59<01:32,  3.45it/s] 46%|████▋     | 273/590 [01:59<01:35,  3.31it/s] 46%|████▋     | 274/590 [01:59<01:34,  3.36it/s] 47%|████▋     | 275/590 [02:00<01:32,  3.39it/s] 47%|████▋     | 276/590 [02:00<01:32,  3.41it/s] 47%|████▋     | 277/590 [02:00<01:31,  3.42it/s] 47%|████▋     | 278/590 [02:01<01:30,  3.44it/s] 47%|████▋     | 279/590 [02:01<01:30,  3.45it/s] 47%|████▋     | 280/590 [02:01<01:29,  3.45it/s] 48%|████▊     | 281/590 [02:02<01:29,  3.46it/s] 48%|████▊     | 282/590 [02:02<01:29,  3.46it/s] 48%|████▊     | 283/590 [02:02<01:28,  3.46it/s] 48%|████▊     | 284/590 [02:02<01:28,  3.45it/s] 48%|████▊     | 285/590 [02:03<01:28,  3.45it/s] 48%|████▊     | 286/590 [02:03<01:27,  3.46it/s] 49%|████▊     | 287/590 [02:03<01:27,  3.46it/s] 49%|████▉     | 288/590 [02:04<01:27,  3.46it/s] 49%|████▉     | 289/590 [02:04<01:26,  3.46it/s] 49%|████▉     | 290/590 [02:04<01:26,  3.46it/s] 49%|████▉     | 291/590 [02:04<01:26,  3.47it/s] 49%|████▉     | 292/590 [02:05<01:26,  3.46it/s] 50%|████▉     | 293/590 [02:05<01:25,  3.46it/s] 50%|████▉     | 294/590 [02:05<01:25,  3.46it/s] 50%|█████     | 295/590 [02:06<01:25,  3.45it/s] 50%|█████     | 296/590 [02:06<01:25,  3.46it/s] 50%|█████     | 297/590 [02:06<01:24,  3.46it/s] 51%|█████     | 298/590 [02:06<01:24,  3.46it/s] 51%|█████     | 299/590 [02:07<01:24,  3.46it/s] 51%|█████     | 300/590 [02:07<01:23,  3.46it/s] 51%|█████     | 301/590 [02:07<01:23,  3.46it/s] 51%|█████     | 302/590 [02:08<01:23,  3.46it/s] 51%|█████▏    | 303/590 [02:08<01:23,  3.45it/s] 52%|█████▏    | 304/590 [02:08<01:22,  3.46it/s] 52%|█████▏    | 305/590 [02:08<01:22,  3.46it/s] 52%|█████▏    | 306/590 [02:09<01:22,  3.44it/s] 52%|█████▏    | 307/590 [02:09<01:22,  3.45it/s] 52%|█████▏    | 308/590 [02:09<01:21,  3.46it/s] 52%|█████▏    | 309/590 [02:10<01:21,  3.46it/s] 53%|█████▎    | 310/590 [02:10<01:20,  3.46it/s] 53%|█████▎    | 311/590 [02:10<01:20,  3.46it/s] 53%|█████▎    | 312/590 [02:10<01:20,  3.47it/s] 53%|█████▎    | 313/590 [02:11<01:19,  3.47it/s] 53%|█████▎    | 314/590 [02:11<01:19,  3.47it/s] 53%|█████▎    | 315/590 [02:11<01:19,  3.47it/s] 54%|█████▎    | 316/590 [02:12<01:19,  3.47it/s] 54%|█████▎    | 317/590 [02:12<01:19,  3.45it/s] 54%|█████▍    | 318/590 [02:12<01:18,  3.45it/s] 54%|█████▍    | 319/590 [02:13<01:18,  3.45it/s] 54%|█████▍    | 320/590 [02:13<01:18,  3.45it/s] 54%|█████▍    | 321/590 [02:13<01:17,  3.45it/s] 55%|█████▍    | 322/590 [02:13<01:17,  3.45it/s] 55%|█████▍    | 323/590 [02:14<01:17,  3.44it/s] 55%|█████▍    | 324/590 [02:14<01:17,  3.44it/s] 55%|█████▌    | 325/590 [02:14<01:16,  3.45it/s] 55%|█████▌    | 326/590 [02:15<01:16,  3.45it/s] 55%|█████▌    | 327/590 [02:15<01:16,  3.45it/s] 56%|█████▌    | 328/590 [02:15<01:16,  3.44it/s] 56%|█████▌    | 329/590 [02:15<01:15,  3.44it/s] 56%|█████▌    | 330/590 [02:16<01:15,  3.44it/s] 56%|█████▌    | 331/590 [02:16<01:15,  3.45it/s] 56%|█████▋    | 332/590 [02:16<01:14,  3.45it/s] 56%|█████▋    | 333/590 [02:17<01:14,  3.45it/s] 57%|█████▋    | 334/590 [02:17<01:14,  3.44it/s] 57%|█████▋    | 335/590 [02:17<01:13,  3.45it/s] 57%|█████▋    | 336/590 [02:17<01:13,  3.45it/s] 57%|█████▋    | 337/590 [02:18<01:13,  3.45it/s] 57%|█████▋    | 338/590 [02:18<01:13,  3.45it/s] 57%|█████▋    | 339/590 [02:18<01:13,  3.44it/s] 58%|█████▊    | 340/590 [02:19<01:12,  3.44it/s] 58%|█████▊    | 341/590 [02:19<01:12,  3.44it/s] 58%|█████▊    | 342/590 [02:19<01:12,  3.44it/s] 58%|█████▊    | 343/590 [02:19<01:11,  3.45it/s] 58%|█████▊    | 344/590 [02:20<01:11,  3.45it/s] 58%|█████▊    | 345/590 [02:20<01:11,  3.44it/s] 59%|█████▊    | 346/590 [02:20<01:10,  3.45it/s] 59%|█████▉    | 347/590 [02:21<01:10,  3.44it/s] 59%|█████▉    | 348/590 [02:21<01:10,  3.45it/s] 59%|█████▉    | 349/590 [02:21<01:09,  3.45it/s] 59%|█████▉    | 350/590 [02:21<01:09,  3.44it/s] 59%|█████▉    | 351/590 [02:22<01:09,  3.44it/s] 60%|█████▉    | 352/590 [02:22<01:09,  3.45it/s] 60%|█████▉    | 353/590 [02:22<01:08,  3.45it/s] 60%|██████    | 354/590 [02:23<01:08,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:27:34,908 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:27:34,909 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:27:34,909 >>   Batch size = 8
{'eval_loss': 0.9097402691841125, 'eval_runtime': 13.2169, 'eval_samples_per_second': 369.377, 'eval_steps_per_second': 46.229, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.08it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.26it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.55it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.82it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.42it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.06it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.74it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.39it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.36it/s][A
  9%|▊         | 53/611 [00:01<00:11, 46.52it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.57it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.55it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.47it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.43it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.47it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.39it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.31it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.26it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.32it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.46it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.47it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.46it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.42it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.37it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.34it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.21it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.24it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.32it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.36it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.47it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.53it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.47it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.40it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.32it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.35it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.41it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.25it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.26it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.39it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.42it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.43it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.32it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.20it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.13it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.23it/s][A
 38%|███▊      | 233/611 [00:05<00:08, 46.15it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.26it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.36it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.39it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.43it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.37it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.26it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.33it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.36it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.37it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.33it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.30it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.40it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.43it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.40it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.32it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.29it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.26it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.31it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.37it/s][A
 55%|█████▍    | 333/611 [00:07<00:06, 46.25it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.32it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.37it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.31it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.33it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.28it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.30it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.41it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.40it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.34it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.25it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.24it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.26it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.23it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.29it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.34it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.35it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.45it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.44it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.26it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.23it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.06it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.09it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.16it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.28it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.35it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.35it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.38it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.25it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.26it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.21it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.12it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.15it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.01it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.04it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.00it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.02it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 45.86it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 45.96it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 45.98it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.06it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.22it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.28it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.35it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.35it/s][A
 91%|█████████▏| 558/611 [00:12<00:01, 46.37it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.34it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.25it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.24it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.23it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.26it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.25it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.34it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.39it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.43it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.38it/s][A                                                 
                                                 [A 60%|██████    | 354/590 [02:36<01:08,  3.45it/s]
100%|██████████| 611/611 [00:13<00:00, 46.38it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:27:48,142 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-29 00:27:48,156 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:27:50,397 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:27:50,412 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:27:50,428 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:44<25:25,  6.49s/it] 60%|██████    | 356/590 [02:44<18:03,  4.63s/it] 61%|██████    | 357/590 [02:44<12:55,  3.33s/it] 61%|██████    | 358/590 [02:44<09:20,  2.42s/it] 61%|██████    | 359/590 [02:45<06:50,  1.78s/it] 61%|██████    | 360/590 [02:45<05:06,  1.33s/it] 61%|██████    | 361/590 [02:45<03:53,  1.02s/it] 61%|██████▏   | 362/590 [02:46<03:02,  1.25it/s] 62%|██████▏   | 363/590 [02:46<02:26,  1.55it/s] 62%|██████▏   | 364/590 [02:46<02:01,  1.85it/s] 62%|██████▏   | 365/590 [02:47<01:44,  2.16it/s] 62%|██████▏   | 366/590 [02:47<01:32,  2.43it/s] 62%|██████▏   | 367/590 [02:47<01:23,  2.67it/s] 62%|██████▏   | 368/590 [02:47<01:17,  2.86it/s] 63%|██████▎   | 369/590 [02:48<01:13,  3.02it/s] 63%|██████▎   | 370/590 [02:48<01:10,  3.14it/s] 63%|██████▎   | 371/590 [02:48<01:07,  3.23it/s] 63%|██████▎   | 372/590 [02:49<01:06,  3.30it/s] 63%|██████▎   | 373/590 [02:49<01:04,  3.34it/s] 63%|██████▎   | 374/590 [02:49<01:03,  3.38it/s] 64%|██████▎   | 375/590 [02:49<01:03,  3.40it/s] 64%|██████▎   | 376/590 [02:50<01:02,  3.42it/s] 64%|██████▍   | 377/590 [02:50<01:02,  3.42it/s] 64%|██████▍   | 378/590 [02:50<01:02,  3.42it/s] 64%|██████▍   | 379/590 [02:51<01:01,  3.43it/s] 64%|██████▍   | 380/590 [02:51<01:01,  3.44it/s] 65%|██████▍   | 381/590 [02:51<01:00,  3.45it/s] 65%|██████▍   | 382/590 [02:51<01:00,  3.45it/s] 65%|██████▍   | 383/590 [02:52<00:59,  3.45it/s] 65%|██████▌   | 384/590 [02:52<00:59,  3.46it/s] 65%|██████▌   | 385/590 [02:52<00:59,  3.46it/s] 65%|██████▌   | 386/590 [02:53<00:58,  3.46it/s] 66%|██████▌   | 387/590 [02:53<00:58,  3.46it/s] 66%|██████▌   | 388/590 [02:53<00:58,  3.46it/s] 66%|██████▌   | 389/590 [02:53<01:00,  3.34it/s] 66%|██████▌   | 390/590 [02:54<00:59,  3.38it/s] 66%|██████▋   | 391/590 [02:54<00:58,  3.40it/s] 66%|██████▋   | 392/590 [02:54<00:58,  3.41it/s] 67%|██████▋   | 393/590 [02:55<00:57,  3.42it/s] 67%|██████▋   | 394/590 [02:55<00:57,  3.44it/s] 67%|██████▋   | 395/590 [02:55<00:56,  3.44it/s] 67%|██████▋   | 396/590 [02:56<00:56,  3.45it/s] 67%|██████▋   | 397/590 [02:56<00:55,  3.45it/s] 67%|██████▋   | 398/590 [02:56<00:55,  3.45it/s] 68%|██████▊   | 399/590 [02:56<00:55,  3.46it/s] 68%|██████▊   | 400/590 [02:57<00:54,  3.46it/s] 68%|██████▊   | 401/590 [02:57<00:54,  3.45it/s] 68%|██████▊   | 402/590 [02:57<00:54,  3.46it/s] 68%|██████▊   | 403/590 [02:58<00:54,  3.45it/s] 68%|██████▊   | 404/590 [02:58<00:53,  3.45it/s] 69%|██████▊   | 405/590 [02:58<00:53,  3.45it/s] 69%|██████▉   | 406/590 [02:58<00:53,  3.46it/s] 69%|██████▉   | 407/590 [02:59<00:52,  3.46it/s] 69%|██████▉   | 408/590 [02:59<00:52,  3.46it/s] 69%|██████▉   | 409/590 [02:59<00:52,  3.46it/s] 69%|██████▉   | 410/590 [03:00<00:52,  3.46it/s] 70%|██████▉   | 411/590 [03:00<00:51,  3.46it/s] 70%|██████▉   | 412/590 [03:00<00:51,  3.46it/s] 70%|███████   | 413/590 [03:00<00:51,  3.46it/s] 70%|███████   | 414/590 [03:01<00:51,  3.44it/s] 70%|███████   | 415/590 [03:01<00:50,  3.45it/s] 71%|███████   | 416/590 [03:01<00:50,  3.45it/s] 71%|███████   | 417/590 [03:02<00:50,  3.45it/s] 71%|███████   | 418/590 [03:02<00:49,  3.45it/s] 71%|███████   | 419/590 [03:02<00:49,  3.45it/s] 71%|███████   | 420/590 [03:02<00:49,  3.46it/s] 71%|███████▏  | 421/590 [03:03<00:48,  3.46it/s] 72%|███████▏  | 422/590 [03:03<00:48,  3.46it/s] 72%|███████▏  | 423/590 [03:03<00:48,  3.46it/s] 72%|███████▏  | 424/590 [03:04<00:47,  3.46it/s] 72%|███████▏  | 425/590 [03:04<00:47,  3.45it/s] 72%|███████▏  | 426/590 [03:04<00:47,  3.45it/s] 72%|███████▏  | 427/590 [03:04<00:47,  3.45it/s] 73%|███████▎  | 428/590 [03:05<00:46,  3.46it/s] 73%|███████▎  | 429/590 [03:05<00:46,  3.46it/s] 73%|███████▎  | 430/590 [03:05<00:46,  3.45it/s] 73%|███████▎  | 431/590 [03:06<00:46,  3.46it/s] 73%|███████▎  | 432/590 [03:06<00:45,  3.45it/s] 73%|███████▎  | 433/590 [03:06<00:45,  3.45it/s] 74%|███████▎  | 434/590 [03:07<00:45,  3.45it/s] 74%|███████▎  | 435/590 [03:07<00:44,  3.45it/s] 74%|███████▍  | 436/590 [03:07<00:44,  3.44it/s] 74%|███████▍  | 437/590 [03:07<00:44,  3.45it/s] 74%|███████▍  | 438/590 [03:08<00:44,  3.45it/s] 74%|███████▍  | 439/590 [03:08<00:43,  3.45it/s] 75%|███████▍  | 440/590 [03:08<00:43,  3.45it/s] 75%|███████▍  | 441/590 [03:09<00:43,  3.45it/s] 75%|███████▍  | 442/590 [03:09<00:42,  3.45it/s] 75%|███████▌  | 443/590 [03:09<00:42,  3.45it/s] 75%|███████▌  | 444/590 [03:09<00:42,  3.45it/s] 75%|███████▌  | 445/590 [03:10<00:42,  3.45it/s] 76%|███████▌  | 446/590 [03:10<00:41,  3.45it/s] 76%|███████▌  | 447/590 [03:10<00:41,  3.44it/s] 76%|███████▌  | 448/590 [03:11<00:41,  3.44it/s] 76%|███████▌  | 449/590 [03:11<00:40,  3.45it/s] 76%|███████▋  | 450/590 [03:11<00:40,  3.45it/s] 76%|███████▋  | 451/590 [03:11<00:40,  3.45it/s] 77%|███████▋  | 452/590 [03:12<00:40,  3.45it/s] 77%|███████▋  | 453/590 [03:12<00:39,  3.45it/s] 77%|███████▋  | 454/590 [03:12<00:39,  3.45it/s] 77%|███████▋  | 455/590 [03:13<00:39,  3.45it/s] 77%|███████▋  | 456/590 [03:13<00:38,  3.45it/s] 77%|███████▋  | 457/590 [03:13<00:38,  3.45it/s] 78%|███████▊  | 458/590 [03:13<00:38,  3.44it/s] 78%|███████▊  | 459/590 [03:14<00:37,  3.45it/s] 78%|███████▊  | 460/590 [03:14<00:37,  3.45it/s] 78%|███████▊  | 461/590 [03:14<00:37,  3.45it/s] 78%|███████▊  | 462/590 [03:15<00:37,  3.45it/s] 78%|███████▊  | 463/590 [03:15<00:36,  3.45it/s] 79%|███████▊  | 464/590 [03:15<00:36,  3.45it/s] 79%|███████▉  | 465/590 [03:16<00:36,  3.45it/s] 79%|███████▉  | 466/590 [03:16<00:35,  3.45it/s] 79%|███████▉  | 467/590 [03:16<00:35,  3.45it/s] 79%|███████▉  | 468/590 [03:16<00:35,  3.45it/s] 79%|███████▉  | 469/590 [03:17<00:35,  3.44it/s] 80%|███████▉  | 470/590 [03:17<00:34,  3.44it/s] 80%|███████▉  | 471/590 [03:17<00:34,  3.45it/s] 80%|████████  | 472/590 [03:18<00:34,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:28:29,782 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:28:29,782 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:28:29,782 >>   Batch size = 8
{'eval_loss': 0.9171572923660278, 'eval_runtime': 13.2193, 'eval_samples_per_second': 369.308, 'eval_steps_per_second': 46.22, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.11it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.34it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.54it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.90it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.41it/s][A
  5%|▌         | 33/611 [00:00<00:12, 46.87it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.54it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.37it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.42it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.37it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.33it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.49it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.49it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.49it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.39it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.28it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.12it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.17it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.36it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.40it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.47it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.38it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.41it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.37it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.14it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.10it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.16it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.30it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.40it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.41it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.45it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.31it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.34it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.22it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.10it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.17it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.26it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.36it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.33it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.38it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.44it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.31it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.29it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.17it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.23it/s][A
 38%|███▊      | 233/611 [00:05<00:08, 46.30it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.39it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.42it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.44it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.45it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.41it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.27it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.17it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.18it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.25it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.37it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.39it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.41it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.13it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.35it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.20it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.24it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.22it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.31it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.33it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.40it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.40it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.34it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.23it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.24it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.29it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.20it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.24it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.34it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.36it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.24it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.24it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.24it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.20it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.17it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.16it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.22it/s][A
 68%|██████▊   | 418/611 [00:09<00:04, 46.38it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.32it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.36it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.27it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.26it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.26it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.23it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.24it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.37it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.35it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.39it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.32it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.26it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.25it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.21it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.17it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.21it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.33it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.37it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.37it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.32it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.19it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.25it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.24it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.21it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.26it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.32it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.38it/s][A
 91%|█████████▏| 558/611 [00:12<00:01, 46.45it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.40it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.38it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.27it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.20it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.17it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.20it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.36it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.39it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.38it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.35it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:31<00:34,  3.45it/s]
100%|██████████| 611/611 [00:13<00:00, 46.35it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:28:43,025 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-29 00:28:43,042 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:28:45,416 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:28:45,434 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:28:45,442 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:38<12:28,  6.39s/it] 80%|████████  | 474/590 [03:38<08:49,  4.56s/it] 81%|████████  | 475/590 [03:39<06:17,  3.28s/it] 81%|████████  | 476/590 [03:39<04:31,  2.38s/it] 81%|████████  | 477/590 [03:39<03:18,  1.76s/it] 81%|████████  | 478/590 [03:40<02:27,  1.32s/it] 81%|████████  | 479/590 [03:40<01:51,  1.01s/it] 81%|████████▏ | 480/590 [03:40<01:27,  1.26it/s] 82%|████████▏ | 481/590 [03:40<01:09,  1.56it/s] 82%|████████▏ | 482/590 [03:41<00:57,  1.87it/s] 82%|████████▏ | 483/590 [03:41<00:49,  2.17it/s] 82%|████████▏ | 484/590 [03:41<00:43,  2.44it/s] 82%|████████▏ | 485/590 [03:42<00:40,  2.62it/s] 82%|████████▏ | 486/590 [03:42<00:36,  2.83it/s] 83%|████████▎ | 487/590 [03:42<00:34,  2.99it/s] 83%|████████▎ | 488/590 [03:43<00:32,  3.11it/s] 83%|████████▎ | 489/590 [03:43<00:31,  3.21it/s] 83%|████████▎ | 490/590 [03:43<00:30,  3.27it/s] 83%|████████▎ | 491/590 [03:43<00:29,  3.32it/s] 83%|████████▎ | 492/590 [03:44<00:29,  3.37it/s] 84%|████████▎ | 493/590 [03:44<00:28,  3.39it/s] 84%|████████▎ | 494/590 [03:44<00:28,  3.42it/s] 84%|████████▍ | 495/590 [03:45<00:27,  3.43it/s] 84%|████████▍ | 496/590 [03:45<00:28,  3.34it/s] 84%|████████▍ | 497/590 [03:45<00:27,  3.38it/s] 84%|████████▍ | 498/590 [03:45<00:27,  3.41it/s] 85%|████████▍ | 499/590 [03:46<00:26,  3.42it/s] 85%|████████▍ | 500/590 [03:46<00:26,  3.43it/s]                                                  85%|████████▍ | 500/590 [03:46<00:26,  3.43it/s] 85%|████████▍ | 501/590 [03:46<00:25,  3.44it/s] 85%|████████▌ | 502/590 [03:47<00:25,  3.45it/s] 85%|████████▌ | 503/590 [03:47<00:25,  3.45it/s] 85%|████████▌ | 504/590 [03:47<00:24,  3.46it/s] 86%|████████▌ | 505/590 [03:47<00:24,  3.46it/s] 86%|████████▌ | 506/590 [03:48<00:24,  3.46it/s] 86%|████████▌ | 507/590 [03:48<00:24,  3.45it/s] 86%|████████▌ | 508/590 [03:48<00:23,  3.46it/s] 86%|████████▋ | 509/590 [03:49<00:23,  3.46it/s] 86%|████████▋ | 510/590 [03:49<00:23,  3.46it/s] 87%|████████▋ | 511/590 [03:49<00:22,  3.46it/s] 87%|████████▋ | 512/590 [03:50<00:22,  3.46it/s] 87%|████████▋ | 513/590 [03:50<00:22,  3.46it/s] 87%|████████▋ | 514/590 [03:50<00:21,  3.46it/s] 87%|████████▋ | 515/590 [03:50<00:21,  3.46it/s] 87%|████████▋ | 516/590 [03:51<00:21,  3.46it/s] 88%|████████▊ | 517/590 [03:51<00:21,  3.46it/s] 88%|████████▊ | 518/590 [03:51<00:20,  3.46it/s] 88%|████████▊ | 519/590 [03:52<00:20,  3.46it/s] 88%|████████▊ | 520/590 [03:52<00:20,  3.46it/s] 88%|████████▊ | 521/590 [03:52<00:19,  3.46it/s] 88%|████████▊ | 522/590 [03:52<00:19,  3.46it/s] 89%|████████▊ | 523/590 [03:53<00:19,  3.46it/s] 89%|████████▉ | 524/590 [03:53<00:19,  3.46it/s] 89%|████████▉ | 525/590 [03:53<00:18,  3.46it/s] 89%|████████▉ | 526/590 [03:54<00:18,  3.47it/s] 89%|████████▉ | 527/590 [03:54<00:18,  3.46it/s] 89%|████████▉ | 528/590 [03:54<00:17,  3.46it/s] 90%|████████▉ | 529/590 [03:54<00:17,  3.45it/s] 90%|████████▉ | 530/590 [03:55<00:17,  3.46it/s] 90%|█████████ | 531/590 [03:55<00:17,  3.46it/s] 90%|█████████ | 532/590 [03:55<00:16,  3.46it/s] 90%|█████████ | 533/590 [03:56<00:16,  3.46it/s] 91%|█████████ | 534/590 [03:56<00:16,  3.47it/s] 91%|█████████ | 535/590 [03:56<00:15,  3.46it/s] 91%|█████████ | 536/590 [03:56<00:15,  3.46it/s] 91%|█████████ | 537/590 [03:57<00:15,  3.46it/s] 91%|█████████ | 538/590 [03:57<00:15,  3.46it/s] 91%|█████████▏| 539/590 [03:57<00:14,  3.46it/s] 92%|█████████▏| 540/590 [03:58<00:14,  3.45it/s] 92%|█████████▏| 541/590 [03:58<00:14,  3.45it/s] 92%|█████████▏| 542/590 [03:58<00:13,  3.45it/s] 92%|█████████▏| 543/590 [03:58<00:13,  3.45it/s] 92%|█████████▏| 544/590 [03:59<00:13,  3.45it/s] 92%|█████████▏| 545/590 [03:59<00:13,  3.45it/s] 93%|█████████▎| 546/590 [03:59<00:12,  3.46it/s] 93%|█████████▎| 547/590 [04:00<00:12,  3.45it/s] 93%|█████████▎| 548/590 [04:00<00:12,  3.45it/s] 93%|█████████▎| 549/590 [04:00<00:11,  3.46it/s] 93%|█████████▎| 550/590 [04:00<00:11,  3.46it/s] 93%|█████████▎| 551/590 [04:01<00:11,  3.45it/s] 94%|█████████▎| 552/590 [04:01<00:11,  3.45it/s] 94%|█████████▎| 553/590 [04:01<00:10,  3.45it/s] 94%|█████████▍| 554/590 [04:02<00:10,  3.45it/s] 94%|█████████▍| 555/590 [04:02<00:10,  3.45it/s] 94%|█████████▍| 556/590 [04:02<00:09,  3.45it/s] 94%|█████████▍| 557/590 [04:03<00:09,  3.46it/s] 95%|█████████▍| 558/590 [04:03<00:09,  3.46it/s] 95%|█████████▍| 559/590 [04:03<00:08,  3.46it/s] 95%|█████████▍| 560/590 [04:03<00:08,  3.46it/s] 95%|█████████▌| 561/590 [04:04<00:08,  3.46it/s] 95%|█████████▌| 562/590 [04:04<00:08,  3.43it/s] 95%|█████████▌| 563/590 [04:04<00:07,  3.44it/s] 96%|█████████▌| 564/590 [04:05<00:07,  3.44it/s] 96%|█████████▌| 565/590 [04:05<00:07,  3.44it/s] 96%|█████████▌| 566/590 [04:05<00:06,  3.45it/s] 96%|█████████▌| 567/590 [04:05<00:06,  3.45it/s] 96%|█████████▋| 568/590 [04:06<00:06,  3.45it/s] 96%|█████████▋| 569/590 [04:06<00:06,  3.45it/s] 97%|█████████▋| 570/590 [04:06<00:05,  3.45it/s] 97%|█████████▋| 571/590 [04:07<00:05,  3.45it/s] 97%|█████████▋| 572/590 [04:07<00:05,  3.45it/s] 97%|█████████▋| 573/590 [04:07<00:04,  3.44it/s] 97%|█████████▋| 574/590 [04:07<00:04,  3.45it/s] 97%|█████████▋| 575/590 [04:08<00:04,  3.45it/s] 98%|█████████▊| 576/590 [04:08<00:04,  3.45it/s] 98%|█████████▊| 577/590 [04:08<00:03,  3.45it/s] 98%|█████████▊| 578/590 [04:09<00:03,  3.46it/s] 98%|█████████▊| 579/590 [04:09<00:03,  3.45it/s] 98%|█████████▊| 580/590 [04:09<00:02,  3.46it/s] 98%|█████████▊| 581/590 [04:09<00:02,  3.45it/s] 99%|█████████▊| 582/590 [04:10<00:02,  3.45it/s] 99%|█████████▉| 583/590 [04:10<00:02,  3.45it/s] 99%|█████████▉| 584/590 [04:10<00:01,  3.45it/s] 99%|█████████▉| 585/590 [04:11<00:01,  3.45it/s] 99%|█████████▉| 586/590 [04:11<00:01,  3.45it/s] 99%|█████████▉| 587/590 [04:11<00:00,  3.45it/s]100%|█████████▉| 588/590 [04:12<00:00,  3.45it/s]100%|█████████▉| 589/590 [04:12<00:00,  3.45it/s]100%|██████████| 590/590 [04:12<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:29:24,286 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:29:24,286 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:29:24,286 >>   Batch size = 8
{'eval_loss': 0.9225637912750244, 'eval_runtime': 13.2216, 'eval_samples_per_second': 369.245, 'eval_steps_per_second': 46.212, 'epoch': 4.0}
{'loss': 0.6346, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.18it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.22it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.54it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.75it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.39it/s][A
  5%|▌         | 33/611 [00:00<00:12, 46.83it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.60it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.31it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.36it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.48it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.48it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.42it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.47it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.39it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.38it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.23it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.18it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.18it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.28it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.39it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.32it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.37it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.44it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.37it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.30it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.23it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.27it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.35it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.32it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.36it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.38it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.36it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.32it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.26it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.29it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.26it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.22it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.33it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.32it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.41it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.33it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.33it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.32it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.24it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.27it/s][A
 38%|███▊      | 233/611 [00:05<00:08, 46.33it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.36it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.35it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.41it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.34it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.35it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.17it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.27it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.22it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.34it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.39it/s][A
 47%|████▋     | 288/611 [00:06<00:07, 46.00it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.13it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.20it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.19it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.19it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.22it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.22it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.22it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.34it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.41it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.27it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.40it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.27it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.29it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.26it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.31it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.29it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.30it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.28it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.35it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.37it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.34it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.16it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.19it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 45.82it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.02it/s][A
 68%|██████▊   | 418/611 [00:09<00:04, 46.11it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.20it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.35it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.41it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.42it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.17it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.06it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.19it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.18it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.28it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.32it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.39it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.50it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.41it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.22it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.21it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.29it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.23it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.28it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.37it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.39it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.37it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.26it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.27it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.31it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.26it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.19it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.30it/s][A
 91%|█████████▏| 558/611 [00:12<00:01, 46.32it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.41it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.35it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.32it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.28it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.27it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.31it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.21it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.25it/s][A
 99%|█████████▊| 603/611 [00:13<00:00, 46.25it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.34it/s][A                                                 
                                                 [A100%|██████████| 590/590 [04:25<00:00,  3.45it/s]
100%|██████████| 611/611 [00:13<00:00, 46.34it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:29:37,514 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-29 00:29:37,535 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:29:40,081 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:29:40,115 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:29:40,167 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:29:46,008 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:29:46,041 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-118 (score: 0.9041016697883606).
                                                 100%|██████████| 590/590 [04:36<00:00,  3.45it/s]100%|██████████| 590/590 [04:36<00:00,  2.13it/s]
[INFO|trainer.py:1894] 2023-08-29 00:29:48,502 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 00:29:48,680 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:29:52,517 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:29:52,713 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:29:52,804 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:29:53,035 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:29:53,036 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:29:53,036 >>   train_loss               =     0.6303
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:29:53,036 >>   train_runtime            = 0:04:36.75
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:29:53,036 >>   train_samples            =       7570
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:29:53,036 >>   train_samples_per_second =    136.764
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:29:53,036 >>   train_steps_per_second   =      2.132
{'eval_loss': 0.9283949136734009, 'eval_runtime': 13.1908, 'eval_samples_per_second': 370.106, 'eval_steps_per_second': 46.32, 'epoch': 5.0}
{'train_runtime': 276.7546, 'train_samples_per_second': 136.764, 'train_steps_per_second': 2.132, 'train_loss': 0.6303423218807932, 'epoch': 5.0}
08/29/2023 00:29:53 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:29:53,077 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:29:53,078 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:29:53,078 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 58.08it/s]  2%|▏         | 12/611 [00:00<00:11, 51.16it/s]  3%|▎         | 18/611 [00:00<00:12, 49.12it/s]  4%|▍         | 23/611 [00:00<00:12, 48.38it/s]  5%|▍         | 28/611 [00:00<00:12, 47.94it/s]  5%|▌         | 33/611 [00:00<00:12, 47.53it/s]  6%|▌         | 38/611 [00:00<00:12, 47.27it/s]  7%|▋         | 43/611 [00:00<00:12, 47.06it/s]  8%|▊         | 48/611 [00:01<00:12, 46.62it/s]  9%|▊         | 53/611 [00:01<00:11, 46.64it/s]  9%|▉         | 58/611 [00:01<00:11, 46.69it/s] 10%|█         | 63/611 [00:01<00:11, 46.79it/s] 11%|█         | 68/611 [00:01<00:11, 46.78it/s] 12%|█▏        | 73/611 [00:01<00:11, 46.89it/s] 13%|█▎        | 78/611 [00:01<00:11, 46.88it/s] 14%|█▎        | 83/611 [00:01<00:11, 46.87it/s] 14%|█▍        | 88/611 [00:01<00:11, 46.70it/s] 15%|█▌        | 93/611 [00:01<00:11, 46.50it/s] 16%|█▌        | 98/611 [00:02<00:11, 46.52it/s] 17%|█▋        | 103/611 [00:02<00:10, 46.54it/s] 18%|█▊        | 108/611 [00:02<00:10, 46.53it/s] 18%|█▊        | 113/611 [00:02<00:10, 46.73it/s] 19%|█▉        | 118/611 [00:02<00:10, 46.82it/s] 20%|██        | 123/611 [00:02<00:10, 46.76it/s] 21%|██        | 128/611 [00:02<00:10, 46.82it/s] 22%|██▏       | 133/611 [00:02<00:10, 46.78it/s] 23%|██▎       | 138/611 [00:02<00:10, 46.61it/s] 23%|██▎       | 143/611 [00:03<00:10, 46.63it/s] 24%|██▍       | 148/611 [00:03<00:09, 46.60it/s] 25%|██▌       | 153/611 [00:03<00:09, 46.67it/s] 26%|██▌       | 158/611 [00:03<00:09, 46.73it/s] 27%|██▋       | 163/611 [00:03<00:09, 46.82it/s] 27%|██▋       | 168/611 [00:03<00:09, 46.86it/s] 28%|██▊       | 173/611 [00:03<00:09, 46.82it/s] 29%|██▉       | 178/611 [00:03<00:09, 46.70it/s] 30%|██▉       | 183/611 [00:03<00:09, 46.71it/s] 31%|███       | 188/611 [00:03<00:09, 46.73it/s] 32%|███▏      | 193/611 [00:04<00:08, 46.62it/s] 32%|███▏      | 198/611 [00:04<00:08, 46.75it/s] 33%|███▎      | 203/611 [00:04<00:08, 46.74it/s] 34%|███▍      | 208/611 [00:04<00:08, 46.76it/s] 35%|███▍      | 213/611 [00:04<00:08, 46.75it/s] 36%|███▌      | 218/611 [00:04<00:08, 46.79it/s] 36%|███▋      | 223/611 [00:04<00:08, 46.75it/s] 37%|███▋      | 228/611 [00:04<00:08, 46.77it/s] 38%|███▊      | 233/611 [00:04<00:08, 46.77it/s] 39%|███▉      | 238/611 [00:05<00:08, 46.59it/s] 40%|███▉      | 243/611 [00:05<00:07, 46.66it/s] 41%|████      | 248/611 [00:05<00:07, 46.65it/s] 41%|████▏     | 253/611 [00:05<00:07, 46.66it/s] 42%|████▏     | 258/611 [00:05<00:07, 46.75it/s] 43%|████▎     | 263/611 [00:05<00:07, 46.66it/s] 44%|████▍     | 268/611 [00:05<00:07, 46.73it/s] 45%|████▍     | 273/611 [00:05<00:07, 46.72it/s] 45%|████▌     | 278/611 [00:05<00:07, 46.71it/s] 46%|████▋     | 283/611 [00:06<00:07, 46.76it/s] 47%|████▋     | 288/611 [00:06<00:06, 46.56it/s] 48%|████▊     | 293/611 [00:06<00:06, 46.63it/s] 49%|████▉     | 298/611 [00:06<00:06, 46.75it/s] 50%|████▉     | 303/611 [00:06<00:06, 46.77it/s] 50%|█████     | 308/611 [00:06<00:06, 46.76it/s] 51%|█████     | 313/611 [00:06<00:06, 46.76it/s] 52%|█████▏    | 318/611 [00:06<00:06, 46.70it/s] 53%|█████▎    | 323/611 [00:06<00:06, 46.65it/s] 54%|█████▎    | 328/611 [00:06<00:06, 46.63it/s] 55%|█████▍    | 333/611 [00:07<00:05, 46.66it/s] 55%|█████▌    | 338/611 [00:07<00:05, 46.72it/s] 56%|█████▌    | 343/611 [00:07<00:05, 46.68it/s] 57%|█████▋    | 348/611 [00:07<00:05, 46.71it/s] 58%|█████▊    | 353/611 [00:07<00:05, 46.78it/s] 59%|█████▊    | 358/611 [00:07<00:05, 46.77it/s] 59%|█████▉    | 363/611 [00:07<00:05, 46.59it/s] 60%|██████    | 368/611 [00:07<00:05, 46.66it/s] 61%|██████    | 373/611 [00:07<00:05, 46.70it/s] 62%|██████▏   | 378/611 [00:08<00:04, 46.63it/s] 63%|██████▎   | 383/611 [00:08<00:04, 46.68it/s] 64%|██████▎   | 388/611 [00:08<00:04, 46.70it/s] 64%|██████▍   | 393/611 [00:08<00:04, 46.62it/s] 65%|██████▌   | 398/611 [00:08<00:04, 46.68it/s] 66%|██████▌   | 403/611 [00:08<00:04, 46.65it/s] 67%|██████▋   | 408/611 [00:08<00:04, 46.56it/s] 68%|██████▊   | 413/611 [00:08<00:04, 46.61it/s] 68%|██████▊   | 418/611 [00:08<00:04, 46.64it/s] 69%|██████▉   | 423/611 [00:09<00:04, 46.73it/s] 70%|███████   | 428/611 [00:09<00:03, 46.75it/s] 71%|███████   | 433/611 [00:09<00:03, 46.72it/s] 72%|███████▏  | 438/611 [00:09<00:03, 46.60it/s] 73%|███████▎  | 443/611 [00:09<00:03, 46.62it/s] 73%|███████▎  | 448/611 [00:09<00:03, 46.71it/s] 74%|███████▍  | 453/611 [00:09<00:03, 46.67it/s] 75%|███████▍  | 458/611 [00:09<00:03, 46.61it/s] 76%|███████▌  | 463/611 [00:09<00:03, 46.54it/s] 77%|███████▋  | 468/611 [00:09<00:03, 46.66it/s] 77%|███████▋  | 473/611 [00:10<00:02, 46.67it/s] 78%|███████▊  | 478/611 [00:10<00:02, 46.68it/s] 79%|███████▉  | 483/611 [00:10<00:02, 46.71it/s] 80%|███████▉  | 488/611 [00:10<00:02, 46.66it/s] 81%|████████  | 493/611 [00:10<00:02, 46.58it/s] 82%|████████▏ | 498/611 [00:10<00:02, 46.65it/s] 82%|████████▏ | 503/611 [00:10<00:02, 46.58it/s] 83%|████████▎ | 508/611 [00:10<00:02, 46.64it/s] 84%|████████▍ | 513/611 [00:10<00:02, 46.62it/s] 85%|████████▍ | 518/611 [00:11<00:01, 46.62it/s] 86%|████████▌ | 523/611 [00:11<00:01, 46.59it/s] 86%|████████▋ | 528/611 [00:11<00:01, 46.66it/s] 87%|████████▋ | 533/611 [00:11<00:01, 46.69it/s] 88%|████████▊ | 538/611 [00:11<00:01, 46.69it/s] 89%|████████▉ | 543/611 [00:11<00:01, 46.61it/s] 90%|████████▉ | 548/611 [00:11<00:01, 46.58it/s] 91%|█████████ | 553/611 [00:11<00:01, 46.61it/s] 91%|█████████▏| 558/611 [00:11<00:01, 46.62it/s] 92%|█████████▏| 563/611 [00:12<00:01, 46.66it/s] 93%|█████████▎| 568/611 [00:12<00:00, 46.66it/s] 94%|█████████▍| 573/611 [00:12<00:00, 46.59it/s] 95%|█████████▍| 578/611 [00:12<00:00, 46.62it/s] 95%|█████████▌| 583/611 [00:12<00:00, 46.57it/s] 96%|█████████▌| 588/611 [00:12<00:00, 46.66it/s] 97%|█████████▋| 593/611 [00:12<00:00, 46.69it/s] 98%|█████████▊| 598/611 [00:12<00:00, 46.58it/s] 99%|█████████▊| 603/611 [00:12<00:00, 46.65it/s]100%|█████████▉| 608/611 [00:12<00:00, 46.69it/s]100%|██████████| 611/611 [00:13<00:00, 46.76it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:30:06,166 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:06,166 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:06,166 >>   eval_loss               =     0.9041
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:06,166 >>   eval_runtime            = 0:00:13.08
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:06,166 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:06,166 >>   eval_samples_per_second =    373.014
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:06,166 >>   eval_steps_per_second   =     46.684
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:06,166 >>   perplexity              =     2.4697
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:30:12,359 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:30:12,363 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:30:12,363 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:30:12,363 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:30:12,364 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:30:12,667 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:30:12,668 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:30:12,928 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:30:13,937 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:30:13,937 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:30:15,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:30:15,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:30:15,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:30:15,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:30:15,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:30:15,981 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:30:15,982 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:30:16,245 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:30:16,407 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:30:16,407 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-590
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-236
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-354
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.46it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:07,  1.59it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:12,  1.63it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:13,  1.64it/s]Extractor Predicting: 22it [00:13,  1.69it/s]Extractor Predicting: 23it [00:14,  1.68it/s]Extractor Predicting: 24it [00:15,  1.68it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.53it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:18,  1.59it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.65it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:21,  1.61it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.62it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:24,  1.58it/s]Extractor Predicting: 40it [00:25,  1.61it/s]Extractor Predicting: 41it [00:25,  1.58it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:26,  1.59it/s]Extractor Predicting: 44it [00:27,  1.58it/s]Extractor Predicting: 45it [00:28,  1.61it/s]Extractor Predicting: 46it [00:28,  1.60it/s]Extractor Predicting: 47it [00:29,  1.60it/s]Extractor Predicting: 48it [00:30,  1.60it/s]Extractor Predicting: 49it [00:30,  1.59it/s]Extractor Predicting: 50it [00:31,  1.56it/s]Extractor Predicting: 51it [00:32,  1.55it/s]Extractor Predicting: 52it [00:32,  1.59it/s]Extractor Predicting: 53it [00:33,  1.57it/s]Extractor Predicting: 54it [00:33,  1.56it/s]Extractor Predicting: 55it [00:34,  1.52it/s]Extractor Predicting: 56it [00:35,  1.48it/s]Extractor Predicting: 57it [00:36,  1.50it/s]Extractor Predicting: 58it [00:36,  1.50it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:37,  1.51it/s]Extractor Predicting: 61it [00:38,  1.51it/s]Extractor Predicting: 62it [00:39,  1.49it/s]Extractor Predicting: 63it [00:40,  1.50it/s]Extractor Predicting: 64it [00:40,  1.50it/s]Extractor Predicting: 65it [00:41,  1.49it/s]Extractor Predicting: 66it [00:42,  1.46it/s]Extractor Predicting: 67it [00:42,  1.49it/s]Extractor Predicting: 68it [00:43,  1.52it/s]Extractor Predicting: 69it [00:44,  1.51it/s]Extractor Predicting: 70it [00:44,  1.48it/s]Extractor Predicting: 71it [00:45,  1.49it/s]Extractor Predicting: 72it [00:46,  1.46it/s]Extractor Predicting: 73it [00:46,  1.49it/s]Extractor Predicting: 74it [00:47,  1.49it/s]Extractor Predicting: 75it [00:48,  1.50it/s]Extractor Predicting: 76it [00:48,  1.51it/s]Extractor Predicting: 77it [00:49,  1.50it/s]Extractor Predicting: 78it [00:50,  1.53it/s]Extractor Predicting: 79it [00:50,  1.54it/s]Extractor Predicting: 80it [00:51,  1.53it/s]Extractor Predicting: 81it [00:51,  1.53it/s]Extractor Predicting: 82it [00:52,  1.50it/s]Extractor Predicting: 83it [00:53,  1.51it/s]Extractor Predicting: 84it [00:53,  1.50it/s]Extractor Predicting: 85it [00:54,  1.50it/s]Extractor Predicting: 86it [00:55,  1.48it/s]Extractor Predicting: 87it [00:56,  1.47it/s]Extractor Predicting: 88it [00:56,  1.45it/s]Extractor Predicting: 89it [00:57,  1.46it/s]Extractor Predicting: 90it [00:58,  1.45it/s]Extractor Predicting: 91it [00:58,  1.46it/s]Extractor Predicting: 92it [00:59,  1.51it/s]Extractor Predicting: 93it [00:59,  1.58it/s]Extractor Predicting: 94it [01:00,  1.56it/s]Extractor Predicting: 95it [01:01,  1.56it/s]Extractor Predicting: 96it [01:01,  1.56it/s]Extractor Predicting: 97it [01:02,  1.58it/s]Extractor Predicting: 98it [01:03,  1.54it/s]Extractor Predicting: 99it [01:03,  1.49it/s]Extractor Predicting: 100it [01:04,  1.52it/s]Extractor Predicting: 101it [01:05,  1.45it/s]Extractor Predicting: 102it [01:06,  1.44it/s]Extractor Predicting: 103it [01:06,  1.45it/s]Extractor Predicting: 104it [01:07,  1.48it/s]Extractor Predicting: 105it [01:08,  1.49it/s]Extractor Predicting: 106it [01:08,  1.54it/s]Extractor Predicting: 107it [01:09,  1.53it/s]Extractor Predicting: 108it [01:09,  1.56it/s]Extractor Predicting: 109it [01:10,  1.55it/s]Extractor Predicting: 110it [01:11,  1.55it/s]Extractor Predicting: 111it [01:11,  1.58it/s]Extractor Predicting: 112it [01:12,  1.58it/s]Extractor Predicting: 113it [01:13,  1.53it/s]Extractor Predicting: 114it [01:13,  1.52it/s]Extractor Predicting: 115it [01:14,  1.53it/s]Extractor Predicting: 116it [01:15,  1.49it/s]Extractor Predicting: 117it [01:15,  1.47it/s]Extractor Predicting: 118it [01:16,  1.48it/s]Extractor Predicting: 119it [01:17,  1.45it/s]Extractor Predicting: 120it [01:17,  1.43it/s]Extractor Predicting: 121it [01:18,  1.44it/s]Extractor Predicting: 122it [01:19,  1.45it/s]Extractor Predicting: 123it [01:20,  1.35it/s]Extractor Predicting: 124it [01:20,  1.39it/s]Extractor Predicting: 125it [01:21,  1.42it/s]Extractor Predicting: 126it [01:22,  1.44it/s]Extractor Predicting: 127it [01:22,  1.44it/s]Extractor Predicting: 128it [01:23,  1.46it/s]Extractor Predicting: 129it [01:24,  1.49it/s]Extractor Predicting: 130it [01:24,  1.45it/s]Extractor Predicting: 131it [01:25,  1.46it/s]Extractor Predicting: 132it [01:26,  1.50it/s]Extractor Predicting: 133it [01:26,  1.47it/s]Extractor Predicting: 134it [01:27,  1.48it/s]Extractor Predicting: 135it [01:28,  1.46it/s]Extractor Predicting: 136it [01:28,  1.48it/s]Extractor Predicting: 137it [01:29,  1.45it/s]Extractor Predicting: 138it [01:30,  1.47it/s]Extractor Predicting: 139it [01:31,  1.45it/s]Extractor Predicting: 140it [01:31,  1.45it/s]Extractor Predicting: 141it [01:32,  1.46it/s]Extractor Predicting: 142it [01:33,  1.47it/s]Extractor Predicting: 143it [01:33,  1.45it/s]Extractor Predicting: 144it [01:34,  1.49it/s]Extractor Predicting: 145it [01:35,  1.52it/s]Extractor Predicting: 146it [01:35,  1.50it/s]Extractor Predicting: 147it [01:36,  1.47it/s]Extractor Predicting: 148it [01:37,  1.48it/s]Extractor Predicting: 149it [01:37,  1.47it/s]Extractor Predicting: 150it [01:38,  1.46it/s]Extractor Predicting: 151it [01:39,  1.45it/s]Extractor Predicting: 152it [01:39,  1.45it/s]Extractor Predicting: 153it [01:40,  1.45it/s]Extractor Predicting: 154it [01:41,  1.46it/s]Extractor Predicting: 155it [01:41,  1.46it/s]Extractor Predicting: 156it [01:42,  1.41it/s]Extractor Predicting: 157it [01:43,  1.37it/s]Extractor Predicting: 158it [01:44,  1.34it/s]Extractor Predicting: 159it [01:44,  1.38it/s]Extractor Predicting: 160it [01:45,  1.41it/s]Extractor Predicting: 161it [01:46,  1.43it/s]Extractor Predicting: 162it [01:46,  1.43it/s]Extractor Predicting: 163it [01:47,  1.43it/s]Extractor Predicting: 164it [01:48,  1.46it/s]Extractor Predicting: 165it [01:49,  1.46it/s]Extractor Predicting: 166it [01:49,  1.48it/s]Extractor Predicting: 167it [01:50,  1.47it/s]Extractor Predicting: 168it [01:51,  1.47it/s]Extractor Predicting: 169it [01:51,  1.48it/s]Extractor Predicting: 170it [01:52,  1.48it/s]Extractor Predicting: 171it [01:53,  1.49it/s]Extractor Predicting: 172it [01:53,  1.51it/s]Extractor Predicting: 173it [01:54,  1.47it/s]Extractor Predicting: 174it [01:55,  1.47it/s]Extractor Predicting: 175it [01:55,  1.45it/s]Extractor Predicting: 176it [01:56,  1.47it/s]Extractor Predicting: 177it [01:57,  1.44it/s]Extractor Predicting: 178it [01:57,  1.45it/s]Extractor Predicting: 179it [01:58,  1.44it/s]Extractor Predicting: 180it [01:59,  1.44it/s]Extractor Predicting: 181it [01:59,  1.45it/s]Extractor Predicting: 182it [02:00,  1.45it/s]Extractor Predicting: 183it [02:01,  1.41it/s]Extractor Predicting: 184it [02:02,  1.43it/s]Extractor Predicting: 185it [02:02,  1.54it/s]Extractor Predicting: 185it [02:02,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:28,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:28,130 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:28,130 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:28,130 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:28,130 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:32:28,723 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:32:28,724 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:32:29,284 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:32:30,309 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:32:30,309 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:33,351 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:33,362 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:33,362 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:33,362 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:33,362 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:32:33,990 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:32:33,991 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:32:34,593 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:32:34,742 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:32:34,743 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4808510638297872,
  "recall": 0.04629250307251127,
  "score": 0.08445440956651719,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.55it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:12,  1.51it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.59it/s]Extractor Predicting: 31it [00:19,  1.64it/s]Extractor Predicting: 32it [00:20,  1.63it/s]Extractor Predicting: 33it [00:21,  1.63it/s]Extractor Predicting: 34it [00:21,  1.63it/s]Extractor Predicting: 35it [00:22,  1.63it/s]Extractor Predicting: 36it [00:23,  1.61it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:24,  1.62it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:26,  1.60it/s]Extractor Predicting: 43it [00:27,  1.61it/s]Extractor Predicting: 44it [00:28,  1.63it/s]Extractor Predicting: 45it [00:28,  1.64it/s]Extractor Predicting: 46it [00:29,  1.65it/s]Extractor Predicting: 47it [00:29,  1.66it/s]Extractor Predicting: 48it [00:30,  1.66it/s]Extractor Predicting: 49it [00:31,  1.64it/s]Extractor Predicting: 50it [00:31,  1.62it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:32,  1.63it/s]Extractor Predicting: 53it [00:33,  1.62it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:34,  1.57it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.60it/s]Extractor Predicting: 58it [00:36,  1.66it/s]Extractor Predicting: 59it [00:37,  1.62it/s]Extractor Predicting: 60it [00:37,  1.60it/s]Extractor Predicting: 61it [00:38,  1.55it/s]Extractor Predicting: 62it [00:39,  1.53it/s]Extractor Predicting: 63it [00:40,  1.48it/s]Extractor Predicting: 64it [00:40,  1.46it/s]Extractor Predicting: 65it [00:41,  1.45it/s]Extractor Predicting: 66it [00:42,  1.44it/s]Extractor Predicting: 67it [00:42,  1.42it/s]Extractor Predicting: 68it [00:43,  1.44it/s]Extractor Predicting: 69it [00:44,  1.45it/s]Extractor Predicting: 70it [00:44,  1.47it/s]Extractor Predicting: 71it [00:45,  1.47it/s]Extractor Predicting: 72it [00:46,  1.49it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:47,  1.53it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:48,  1.55it/s]Extractor Predicting: 77it [00:49,  1.42it/s]Extractor Predicting: 78it [00:50,  1.45it/s]Extractor Predicting: 79it [00:50,  1.53it/s]Extractor Predicting: 80it [00:51,  1.58it/s]Extractor Predicting: 81it [00:52,  1.56it/s]Extractor Predicting: 82it [00:52,  1.59it/s]Extractor Predicting: 83it [00:53,  1.55it/s]Extractor Predicting: 84it [00:53,  1.55it/s]Extractor Predicting: 85it [00:54,  1.51it/s]Extractor Predicting: 86it [00:55,  1.49it/s]Extractor Predicting: 87it [00:56,  1.49it/s]Extractor Predicting: 88it [00:56,  1.51it/s]Extractor Predicting: 89it [00:57,  1.50it/s]Extractor Predicting: 90it [00:58,  1.51it/s]Extractor Predicting: 91it [00:58,  1.50it/s]Extractor Predicting: 92it [00:59,  1.49it/s]Extractor Predicting: 93it [01:00,  1.52it/s]Extractor Predicting: 94it [01:00,  1.53it/s]Extractor Predicting: 95it [01:01,  1.52it/s]Extractor Predicting: 96it [01:01,  1.51it/s]Extractor Predicting: 97it [01:02,  1.53it/s]Extractor Predicting: 98it [01:03,  1.52it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.51it/s]Extractor Predicting: 101it [01:05,  1.53it/s]Extractor Predicting: 102it [01:05,  1.53it/s]Extractor Predicting: 103it [01:06,  1.50it/s]Extractor Predicting: 104it [01:07,  1.53it/s]Extractor Predicting: 105it [01:07,  1.54it/s]Extractor Predicting: 106it [01:08,  1.56it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:09,  1.56it/s]Extractor Predicting: 109it [01:10,  1.55it/s]Extractor Predicting: 110it [01:11,  1.56it/s]Extractor Predicting: 111it [01:11,  1.55it/s]Extractor Predicting: 112it [01:12,  1.53it/s]Extractor Predicting: 113it [01:13,  1.52it/s]Extractor Predicting: 114it [01:13,  1.52it/s]Extractor Predicting: 115it [01:14,  1.51it/s]Extractor Predicting: 116it [01:15,  1.51it/s]Extractor Predicting: 117it [01:15,  1.55it/s]Extractor Predicting: 118it [01:16,  1.53it/s]Extractor Predicting: 119it [01:16,  1.52it/s]Extractor Predicting: 120it [01:17,  1.54it/s]Extractor Predicting: 121it [01:18,  1.57it/s]Extractor Predicting: 122it [01:18,  1.56it/s]Extractor Predicting: 123it [01:19,  1.54it/s]Extractor Predicting: 124it [01:20,  1.51it/s]Extractor Predicting: 125it [01:20,  1.52it/s]Extractor Predicting: 126it [01:21,  1.51it/s]Extractor Predicting: 127it [01:22,  1.54it/s]Extractor Predicting: 128it [01:22,  1.49it/s]Extractor Predicting: 129it [01:23,  1.51it/s]Extractor Predicting: 130it [01:24,  1.54it/s]Extractor Predicting: 131it [01:24,  1.51it/s]Extractor Predicting: 132it [01:25,  1.51it/s]Extractor Predicting: 133it [01:26,  1.50it/s]Extractor Predicting: 134it [01:26,  1.50it/s]Extractor Predicting: 135it [01:27,  1.50it/s]Extractor Predicting: 136it [01:28,  1.54it/s]Extractor Predicting: 137it [01:28,  1.48it/s]Extractor Predicting: 138it [01:29,  1.50it/s]Extractor Predicting: 139it [01:30,  1.51it/s]Extractor Predicting: 140it [01:30,  1.52it/s]Extractor Predicting: 141it [01:31,  1.51it/s]Extractor Predicting: 142it [01:32,  1.53it/s]Extractor Predicting: 143it [01:32,  1.44it/s]Extractor Predicting: 144it [01:33,  1.50it/s]Extractor Predicting: 145it [01:34,  1.48it/s]Extractor Predicting: 146it [01:34,  1.50it/s]Extractor Predicting: 147it [01:35,  1.54it/s]Extractor Predicting: 148it [01:36,  1.53it/s]Extractor Predicting: 149it [01:36,  1.56it/s]Extractor Predicting: 150it [01:37,  1.57it/s]Extractor Predicting: 151it [01:37,  1.59it/s]Extractor Predicting: 152it [01:38,  1.55it/s]Extractor Predicting: 153it [01:39,  1.56it/s]Extractor Predicting: 154it [01:39,  1.52it/s]Extractor Predicting: 155it [01:40,  1.52it/s]Extractor Predicting: 156it [01:41,  1.57it/s]Extractor Predicting: 157it [01:41,  1.53it/s]Extractor Predicting: 158it [01:42,  1.51it/s]Extractor Predicting: 159it [01:43,  1.53it/s]Extractor Predicting: 160it [01:43,  1.53it/s]Extractor Predicting: 161it [01:44,  1.56it/s]Extractor Predicting: 162it [01:45,  1.54it/s]Extractor Predicting: 163it [01:45,  1.54it/s]Extractor Predicting: 164it [01:46,  1.53it/s]Extractor Predicting: 165it [01:47,  1.48it/s]Extractor Predicting: 166it [01:47,  1.46it/s]Extractor Predicting: 167it [01:48,  1.46it/s]Extractor Predicting: 168it [01:49,  1.49it/s]Extractor Predicting: 169it [01:49,  1.52it/s]Extractor Predicting: 170it [01:50,  1.54it/s]Extractor Predicting: 171it [01:51,  1.54it/s]Extractor Predicting: 172it [01:51,  1.57it/s]Extractor Predicting: 173it [01:52,  1.56it/s]Extractor Predicting: 174it [01:53,  1.55it/s]Extractor Predicting: 175it [01:53,  1.55it/s]Extractor Predicting: 176it [01:54,  1.55it/s]Extractor Predicting: 177it [01:55,  1.54it/s]Extractor Predicting: 178it [01:55,  1.53it/s]Extractor Predicting: 179it [01:56,  1.50it/s]Extractor Predicting: 180it [01:56,  1.54it/s]Extractor Predicting: 181it [01:57,  1.53it/s]Extractor Predicting: 182it [01:58,  1.56it/s]Extractor Predicting: 183it [01:58,  1.58it/s]Extractor Predicting: 184it [01:59,  1.57it/s]Extractor Predicting: 185it [02:00,  1.57it/s]Extractor Predicting: 186it [02:00,  1.53it/s]Extractor Predicting: 187it [02:01,  1.55it/s]Extractor Predicting: 188it [02:02,  1.54it/s]Extractor Predicting: 189it [02:02,  1.54it/s]Extractor Predicting: 190it [02:03,  1.56it/s]Extractor Predicting: 191it [02:04,  1.56it/s]Extractor Predicting: 192it [02:04,  1.60it/s]Extractor Predicting: 193it [02:05,  1.57it/s]Extractor Predicting: 194it [02:05,  1.58it/s]Extractor Predicting: 195it [02:06,  1.56it/s]Extractor Predicting: 196it [02:07,  1.56it/s]Extractor Predicting: 197it [02:07,  1.54it/s]Extractor Predicting: 198it [02:08,  1.53it/s]Extractor Predicting: 199it [02:09,  1.52it/s]Extractor Predicting: 200it [02:09,  1.53it/s]Extractor Predicting: 201it [02:10,  1.54it/s]Extractor Predicting: 202it [02:11,  1.55it/s]Extractor Predicting: 203it [02:11,  1.57it/s]Extractor Predicting: 204it [02:12,  1.56it/s]Extractor Predicting: 205it [02:13,  1.56it/s]Extractor Predicting: 206it [02:13,  1.55it/s]Extractor Predicting: 207it [02:14,  1.57it/s]Extractor Predicting: 208it [02:14,  1.55it/s]Extractor Predicting: 209it [02:15,  1.36it/s]Extractor Predicting: 210it [02:16,  1.46it/s]Extractor Predicting: 211it [02:17,  1.47it/s]Extractor Predicting: 212it [02:17,  1.51it/s]Extractor Predicting: 213it [02:18,  1.52it/s]Extractor Predicting: 214it [02:19,  1.56it/s]Extractor Predicting: 215it [02:19,  1.55it/s]Extractor Predicting: 216it [02:20,  1.53it/s]Extractor Predicting: 217it [02:20,  1.55it/s]Extractor Predicting: 218it [02:21,  1.53it/s]Extractor Predicting: 219it [02:22,  1.53it/s]Extractor Predicting: 220it [02:22,  1.54it/s]Extractor Predicting: 221it [02:23,  1.55it/s]Extractor Predicting: 222it [02:24,  1.50it/s]Extractor Predicting: 223it [02:25,  1.45it/s]Extractor Predicting: 224it [02:25,  1.48it/s]Extractor Predicting: 225it [02:26,  1.50it/s]Extractor Predicting: 226it [02:26,  1.53it/s]Extractor Predicting: 227it [02:27,  1.53it/s]Extractor Predicting: 228it [02:28,  1.56it/s]Extractor Predicting: 229it [02:28,  1.59it/s]Extractor Predicting: 230it [02:29,  1.60it/s]Extractor Predicting: 231it [02:30,  1.60it/s]Extractor Predicting: 232it [02:30,  1.59it/s]Extractor Predicting: 233it [02:31,  1.58it/s]Extractor Predicting: 234it [02:31,  1.58it/s]Extractor Predicting: 235it [02:32,  1.55it/s]Extractor Predicting: 236it [02:33,  1.56it/s]Extractor Predicting: 237it [02:33,  1.56it/s]Extractor Predicting: 238it [02:34,  1.53it/s]Extractor Predicting: 239it [02:35,  1.55it/s]Extractor Predicting: 240it [02:35,  1.54it/s]Extractor Predicting: 241it [02:36,  1.56it/s]Extractor Predicting: 242it [02:37,  1.53it/s]Extractor Predicting: 243it [02:37,  1.49it/s]Extractor Predicting: 244it [02:38,  1.50it/s]Extractor Predicting: 245it [02:39,  1.54it/s]Extractor Predicting: 246it [02:39,  1.53it/s]Extractor Predicting: 247it [02:40,  1.56it/s]Extractor Predicting: 248it [02:41,  1.50it/s]Extractor Predicting: 249it [02:41,  1.50it/s]Extractor Predicting: 250it [02:42,  1.52it/s]Extractor Predicting: 251it [02:43,  1.52it/s]Extractor Predicting: 252it [02:43,  1.51it/s]Extractor Predicting: 253it [02:44,  1.47it/s]Extractor Predicting: 254it [02:45,  1.45it/s]Extractor Predicting: 255it [02:45,  1.47it/s]Extractor Predicting: 256it [02:46,  1.49it/s]Extractor Predicting: 257it [02:47,  1.50it/s]Extractor Predicting: 258it [02:47,  1.50it/s]Extractor Predicting: 259it [02:48,  1.50it/s]Extractor Predicting: 260it [02:49,  1.47it/s]Extractor Predicting: 261it [02:49,  1.50it/s]Extractor Predicting: 262it [02:50,  1.49it/s]Extractor Predicting: 263it [02:51,  1.49it/s]Extractor Predicting: 264it [02:51,  1.50it/s]Extractor Predicting: 265it [02:52,  1.52it/s]Extractor Predicting: 266it [02:53,  1.47it/s]Extractor Predicting: 267it [02:53,  1.47it/s]Extractor Predicting: 268it [02:54,  1.46it/s]Extractor Predicting: 269it [02:55,  1.48it/s]Extractor Predicting: 270it [02:55,  1.46it/s]Extractor Predicting: 271it [02:56,  1.47it/s]Extractor Predicting: 272it [02:57,  1.47it/s]Extractor Predicting: 273it [02:58,  1.48it/s]Extractor Predicting: 274it [02:58,  1.45it/s]Extractor Predicting: 275it [02:59,  1.46it/s]Extractor Predicting: 276it [03:00,  1.47it/s]Extractor Predicting: 277it [03:00,  1.47it/s]Extractor Predicting: 278it [03:01,  1.49it/s]Extractor Predicting: 279it [03:02,  1.48it/s]Extractor Predicting: 280it [03:02,  1.48it/s]Extractor Predicting: 281it [03:03,  1.44it/s]Extractor Predicting: 282it [03:04,  1.44it/s]Extractor Predicting: 283it [03:04,  1.46it/s]Extractor Predicting: 284it [03:05,  1.48it/s]Extractor Predicting: 285it [03:06,  1.47it/s]Extractor Predicting: 286it [03:06,  1.49it/s]Extractor Predicting: 287it [03:07,  1.44it/s]Extractor Predicting: 288it [03:08,  1.48it/s]Extractor Predicting: 289it [03:08,  1.49it/s]Extractor Predicting: 290it [03:09,  1.45it/s]Extractor Predicting: 291it [03:10,  1.43it/s]Extractor Predicting: 292it [03:10,  1.46it/s]Extractor Predicting: 293it [03:11,  1.47it/s]Extractor Predicting: 294it [03:12,  1.45it/s]Extractor Predicting: 295it [03:13,  1.46it/s]Extractor Predicting: 296it [03:13,  1.49it/s]Extractor Predicting: 297it [03:14,  1.49it/s]Extractor Predicting: 298it [03:15,  1.47it/s]Extractor Predicting: 299it [03:15,  1.48it/s]Extractor Predicting: 300it [03:16,  1.46it/s]Extractor Predicting: 301it [03:17,  1.49it/s]Extractor Predicting: 302it [03:17,  1.45it/s]Extractor Predicting: 303it [03:18,  1.48it/s]Extractor Predicting: 304it [03:19,  1.50it/s]Extractor Predicting: 305it [03:19,  1.53it/s]Extractor Predicting: 306it [03:20,  1.55it/s]Extractor Predicting: 307it [03:21,  1.52it/s]Extractor Predicting: 308it [03:21,  1.54it/s]Extractor Predicting: 309it [03:22,  1.51it/s]Extractor Predicting: 310it [03:23,  1.51it/s]Extractor Predicting: 311it [03:23,  1.47it/s]Extractor Predicting: 312it [03:24,  1.49it/s]Extractor Predicting: 313it [03:25,  1.44it/s]Extractor Predicting: 314it [03:25,  1.44it/s]Extractor Predicting: 315it [03:26,  1.32it/s]Extractor Predicting: 316it [03:27,  1.40it/s]Extractor Predicting: 317it [03:28,  1.44it/s]Extractor Predicting: 318it [03:28,  1.49it/s]Extractor Predicting: 319it [03:29,  1.47it/s]Extractor Predicting: 320it [03:30,  1.46it/s]Extractor Predicting: 321it [03:30,  1.46it/s]Extractor Predicting: 322it [03:31,  1.48it/s]Extractor Predicting: 323it [03:32,  1.44it/s]Extractor Predicting: 324it [03:32,  1.46it/s]Extractor Predicting: 325it [03:33,  1.49it/s]Extractor Predicting: 326it [03:34,  1.49it/s]Extractor Predicting: 327it [03:34,  1.50it/s]Extractor Predicting: 328it [03:35,  1.47it/s]Extractor Predicting: 329it [03:36,  1.48it/s]Extractor Predicting: 330it [03:36,  1.47it/s]Extractor Predicting: 331it [03:37,  1.48it/s]Extractor Predicting: 332it [03:38,  1.48it/s]Extractor Predicting: 333it [03:38,  1.46it/s]Extractor Predicting: 334it [03:39,  1.48it/s]Extractor Predicting: 335it [03:40,  1.46it/s]Extractor Predicting: 336it [03:40,  1.40it/s]Extractor Predicting: 337it [03:41,  1.41it/s]Extractor Predicting: 338it [03:42,  1.42it/s]Extractor Predicting: 339it [03:43,  1.44it/s]Extractor Predicting: 340it [03:43,  1.44it/s]Extractor Predicting: 341it [03:44,  1.43it/s]Extractor Predicting: 342it [03:45,  1.43it/s]Extractor Predicting: 343it [03:45,  1.46it/s]Extractor Predicting: 344it [03:46,  1.45it/s]Extractor Predicting: 345it [03:47,  1.42it/s]Extractor Predicting: 346it [03:47,  1.43it/s]Extractor Predicting: 347it [03:48,  1.44it/s]Extractor Predicting: 348it [03:49,  1.46it/s]Extractor Predicting: 349it [03:49,  1.47it/s]Extractor Predicting: 350it [03:50,  1.45it/s]Extractor Predicting: 351it [03:51,  1.48it/s]Extractor Predicting: 352it [03:52,  1.44it/s]Extractor Predicting: 353it [03:52,  1.42it/s]Extractor Predicting: 354it [03:53,  1.45it/s]Extractor Predicting: 355it [03:54,  1.47it/s]Extractor Predicting: 356it [03:54,  1.47it/s]Extractor Predicting: 357it [03:55,  1.48it/s]Extractor Predicting: 358it [03:56,  1.48it/s]Extractor Predicting: 359it [03:56,  1.47it/s]Extractor Predicting: 360it [03:57,  1.44it/s]Extractor Predicting: 361it [03:58,  1.44it/s]Extractor Predicting: 362it [03:58,  1.45it/s]Extractor Predicting: 363it [03:59,  1.49it/s]Extractor Predicting: 364it [04:00,  1.50it/s]Extractor Predicting: 365it [04:00,  1.47it/s]Extractor Predicting: 366it [04:01,  1.43it/s]Extractor Predicting: 367it [04:02,  1.43it/s]Extractor Predicting: 368it [04:03,  1.42it/s]Extractor Predicting: 369it [04:03,  1.37it/s]Extractor Predicting: 370it [04:04,  1.38it/s]Extractor Predicting: 371it [04:05,  1.41it/s]Extractor Predicting: 372it [04:05,  1.67it/s]Extractor Predicting: 372it [04:05,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:49,610 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:49,615 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:49,615 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:49,615 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:49,615 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:36:50,260 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:36:50,261 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:36:50,834 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:36:51,891 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:36:51,891 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:54,800 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:54,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:54,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:54,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:54,806 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:36:55,466 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:36:55,467 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:36:56,041 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:36:56,204 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:36:56,204 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.32397003745318353,
  "recall": 0.05823608617594255,
  "score": 0.09872550884534906,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:09,  1.58it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:13,  1.54it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.46it/s]Extractor Predicting: 25it [00:16,  1.49it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:17,  1.47it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:23,  1.50it/s]Extractor Predicting: 37it [00:24,  1.48it/s]Extractor Predicting: 38it [00:25,  1.44it/s]Extractor Predicting: 39it [00:25,  1.41it/s]Extractor Predicting: 40it [00:26,  1.40it/s]Extractor Predicting: 41it [00:27,  1.40it/s]Extractor Predicting: 42it [00:28,  1.42it/s]Extractor Predicting: 43it [00:28,  1.42it/s]Extractor Predicting: 44it [00:29,  1.44it/s]Extractor Predicting: 45it [00:30,  1.41it/s]Extractor Predicting: 46it [00:30,  1.44it/s]Extractor Predicting: 47it [00:31,  1.41it/s]Extractor Predicting: 48it [00:32,  1.42it/s]Extractor Predicting: 49it [00:33,  1.42it/s]Extractor Predicting: 50it [00:33,  1.42it/s]Extractor Predicting: 51it [00:34,  1.40it/s]Extractor Predicting: 52it [00:35,  1.39it/s]Extractor Predicting: 53it [00:35,  1.41it/s]Extractor Predicting: 54it [00:36,  1.40it/s]Extractor Predicting: 55it [00:37,  1.44it/s]Extractor Predicting: 56it [00:37,  1.43it/s]Extractor Predicting: 57it [00:38,  1.42it/s]Extractor Predicting: 58it [00:39,  1.40it/s]Extractor Predicting: 59it [00:40,  1.40it/s]Extractor Predicting: 60it [00:40,  1.40it/s]Extractor Predicting: 61it [00:41,  1.37it/s]Extractor Predicting: 62it [00:42,  1.36it/s]Extractor Predicting: 63it [00:42,  1.49it/s]Extractor Predicting: 63it [00:42,  1.47it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:37:40,264 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:37:40,265 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:37:40,269 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:37:40,270 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:37:40,274 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:37:43,414 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:37:43,420 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:37:43,459 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:37:43,459 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:37:43,468 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:43,475 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:43,475 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:43,475 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:43,475 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:43,475 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:43,475 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5075757575757576,
  "recall": 0.04015582858855259,
  "score": 0.07442377117467369,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:37:43,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:44,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:45,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:45,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:46,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:47,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:47,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:48,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:49,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:50,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:51,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:51,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:52,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:53,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:53,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:54,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:55,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:56,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:56,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:57,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:58,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:59,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:59,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:00,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:01,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:18<04:15, 18.26s/it][WARNING|generation_utils.py:914] 2023-08-29 00:38:01,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:02,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:03,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:04,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:04,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:05,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:06,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:06,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:07,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:08,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:08,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:09,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:10,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:10,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:11,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:12,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:12,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:13,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:13,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:14,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:15,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:15,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:16,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:34, 16.48s/it][WARNING|generation_utils.py:914] 2023-08-29 00:38:17,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:17,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:18,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:18,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:19,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:20,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:20,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:21,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:22,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:22,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:23,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:23,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:24,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:25,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:25,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:26,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:26,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:27,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:28,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:28,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:45<02:53, 14.48s/it][WARNING|generation_utils.py:914] 2023-08-29 00:38:29,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:30,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:30,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:31,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:32,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:33,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:33,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:34,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:35,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:35,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:36,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:37,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:38,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:38,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:39,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:40,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:40,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:42,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:42,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:43,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:44,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:45,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:02<02:48, 15.32s/it][WARNING|generation_utils.py:914] 2023-08-29 00:38:45,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:46,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:47,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:47,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:48,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:49,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:50,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:50,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:51,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:52,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:52,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:53,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:54,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:54,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:55,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:56,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:57,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:57,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:58,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:59,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:59,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:00,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:01,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:01,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:02,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:19<02:40, 16.04s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:03,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:03,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:04,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:05,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:06,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:06,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:07,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:08,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:08,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:09,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:10,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:11,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:11,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:12,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:13,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:13,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:14,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:15,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:16,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:16,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:17,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:18,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:35<02:23, 15.90s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:18,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:19,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:20,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:20,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:21,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:22,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:22,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:23,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:24,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:25,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:25,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:26,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:27,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:28,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:28,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:29,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:30,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:30,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:31,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:32,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:49<02:02, 15.33s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:33,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:33,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:34,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:35,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:35,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:36,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:37,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:37,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:38,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:39,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:39,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:40,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:41,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:41,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:42,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:43,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:43,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:44,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:45,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:46,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:46,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:47,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:04<01:46, 15.20s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:47,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:48,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:49,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:50,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:50,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:51,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:52,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:52,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:53,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:54,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:55,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:55,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:56,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:57,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:57,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:58,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:59,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:00,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:00,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:01,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:02,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:03,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:04,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:05,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:05,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:06,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:23<01:38, 16.49s/it][WARNING|generation_utils.py:914] 2023-08-29 00:40:07,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:08,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:08,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:09,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:10,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:10,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:11,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:12,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:13,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:13,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:14,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:15,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:15,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:16,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:17,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:17,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:18,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:19,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:20,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:20,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:22,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:39<01:20, 16.18s/it][WARNING|generation_utils.py:914] 2023-08-29 00:40:22,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:23,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:23,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:24,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:25,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:25,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:26,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:27,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:27,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:28,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:28,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:29,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:29,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:30,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:31,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:31,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:32,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:33,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:33,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:34,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:51<00:59, 14.91s/it][WARNING|generation_utils.py:914] 2023-08-29 00:40:34,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:35,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:36,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:36,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:37,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:38,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:39,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:39,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:40,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:41,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:41,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:42,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:43,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:43,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:44,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:45,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:46,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:46,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:47,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:48,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:48,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:49,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:50,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:51,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:51,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:08<00:47, 15.78s/it][WARNING|generation_utils.py:914] 2023-08-29 00:40:52,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:53,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:54,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:54,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:55,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:56,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:57,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:57,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:58,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:59,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:00,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:00,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:01,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:02,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:03,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:04,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:04,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:05,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:06,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:07,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:07,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:08,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:09,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:10,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:27<00:33, 16.66s/it][WARNING|generation_utils.py:914] 2023-08-29 00:41:11,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:11,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:12,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:13,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:13,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:14,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:14,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:15,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:16,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:16,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:17,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:18,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:18,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:19,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:19,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:20,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:21,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:21,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:22,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:22,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:39<00:15, 15.32s/it][WARNING|generation_utils.py:914] 2023-08-29 00:41:23,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:24,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:24,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:25,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:26,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:27,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:28,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:28,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:29,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:30,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:31,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:32,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:32,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:33,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:34,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:35,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:35,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:36,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:37,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:38,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:38,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:39,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:40,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:41,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:58<00:00, 16.37s/it]Generating: 100%|██████████| 15/15 [03:58<00:00, 15.90s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:48,985 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:48,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:48,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:48,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:48,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:41:49,836 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:41:49,837 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:41:50,419 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:41:51,476 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:41:51,476 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:54,454 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:54,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:54,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:54,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:54,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:41:55,178 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:41:55,179 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:41:55,784 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:41:55,941 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:41:55,941 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : conflict . Context : Later in the year ( 1143 ) , he served as ambassador of the Austroamerican Empire from 1163 to 1189 until the death of Emperor Pius IX of Breslau on April 4 , 1185 . Head Entity : Emperor Pius IX of Breslau , Tail Entity : emperor .\n']
['Relation : conflict . Context : Later in the year ( 1143 ) , he served as ambassador of the Austroamerican Empire from 1163 to 1189 until the death of Emperor Pius IX of Breslau on April 4 , 1185 . Head Entity : Emperor Pius IX of Breslau , Tail Entity : emperor .\n', 'Relation : conflict . Context : After the war the British Empire expanded under his command into the Eastern Cape and Western Cape , South Africa to become part of the Commonwealth . Head Entity : Western Cape , Tail Entity : Australia .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 189, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 458, 'raw': 608}
{'target': 600, 'success': 487, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 606, 'raw': 800}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7575, 'errors': {''}}
['Relation : developer . Context : Later in 2008 , the series became a part of a PlayStation Portable video game entitled Project Gotham Knight II , released in North America for the Game Boy Advance . Head Entity : Project Gotham Knight II , Tail Entity : Nintendo .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : developer .', 'success_rate': 0.8288043478260869, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 411, 'raw': 416}
{'target': 600, 'success': 443, 'raw': 448}
{'target': 600, 'success': 475, 'raw': 480}
{'target': 600, 'success': 506, 'raw': 512}
{'target': 600, 'success': 537, 'raw': 544}
{'target': 600, 'success': 568, 'raw': 576}
{'target': 600, 'success': 598, 'raw': 608}
{'target': 600, 'success': 630, 'raw': 640}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.984375, 'errors': {''}}
['Relation : subsidiary . Context : Later in 2008 , the band became a part of a reunion tour of other acts at the end of 2010 , led by Rick Rubin s first album , titled , The Last Time , released on Warner Bros . Records . Head Entity : Warner Bros . Records , Tail Entity : Warner Bros . Entertainment .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 395, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8707386363636364, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best New Artist at the 2004 Grammy Awards . Head Entity : Best New Artist at the 2004 Grammy Awards , Tail Entity : music .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 328, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 451, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 524, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 570, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 624, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.75, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lyrics by . Context : Later in 2008 , the band recorded a single called The Day After , released on their 2011 album I Am , in which they sang the theme song to a song by the Beatles , titled Never Say Never . Head Entity : I Am , Tail Entity : John Lennon .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.975, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 520, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.775, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 565, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 438, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 502, 'raw': 512}
{'target': 600, 'success': 534, 'raw': 544}
{'target': 600, 'success': 565, 'raw': 576}
{'target': 600, 'success': 597, 'raw': 608}
{'target': 600, 'success': 629, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9828125, 'errors': {''}}
['Relation : twinned administrative body . Context : Later in the year ( 1143 ) , he served as a chancellor in the Legislative Assembly of the Kingdom of Great Britain from 1164 to 1172 , serving from 1176 , to 1185 , and finally 1241 . Head Entity : State Assembly , Tail Entity : Kingdom of Great Britain .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 11154
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11254, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.59it/s]Extractor Estimating: 2it [00:01,  1.52it/s]Extractor Estimating: 3it [00:01,  1.58it/s]Extractor Estimating: 4it [00:02,  1.61it/s]Extractor Estimating: 5it [00:03,  1.62it/s]Extractor Estimating: 6it [00:03,  1.61it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.51it/s]Extractor Estimating: 10it [00:06,  1.54it/s]Extractor Estimating: 11it [00:07,  1.57it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.54it/s]Extractor Estimating: 14it [00:08,  1.57it/s]Extractor Estimating: 15it [00:09,  1.63it/s]Extractor Estimating: 16it [00:10,  1.58it/s]Extractor Estimating: 17it [00:10,  1.61it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:12,  1.56it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:13,  1.62it/s]Extractor Estimating: 22it [00:13,  1.60it/s]Extractor Estimating: 23it [00:14,  1.59it/s]Extractor Estimating: 24it [00:15,  1.62it/s]Extractor Estimating: 25it [00:15,  1.63it/s]Extractor Estimating: 26it [00:16,  1.49it/s]Extractor Estimating: 27it [00:17,  1.57it/s]Extractor Estimating: 28it [00:17,  1.60it/s]Extractor Estimating: 29it [00:18,  1.61it/s]Extractor Estimating: 30it [00:18,  1.58it/s]Extractor Estimating: 31it [00:19,  1.60it/s]Extractor Estimating: 32it [00:20,  1.60it/s]Extractor Estimating: 33it [00:20,  1.61it/s]Extractor Estimating: 34it [00:21,  1.63it/s]Extractor Estimating: 35it [00:21,  1.66it/s]Extractor Estimating: 36it [00:22,  1.63it/s]Extractor Estimating: 37it [00:23,  1.69it/s]Extractor Estimating: 38it [00:23,  1.66it/s]Extractor Estimating: 39it [00:24,  1.59it/s]Extractor Estimating: 40it [00:25,  1.65it/s]Extractor Estimating: 41it [00:25,  1.67it/s]Extractor Estimating: 42it [00:26,  1.65it/s]Extractor Estimating: 43it [00:26,  1.65it/s]Extractor Estimating: 44it [00:27,  1.61it/s]Extractor Estimating: 45it [00:28,  1.62it/s]Extractor Estimating: 46it [00:28,  1.68it/s]Extractor Estimating: 47it [00:29,  1.69it/s]Extractor Estimating: 48it [00:29,  1.67it/s]Extractor Estimating: 49it [00:30,  1.71it/s]Extractor Estimating: 50it [00:31,  1.69it/s]Extractor Estimating: 51it [00:31,  1.79it/s]Extractor Estimating: 52it [00:32,  1.84it/s]Extractor Estimating: 53it [00:32,  1.90it/s]Extractor Estimating: 54it [00:33,  1.82it/s]Extractor Estimating: 55it [00:33,  1.88it/s]Extractor Estimating: 56it [00:34,  1.92it/s]Extractor Estimating: 57it [00:34,  1.98it/s]Extractor Estimating: 58it [00:35,  1.98it/s]Extractor Estimating: 59it [00:35,  1.95it/s]Extractor Estimating: 60it [00:36,  1.99it/s]Extractor Estimating: 61it [00:36,  2.08it/s]Extractor Estimating: 62it [00:36,  2.10it/s]Extractor Estimating: 63it [00:37,  2.09it/s]Extractor Estimating: 64it [00:37,  2.05it/s]Extractor Estimating: 65it [00:38,  2.08it/s]Extractor Estimating: 66it [00:38,  2.09it/s]Extractor Estimating: 67it [00:39,  1.98it/s]Extractor Estimating: 68it [00:39,  2.02it/s]Extractor Estimating: 69it [00:40,  2.03it/s]Extractor Estimating: 70it [00:40,  2.08it/s]Extractor Estimating: 71it [00:41,  2.08it/s]Extractor Estimating: 72it [00:41,  2.12it/s]Extractor Estimating: 73it [00:42,  2.10it/s]Extractor Estimating: 74it [00:42,  2.03it/s]Extractor Estimating: 75it [00:43,  1.94it/s]Extractor Estimating: 76it [00:44,  1.79it/s]Extractor Estimating: 77it [00:44,  1.65it/s]Extractor Estimating: 78it [00:45,  1.60it/s]Extractor Estimating: 79it [00:46,  1.61it/s]Extractor Estimating: 80it [00:46,  1.60it/s]Extractor Estimating: 81it [00:47,  1.60it/s]Extractor Estimating: 82it [00:47,  1.60it/s]Extractor Estimating: 83it [00:48,  1.65it/s]Extractor Estimating: 84it [00:49,  1.63it/s]Extractor Estimating: 85it [00:49,  1.59it/s]Extractor Estimating: 86it [00:50,  1.53it/s]Extractor Estimating: 87it [00:51,  1.54it/s]Extractor Estimating: 88it [00:51,  1.51it/s]Extractor Estimating: 89it [00:52,  1.50it/s]Extractor Estimating: 90it [00:53,  1.51it/s]Extractor Estimating: 91it [00:53,  1.51it/s]Extractor Estimating: 92it [00:54,  1.53it/s]Extractor Estimating: 93it [00:55,  1.57it/s]Extractor Estimating: 94it [00:55,  1.59it/s]Extractor Estimating: 95it [00:56,  1.53it/s]Extractor Estimating: 96it [00:56,  1.57it/s]Extractor Estimating: 97it [00:57,  1.55it/s]Extractor Estimating: 98it [00:58,  1.61it/s]Extractor Estimating: 99it [00:58,  1.59it/s]Extractor Estimating: 100it [00:59,  1.57it/s]Extractor Estimating: 101it [01:00,  1.56it/s]Extractor Estimating: 102it [01:00,  1.54it/s]Extractor Estimating: 103it [01:01,  1.52it/s]Extractor Estimating: 104it [01:02,  1.57it/s]Extractor Estimating: 105it [01:02,  1.58it/s]Extractor Estimating: 106it [01:03,  1.56it/s]Extractor Estimating: 107it [01:04,  1.56it/s]Extractor Estimating: 108it [01:04,  1.57it/s]Extractor Estimating: 109it [01:05,  1.57it/s]Extractor Estimating: 110it [01:05,  1.51it/s]Extractor Estimating: 111it [01:06,  1.54it/s]Extractor Estimating: 112it [01:07,  1.49it/s]Extractor Estimating: 113it [01:08,  1.47it/s]Extractor Estimating: 114it [01:08,  1.50it/s]Extractor Estimating: 115it [01:09,  1.47it/s]Extractor Estimating: 116it [01:10,  1.51it/s]Extractor Estimating: 117it [01:10,  1.58it/s]Extractor Estimating: 118it [01:11,  1.58it/s]Extractor Estimating: 119it [01:11,  1.56it/s]Extractor Estimating: 120it [01:12,  1.57it/s]Extractor Estimating: 121it [01:13,  1.56it/s]Extractor Estimating: 122it [01:13,  1.58it/s]Extractor Estimating: 123it [01:14,  1.55it/s]Extractor Estimating: 124it [01:15,  1.55it/s]Extractor Estimating: 125it [01:15,  1.54it/s]Extractor Estimating: 126it [01:16,  1.52it/s]Extractor Estimating: 127it [01:17,  1.46it/s]Extractor Estimating: 128it [01:17,  1.51it/s]Extractor Estimating: 129it [01:18,  1.53it/s]Extractor Estimating: 130it [01:19,  1.51it/s]Extractor Estimating: 131it [01:19,  1.54it/s]Extractor Estimating: 132it [01:20,  1.54it/s]Extractor Estimating: 133it [01:20,  1.56it/s]Extractor Estimating: 134it [01:21,  1.55it/s]Extractor Estimating: 135it [01:22,  1.56it/s]Extractor Estimating: 136it [01:22,  1.56it/s]Extractor Estimating: 137it [01:23,  1.57it/s]Extractor Estimating: 138it [01:24,  1.44it/s]Extractor Estimating: 139it [01:24,  1.48it/s]Extractor Estimating: 140it [01:25,  1.52it/s]Extractor Estimating: 141it [01:26,  1.54it/s]Extractor Estimating: 142it [01:26,  1.52it/s]Extractor Estimating: 143it [01:27,  1.53it/s]Extractor Estimating: 144it [01:28,  1.57it/s]Extractor Estimating: 145it [01:28,  1.59it/s]Extractor Estimating: 146it [01:29,  1.59it/s]Extractor Estimating: 147it [01:30,  1.60it/s]Extractor Estimating: 148it [01:30,  1.57it/s]Extractor Estimating: 149it [01:31,  1.51it/s]Extractor Estimating: 150it [01:32,  1.50it/s]Extractor Estimating: 151it [01:32,  1.57it/s]Extractor Estimating: 152it [01:33,  1.55it/s]Extractor Estimating: 153it [01:33,  1.58it/s]Extractor Estimating: 154it [01:34,  1.56it/s]Extractor Estimating: 155it [01:35,  1.56it/s]Extractor Estimating: 156it [01:35,  1.57it/s]Extractor Estimating: 157it [01:36,  1.58it/s]Extractor Estimating: 158it [01:37,  1.57it/s]Extractor Estimating: 159it [01:37,  1.56it/s]Extractor Estimating: 160it [01:38,  1.56it/s]Extractor Estimating: 161it [01:39,  1.58it/s]Extractor Estimating: 162it [01:39,  1.56it/s]Extractor Estimating: 163it [01:40,  1.57it/s]Extractor Estimating: 164it [01:40,  1.58it/s]Extractor Estimating: 165it [01:41,  1.56it/s]Extractor Estimating: 166it [01:42,  1.62it/s]Extractor Estimating: 167it [01:42,  1.63it/s]Extractor Estimating: 168it [01:43,  1.61it/s]Extractor Estimating: 169it [01:44,  1.59it/s]Extractor Estimating: 170it [01:44,  1.58it/s]Extractor Estimating: 171it [01:45,  1.58it/s]Extractor Estimating: 172it [01:45,  1.57it/s]Extractor Estimating: 173it [01:46,  1.55it/s]Extractor Estimating: 174it [01:47,  1.53it/s]Extractor Estimating: 175it [01:47,  1.56it/s]Extractor Estimating: 176it [01:48,  1.61it/s]Extractor Estimating: 177it [01:49,  1.60it/s]Extractor Estimating: 178it [01:49,  1.56it/s]Extractor Estimating: 179it [01:50,  1.55it/s]Extractor Estimating: 180it [01:51,  1.57it/s]Extractor Estimating: 181it [01:51,  1.58it/s]Extractor Estimating: 182it [01:52,  1.55it/s]Extractor Estimating: 183it [01:53,  1.56it/s]Extractor Estimating: 184it [01:53,  1.55it/s]Extractor Estimating: 185it [01:54,  1.57it/s]Extractor Estimating: 186it [01:54,  1.61it/s]Extractor Estimating: 187it [01:55,  1.61it/s]Extractor Estimating: 188it [01:56,  1.62it/s]Extractor Estimating: 189it [01:56,  1.60it/s]Extractor Estimating: 190it [01:57,  1.65it/s]Extractor Estimating: 191it [01:57,  1.64it/s]Extractor Estimating: 192it [01:58,  1.59it/s]Extractor Estimating: 193it [01:59,  1.60it/s]Extractor Estimating: 194it [01:59,  1.61it/s]Extractor Estimating: 195it [02:00,  1.63it/s]Extractor Estimating: 196it [02:01,  1.59it/s]Extractor Estimating: 197it [02:01,  1.60it/s]Extractor Estimating: 198it [02:02,  1.61it/s]Extractor Estimating: 199it [02:02,  1.62it/s]Extractor Estimating: 200it [02:03,  1.60it/s]Extractor Estimating: 201it [02:04,  1.62it/s]Extractor Estimating: 202it [02:04,  1.60it/s]Extractor Estimating: 203it [02:05,  1.57it/s]Extractor Estimating: 204it [02:06,  1.55it/s]Extractor Estimating: 205it [02:06,  1.44it/s]Extractor Estimating: 206it [02:07,  1.47it/s]Extractor Estimating: 207it [02:08,  1.48it/s]Extractor Estimating: 208it [02:08,  1.46it/s]Extractor Estimating: 209it [02:09,  1.44it/s]Extractor Estimating: 210it [02:10,  1.51it/s]Extractor Estimating: 211it [02:10,  1.51it/s]Extractor Estimating: 212it [02:11,  1.56it/s]Extractor Estimating: 213it [02:12,  1.54it/s]Extractor Estimating: 214it [02:12,  1.52it/s]Extractor Estimating: 215it [02:13,  1.54it/s]Extractor Estimating: 216it [02:14,  1.57it/s]Extractor Estimating: 217it [02:14,  1.54it/s]Extractor Estimating: 218it [02:15,  1.57it/s]Extractor Estimating: 219it [02:16,  1.51it/s]Extractor Estimating: 220it [02:16,  1.56it/s]Extractor Estimating: 221it [02:17,  1.52it/s]Extractor Estimating: 222it [02:18,  1.48it/s]Extractor Estimating: 223it [02:18,  1.49it/s]Extractor Estimating: 224it [02:19,  1.49it/s]Extractor Estimating: 225it [02:20,  1.48it/s]Extractor Estimating: 226it [02:20,  1.46it/s]Extractor Estimating: 227it [02:21,  1.46it/s]Extractor Estimating: 228it [02:22,  1.51it/s]Extractor Estimating: 229it [02:22,  1.52it/s]Extractor Estimating: 230it [02:23,  1.50it/s]Extractor Estimating: 231it [02:24,  1.51it/s]Extractor Estimating: 232it [02:24,  1.44it/s]Extractor Estimating: 233it [02:25,  1.46it/s]Extractor Estimating: 234it [02:26,  1.46it/s]Extractor Estimating: 235it [02:26,  1.49it/s]Extractor Estimating: 236it [02:27,  1.48it/s]Extractor Estimating: 237it [02:28,  1.48it/s]Extractor Estimating: 238it [02:28,  1.48it/s]Extractor Estimating: 239it [02:29,  1.49it/s]Extractor Estimating: 240it [02:30,  1.51it/s]Extractor Estimating: 241it [02:30,  1.51it/s]Extractor Estimating: 242it [02:31,  1.50it/s]Extractor Estimating: 243it [02:32,  1.50it/s]Extractor Estimating: 244it [02:32,  1.48it/s]Extractor Estimating: 245it [02:33,  1.50it/s]Extractor Estimating: 246it [02:34,  1.48it/s]Extractor Estimating: 247it [02:34,  1.51it/s]Extractor Estimating: 248it [02:35,  1.52it/s]Extractor Estimating: 249it [02:36,  1.52it/s]Extractor Estimating: 250it [02:36,  1.53it/s]Extractor Estimating: 251it [02:37,  1.52it/s]Extractor Estimating: 252it [02:38,  1.53it/s]Extractor Estimating: 253it [02:38,  1.53it/s]Extractor Estimating: 254it [02:39,  1.50it/s]Extractor Estimating: 255it [02:40,  1.52it/s]Extractor Estimating: 256it [02:40,  1.51it/s]Extractor Estimating: 257it [02:41,  1.53it/s]Extractor Estimating: 258it [02:42,  1.54it/s]Extractor Estimating: 259it [02:42,  1.52it/s]Extractor Estimating: 260it [02:43,  1.55it/s]Extractor Estimating: 261it [02:44,  1.54it/s]Extractor Estimating: 262it [02:44,  1.53it/s]Extractor Estimating: 263it [02:45,  1.53it/s]Extractor Estimating: 264it [02:45,  1.55it/s]Extractor Estimating: 265it [02:46,  1.53it/s]Extractor Estimating: 266it [02:47,  1.51it/s]Extractor Estimating: 267it [02:47,  1.51it/s]Extractor Estimating: 268it [02:48,  1.51it/s]Extractor Estimating: 269it [02:49,  1.53it/s]Extractor Estimating: 270it [02:49,  1.53it/s]Extractor Estimating: 271it [02:50,  1.53it/s]Extractor Estimating: 272it [02:51,  1.53it/s]Extractor Estimating: 273it [02:51,  1.54it/s]Extractor Estimating: 274it [02:52,  1.54it/s]Extractor Estimating: 275it [02:53,  1.53it/s]Extractor Estimating: 276it [02:53,  1.54it/s]Extractor Estimating: 277it [02:54,  1.55it/s]Extractor Estimating: 278it [02:55,  1.51it/s]Extractor Estimating: 279it [02:55,  1.52it/s]Extractor Estimating: 280it [02:56,  1.52it/s]Extractor Estimating: 281it [02:57,  1.54it/s]Extractor Estimating: 282it [02:57,  1.54it/s]Extractor Estimating: 283it [02:58,  1.41it/s]Extractor Estimating: 284it [02:59,  1.47it/s]Extractor Estimating: 285it [02:59,  1.48it/s]Extractor Estimating: 286it [03:00,  1.51it/s]Extractor Estimating: 287it [03:01,  1.50it/s]Extractor Estimating: 288it [03:01,  1.51it/s]Extractor Estimating: 289it [03:02,  1.51it/s]Extractor Estimating: 290it [03:03,  1.53it/s]Extractor Estimating: 291it [03:03,  1.55it/s]Extractor Estimating: 292it [03:04,  1.53it/s]Extractor Estimating: 293it [03:05,  1.54it/s]Extractor Estimating: 294it [03:05,  1.51it/s]Extractor Estimating: 295it [03:06,  1.52it/s]Extractor Estimating: 296it [03:07,  1.53it/s]Extractor Estimating: 297it [03:07,  1.51it/s]Extractor Estimating: 298it [03:08,  1.49it/s]Extractor Estimating: 299it [03:09,  1.52it/s]Extractor Estimating: 300it [03:09,  1.50it/s]Extractor Estimating: 301it [03:10,  1.48it/s]Extractor Estimating: 302it [03:11,  1.48it/s]Extractor Estimating: 303it [03:11,  1.44it/s]Extractor Estimating: 304it [03:12,  1.46it/s]Extractor Estimating: 305it [03:13,  1.51it/s]Extractor Estimating: 306it [03:13,  1.50it/s]Extractor Estimating: 307it [03:14,  1.50it/s]Extractor Estimating: 308it [03:15,  1.50it/s]Extractor Estimating: 309it [03:15,  1.52it/s]Extractor Estimating: 310it [03:16,  1.56it/s]Extractor Estimating: 311it [03:17,  1.55it/s]Extractor Estimating: 312it [03:17,  1.54it/s]Extractor Estimating: 313it [03:18,  1.53it/s]Extractor Estimating: 314it [03:19,  1.51it/s]Extractor Estimating: 315it [03:19,  1.47it/s]Extractor Estimating: 316it [03:20,  1.46it/s]Extractor Estimating: 317it [03:21,  1.47it/s]Extractor Estimating: 318it [03:21,  1.47it/s]Extractor Estimating: 319it [03:22,  1.49it/s]Extractor Estimating: 320it [03:23,  1.49it/s]Extractor Estimating: 321it [03:23,  1.45it/s]Extractor Estimating: 322it [03:24,  1.50it/s]Extractor Estimating: 323it [03:25,  1.51it/s]Extractor Estimating: 324it [03:25,  1.52it/s]Extractor Estimating: 325it [03:26,  1.51it/s]Extractor Estimating: 326it [03:27,  1.50it/s]Extractor Estimating: 327it [03:27,  1.50it/s]Extractor Estimating: 328it [03:28,  1.50it/s]Extractor Estimating: 329it [03:29,  1.51it/s]Extractor Estimating: 330it [03:29,  1.51it/s]Extractor Estimating: 331it [03:30,  1.54it/s]Extractor Estimating: 332it [03:31,  1.52it/s]Extractor Estimating: 333it [03:31,  1.53it/s]Extractor Estimating: 334it [03:32,  1.52it/s]Extractor Estimating: 335it [03:33,  1.49it/s]Extractor Estimating: 336it [03:33,  1.50it/s]Extractor Estimating: 337it [03:34,  1.50it/s]Extractor Estimating: 338it [03:35,  1.49it/s]Extractor Estimating: 339it [03:35,  1.50it/s]Extractor Estimating: 340it [03:36,  1.50it/s]Extractor Estimating: 341it [03:37,  1.51it/s]Extractor Estimating: 342it [03:37,  1.51it/s]Extractor Estimating: 343it [03:38,  1.51it/s]Extractor Estimating: 344it [03:39,  1.48it/s]Extractor Estimating: 345it [03:39,  1.47it/s]Extractor Estimating: 346it [03:40,  1.47it/s]Extractor Estimating: 347it [03:41,  1.49it/s]Extractor Estimating: 348it [03:41,  1.50it/s]Extractor Estimating: 349it [03:42,  1.52it/s]Extractor Estimating: 350it [03:43,  1.52it/s]Extractor Estimating: 351it [03:43,  1.54it/s]Extractor Estimating: 352it [03:44,  1.59it/s]Extractor Estimating: 353it [03:44,  1.59it/s]Extractor Estimating: 354it [03:45,  1.63it/s]Extractor Estimating: 355it [03:46,  1.57it/s]Extractor Estimating: 356it [03:46,  1.57it/s]Extractor Estimating: 357it [03:47,  1.58it/s]Extractor Estimating: 358it [03:48,  1.57it/s]Extractor Estimating: 359it [03:48,  1.63it/s]Extractor Estimating: 360it [03:49,  1.64it/s]Extractor Estimating: 361it [03:49,  1.61it/s]Extractor Estimating: 362it [03:50,  1.56it/s]Extractor Estimating: 363it [03:51,  1.57it/s]Extractor Estimating: 364it [03:51,  1.55it/s]Extractor Estimating: 365it [03:52,  1.59it/s]Extractor Estimating: 366it [03:53,  1.64it/s]Extractor Estimating: 367it [03:53,  1.63it/s]Extractor Estimating: 368it [03:54,  1.66it/s]Extractor Estimating: 369it [03:54,  1.57it/s]Extractor Estimating: 370it [03:55,  1.51it/s]Extractor Estimating: 371it [03:56,  1.52it/s]Extractor Estimating: 372it [03:57,  1.42it/s]Extractor Estimating: 373it [03:57,  1.49it/s]Extractor Estimating: 374it [03:58,  1.51it/s]Extractor Estimating: 375it [03:58,  1.82it/s]Extractor Estimating: 375it [03:58,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:14,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:14,996 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:14,996 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:14,996 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:14,996 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:46:15,629 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:46:15,630 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:46:16,189 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:46:17,252 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:46:17,252 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:20,225 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:20,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:20,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:20,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:20,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:46:20,902 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:46:20,903 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:46:21,487 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:46:21,643 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:46:21,643 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 03:13:08,445 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 03:13:08,463 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7491 mean pseudo reward: 0.9563534453019609
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 22821
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22921, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22921, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.072, loss:672.4355
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.091, loss:642.3390
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.081, loss:639.2945
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.074, loss:628.1695
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.075, loss:614.2883
>> valid entity prec:0.5997, rec:0.5983, f1:0.5990
>> valid relation prec:0.3059, rec:0.0412, f1:0.0726
>> valid relation with NER prec:0.3059, rec:0.0412, f1:0.0726
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.830, loss:628.8079
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.065, loss:609.0778
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.073, loss:623.5245
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.072, loss:633.2108
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.073, loss:617.9642
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5180, rec:0.6627, f1:0.5815
>> valid relation prec:0.2027, rec:0.0402, f1:0.0671
>> valid relation with NER prec:0.2027, rec:0.0402, f1:0.0671
g_step 1100, step 161, avg_time 2.833, loss:608.9251
g_step 1200, step 261, avg_time 1.062, loss:629.2930
g_step 1300, step 48, avg_time 1.084, loss:604.9723
g_step 1400, step 148, avg_time 1.070, loss:595.7207
g_step 1500, step 248, avg_time 1.071, loss:588.3660
>> valid entity prec:0.6008, rec:0.5128, f1:0.5533
>> valid relation prec:0.2414, rec:0.0361, f1:0.0628
>> valid relation with NER prec:0.2414, rec:0.0361, f1:0.0628
g_step 1600, step 35, avg_time 2.822, loss:574.0311
g_step 1700, step 135, avg_time 1.074, loss:595.2881
g_step 1800, step 235, avg_time 1.074, loss:574.5715
g_step 1900, step 22, avg_time 1.065, loss:548.2248
g_step 2000, step 122, avg_time 1.068, loss:556.2844
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5856, rec:0.5253, f1:0.5538
>> valid relation prec:0.2164, rec:0.0367, f1:0.0627
>> valid relation with NER prec:0.2164, rec:0.0367, f1:0.0627
g_step 2100, step 222, avg_time 2.833, loss:556.7962
g_step 2200, step 9, avg_time 1.065, loss:538.0788
g_step 2300, step 109, avg_time 1.074, loss:522.4534
g_step 2400, step 209, avg_time 1.072, loss:524.3333
g_step 2500, step 309, avg_time 1.067, loss:525.3181
>> valid entity prec:0.5614, rec:0.5918, f1:0.5762
>> valid relation prec:0.2871, rec:0.0543, f1:0.0913
>> valid relation with NER prec:0.2871, rec:0.0543, f1:0.0913
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.810, loss:500.1803
g_step 2700, step 196, avg_time 1.075, loss:499.4908
g_step 2800, step 296, avg_time 1.070, loss:511.0001
g_step 2900, step 83, avg_time 1.069, loss:468.2883
g_step 3000, step 183, avg_time 1.075, loss:495.4590
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5888, rec:0.4896, f1:0.5346
>> valid relation prec:0.2849, rec:0.0494, f1:0.0842
>> valid relation with NER prec:0.2849, rec:0.0494, f1:0.0842
g_step 3100, step 283, avg_time 2.815, loss:506.4926
g_step 3200, step 70, avg_time 1.064, loss:461.6864
g_step 3300, step 170, avg_time 1.076, loss:475.9097
g_step 3400, step 270, avg_time 1.065, loss:472.8187
g_step 3500, step 57, avg_time 1.059, loss:440.9161
>> valid entity prec:0.5591, rec:0.4584, f1:0.5038
>> valid relation prec:0.2473, rec:0.0324, f1:0.0573
>> valid relation with NER prec:0.2473, rec:0.0324, f1:0.0573
g_step 3600, step 157, avg_time 2.822, loss:447.1719
g_step 3700, step 257, avg_time 1.071, loss:468.2072
g_step 3800, step 44, avg_time 1.060, loss:447.7565
g_step 3900, step 144, avg_time 1.079, loss:430.7771
g_step 4000, step 244, avg_time 1.075, loss:464.4115
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5895, rec:0.4990, f1:0.5405
>> valid relation prec:0.3040, rec:0.0594, f1:0.0994
>> valid relation with NER prec:0.3040, rec:0.0594, f1:0.0994
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 31, avg_time 2.812, loss:429.3160
g_step 4200, step 131, avg_time 1.063, loss:407.7774
g_step 4300, step 231, avg_time 1.085, loss:432.8402
g_step 4400, step 18, avg_time 1.068, loss:433.4457
g_step 4500, step 118, avg_time 1.082, loss:393.0905
>> valid entity prec:0.5396, rec:0.5872, f1:0.5624
>> valid relation prec:0.1739, rec:0.0482, f1:0.0754
>> valid relation with NER prec:0.1739, rec:0.0482, f1:0.0754
g_step 4600, step 218, avg_time 2.816, loss:417.0723
g_step 4700, step 5, avg_time 1.056, loss:407.6455
g_step 4800, step 105, avg_time 1.070, loss:379.4189
g_step 4900, step 205, avg_time 1.073, loss:396.8639
g_step 5000, step 305, avg_time 1.069, loss:407.2133
learning rate was adjusted to 0.0008
>> valid entity prec:0.5182, rec:0.4781, f1:0.4974
>> valid relation prec:0.1637, rec:0.0344, f1:0.0569
>> valid relation with NER prec:0.1637, rec:0.0344, f1:0.0569
g_step 5100, step 92, avg_time 2.818, loss:373.2380
g_step 5200, step 192, avg_time 1.063, loss:378.8868
g_step 5300, step 292, avg_time 1.076, loss:391.0444
g_step 5400, step 79, avg_time 1.068, loss:362.8265
g_step 5500, step 179, avg_time 1.062, loss:359.8286
>> valid entity prec:0.5393, rec:0.4762, f1:0.5058
>> valid relation prec:0.2101, rec:0.0375, f1:0.0637
>> valid relation with NER prec:0.2101, rec:0.0375, f1:0.0637
g_step 5600, step 279, avg_time 2.827, loss:388.5667
g_step 5700, step 66, avg_time 1.066, loss:348.7431
g_step 5800, step 166, avg_time 1.070, loss:349.0072
g_step 5900, step 266, avg_time 1.068, loss:365.8336
g_step 6000, step 53, avg_time 1.068, loss:346.6072
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5520, rec:0.4279, f1:0.4821
>> valid relation prec:0.2197, rec:0.0449, f1:0.0745
>> valid relation with NER prec:0.2197, rec:0.0449, f1:0.0745
g_step 6100, step 153, avg_time 2.824, loss:348.3501
g_step 6200, step 253, avg_time 1.071, loss:349.3453
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 03:13:08 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 03:13:08 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_03-13-08_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 03:13:09 - WARNING - datasets.builder -   Using custom data configuration default-6dd83df94bf25174
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6dd83df94bf25174/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 03:13:09,731 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:13:09,732 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:13:09,733 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:13:09,734 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:13:09,741 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:13:09,744 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:13:09,744 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:13:09,745 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:13:09,745 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:13:09,745 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:13:09,745 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 03:13:09,888 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:13:12,987 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 03:13:12,990 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6dd83df94bf25174/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.22ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.01ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.40ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.54ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.62ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.69ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.71ba/s]100%|██████████| 8/8 [00:01<00:00,  5.51ba/s]100%|██████████| 8/8 [00:01<00:00,  4.79ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.93ba/s] 40%|████      | 2/5 [00:00<00:00,  4.31ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.57ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.92ba/s]100%|██████████| 5/5 [00:01<00:00,  4.31ba/s]100%|██████████| 5/5 [00:01<00:00,  4.11ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.83ba/s] 25%|██▌       | 2/8 [00:00<00:00,  8.84ba/s] 50%|█████     | 4/8 [00:00<00:00,  9.81ba/s] 75%|███████▌  | 6/8 [00:00<00:00, 10.22ba/s]100%|██████████| 8/8 [00:00<00:00, 11.35ba/s]100%|██████████| 8/8 [00:00<00:00, 10.59ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.46ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.00ba/s]100%|██████████| 5/5 [00:00<00:00, 10.55ba/s]100%|██████████| 5/5 [00:00<00:00, 10.30ba/s]
[INFO|trainer.py:414] 2023-08-29 03:13:17,513 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 03:13:17,528 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 03:13:17,528 >>   Num examples = 7546
[INFO|trainer.py:1149] 2023-08-29 03:13:17,528 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 03:13:17,528 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 03:13:17,528 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 03:13:17,528 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 03:13:17,528 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:55,  3.36it/s]  0%|          | 2/590 [00:00<02:50,  3.45it/s]  1%|          | 3/590 [00:00<02:49,  3.47it/s]  1%|          | 4/590 [00:01<02:48,  3.49it/s]  1%|          | 5/590 [00:01<02:47,  3.49it/s]  1%|          | 6/590 [00:01<02:47,  3.50it/s]  1%|          | 7/590 [00:02<02:47,  3.49it/s]  1%|▏         | 8/590 [00:02<02:46,  3.49it/s]  2%|▏         | 9/590 [00:02<02:46,  3.50it/s]  2%|▏         | 10/590 [00:02<02:45,  3.50it/s]  2%|▏         | 11/590 [00:03<02:45,  3.50it/s]  2%|▏         | 12/590 [00:03<02:45,  3.50it/s]  2%|▏         | 13/590 [00:03<02:44,  3.50it/s]  2%|▏         | 14/590 [00:04<02:44,  3.50it/s]  3%|▎         | 15/590 [00:04<02:44,  3.50it/s]  3%|▎         | 16/590 [00:04<02:43,  3.50it/s]  3%|▎         | 17/590 [00:04<02:43,  3.50it/s]  3%|▎         | 18/590 [00:05<02:43,  3.49it/s]  3%|▎         | 19/590 [00:05<02:43,  3.50it/s]  3%|▎         | 20/590 [00:05<02:42,  3.50it/s]  4%|▎         | 21/590 [00:06<02:42,  3.50it/s]  4%|▎         | 22/590 [00:06<02:42,  3.50it/s]  4%|▍         | 23/590 [00:06<02:41,  3.50it/s]  4%|▍         | 24/590 [00:06<02:41,  3.50it/s]  4%|▍         | 25/590 [00:07<02:41,  3.50it/s]  4%|▍         | 26/590 [00:07<02:41,  3.50it/s]  5%|▍         | 27/590 [00:07<02:40,  3.50it/s]  5%|▍         | 28/590 [00:08<02:40,  3.50it/s]  5%|▍         | 29/590 [00:08<02:40,  3.49it/s]  5%|▌         | 30/590 [00:08<02:40,  3.49it/s]  5%|▌         | 31/590 [00:08<02:39,  3.49it/s]  5%|▌         | 32/590 [00:09<02:39,  3.50it/s]  6%|▌         | 33/590 [00:09<02:39,  3.49it/s]  6%|▌         | 34/590 [00:09<02:39,  3.50it/s]  6%|▌         | 35/590 [00:10<02:38,  3.50it/s]  6%|▌         | 36/590 [00:10<02:38,  3.50it/s]  6%|▋         | 37/590 [00:10<02:38,  3.50it/s]  6%|▋         | 38/590 [00:10<02:37,  3.49it/s]  7%|▋         | 39/590 [00:11<02:37,  3.49it/s]  7%|▋         | 40/590 [00:11<02:39,  3.45it/s]  7%|▋         | 41/590 [00:11<02:38,  3.46it/s]  7%|▋         | 42/590 [00:12<02:38,  3.47it/s]  7%|▋         | 43/590 [00:12<02:37,  3.47it/s]  7%|▋         | 44/590 [00:12<02:37,  3.47it/s]  8%|▊         | 45/590 [00:12<02:36,  3.48it/s]  8%|▊         | 46/590 [00:13<02:36,  3.48it/s]  8%|▊         | 47/590 [00:13<02:35,  3.48it/s]  8%|▊         | 48/590 [00:13<02:35,  3.49it/s]  8%|▊         | 49/590 [00:14<02:35,  3.48it/s]  8%|▊         | 50/590 [00:14<02:35,  3.48it/s]  9%|▊         | 51/590 [00:14<02:35,  3.46it/s]  9%|▉         | 52/590 [00:14<02:35,  3.46it/s]  9%|▉         | 53/590 [00:15<02:35,  3.46it/s]  9%|▉         | 54/590 [00:15<02:34,  3.46it/s]  9%|▉         | 55/590 [00:15<02:34,  3.47it/s]  9%|▉         | 56/590 [00:16<02:33,  3.48it/s] 10%|▉         | 57/590 [00:16<02:33,  3.48it/s] 10%|▉         | 58/590 [00:16<02:32,  3.48it/s] 10%|█         | 59/590 [00:16<02:32,  3.49it/s] 10%|█         | 60/590 [00:17<02:31,  3.49it/s] 10%|█         | 61/590 [00:17<02:31,  3.49it/s] 11%|█         | 62/590 [00:17<02:37,  3.34it/s] 11%|█         | 63/590 [00:18<02:35,  3.39it/s] 11%|█         | 64/590 [00:18<02:33,  3.42it/s] 11%|█         | 65/590 [00:18<02:32,  3.44it/s] 11%|█         | 66/590 [00:18<02:31,  3.45it/s] 11%|█▏        | 67/590 [00:19<02:30,  3.46it/s] 12%|█▏        | 68/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 69/590 [00:19<02:29,  3.47it/s] 12%|█▏        | 70/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 71/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 72/590 [00:20<02:28,  3.48it/s] 12%|█▏        | 73/590 [00:20<02:29,  3.47it/s] 13%|█▎        | 74/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 75/590 [00:21<02:28,  3.48it/s] 13%|█▎        | 76/590 [00:21<02:27,  3.48it/s] 13%|█▎        | 77/590 [00:22<02:27,  3.48it/s] 13%|█▎        | 78/590 [00:22<02:27,  3.48it/s] 13%|█▎        | 79/590 [00:22<02:26,  3.49it/s] 14%|█▎        | 80/590 [00:22<02:26,  3.48it/s] 14%|█▎        | 81/590 [00:23<02:26,  3.48it/s] 14%|█▍        | 82/590 [00:23<02:25,  3.49it/s] 14%|█▍        | 83/590 [00:23<02:25,  3.49it/s] 14%|█▍        | 84/590 [00:24<02:25,  3.47it/s] 14%|█▍        | 85/590 [00:24<02:25,  3.47it/s] 15%|█▍        | 86/590 [00:24<02:24,  3.48it/s] 15%|█▍        | 87/590 [00:25<02:24,  3.48it/s] 15%|█▍        | 88/590 [00:25<02:24,  3.48it/s] 15%|█▌        | 89/590 [00:25<02:23,  3.48it/s] 15%|█▌        | 90/590 [00:25<02:23,  3.49it/s] 15%|█▌        | 91/590 [00:26<02:23,  3.48it/s] 16%|█▌        | 92/590 [00:26<02:22,  3.48it/s] 16%|█▌        | 93/590 [00:26<02:22,  3.49it/s] 16%|█▌        | 94/590 [00:27<02:22,  3.48it/s] 16%|█▌        | 95/590 [00:27<02:22,  3.47it/s] 16%|█▋        | 96/590 [00:27<02:22,  3.48it/s] 16%|█▋        | 97/590 [00:27<02:21,  3.48it/s] 17%|█▋        | 98/590 [00:28<02:21,  3.48it/s] 17%|█▋        | 99/590 [00:28<02:21,  3.48it/s] 17%|█▋        | 100/590 [00:28<02:20,  3.48it/s] 17%|█▋        | 101/590 [00:29<02:20,  3.48it/s] 17%|█▋        | 102/590 [00:29<02:19,  3.49it/s] 17%|█▋        | 103/590 [00:29<02:19,  3.48it/s] 18%|█▊        | 104/590 [00:29<02:19,  3.48it/s] 18%|█▊        | 105/590 [00:30<02:19,  3.48it/s] 18%|█▊        | 106/590 [00:30<02:19,  3.48it/s] 18%|█▊        | 107/590 [00:30<02:18,  3.48it/s] 18%|█▊        | 108/590 [00:31<02:18,  3.48it/s] 18%|█▊        | 109/590 [00:31<02:18,  3.48it/s] 19%|█▊        | 110/590 [00:31<02:17,  3.48it/s] 19%|█▉        | 111/590 [00:31<02:17,  3.48it/s] 19%|█▉        | 112/590 [00:32<02:17,  3.48it/s] 19%|█▉        | 113/590 [00:32<02:17,  3.48it/s] 19%|█▉        | 114/590 [00:32<02:16,  3.48it/s] 19%|█▉        | 115/590 [00:33<02:16,  3.48it/s] 20%|█▉        | 116/590 [00:33<02:16,  3.48it/s] 20%|█▉        | 117/590 [00:33<02:16,  3.47it/s] 20%|██        | 118/590 [00:33<02:12,  3.55it/s][INFO|trainer.py:2140] 2023-08-29 03:13:51,421 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:13:51,421 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:13:51,422 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.48it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.38it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.65it/s][A
  4%|▍         | 23/611 [00:00<00:12, 48.09it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.71it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.53it/s][A
  6%|▌         | 38/611 [00:00<00:12, 47.20it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.93it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.80it/s][A
  9%|▊         | 53/611 [00:01<00:11, 46.81it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.78it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.68it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.75it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.85it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.92it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.95it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.94it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.82it/s][A
 16%|█▌        | 98/611 [00:02<00:10, 46.79it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.80it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.77it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.71it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.72it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.75it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.78it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.87it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.83it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.76it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.77it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.78it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.75it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.76it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.75it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.80it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.89it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.86it/s][A
 31%|███       | 188/611 [00:03<00:09, 46.85it/s][A
 32%|███▏      | 193/611 [00:04<00:08, 46.66it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.77it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.78it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.78it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.75it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.80it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.85it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.79it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.83it/s][A
 39%|███▉      | 238/611 [00:05<00:07, 46.80it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.78it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.82it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.81it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.65it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.70it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.70it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.76it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.80it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.80it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.82it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.77it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.79it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.80it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.80it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.71it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.64it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.73it/s][A
 54%|█████▎    | 328/611 [00:06<00:06, 46.71it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.71it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.68it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.70it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.32it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.63it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.59it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.59it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.65it/s][A
 61%|██████    | 373/611 [00:07<00:05, 46.75it/s][A
 62%|██████▏   | 378/611 [00:08<00:04, 46.68it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.71it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.75it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.76it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.77it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.79it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.78it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.71it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.79it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.79it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.74it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.71it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.74it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.60it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.71it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.74it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.75it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.77it/s][A
 77%|███████▋  | 468/611 [00:09<00:03, 46.77it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.81it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.78it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.66it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.68it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.72it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.70it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.75it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.75it/s][A
 84%|████████▍ | 513/611 [00:10<00:02, 46.78it/s][A
 85%|████████▍ | 518/611 [00:11<00:01, 46.77it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.78it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.70it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.73it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.74it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.75it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.76it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.74it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.76it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.77it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.78it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.73it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.67it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.67it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.71it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.74it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.71it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.74it/s][A
100%|█████████▉| 608/611 [00:12<00:00, 46.76it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:46<02:12,  3.55it/s]
100%|██████████| 611/611 [00:13<00:00, 46.76it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:14:04,509 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-29 03:14:04,526 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:14:06,968 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:14:06,988 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:14:06,998 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:54<49:49,  6.35s/it] 20%|██        | 120/590 [00:54<35:29,  4.53s/it] 21%|██        | 121/590 [00:54<25:27,  3.26s/it] 21%|██        | 122/590 [00:55<18:27,  2.37s/it] 21%|██        | 123/590 [00:55<13:33,  1.74s/it] 21%|██        | 124/590 [00:55<10:08,  1.31s/it] 21%|██        | 125/590 [00:56<07:44,  1.00it/s] 21%|██▏       | 126/590 [00:56<06:04,  1.27it/s] 22%|██▏       | 127/590 [00:56<04:54,  1.57it/s] 22%|██▏       | 128/590 [00:56<04:05,  1.88it/s] 22%|██▏       | 129/590 [00:57<03:31,  2.18it/s] 22%|██▏       | 130/590 [00:57<03:07,  2.46it/s] 22%|██▏       | 131/590 [00:57<02:50,  2.69it/s] 22%|██▏       | 132/590 [00:58<02:38,  2.89it/s] 23%|██▎       | 133/590 [00:58<02:30,  3.04it/s] 23%|██▎       | 134/590 [00:58<02:24,  3.16it/s] 23%|██▎       | 135/590 [00:58<02:19,  3.25it/s] 23%|██▎       | 136/590 [00:59<02:16,  3.32it/s] 23%|██▎       | 137/590 [00:59<02:14,  3.37it/s] 23%|██▎       | 138/590 [00:59<02:13,  3.40it/s] 24%|██▎       | 139/590 [01:00<02:11,  3.42it/s] 24%|██▎       | 140/590 [01:00<02:10,  3.44it/s] 24%|██▍       | 141/590 [01:00<02:09,  3.45it/s] 24%|██▍       | 142/590 [01:00<02:09,  3.46it/s] 24%|██▍       | 143/590 [01:01<02:09,  3.46it/s] 24%|██▍       | 144/590 [01:01<02:08,  3.47it/s] 25%|██▍       | 145/590 [01:01<02:08,  3.47it/s] 25%|██▍       | 146/590 [01:02<02:07,  3.47it/s] 25%|██▍       | 147/590 [01:02<02:07,  3.48it/s] 25%|██▌       | 148/590 [01:02<02:07,  3.48it/s] 25%|██▌       | 149/590 [01:03<02:06,  3.48it/s] 25%|██▌       | 150/590 [01:03<02:06,  3.48it/s] 26%|██▌       | 151/590 [01:03<02:06,  3.48it/s] 26%|██▌       | 152/590 [01:03<02:05,  3.48it/s] 26%|██▌       | 153/590 [01:04<02:05,  3.47it/s] 26%|██▌       | 154/590 [01:04<02:05,  3.47it/s] 26%|██▋       | 155/590 [01:04<02:05,  3.47it/s] 26%|██▋       | 156/590 [01:05<02:05,  3.47it/s] 27%|██▋       | 157/590 [01:05<02:04,  3.47it/s] 27%|██▋       | 158/590 [01:05<02:04,  3.48it/s] 27%|██▋       | 159/590 [01:05<02:03,  3.48it/s] 27%|██▋       | 160/590 [01:06<02:03,  3.48it/s] 27%|██▋       | 161/590 [01:06<02:03,  3.48it/s] 27%|██▋       | 162/590 [01:06<02:02,  3.48it/s] 28%|██▊       | 163/590 [01:07<02:02,  3.47it/s] 28%|██▊       | 164/590 [01:07<02:03,  3.46it/s] 28%|██▊       | 165/590 [01:07<02:03,  3.45it/s] 28%|██▊       | 166/590 [01:07<02:02,  3.45it/s] 28%|██▊       | 167/590 [01:08<02:02,  3.46it/s] 28%|██▊       | 168/590 [01:08<02:01,  3.47it/s] 29%|██▊       | 169/590 [01:08<02:01,  3.47it/s] 29%|██▉       | 170/590 [01:09<02:01,  3.47it/s] 29%|██▉       | 171/590 [01:09<02:00,  3.47it/s] 29%|██▉       | 172/590 [01:09<02:00,  3.48it/s] 29%|██▉       | 173/590 [01:09<01:59,  3.48it/s] 29%|██▉       | 174/590 [01:10<01:59,  3.48it/s] 30%|██▉       | 175/590 [01:10<02:04,  3.34it/s] 30%|██▉       | 176/590 [01:10<02:02,  3.38it/s] 30%|███       | 177/590 [01:11<02:01,  3.41it/s] 30%|███       | 178/590 [01:11<02:00,  3.43it/s] 30%|███       | 179/590 [01:11<01:59,  3.44it/s] 31%|███       | 180/590 [01:11<01:58,  3.45it/s] 31%|███       | 181/590 [01:12<01:58,  3.46it/s] 31%|███       | 182/590 [01:12<01:57,  3.46it/s] 31%|███       | 183/590 [01:12<01:57,  3.47it/s] 31%|███       | 184/590 [01:13<01:57,  3.47it/s] 31%|███▏      | 185/590 [01:13<01:56,  3.47it/s] 32%|███▏      | 186/590 [01:13<01:56,  3.47it/s] 32%|███▏      | 187/590 [01:13<01:55,  3.48it/s] 32%|███▏      | 188/590 [01:14<01:55,  3.47it/s] 32%|███▏      | 189/590 [01:14<01:55,  3.47it/s] 32%|███▏      | 190/590 [01:14<01:55,  3.47it/s] 32%|███▏      | 191/590 [01:15<01:54,  3.47it/s] 33%|███▎      | 192/590 [01:15<01:54,  3.47it/s] 33%|███▎      | 193/590 [01:15<01:54,  3.47it/s] 33%|███▎      | 194/590 [01:16<01:53,  3.48it/s] 33%|███▎      | 195/590 [01:16<01:54,  3.46it/s] 33%|███▎      | 196/590 [01:16<01:53,  3.46it/s] 33%|███▎      | 197/590 [01:16<01:53,  3.47it/s] 34%|███▎      | 198/590 [01:17<01:53,  3.47it/s] 34%|███▎      | 199/590 [01:17<01:52,  3.47it/s] 34%|███▍      | 200/590 [01:17<01:52,  3.47it/s] 34%|███▍      | 201/590 [01:18<01:51,  3.47it/s] 34%|███▍      | 202/590 [01:18<01:51,  3.47it/s] 34%|███▍      | 203/590 [01:18<01:51,  3.47it/s] 35%|███▍      | 204/590 [01:18<01:51,  3.47it/s] 35%|███▍      | 205/590 [01:19<01:50,  3.48it/s] 35%|███▍      | 206/590 [01:19<01:50,  3.46it/s] 35%|███▌      | 207/590 [01:19<01:50,  3.47it/s] 35%|███▌      | 208/590 [01:20<01:50,  3.47it/s] 35%|███▌      | 209/590 [01:20<01:49,  3.47it/s] 36%|███▌      | 210/590 [01:20<01:49,  3.47it/s] 36%|███▌      | 211/590 [01:20<01:49,  3.47it/s] 36%|███▌      | 212/590 [01:21<01:48,  3.47it/s] 36%|███▌      | 213/590 [01:21<01:48,  3.47it/s] 36%|███▋      | 214/590 [01:21<01:48,  3.47it/s] 36%|███▋      | 215/590 [01:22<01:48,  3.47it/s] 37%|███▋      | 216/590 [01:22<01:47,  3.47it/s] 37%|███▋      | 217/590 [01:22<01:47,  3.46it/s] 37%|███▋      | 218/590 [01:22<01:47,  3.46it/s] 37%|███▋      | 219/590 [01:23<01:47,  3.46it/s] 37%|███▋      | 220/590 [01:23<01:46,  3.46it/s] 37%|███▋      | 221/590 [01:23<01:46,  3.47it/s] 38%|███▊      | 222/590 [01:24<01:46,  3.47it/s] 38%|███▊      | 223/590 [01:24<01:45,  3.47it/s] 38%|███▊      | 224/590 [01:24<01:45,  3.47it/s] 38%|███▊      | 225/590 [01:24<01:45,  3.47it/s] 38%|███▊      | 226/590 [01:25<01:44,  3.47it/s] 38%|███▊      | 227/590 [01:25<01:44,  3.47it/s] 39%|███▊      | 228/590 [01:25<01:44,  3.46it/s] 39%|███▉      | 229/590 [01:26<01:44,  3.46it/s] 39%|███▉      | 230/590 [01:26<01:43,  3.46it/s] 39%|███▉      | 231/590 [01:26<01:43,  3.47it/s] 39%|███▉      | 232/590 [01:26<01:43,  3.47it/s] 39%|███▉      | 233/590 [01:27<01:42,  3.47it/s] 40%|███▉      | 234/590 [01:27<01:42,  3.47it/s] 40%|███▉      | 235/590 [01:27<01:42,  3.47it/s] 40%|████      | 236/590 [01:28<01:39,  3.56it/s][INFO|trainer.py:2140] 2023-08-29 03:14:45,621 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:14:45,621 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:14:45,621 >>   Batch size = 8
{'eval_loss': 0.9156873822212219, 'eval_runtime': 13.0664, 'eval_samples_per_second': 373.63, 'eval_steps_per_second': 46.761, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.31it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.55it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.53it/s][A
  4%|▍         | 23/611 [00:00<00:12, 48.03it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.58it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.25it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.99it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.70it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.65it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.15it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.37it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.49it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.57it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.66it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.50it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.51it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.50it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.50it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.53it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.59it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.63it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.67it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.71it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.61it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.61it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.51it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.52it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.59it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.65it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.62it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.66it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.70it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.64it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.61it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.53it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.50it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.50it/s][A
 32%|███▏      | 193/611 [00:04<00:08, 46.49it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.47it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.54it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.59it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.64it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.67it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.61it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.55it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.45it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.51it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.51it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.55it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.53it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.36it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.58it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.61it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.56it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.50it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 44.94it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.15it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.33it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.41it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.48it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.46it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.58it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.56it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.46it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.35it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.39it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.48it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.56it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.60it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.64it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.57it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.58it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.57it/s][A
 61%|██████    | 373/611 [00:07<00:05, 46.47it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.33it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.40it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.44it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.56it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.62it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.58it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.62it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.58it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.53it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.47it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.46it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.40it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.44it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.50it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.56it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.57it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.37it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.40it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.25it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.34it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.29it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.32it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.29it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.27it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.41it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.40it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.37it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.31it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.29it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.32it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.28it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.38it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.41it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.41it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.44it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.43it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.32it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.34it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.28it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.31it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.31it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.40it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.43it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.34it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.39it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.36it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.43it/s][A                                                 
                                                 [A 40%|████      | 236/590 [01:41<01:39,  3.56it/s]
100%|██████████| 611/611 [00:13<00:00, 46.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:14:58,776 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-29 03:14:58,791 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:15:00,994 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:15:01,014 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:15:01,025 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:48<37:33,  6.38s/it] 40%|████      | 238/590 [01:49<26:43,  4.56s/it] 41%|████      | 239/590 [01:49<19:09,  3.28s/it] 41%|████      | 240/590 [01:49<13:52,  2.38s/it] 41%|████      | 241/590 [01:49<10:11,  1.75s/it] 41%|████      | 242/590 [01:50<07:36,  1.31s/it] 41%|████      | 243/590 [01:50<05:48,  1.01s/it] 41%|████▏     | 244/590 [01:50<04:33,  1.27it/s] 42%|████▏     | 245/590 [01:51<03:40,  1.56it/s] 42%|████▏     | 246/590 [01:51<03:03,  1.87it/s] 42%|████▏     | 247/590 [01:51<02:37,  2.17it/s] 42%|████▏     | 248/590 [01:51<02:19,  2.45it/s] 42%|████▏     | 249/590 [01:52<02:07,  2.68it/s] 42%|████▏     | 250/590 [01:52<01:58,  2.88it/s] 43%|████▎     | 251/590 [01:52<01:51,  3.04it/s] 43%|████▎     | 252/590 [01:53<01:47,  3.15it/s] 43%|████▎     | 253/590 [01:53<01:43,  3.25it/s] 43%|████▎     | 254/590 [01:53<01:41,  3.31it/s] 43%|████▎     | 255/590 [01:53<01:39,  3.36it/s] 43%|████▎     | 256/590 [01:54<01:38,  3.39it/s] 44%|████▎     | 257/590 [01:54<01:37,  3.42it/s] 44%|████▎     | 258/590 [01:54<01:36,  3.44it/s] 44%|████▍     | 259/590 [01:55<01:36,  3.44it/s] 44%|████▍     | 260/590 [01:55<01:36,  3.44it/s] 44%|████▍     | 261/590 [01:55<01:35,  3.45it/s] 44%|████▍     | 262/590 [01:55<01:34,  3.46it/s] 45%|████▍     | 263/590 [01:56<01:34,  3.46it/s] 45%|████▍     | 264/590 [01:56<01:34,  3.47it/s] 45%|████▍     | 265/590 [01:56<01:33,  3.47it/s] 45%|████▌     | 266/590 [01:57<01:33,  3.47it/s] 45%|████▌     | 267/590 [01:57<01:33,  3.47it/s] 45%|████▌     | 268/590 [01:57<01:32,  3.46it/s] 46%|████▌     | 269/590 [01:57<01:32,  3.46it/s] 46%|████▌     | 270/590 [01:58<01:32,  3.47it/s] 46%|████▌     | 271/590 [01:58<01:34,  3.36it/s] 46%|████▌     | 272/590 [01:58<01:33,  3.40it/s] 46%|████▋     | 273/590 [01:59<01:32,  3.42it/s] 46%|████▋     | 274/590 [01:59<01:32,  3.43it/s] 47%|████▋     | 275/590 [01:59<01:31,  3.44it/s] 47%|████▋     | 276/590 [01:59<01:31,  3.45it/s] 47%|████▋     | 277/590 [02:00<01:30,  3.45it/s] 47%|████▋     | 278/590 [02:00<01:30,  3.45it/s] 47%|████▋     | 279/590 [02:00<01:30,  3.45it/s] 47%|████▋     | 280/590 [02:01<01:29,  3.45it/s] 48%|████▊     | 281/590 [02:01<01:29,  3.46it/s] 48%|████▊     | 282/590 [02:01<01:33,  3.31it/s] 48%|████▊     | 283/590 [02:02<01:31,  3.36it/s] 48%|████▊     | 284/590 [02:02<01:30,  3.40it/s] 48%|████▊     | 285/590 [02:02<01:29,  3.42it/s] 48%|████▊     | 286/590 [02:02<01:28,  3.44it/s] 49%|████▊     | 287/590 [02:03<01:27,  3.44it/s] 49%|████▉     | 288/590 [02:03<01:27,  3.45it/s] 49%|████▉     | 289/590 [02:03<01:26,  3.46it/s] 49%|████▉     | 290/590 [02:04<01:26,  3.47it/s] 49%|████▉     | 291/590 [02:04<01:26,  3.46it/s] 49%|████▉     | 292/590 [02:04<01:26,  3.46it/s] 50%|████▉     | 293/590 [02:04<01:26,  3.44it/s] 50%|████▉     | 294/590 [02:05<01:25,  3.45it/s] 50%|█████     | 295/590 [02:05<01:25,  3.45it/s] 50%|█████     | 296/590 [02:05<01:25,  3.45it/s] 50%|█████     | 297/590 [02:06<01:24,  3.45it/s] 51%|█████     | 298/590 [02:06<01:24,  3.45it/s] 51%|█████     | 299/590 [02:06<01:24,  3.46it/s] 51%|█████     | 300/590 [02:06<01:23,  3.46it/s] 51%|█████     | 301/590 [02:07<01:23,  3.46it/s] 51%|█████     | 302/590 [02:07<01:23,  3.46it/s] 51%|█████▏    | 303/590 [02:07<01:22,  3.47it/s] 52%|█████▏    | 304/590 [02:08<01:26,  3.32it/s] 52%|█████▏    | 305/590 [02:08<01:24,  3.36it/s] 52%|█████▏    | 306/590 [02:08<01:23,  3.39it/s] 52%|█████▏    | 307/590 [02:09<01:22,  3.42it/s] 52%|█████▏    | 308/590 [02:09<01:22,  3.43it/s] 52%|█████▏    | 309/590 [02:09<01:21,  3.44it/s] 53%|█████▎    | 310/590 [02:09<01:21,  3.45it/s] 53%|█████▎    | 311/590 [02:10<01:20,  3.46it/s] 53%|█████▎    | 312/590 [02:10<01:20,  3.46it/s] 53%|█████▎    | 313/590 [02:10<01:19,  3.46it/s] 53%|█████▎    | 314/590 [02:11<01:19,  3.47it/s] 53%|█████▎    | 315/590 [02:11<01:21,  3.39it/s] 54%|█████▎    | 316/590 [02:11<01:20,  3.41it/s] 54%|█████▎    | 317/590 [02:11<01:19,  3.43it/s] 54%|█████▍    | 318/590 [02:12<01:19,  3.44it/s] 54%|█████▍    | 319/590 [02:12<01:18,  3.45it/s] 54%|█████▍    | 320/590 [02:12<01:18,  3.46it/s] 54%|█████▍    | 321/590 [02:13<01:17,  3.46it/s] 55%|█████▍    | 322/590 [02:13<01:17,  3.46it/s] 55%|█████▍    | 323/590 [02:13<01:17,  3.46it/s] 55%|█████▍    | 324/590 [02:13<01:16,  3.46it/s] 55%|█████▌    | 325/590 [02:14<01:16,  3.46it/s] 55%|█████▌    | 326/590 [02:14<01:16,  3.46it/s] 55%|█████▌    | 327/590 [02:14<01:15,  3.47it/s] 56%|█████▌    | 328/590 [02:15<01:15,  3.46it/s] 56%|█████▌    | 329/590 [02:15<01:15,  3.47it/s] 56%|█████▌    | 330/590 [02:15<01:14,  3.47it/s] 56%|█████▌    | 331/590 [02:15<01:14,  3.47it/s] 56%|█████▋    | 332/590 [02:16<01:14,  3.47it/s] 56%|█████▋    | 333/590 [02:16<01:14,  3.47it/s] 57%|█████▋    | 334/590 [02:16<01:13,  3.47it/s] 57%|█████▋    | 335/590 [02:17<01:13,  3.47it/s] 57%|█████▋    | 336/590 [02:17<01:13,  3.47it/s] 57%|█████▋    | 337/590 [02:17<01:13,  3.46it/s] 57%|█████▋    | 338/590 [02:17<01:12,  3.46it/s] 57%|█████▋    | 339/590 [02:18<01:12,  3.47it/s] 58%|█████▊    | 340/590 [02:18<01:12,  3.46it/s] 58%|█████▊    | 341/590 [02:18<01:11,  3.47it/s] 58%|█████▊    | 342/590 [02:19<01:11,  3.47it/s] 58%|█████▊    | 343/590 [02:19<01:11,  3.47it/s] 58%|█████▊    | 344/590 [02:19<01:10,  3.47it/s] 58%|█████▊    | 345/590 [02:20<01:10,  3.47it/s] 59%|█████▊    | 346/590 [02:20<01:10,  3.47it/s] 59%|█████▉    | 347/590 [02:20<01:10,  3.46it/s] 59%|█████▉    | 348/590 [02:20<01:09,  3.46it/s] 59%|█████▉    | 349/590 [02:21<01:09,  3.45it/s] 59%|█████▉    | 350/590 [02:21<01:09,  3.46it/s] 59%|█████▉    | 351/590 [02:21<01:09,  3.45it/s] 60%|█████▉    | 352/590 [02:22<01:08,  3.45it/s] 60%|█████▉    | 353/590 [02:22<01:08,  3.46it/s] 60%|██████    | 354/590 [02:22<01:06,  3.55it/s][INFO|trainer.py:2140] 2023-08-29 03:15:40,115 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:15:40,115 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:15:40,115 >>   Batch size = 8
{'eval_loss': 0.9236816167831421, 'eval_runtime': 13.1416, 'eval_samples_per_second': 371.491, 'eval_steps_per_second': 46.493, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.24it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.44it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.63it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.97it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.51it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.11it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.75it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.49it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.43it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.50it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.50it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.43it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.58it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.62it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.54it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.37it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.27it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.36it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.31it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.38it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.43it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.45it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.42it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.44it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.36it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.33it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.29it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.26it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.34it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.44it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.47it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.49it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.37it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.32it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.28it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.28it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.26it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.37it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.40it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.34it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.42it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.39it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.31it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.22it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.12it/s][A
 38%|███▊      | 233/611 [00:05<00:08, 46.17it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.16it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.30it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.39it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.41it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.40it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.20it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.32it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.30it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.27it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.35it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.40it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.32it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.38it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.39it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.38it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.24it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.29it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.24it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.27it/s][A
 55%|█████▍    | 333/611 [00:07<00:06, 46.33it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.38it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.41it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.45it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.35it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.29it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.27it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.23it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.29it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.23it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.38it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.41it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.42it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.32it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.03it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 45.99it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.08it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.16it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.18it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.20it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.30it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.29it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.34it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.22it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.25it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.11it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.23it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.23it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.25it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.30it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.29it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.23it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.31it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.29it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.29it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.26it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.26it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.30it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.24it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.32it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.30it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 45.41it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 45.71it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 45.98it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.15it/s][A
 91%|█████████▏| 558/611 [00:12<00:01, 46.13it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.26it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.07it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.13it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.18it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.16it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.25it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.32it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.40it/s][A
 99%|█████████▊| 603/611 [00:13<00:00, 46.29it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.21it/s][A                                                 
                                                 [A 60%|██████    | 354/590 [02:35<01:06,  3.55it/s]
100%|██████████| 611/611 [00:13<00:00, 46.21it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:15:53,320 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-29 03:15:53,339 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:15:55,591 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:15:55,604 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:15:55,612 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:42<24:17,  6.20s/it] 60%|██████    | 356/590 [02:42<17:16,  4.43s/it] 61%|██████    | 357/590 [02:43<12:22,  3.19s/it] 61%|██████    | 358/590 [02:43<08:57,  2.32s/it] 61%|██████    | 359/590 [02:43<06:34,  1.71s/it] 61%|██████    | 360/590 [02:44<04:55,  1.28s/it] 61%|██████    | 361/590 [02:44<03:45,  1.02it/s] 61%|██████▏   | 362/590 [02:44<02:57,  1.29it/s] 62%|██████▏   | 363/590 [02:44<02:22,  1.59it/s] 62%|██████▏   | 364/590 [02:45<01:59,  1.90it/s] 62%|██████▏   | 365/590 [02:45<01:42,  2.19it/s] 62%|██████▏   | 366/590 [02:45<01:30,  2.47it/s] 62%|██████▏   | 367/590 [02:46<01:22,  2.70it/s] 62%|██████▏   | 368/590 [02:46<01:16,  2.89it/s] 63%|██████▎   | 369/590 [02:46<01:12,  3.04it/s] 63%|██████▎   | 370/590 [02:46<01:09,  3.16it/s] 63%|██████▎   | 371/590 [02:47<01:07,  3.25it/s] 63%|██████▎   | 372/590 [02:47<01:05,  3.31it/s] 63%|██████▎   | 373/590 [02:47<01:04,  3.36it/s] 63%|██████▎   | 374/590 [02:48<01:03,  3.39it/s] 64%|██████▎   | 375/590 [02:48<01:03,  3.40it/s] 64%|██████▎   | 376/590 [02:48<01:02,  3.42it/s] 64%|██████▍   | 377/590 [02:48<01:02,  3.44it/s] 64%|██████▍   | 378/590 [02:49<01:01,  3.44it/s] 64%|██████▍   | 379/590 [02:49<01:01,  3.45it/s] 64%|██████▍   | 380/590 [02:49<01:00,  3.46it/s] 65%|██████▍   | 381/590 [02:50<01:00,  3.46it/s] 65%|██████▍   | 382/590 [02:50<01:00,  3.46it/s] 65%|██████▍   | 383/590 [02:50<00:59,  3.46it/s] 65%|██████▌   | 384/590 [02:50<00:59,  3.46it/s] 65%|██████▌   | 385/590 [02:51<00:59,  3.46it/s] 65%|██████▌   | 386/590 [02:51<00:59,  3.41it/s] 66%|██████▌   | 387/590 [02:51<00:59,  3.43it/s] 66%|██████▌   | 388/590 [02:52<00:58,  3.44it/s] 66%|██████▌   | 389/590 [02:52<00:58,  3.45it/s] 66%|██████▌   | 390/590 [02:52<00:57,  3.45it/s] 66%|██████▋   | 391/590 [02:53<00:57,  3.46it/s] 66%|██████▋   | 392/590 [02:53<00:57,  3.46it/s] 67%|██████▋   | 393/590 [02:53<00:56,  3.46it/s] 67%|██████▋   | 394/590 [02:53<00:56,  3.46it/s] 67%|██████▋   | 395/590 [02:54<00:56,  3.46it/s] 67%|██████▋   | 396/590 [02:54<00:55,  3.47it/s] 67%|██████▋   | 397/590 [02:54<00:55,  3.46it/s] 67%|██████▋   | 398/590 [02:55<00:55,  3.46it/s] 68%|██████▊   | 399/590 [02:55<00:55,  3.46it/s] 68%|██████▊   | 400/590 [02:55<00:54,  3.46it/s] 68%|██████▊   | 401/590 [02:55<00:54,  3.46it/s] 68%|██████▊   | 402/590 [02:56<00:54,  3.46it/s] 68%|██████▊   | 403/590 [02:56<00:53,  3.47it/s] 68%|██████▊   | 404/590 [02:56<00:53,  3.46it/s] 69%|██████▊   | 405/590 [02:57<00:53,  3.46it/s] 69%|██████▉   | 406/590 [02:57<00:53,  3.47it/s] 69%|██████▉   | 407/590 [02:57<00:52,  3.46it/s] 69%|██████▉   | 408/590 [02:57<00:52,  3.46it/s] 69%|██████▉   | 409/590 [02:58<00:52,  3.46it/s] 69%|██████▉   | 410/590 [02:58<00:51,  3.46it/s] 70%|██████▉   | 411/590 [02:58<00:51,  3.46it/s] 70%|██████▉   | 412/590 [02:59<00:51,  3.47it/s] 70%|███████   | 413/590 [02:59<00:51,  3.46it/s] 70%|███████   | 414/590 [02:59<00:50,  3.46it/s] 70%|███████   | 415/590 [02:59<00:50,  3.47it/s] 71%|███████   | 416/590 [03:00<00:50,  3.47it/s] 71%|███████   | 417/590 [03:00<00:49,  3.47it/s] 71%|███████   | 418/590 [03:00<00:49,  3.46it/s] 71%|███████   | 419/590 [03:01<00:50,  3.42it/s] 71%|███████   | 420/590 [03:01<00:49,  3.43it/s] 71%|███████▏  | 421/590 [03:01<00:49,  3.44it/s] 72%|███████▏  | 422/590 [03:01<00:48,  3.45it/s] 72%|███████▏  | 423/590 [03:02<00:48,  3.45it/s] 72%|███████▏  | 424/590 [03:02<00:48,  3.45it/s] 72%|███████▏  | 425/590 [03:02<00:47,  3.45it/s] 72%|███████▏  | 426/590 [03:03<00:47,  3.46it/s] 72%|███████▏  | 427/590 [03:03<00:47,  3.46it/s] 73%|███████▎  | 428/590 [03:03<00:46,  3.46it/s] 73%|███████▎  | 429/590 [03:04<00:46,  3.46it/s] 73%|███████▎  | 430/590 [03:04<00:46,  3.44it/s] 73%|███████▎  | 431/590 [03:04<00:46,  3.45it/s] 73%|███████▎  | 432/590 [03:04<00:45,  3.45it/s] 73%|███████▎  | 433/590 [03:05<00:45,  3.45it/s] 74%|███████▎  | 434/590 [03:05<00:45,  3.45it/s] 74%|███████▎  | 435/590 [03:05<00:44,  3.46it/s] 74%|███████▍  | 436/590 [03:06<00:44,  3.46it/s] 74%|███████▍  | 437/590 [03:06<00:44,  3.46it/s] 74%|███████▍  | 438/590 [03:06<00:43,  3.46it/s] 74%|███████▍  | 439/590 [03:06<00:43,  3.46it/s] 75%|███████▍  | 440/590 [03:07<00:43,  3.46it/s] 75%|███████▍  | 441/590 [03:07<00:43,  3.44it/s] 75%|███████▍  | 442/590 [03:07<00:42,  3.45it/s] 75%|███████▌  | 443/590 [03:08<00:42,  3.45it/s] 75%|███████▌  | 444/590 [03:08<00:42,  3.45it/s] 75%|███████▌  | 445/590 [03:08<00:41,  3.46it/s] 76%|███████▌  | 446/590 [03:08<00:41,  3.46it/s] 76%|███████▌  | 447/590 [03:09<00:41,  3.46it/s] 76%|███████▌  | 448/590 [03:09<00:41,  3.46it/s] 76%|███████▌  | 449/590 [03:09<00:40,  3.46it/s] 76%|███████▋  | 450/590 [03:10<00:40,  3.46it/s] 76%|███████▋  | 451/590 [03:10<00:40,  3.46it/s] 77%|███████▋  | 452/590 [03:10<00:40,  3.45it/s] 77%|███████▋  | 453/590 [03:10<00:39,  3.45it/s] 77%|███████▋  | 454/590 [03:11<00:39,  3.46it/s] 77%|███████▋  | 455/590 [03:11<00:39,  3.46it/s] 77%|███████▋  | 456/590 [03:11<00:38,  3.46it/s] 77%|███████▋  | 457/590 [03:12<00:38,  3.46it/s] 78%|███████▊  | 458/590 [03:12<00:38,  3.46it/s] 78%|███████▊  | 459/590 [03:12<00:37,  3.46it/s] 78%|███████▊  | 460/590 [03:12<00:37,  3.46it/s] 78%|███████▊  | 461/590 [03:13<00:37,  3.46it/s] 78%|███████▊  | 462/590 [03:13<00:37,  3.46it/s] 78%|███████▊  | 463/590 [03:13<00:36,  3.45it/s] 79%|███████▊  | 464/590 [03:14<00:36,  3.45it/s] 79%|███████▉  | 465/590 [03:14<00:36,  3.45it/s] 79%|███████▉  | 466/590 [03:14<00:35,  3.45it/s] 79%|███████▉  | 467/590 [03:15<00:35,  3.45it/s] 79%|███████▉  | 468/590 [03:15<00:35,  3.45it/s] 79%|███████▉  | 469/590 [03:15<00:34,  3.46it/s] 80%|███████▉  | 470/590 [03:15<00:34,  3.46it/s] 80%|███████▉  | 471/590 [03:16<00:34,  3.46it/s] 80%|████████  | 472/590 [03:16<00:33,  3.55it/s][INFO|trainer.py:2140] 2023-08-29 03:16:33,955 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:16:33,955 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:16:33,955 >>   Batch size = 8
{'eval_loss': 0.9407636523246765, 'eval_runtime': 13.1896, 'eval_samples_per_second': 370.14, 'eval_steps_per_second': 46.324, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.75it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.21it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.57it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.92it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.42it/s][A
  5%|▌         | 33/611 [00:00<00:12, 46.95it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.53it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.37it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.41it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.46it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.48it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.42it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.47it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.52it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.42it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.28it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.17it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.29it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.27it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.36it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.38it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.35it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.44it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.41it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.11it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.18it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.20it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.31it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.35it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.44it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.46it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.37it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.39it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.12it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.21it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.23it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.24it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.24it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.33it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.42it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.39it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.33it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.29it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.15it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.25it/s][A
 38%|███▊      | 233/611 [00:05<00:08, 46.26it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.32it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.39it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.47it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.35it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.32it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.24it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.28it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.25it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.30it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.29it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.34it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.46it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.47it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.32it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.33it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.26it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.20it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.18it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.27it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.39it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.41it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.45it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.36it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.30it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.23it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.19it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.21it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.22it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.16it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.35it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.36it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.41it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.31it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.25it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.14it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.19it/s][A
 68%|██████▊   | 418/611 [00:09<00:04, 46.27it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.15it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.32it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.39it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.29it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.32it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.27it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.26it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.23it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.23it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.22it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.29it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.37it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.42it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.39it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.26it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.15it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.20it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.20it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.26it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.35it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.36it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.27it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.32it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.30it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.22it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.19it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.24it/s][A
 91%|█████████▏| 558/611 [00:12<00:01, 46.16it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.27it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.35it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.38it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.39it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.34it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.22it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.28it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.20it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.29it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.24it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:29<00:33,  3.55it/s]
100%|██████████| 611/611 [00:13<00:00, 46.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:16:47,162 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-29 03:16:47,196 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:16:49,432 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:16:49,462 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:16:49,482 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:37<12:29,  6.41s/it] 80%|████████  | 474/590 [03:37<08:50,  4.57s/it] 81%|████████  | 475/590 [03:37<06:18,  3.29s/it] 81%|████████  | 476/590 [03:37<04:32,  2.39s/it] 81%|████████  | 477/590 [03:38<03:18,  1.76s/it] 81%|████████  | 478/590 [03:38<02:27,  1.32s/it] 81%|████████  | 479/590 [03:38<01:51,  1.01s/it] 81%|████████▏ | 480/590 [03:39<01:27,  1.26it/s] 82%|████████▏ | 481/590 [03:39<01:09,  1.56it/s] 82%|████████▏ | 482/590 [03:39<00:57,  1.87it/s] 82%|████████▏ | 483/590 [03:40<00:49,  2.17it/s] 82%|████████▏ | 484/590 [03:40<00:43,  2.45it/s] 82%|████████▏ | 485/590 [03:40<00:39,  2.68it/s] 82%|████████▏ | 486/590 [03:40<00:36,  2.87it/s] 83%|████████▎ | 487/590 [03:41<00:33,  3.03it/s] 83%|████████▎ | 488/590 [03:41<00:32,  3.15it/s] 83%|████████▎ | 489/590 [03:41<00:31,  3.24it/s] 83%|████████▎ | 490/590 [03:42<00:30,  3.30it/s] 83%|████████▎ | 491/590 [03:42<00:29,  3.35it/s] 83%|████████▎ | 492/590 [03:42<00:28,  3.39it/s] 84%|████████▎ | 493/590 [03:42<00:28,  3.41it/s] 84%|████████▎ | 494/590 [03:43<00:27,  3.43it/s] 84%|████████▍ | 495/590 [03:43<00:27,  3.44it/s] 84%|████████▍ | 496/590 [03:43<00:27,  3.45it/s] 84%|████████▍ | 497/590 [03:44<00:26,  3.45it/s] 84%|████████▍ | 498/590 [03:44<00:26,  3.46it/s] 85%|████████▍ | 499/590 [03:44<00:26,  3.46it/s] 85%|████████▍ | 500/590 [03:44<00:26,  3.46it/s]                                                  85%|████████▍ | 500/590 [03:44<00:26,  3.46it/s] 85%|████████▍ | 501/590 [03:45<00:25,  3.46it/s] 85%|████████▌ | 502/590 [03:45<00:25,  3.46it/s] 85%|████████▌ | 503/590 [03:45<00:25,  3.47it/s] 85%|████████▌ | 504/590 [03:46<00:24,  3.47it/s] 86%|████████▌ | 505/590 [03:46<00:24,  3.47it/s] 86%|████████▌ | 506/590 [03:46<00:24,  3.47it/s] 86%|████████▌ | 507/590 [03:46<00:23,  3.47it/s] 86%|████████▌ | 508/590 [03:47<00:23,  3.47it/s] 86%|████████▋ | 509/590 [03:47<00:23,  3.47it/s] 86%|████████▋ | 510/590 [03:47<00:23,  3.47it/s] 87%|████████▋ | 511/590 [03:48<00:22,  3.47it/s] 87%|████████▋ | 512/590 [03:48<00:22,  3.47it/s] 87%|████████▋ | 513/590 [03:48<00:22,  3.47it/s] 87%|████████▋ | 514/590 [03:48<00:21,  3.47it/s] 87%|████████▋ | 515/590 [03:49<00:21,  3.47it/s] 87%|████████▋ | 516/590 [03:49<00:21,  3.47it/s] 88%|████████▊ | 517/590 [03:49<00:21,  3.45it/s] 88%|████████▊ | 518/590 [03:50<00:20,  3.45it/s] 88%|████████▊ | 519/590 [03:50<00:20,  3.47it/s] 88%|████████▊ | 520/590 [03:50<00:20,  3.46it/s] 88%|████████▊ | 521/590 [03:50<00:19,  3.47it/s] 88%|████████▊ | 522/590 [03:51<00:19,  3.47it/s] 89%|████████▊ | 523/590 [03:51<00:19,  3.47it/s] 89%|████████▉ | 524/590 [03:51<00:19,  3.47it/s] 89%|████████▉ | 525/590 [03:52<00:18,  3.47it/s] 89%|████████▉ | 526/590 [03:52<00:18,  3.47it/s] 89%|████████▉ | 527/590 [03:52<00:18,  3.47it/s] 89%|████████▉ | 528/590 [03:52<00:17,  3.46it/s] 90%|████████▉ | 529/590 [03:53<00:17,  3.46it/s] 90%|████████▉ | 530/590 [03:53<00:17,  3.46it/s] 90%|█████████ | 531/590 [03:53<00:17,  3.46it/s] 90%|█████████ | 532/590 [03:54<00:16,  3.47it/s] 90%|█████████ | 533/590 [03:54<00:16,  3.47it/s] 91%|█████████ | 534/590 [03:54<00:16,  3.46it/s] 91%|█████████ | 535/590 [03:55<00:15,  3.46it/s] 91%|█████████ | 536/590 [03:55<00:15,  3.46it/s] 91%|█████████ | 537/590 [03:55<00:15,  3.46it/s] 91%|█████████ | 538/590 [03:55<00:15,  3.46it/s] 91%|█████████▏| 539/590 [03:56<00:14,  3.45it/s] 92%|█████████▏| 540/590 [03:56<00:14,  3.46it/s] 92%|█████████▏| 541/590 [03:56<00:14,  3.46it/s] 92%|█████████▏| 542/590 [03:57<00:13,  3.46it/s] 92%|█████████▏| 543/590 [03:57<00:13,  3.46it/s] 92%|█████████▏| 544/590 [03:57<00:13,  3.46it/s] 92%|█████████▏| 545/590 [03:57<00:12,  3.46it/s] 93%|█████████▎| 546/590 [03:58<00:12,  3.46it/s] 93%|█████████▎| 547/590 [03:58<00:12,  3.46it/s] 93%|█████████▎| 548/590 [03:58<00:12,  3.47it/s] 93%|█████████▎| 549/590 [03:59<00:11,  3.47it/s] 93%|█████████▎| 550/590 [03:59<00:11,  3.46it/s] 93%|█████████▎| 551/590 [03:59<00:11,  3.46it/s] 94%|█████████▎| 552/590 [03:59<00:10,  3.46it/s] 94%|█████████▎| 553/590 [04:00<00:10,  3.46it/s] 94%|█████████▍| 554/590 [04:00<00:10,  3.45it/s] 94%|█████████▍| 555/590 [04:00<00:10,  3.46it/s] 94%|█████████▍| 556/590 [04:01<00:09,  3.46it/s] 94%|█████████▍| 557/590 [04:01<00:09,  3.46it/s] 95%|█████████▍| 558/590 [04:01<00:09,  3.46it/s] 95%|█████████▍| 559/590 [04:01<00:08,  3.46it/s] 95%|█████████▍| 560/590 [04:02<00:08,  3.46it/s] 95%|█████████▌| 561/590 [04:02<00:08,  3.45it/s] 95%|█████████▌| 562/590 [04:02<00:08,  3.46it/s] 95%|█████████▌| 563/590 [04:03<00:07,  3.45it/s] 96%|█████████▌| 564/590 [04:03<00:07,  3.46it/s] 96%|█████████▌| 565/590 [04:03<00:07,  3.45it/s] 96%|█████████▌| 566/590 [04:03<00:06,  3.46it/s] 96%|█████████▌| 567/590 [04:04<00:06,  3.46it/s] 96%|█████████▋| 568/590 [04:04<00:06,  3.46it/s] 96%|█████████▋| 569/590 [04:04<00:06,  3.46it/s] 97%|█████████▋| 570/590 [04:05<00:05,  3.46it/s] 97%|█████████▋| 571/590 [04:05<00:05,  3.46it/s] 97%|█████████▋| 572/590 [04:05<00:05,  3.45it/s] 97%|█████████▋| 573/590 [04:06<00:04,  3.45it/s] 97%|█████████▋| 574/590 [04:06<00:04,  3.45it/s] 97%|█████████▋| 575/590 [04:06<00:04,  3.45it/s] 98%|█████████▊| 576/590 [04:06<00:04,  3.45it/s] 98%|█████████▊| 577/590 [04:07<00:03,  3.46it/s] 98%|█████████▊| 578/590 [04:07<00:03,  3.46it/s] 98%|█████████▊| 579/590 [04:07<00:03,  3.46it/s] 98%|█████████▊| 580/590 [04:08<00:02,  3.46it/s] 98%|█████████▊| 581/590 [04:08<00:02,  3.46it/s] 99%|█████████▊| 582/590 [04:08<00:02,  3.46it/s] 99%|█████████▉| 583/590 [04:08<00:02,  3.45it/s] 99%|█████████▉| 584/590 [04:09<00:01,  3.45it/s] 99%|█████████▉| 585/590 [04:09<00:01,  3.45it/s] 99%|█████████▉| 586/590 [04:09<00:01,  3.46it/s] 99%|█████████▉| 587/590 [04:10<00:00,  3.46it/s]100%|█████████▉| 588/590 [04:10<00:00,  3.45it/s]100%|█████████▉| 589/590 [04:10<00:00,  3.46it/s]100%|██████████| 590/590 [04:10<00:00,  3.55it/s][INFO|trainer.py:2140] 2023-08-29 03:17:28,431 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:17:28,431 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:17:28,431 >>   Batch size = 8
{'eval_loss': 0.9458978772163391, 'eval_runtime': 13.1888, 'eval_samples_per_second': 370.162, 'eval_steps_per_second': 46.327, 'epoch': 4.0}
{'loss': 0.5572, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.65it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.38it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.52it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.79it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.22it/s][A
  5%|▌         | 33/611 [00:00<00:12, 46.75it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.54it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.31it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.39it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.43it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.36it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.50it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.54it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.48it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.40it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.27it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.14it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.14it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.30it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.36it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.43it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.51it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.46it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.35it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.20it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.15it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.19it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.28it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.39it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.31it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.39it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.45it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.36it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.29it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.20it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.16it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.35it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.30it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.44it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.43it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.42it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.33it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.27it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.16it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.18it/s][A
 38%|███▊      | 233/611 [00:05<00:08, 46.32it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.40it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.32it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.38it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.33it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.31it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.30it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.20it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.14it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.22it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.34it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.42it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.34it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.42it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.34it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.34it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.30it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.18it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.24it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.25it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.35it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.37it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.31it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.36it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.31it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.30it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.17it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.20it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.29it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.23it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.24it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.31it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.26it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.30it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.25it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.22it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.19it/s][A
 68%|██████▊   | 418/611 [00:09<00:04, 46.19it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.29it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.34it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.37it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.33it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.25it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.31it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.27it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.28it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.26it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.28it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.26it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.21it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.37it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.13it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.23it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.16it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.09it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.23it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.25it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.31it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.31it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.31it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.37it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.32it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.33it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.31it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.30it/s][A
 91%|█████████▏| 558/611 [00:12<00:01, 46.30it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.26it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.32it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.39it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.39it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.40it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.35it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.33it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.35it/s][A
 99%|█████████▊| 603/611 [00:13<00:00, 46.32it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.34it/s][A                                                 
                                                 [A100%|██████████| 590/590 [04:24<00:00,  3.55it/s]
100%|██████████| 611/611 [00:13<00:00, 46.34it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:17:41,636 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-29 03:17:41,662 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:17:43,955 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:17:43,977 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:17:43,993 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 03:17:48,596 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 03:17:48,598 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-118 (score: 0.9156873822212219).
                                                 100%|██████████| 590/590 [04:32<00:00,  3.55it/s]100%|██████████| 590/590 [04:32<00:00,  2.16it/s]
[INFO|trainer.py:1894] 2023-08-29 03:17:50,269 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 03:17:50,287 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:17:52,685 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:17:52,703 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:17:52,715 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:17:52,915 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:17:52,915 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:17:52,915 >>   train_loss               =      0.553
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:17:52,915 >>   train_runtime            = 0:04:32.73
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:17:52,915 >>   train_samples            =       7546
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:17:52,915 >>   train_samples_per_second =    138.338
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:17:52,915 >>   train_steps_per_second   =      2.163
{'eval_loss': 0.951994001865387, 'eval_runtime': 13.1886, 'eval_samples_per_second': 370.168, 'eval_steps_per_second': 46.328, 'epoch': 5.0}
{'train_runtime': 272.7386, 'train_samples_per_second': 138.338, 'train_steps_per_second': 2.163, 'train_loss': 0.5530402619959944, 'epoch': 5.0}
08/29/2023 03:17:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 03:17:52,969 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:17:52,969 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 03:17:52,969 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 58.62it/s]  2%|▏         | 12/611 [00:00<00:11, 51.18it/s]  3%|▎         | 18/611 [00:00<00:12, 49.17it/s]  4%|▍         | 23/611 [00:00<00:12, 48.28it/s]  5%|▍         | 28/611 [00:00<00:12, 47.84it/s]  5%|▌         | 33/611 [00:00<00:12, 47.59it/s]  6%|▌         | 38/611 [00:00<00:12, 47.42it/s]  7%|▋         | 43/611 [00:00<00:12, 46.96it/s]  8%|▊         | 48/611 [00:01<00:12, 46.60it/s]  9%|▊         | 53/611 [00:01<00:12, 46.46it/s]  9%|▉         | 58/611 [00:01<00:11, 46.64it/s] 10%|█         | 63/611 [00:01<00:12, 44.35it/s] 11%|█▏        | 69/611 [00:01<00:11, 46.10it/s] 12%|█▏        | 74/611 [00:01<00:11, 46.32it/s] 13%|█▎        | 79/611 [00:01<00:11, 46.47it/s] 14%|█▎        | 84/611 [00:01<00:11, 46.64it/s] 15%|█▍        | 89/611 [00:01<00:11, 46.65it/s] 15%|█▌        | 94/611 [00:01<00:11, 46.64it/s] 16%|█▌        | 99/611 [00:02<00:11, 46.53it/s] 17%|█▋        | 104/611 [00:02<00:10, 46.64it/s] 18%|█▊        | 109/611 [00:02<00:10, 46.58it/s] 19%|█▊        | 114/611 [00:02<00:10, 46.62it/s] 19%|█▉        | 119/611 [00:02<00:10, 46.72it/s] 20%|██        | 124/611 [00:02<00:10, 46.69it/s] 21%|██        | 129/611 [00:02<00:10, 46.73it/s] 22%|██▏       | 134/611 [00:02<00:10, 46.80it/s] 23%|██▎       | 139/611 [00:02<00:10, 46.58it/s] 24%|██▎       | 144/611 [00:03<00:10, 46.65it/s] 24%|██▍       | 149/611 [00:03<00:09, 46.65it/s] 25%|██▌       | 154/611 [00:03<00:09, 46.60it/s] 26%|██▌       | 159/611 [00:03<00:09, 46.54it/s] 27%|██▋       | 164/611 [00:03<00:09, 46.72it/s] 28%|██▊       | 169/611 [00:03<00:09, 46.65it/s] 28%|██▊       | 174/611 [00:03<00:09, 46.69it/s] 29%|██▉       | 179/611 [00:03<00:09, 46.66it/s] 30%|███       | 184/611 [00:03<00:09, 46.72it/s] 31%|███       | 189/611 [00:04<00:09, 46.71it/s] 32%|███▏      | 194/611 [00:04<00:08, 46.65it/s] 33%|███▎      | 199/611 [00:04<00:08, 46.46it/s] 33%|███▎      | 204/611 [00:04<00:08, 46.65it/s] 34%|███▍      | 209/611 [00:04<00:08, 46.75it/s] 35%|███▌      | 214/611 [00:04<00:08, 46.76it/s] 36%|███▌      | 219/611 [00:04<00:08, 46.67it/s] 37%|███▋      | 224/611 [00:04<00:08, 46.69it/s] 37%|███▋      | 229/611 [00:04<00:08, 46.65it/s] 38%|███▊      | 234/611 [00:04<00:08, 46.70it/s] 39%|███▉      | 239/611 [00:05<00:07, 46.70it/s] 40%|███▉      | 244/611 [00:05<00:07, 46.64it/s] 41%|████      | 249/611 [00:05<00:07, 46.68it/s] 42%|████▏     | 254/611 [00:05<00:07, 46.46it/s] 42%|████▏     | 259/611 [00:05<00:07, 46.53it/s] 43%|████▎     | 264/611 [00:05<00:07, 46.61it/s] 44%|████▍     | 269/611 [00:05<00:07, 46.65it/s] 45%|████▍     | 274/611 [00:05<00:07, 46.61it/s] 46%|████▌     | 279/611 [00:05<00:07, 46.69it/s] 46%|████▋     | 284/611 [00:06<00:07, 46.58it/s] 47%|████▋     | 289/611 [00:06<00:06, 46.60it/s] 48%|████▊     | 294/611 [00:06<00:06, 46.66it/s] 49%|████▉     | 299/611 [00:06<00:06, 46.67it/s] 50%|████▉     | 304/611 [00:06<00:06, 46.71it/s] 51%|█████     | 309/611 [00:06<00:06, 46.74it/s] 51%|█████▏    | 314/611 [00:06<00:06, 46.53it/s] 52%|█████▏    | 319/611 [00:06<00:06, 46.55it/s] 53%|█████▎    | 324/611 [00:06<00:06, 46.53it/s] 54%|█████▍    | 329/611 [00:07<00:06, 46.66it/s] 55%|█████▍    | 334/611 [00:07<00:05, 46.69it/s] 55%|█████▌    | 339/611 [00:07<00:05, 46.67it/s] 56%|█████▋    | 344/611 [00:07<00:05, 46.60it/s] 57%|█████▋    | 349/611 [00:07<00:05, 46.65it/s] 58%|█████▊    | 354/611 [00:07<00:05, 46.64it/s] 59%|█████▉    | 359/611 [00:07<00:05, 46.66it/s] 60%|█████▉    | 364/611 [00:07<00:05, 46.61it/s] 60%|██████    | 369/611 [00:07<00:05, 46.65it/s] 61%|██████    | 374/611 [00:08<00:05, 46.54it/s] 62%|██████▏   | 379/611 [00:08<00:04, 46.60it/s] 63%|██████▎   | 384/611 [00:08<00:04, 46.65it/s] 64%|██████▎   | 389/611 [00:08<00:04, 46.67it/s] 64%|██████▍   | 394/611 [00:08<00:04, 46.68it/s] 65%|██████▌   | 399/611 [00:08<00:04, 46.73it/s] 66%|██████▌   | 404/611 [00:08<00:04, 46.63it/s] 67%|██████▋   | 409/611 [00:08<00:04, 46.65it/s] 68%|██████▊   | 414/611 [00:08<00:04, 46.60it/s] 69%|██████▊   | 419/611 [00:08<00:04, 46.70it/s] 69%|██████▉   | 424/611 [00:09<00:04, 46.69it/s] 70%|███████   | 429/611 [00:09<00:03, 46.66it/s] 71%|███████   | 434/611 [00:09<00:03, 46.59it/s] 72%|███████▏  | 439/611 [00:09<00:03, 46.63it/s] 73%|███████▎  | 444/611 [00:09<00:03, 46.68it/s] 73%|███████▎  | 449/611 [00:09<00:03, 46.65it/s] 74%|███████▍  | 454/611 [00:09<00:03, 46.65it/s] 75%|███████▌  | 459/611 [00:09<00:03, 46.69it/s] 76%|███████▌  | 464/611 [00:09<00:03, 46.59it/s] 77%|███████▋  | 469/611 [00:10<00:03, 46.67it/s] 78%|███████▊  | 474/611 [00:10<00:02, 46.71it/s] 78%|███████▊  | 479/611 [00:10<00:02, 46.61it/s] 79%|███████▉  | 484/611 [00:10<00:02, 46.66it/s] 80%|████████  | 489/611 [00:10<00:02, 46.69it/s] 81%|████████  | 494/611 [00:10<00:02, 46.56it/s] 82%|████████▏ | 499/611 [00:10<00:02, 46.62it/s] 82%|████████▏ | 504/611 [00:10<00:02, 46.62it/s] 83%|████████▎ | 509/611 [00:10<00:02, 46.64it/s] 84%|████████▍ | 514/611 [00:11<00:02, 46.68it/s] 85%|████████▍ | 519/611 [00:11<00:01, 46.38it/s] 86%|████████▌ | 524/611 [00:11<00:01, 46.67it/s] 87%|████████▋ | 529/611 [00:11<00:01, 46.64it/s] 87%|████████▋ | 534/611 [00:11<00:01, 46.59it/s] 88%|████████▊ | 539/611 [00:11<00:01, 46.67it/s] 89%|████████▉ | 544/611 [00:11<00:01, 46.70it/s] 90%|████████▉ | 549/611 [00:11<00:01, 46.65it/s] 91%|█████████ | 554/611 [00:11<00:01, 46.60it/s] 91%|█████████▏| 559/611 [00:11<00:01, 46.61it/s] 92%|█████████▏| 564/611 [00:12<00:01, 46.64it/s] 93%|█████████▎| 569/611 [00:12<00:00, 46.59it/s] 94%|█████████▍| 574/611 [00:12<00:00, 46.49it/s] 95%|█████████▍| 579/611 [00:12<00:00, 46.60it/s] 96%|█████████▌| 584/611 [00:12<00:00, 46.59it/s] 96%|█████████▋| 589/611 [00:12<00:00, 46.59it/s] 97%|█████████▋| 594/611 [00:12<00:00, 46.61it/s] 98%|█████████▊| 599/611 [00:12<00:00, 46.65it/s] 99%|█████████▉| 604/611 [00:12<00:00, 46.67it/s]100%|█████████▉| 609/611 [00:13<00:00, 46.70it/s]100%|██████████| 611/611 [00:13<00:00, 46.70it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:18:06,076 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:18:06,076 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:18:06,076 >>   eval_loss               =     0.9157
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:18:06,076 >>   eval_runtime            = 0:00:13.10
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:18:06,076 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:18:06,076 >>   eval_samples_per_second =    372.486
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:18:06,076 >>   eval_steps_per_second   =     46.618
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:18:06,076 >>   perplexity              =     2.4985
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:11,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:11,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:11,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:11,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:11,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:18:11,985 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:18:11,986 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:18:12,368 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:18:13,380 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:18:13,380 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:15,081 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:15,087 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:15,087 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:15,087 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:15,087 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:18:15,396 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:18:15,397 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:18:16,062 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:18:16,197 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:18:16,197 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-354
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-236
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-590
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.44it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.54it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.58it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:11,  1.58it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:12,  1.62it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:13,  1.63it/s]Extractor Predicting: 22it [00:14,  1.67it/s]Extractor Predicting: 23it [00:14,  1.67it/s]Extractor Predicting: 24it [00:15,  1.67it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:17,  1.62it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:19,  1.63it/s]Extractor Predicting: 31it [00:19,  1.63it/s]Extractor Predicting: 32it [00:20,  1.67it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:21,  1.62it/s]Extractor Predicting: 35it [00:22,  1.62it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:23,  1.63it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.63it/s]Extractor Predicting: 41it [00:25,  1.59it/s]Extractor Predicting: 42it [00:26,  1.60it/s]Extractor Predicting: 43it [00:27,  1.60it/s]Extractor Predicting: 44it [00:27,  1.59it/s]Extractor Predicting: 45it [00:28,  1.63it/s]Extractor Predicting: 46it [00:28,  1.61it/s]Extractor Predicting: 47it [00:29,  1.61it/s]Extractor Predicting: 48it [00:30,  1.60it/s]Extractor Predicting: 49it [00:30,  1.60it/s]Extractor Predicting: 50it [00:31,  1.56it/s]Extractor Predicting: 51it [00:32,  1.56it/s]Extractor Predicting: 52it [00:32,  1.60it/s]Extractor Predicting: 53it [00:33,  1.58it/s]Extractor Predicting: 54it [00:33,  1.57it/s]Extractor Predicting: 55it [00:34,  1.52it/s]Extractor Predicting: 56it [00:35,  1.49it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:36,  1.51it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:37,  1.52it/s]Extractor Predicting: 61it [00:38,  1.51it/s]Extractor Predicting: 62it [00:39,  1.49it/s]Extractor Predicting: 63it [00:40,  1.50it/s]Extractor Predicting: 64it [00:40,  1.50it/s]Extractor Predicting: 65it [00:41,  1.49it/s]Extractor Predicting: 66it [00:42,  1.46it/s]Extractor Predicting: 67it [00:42,  1.49it/s]Extractor Predicting: 68it [00:43,  1.52it/s]Extractor Predicting: 69it [00:44,  1.51it/s]Extractor Predicting: 70it [00:44,  1.49it/s]Extractor Predicting: 71it [00:45,  1.49it/s]Extractor Predicting: 72it [00:46,  1.47it/s]Extractor Predicting: 73it [00:46,  1.49it/s]Extractor Predicting: 74it [00:47,  1.49it/s]Extractor Predicting: 75it [00:48,  1.50it/s]Extractor Predicting: 76it [00:48,  1.50it/s]Extractor Predicting: 77it [00:49,  1.50it/s]Extractor Predicting: 78it [00:50,  1.53it/s]Extractor Predicting: 79it [00:50,  1.54it/s]Extractor Predicting: 80it [00:51,  1.53it/s]Extractor Predicting: 81it [00:51,  1.54it/s]Extractor Predicting: 82it [00:52,  1.38it/s]Extractor Predicting: 83it [00:53,  1.42it/s]Extractor Predicting: 84it [00:54,  1.44it/s]Extractor Predicting: 85it [00:54,  1.46it/s]Extractor Predicting: 86it [00:55,  1.45it/s]Extractor Predicting: 87it [00:56,  1.45it/s]Extractor Predicting: 88it [00:56,  1.43it/s]Extractor Predicting: 89it [00:57,  1.44it/s]Extractor Predicting: 90it [00:58,  1.44it/s]Extractor Predicting: 91it [00:59,  1.45it/s]Extractor Predicting: 92it [00:59,  1.50it/s]Extractor Predicting: 93it [01:00,  1.57it/s]Extractor Predicting: 94it [01:00,  1.56it/s]Extractor Predicting: 95it [01:01,  1.55it/s]Extractor Predicting: 96it [01:02,  1.56it/s]Extractor Predicting: 97it [01:02,  1.58it/s]Extractor Predicting: 98it [01:03,  1.54it/s]Extractor Predicting: 99it [01:04,  1.48it/s]Extractor Predicting: 100it [01:04,  1.50it/s]Extractor Predicting: 101it [01:05,  1.44it/s]Extractor Predicting: 102it [01:06,  1.43it/s]Extractor Predicting: 103it [01:06,  1.45it/s]Extractor Predicting: 104it [01:07,  1.47it/s]Extractor Predicting: 105it [01:08,  1.49it/s]Extractor Predicting: 106it [01:08,  1.54it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:10,  1.56it/s]Extractor Predicting: 109it [01:10,  1.55it/s]Extractor Predicting: 110it [01:11,  1.55it/s]Extractor Predicting: 111it [01:12,  1.58it/s]Extractor Predicting: 112it [01:12,  1.59it/s]Extractor Predicting: 113it [01:13,  1.53it/s]Extractor Predicting: 114it [01:14,  1.52it/s]Extractor Predicting: 115it [01:14,  1.52it/s]Extractor Predicting: 116it [01:15,  1.49it/s]Extractor Predicting: 117it [01:16,  1.48it/s]Extractor Predicting: 118it [01:16,  1.48it/s]Extractor Predicting: 119it [01:17,  1.46it/s]Extractor Predicting: 120it [01:18,  1.43it/s]Extractor Predicting: 121it [01:18,  1.45it/s]Extractor Predicting: 122it [01:19,  1.45it/s]Extractor Predicting: 123it [01:20,  1.48it/s]Extractor Predicting: 124it [01:20,  1.48it/s]Extractor Predicting: 125it [01:21,  1.50it/s]Extractor Predicting: 126it [01:22,  1.49it/s]Extractor Predicting: 127it [01:22,  1.49it/s]Extractor Predicting: 128it [01:23,  1.49it/s]Extractor Predicting: 129it [01:24,  1.51it/s]Extractor Predicting: 130it [01:24,  1.47it/s]Extractor Predicting: 131it [01:25,  1.47it/s]Extractor Predicting: 132it [01:26,  1.50it/s]Extractor Predicting: 133it [01:26,  1.48it/s]Extractor Predicting: 134it [01:27,  1.48it/s]Extractor Predicting: 135it [01:28,  1.47it/s]Extractor Predicting: 136it [01:28,  1.49it/s]Extractor Predicting: 137it [01:29,  1.46it/s]Extractor Predicting: 138it [01:30,  1.48it/s]Extractor Predicting: 139it [01:31,  1.46it/s]Extractor Predicting: 140it [01:31,  1.45it/s]Extractor Predicting: 141it [01:32,  1.47it/s]Extractor Predicting: 142it [01:33,  1.48it/s]Extractor Predicting: 143it [01:33,  1.46it/s]Extractor Predicting: 144it [01:34,  1.49it/s]Extractor Predicting: 145it [01:34,  1.53it/s]Extractor Predicting: 146it [01:35,  1.51it/s]Extractor Predicting: 147it [01:36,  1.48it/s]Extractor Predicting: 148it [01:37,  1.49it/s]Extractor Predicting: 149it [01:37,  1.48it/s]Extractor Predicting: 150it [01:38,  1.46it/s]Extractor Predicting: 151it [01:39,  1.46it/s]Extractor Predicting: 152it [01:39,  1.46it/s]Extractor Predicting: 153it [01:40,  1.46it/s]Extractor Predicting: 154it [01:41,  1.47it/s]Extractor Predicting: 155it [01:41,  1.47it/s]Extractor Predicting: 156it [01:42,  1.41it/s]Extractor Predicting: 157it [01:43,  1.38it/s]Extractor Predicting: 158it [01:44,  1.35it/s]Extractor Predicting: 159it [01:44,  1.38it/s]Extractor Predicting: 160it [01:45,  1.41it/s]Extractor Predicting: 161it [01:46,  1.31it/s]Extractor Predicting: 162it [01:47,  1.35it/s]Extractor Predicting: 163it [01:47,  1.37it/s]Extractor Predicting: 164it [01:48,  1.42it/s]Extractor Predicting: 165it [01:49,  1.42it/s]Extractor Predicting: 166it [01:49,  1.46it/s]Extractor Predicting: 167it [01:50,  1.46it/s]Extractor Predicting: 168it [01:51,  1.46it/s]Extractor Predicting: 169it [01:51,  1.47it/s]Extractor Predicting: 170it [01:52,  1.47it/s]Extractor Predicting: 171it [01:53,  1.49it/s]Extractor Predicting: 172it [01:53,  1.52it/s]Extractor Predicting: 173it [01:54,  1.47it/s]Extractor Predicting: 174it [01:55,  1.49it/s]Extractor Predicting: 175it [01:55,  1.46it/s]Extractor Predicting: 176it [01:56,  1.48it/s]Extractor Predicting: 177it [01:57,  1.45it/s]Extractor Predicting: 178it [01:57,  1.46it/s]Extractor Predicting: 179it [01:58,  1.46it/s]Extractor Predicting: 180it [01:59,  1.46it/s]Extractor Predicting: 181it [01:59,  1.46it/s]Extractor Predicting: 182it [02:00,  1.46it/s]Extractor Predicting: 183it [02:01,  1.42it/s]Extractor Predicting: 184it [02:02,  1.44it/s]Extractor Predicting: 185it [02:02,  1.54it/s]Extractor Predicting: 185it [02:02,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:20:27,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:20:27,967 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:20:27,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:20:27,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:20:27,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:20:28,593 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:20:28,594 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:20:29,175 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:20:30,214 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:20:30,215 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:20:33,045 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:20:33,049 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:20:33,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:20:33,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:20:33,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:20:33,816 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:20:33,817 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:20:34,403 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:20:34,554 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:20:34,554 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.444,
  "recall": 0.0454731667349447,
  "score": 0.08249721293199555,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.46it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.49it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:19,  1.59it/s]Extractor Predicting: 31it [00:20,  1.64it/s]Extractor Predicting: 32it [00:20,  1.63it/s]Extractor Predicting: 33it [00:21,  1.63it/s]Extractor Predicting: 34it [00:22,  1.63it/s]Extractor Predicting: 35it [00:22,  1.63it/s]Extractor Predicting: 36it [00:23,  1.61it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:24,  1.62it/s]Extractor Predicting: 39it [00:25,  1.56it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:27,  1.60it/s]Extractor Predicting: 43it [00:27,  1.61it/s]Extractor Predicting: 44it [00:28,  1.62it/s]Extractor Predicting: 45it [00:28,  1.63it/s]Extractor Predicting: 46it [00:29,  1.66it/s]Extractor Predicting: 47it [00:30,  1.66it/s]Extractor Predicting: 48it [00:30,  1.66it/s]Extractor Predicting: 49it [00:31,  1.64it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:33,  1.63it/s]Extractor Predicting: 53it [00:33,  1.62it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:35,  1.58it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.60it/s]Extractor Predicting: 58it [00:36,  1.66it/s]Extractor Predicting: 59it [00:37,  1.62it/s]Extractor Predicting: 60it [00:38,  1.60it/s]Extractor Predicting: 61it [00:38,  1.56it/s]Extractor Predicting: 62it [00:39,  1.53it/s]Extractor Predicting: 63it [00:40,  1.48it/s]Extractor Predicting: 64it [00:40,  1.46it/s]Extractor Predicting: 65it [00:41,  1.45it/s]Extractor Predicting: 66it [00:42,  1.44it/s]Extractor Predicting: 67it [00:43,  1.43it/s]Extractor Predicting: 68it [00:43,  1.44it/s]Extractor Predicting: 69it [00:44,  1.45it/s]Extractor Predicting: 70it [00:45,  1.47it/s]Extractor Predicting: 71it [00:45,  1.48it/s]Extractor Predicting: 72it [00:46,  1.49it/s]Extractor Predicting: 73it [00:47,  1.53it/s]Extractor Predicting: 74it [00:47,  1.53it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:48,  1.55it/s]Extractor Predicting: 77it [00:49,  1.56it/s]Extractor Predicting: 78it [00:50,  1.56it/s]Extractor Predicting: 79it [00:50,  1.61it/s]Extractor Predicting: 80it [00:51,  1.64it/s]Extractor Predicting: 81it [00:52,  1.60it/s]Extractor Predicting: 82it [00:52,  1.62it/s]Extractor Predicting: 83it [00:53,  1.58it/s]Extractor Predicting: 84it [00:53,  1.56it/s]Extractor Predicting: 85it [00:54,  1.52it/s]Extractor Predicting: 86it [00:55,  1.50it/s]Extractor Predicting: 87it [00:56,  1.50it/s]Extractor Predicting: 88it [00:56,  1.52it/s]Extractor Predicting: 89it [00:57,  1.51it/s]Extractor Predicting: 90it [00:57,  1.52it/s]Extractor Predicting: 91it [00:58,  1.51it/s]Extractor Predicting: 92it [00:59,  1.50it/s]Extractor Predicting: 93it [00:59,  1.53it/s]Extractor Predicting: 94it [01:00,  1.53it/s]Extractor Predicting: 95it [01:01,  1.52it/s]Extractor Predicting: 96it [01:01,  1.51it/s]Extractor Predicting: 97it [01:02,  1.53it/s]Extractor Predicting: 98it [01:03,  1.52it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.51it/s]Extractor Predicting: 101it [01:05,  1.53it/s]Extractor Predicting: 102it [01:05,  1.54it/s]Extractor Predicting: 103it [01:06,  1.50it/s]Extractor Predicting: 104it [01:07,  1.53it/s]Extractor Predicting: 105it [01:07,  1.54it/s]Extractor Predicting: 106it [01:08,  1.56it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:09,  1.56it/s]Extractor Predicting: 109it [01:10,  1.55it/s]Extractor Predicting: 110it [01:11,  1.57it/s]Extractor Predicting: 111it [01:11,  1.56it/s]Extractor Predicting: 112it [01:12,  1.54it/s]Extractor Predicting: 113it [01:12,  1.53it/s]Extractor Predicting: 114it [01:13,  1.53it/s]Extractor Predicting: 115it [01:14,  1.52it/s]Extractor Predicting: 116it [01:14,  1.52it/s]Extractor Predicting: 117it [01:15,  1.55it/s]Extractor Predicting: 118it [01:16,  1.53it/s]Extractor Predicting: 119it [01:16,  1.52it/s]Extractor Predicting: 120it [01:17,  1.54it/s]Extractor Predicting: 121it [01:18,  1.58it/s]Extractor Predicting: 122it [01:18,  1.56it/s]Extractor Predicting: 123it [01:19,  1.54it/s]Extractor Predicting: 124it [01:20,  1.52it/s]Extractor Predicting: 125it [01:20,  1.52it/s]Extractor Predicting: 126it [01:21,  1.51it/s]Extractor Predicting: 127it [01:22,  1.55it/s]Extractor Predicting: 128it [01:22,  1.50it/s]Extractor Predicting: 129it [01:23,  1.51it/s]Extractor Predicting: 130it [01:24,  1.54it/s]Extractor Predicting: 131it [01:24,  1.52it/s]Extractor Predicting: 132it [01:25,  1.51it/s]Extractor Predicting: 133it [01:26,  1.51it/s]Extractor Predicting: 134it [01:26,  1.51it/s]Extractor Predicting: 135it [01:27,  1.52it/s]Extractor Predicting: 136it [01:28,  1.55it/s]Extractor Predicting: 137it [01:28,  1.50it/s]Extractor Predicting: 138it [01:29,  1.51it/s]Extractor Predicting: 139it [01:30,  1.38it/s]Extractor Predicting: 140it [01:30,  1.43it/s]Extractor Predicting: 141it [01:31,  1.44it/s]Extractor Predicting: 142it [01:32,  1.48it/s]Extractor Predicting: 143it [01:33,  1.41it/s]Extractor Predicting: 144it [01:33,  1.48it/s]Extractor Predicting: 145it [01:34,  1.46it/s]Extractor Predicting: 146it [01:34,  1.49it/s]Extractor Predicting: 147it [01:35,  1.54it/s]Extractor Predicting: 148it [01:36,  1.52it/s]Extractor Predicting: 149it [01:36,  1.57it/s]Extractor Predicting: 150it [01:37,  1.58it/s]Extractor Predicting: 151it [01:38,  1.60it/s]Extractor Predicting: 152it [01:38,  1.56it/s]Extractor Predicting: 153it [01:39,  1.57it/s]Extractor Predicting: 154it [01:40,  1.53it/s]Extractor Predicting: 155it [01:40,  1.52it/s]Extractor Predicting: 156it [01:41,  1.57it/s]Extractor Predicting: 157it [01:41,  1.53it/s]Extractor Predicting: 158it [01:42,  1.52it/s]Extractor Predicting: 159it [01:43,  1.54it/s]Extractor Predicting: 160it [01:43,  1.53it/s]Extractor Predicting: 161it [01:44,  1.56it/s]Extractor Predicting: 162it [01:45,  1.55it/s]Extractor Predicting: 163it [01:45,  1.54it/s]Extractor Predicting: 164it [01:46,  1.53it/s]Extractor Predicting: 165it [01:47,  1.49it/s]Extractor Predicting: 166it [01:47,  1.47it/s]Extractor Predicting: 167it [01:48,  1.47it/s]Extractor Predicting: 168it [01:49,  1.49it/s]Extractor Predicting: 169it [01:49,  1.52it/s]Extractor Predicting: 170it [01:50,  1.54it/s]Extractor Predicting: 171it [01:51,  1.55it/s]Extractor Predicting: 172it [01:51,  1.58it/s]Extractor Predicting: 173it [01:52,  1.57it/s]Extractor Predicting: 174it [01:53,  1.55it/s]Extractor Predicting: 175it [01:53,  1.55it/s]Extractor Predicting: 176it [01:54,  1.56it/s]Extractor Predicting: 177it [01:55,  1.54it/s]Extractor Predicting: 178it [01:55,  1.53it/s]Extractor Predicting: 179it [01:56,  1.50it/s]Extractor Predicting: 180it [01:57,  1.54it/s]Extractor Predicting: 181it [01:57,  1.54it/s]Extractor Predicting: 182it [01:58,  1.56it/s]Extractor Predicting: 183it [01:58,  1.59it/s]Extractor Predicting: 184it [01:59,  1.57it/s]Extractor Predicting: 185it [02:00,  1.58it/s]Extractor Predicting: 186it [02:00,  1.53it/s]Extractor Predicting: 187it [02:01,  1.55it/s]Extractor Predicting: 188it [02:02,  1.54it/s]Extractor Predicting: 189it [02:02,  1.55it/s]Extractor Predicting: 190it [02:03,  1.56it/s]Extractor Predicting: 191it [02:04,  1.56it/s]Extractor Predicting: 192it [02:04,  1.60it/s]Extractor Predicting: 193it [02:05,  1.58it/s]Extractor Predicting: 194it [02:05,  1.58it/s]Extractor Predicting: 195it [02:06,  1.56it/s]Extractor Predicting: 196it [02:07,  1.56it/s]Extractor Predicting: 197it [02:07,  1.56it/s]Extractor Predicting: 198it [02:08,  1.54it/s]Extractor Predicting: 199it [02:09,  1.53it/s]Extractor Predicting: 200it [02:09,  1.53it/s]Extractor Predicting: 201it [02:10,  1.55it/s]Extractor Predicting: 202it [02:11,  1.55it/s]Extractor Predicting: 203it [02:11,  1.57it/s]Extractor Predicting: 204it [02:12,  1.56it/s]Extractor Predicting: 205it [02:13,  1.56it/s]Extractor Predicting: 206it [02:13,  1.56it/s]Extractor Predicting: 207it [02:14,  1.57it/s]Extractor Predicting: 208it [02:14,  1.54it/s]Extractor Predicting: 209it [02:15,  1.52it/s]Extractor Predicting: 210it [02:16,  1.58it/s]Extractor Predicting: 211it [02:16,  1.56it/s]Extractor Predicting: 212it [02:17,  1.57it/s]Extractor Predicting: 213it [02:18,  1.57it/s]Extractor Predicting: 214it [02:18,  1.60it/s]Extractor Predicting: 215it [02:19,  1.58it/s]Extractor Predicting: 216it [02:20,  1.55it/s]Extractor Predicting: 217it [02:20,  1.56it/s]Extractor Predicting: 218it [02:21,  1.55it/s]Extractor Predicting: 219it [02:22,  1.55it/s]Extractor Predicting: 220it [02:22,  1.56it/s]Extractor Predicting: 221it [02:23,  1.57it/s]Extractor Predicting: 222it [02:23,  1.52it/s]Extractor Predicting: 223it [02:24,  1.47it/s]Extractor Predicting: 224it [02:25,  1.49it/s]Extractor Predicting: 225it [02:25,  1.52it/s]Extractor Predicting: 226it [02:26,  1.55it/s]Extractor Predicting: 227it [02:27,  1.54it/s]Extractor Predicting: 228it [02:27,  1.57it/s]Extractor Predicting: 229it [02:28,  1.60it/s]Extractor Predicting: 230it [02:29,  1.61it/s]Extractor Predicting: 231it [02:29,  1.61it/s]Extractor Predicting: 232it [02:30,  1.60it/s]Extractor Predicting: 233it [02:30,  1.59it/s]Extractor Predicting: 234it [02:31,  1.59it/s]Extractor Predicting: 235it [02:32,  1.56it/s]Extractor Predicting: 236it [02:32,  1.56it/s]Extractor Predicting: 237it [02:33,  1.57it/s]Extractor Predicting: 238it [02:34,  1.54it/s]Extractor Predicting: 239it [02:34,  1.56it/s]Extractor Predicting: 240it [02:35,  1.55it/s]Extractor Predicting: 241it [02:36,  1.56it/s]Extractor Predicting: 242it [02:36,  1.54it/s]Extractor Predicting: 243it [02:37,  1.51it/s]Extractor Predicting: 244it [02:38,  1.51it/s]Extractor Predicting: 245it [02:38,  1.54it/s]Extractor Predicting: 246it [02:39,  1.54it/s]Extractor Predicting: 247it [02:40,  1.57it/s]Extractor Predicting: 248it [02:40,  1.51it/s]Extractor Predicting: 249it [02:41,  1.50it/s]Extractor Predicting: 250it [02:42,  1.53it/s]Extractor Predicting: 251it [02:42,  1.53it/s]Extractor Predicting: 252it [02:43,  1.36it/s]Extractor Predicting: 253it [02:44,  1.37it/s]Extractor Predicting: 254it [02:45,  1.39it/s]Extractor Predicting: 255it [02:45,  1.43it/s]Extractor Predicting: 256it [02:46,  1.45it/s]Extractor Predicting: 257it [02:47,  1.47it/s]Extractor Predicting: 258it [02:47,  1.49it/s]Extractor Predicting: 259it [02:48,  1.49it/s]Extractor Predicting: 260it [02:49,  1.47it/s]Extractor Predicting: 261it [02:49,  1.49it/s]Extractor Predicting: 262it [02:50,  1.48it/s]Extractor Predicting: 263it [02:51,  1.49it/s]Extractor Predicting: 264it [02:51,  1.50it/s]Extractor Predicting: 265it [02:52,  1.52it/s]Extractor Predicting: 266it [02:53,  1.47it/s]Extractor Predicting: 267it [02:53,  1.48it/s]Extractor Predicting: 268it [02:54,  1.46it/s]Extractor Predicting: 269it [02:55,  1.48it/s]Extractor Predicting: 270it [02:55,  1.46it/s]Extractor Predicting: 271it [02:56,  1.47it/s]Extractor Predicting: 272it [02:57,  1.47it/s]Extractor Predicting: 273it [02:57,  1.48it/s]Extractor Predicting: 274it [02:58,  1.45it/s]Extractor Predicting: 275it [02:59,  1.46it/s]Extractor Predicting: 276it [02:59,  1.47it/s]Extractor Predicting: 277it [03:00,  1.48it/s]Extractor Predicting: 278it [03:01,  1.49it/s]Extractor Predicting: 279it [03:01,  1.48it/s]Extractor Predicting: 280it [03:02,  1.49it/s]Extractor Predicting: 281it [03:03,  1.45it/s]Extractor Predicting: 282it [03:03,  1.44it/s]Extractor Predicting: 283it [03:04,  1.46it/s]Extractor Predicting: 284it [03:05,  1.49it/s]Extractor Predicting: 285it [03:05,  1.48it/s]Extractor Predicting: 286it [03:06,  1.50it/s]Extractor Predicting: 287it [03:07,  1.45it/s]Extractor Predicting: 288it [03:07,  1.48it/s]Extractor Predicting: 289it [03:08,  1.50it/s]Extractor Predicting: 290it [03:09,  1.47it/s]Extractor Predicting: 291it [03:10,  1.44it/s]Extractor Predicting: 292it [03:10,  1.47it/s]Extractor Predicting: 293it [03:11,  1.47it/s]Extractor Predicting: 294it [03:12,  1.45it/s]Extractor Predicting: 295it [03:12,  1.46it/s]Extractor Predicting: 296it [03:13,  1.49it/s]Extractor Predicting: 297it [03:14,  1.50it/s]Extractor Predicting: 298it [03:14,  1.48it/s]Extractor Predicting: 299it [03:15,  1.48it/s]Extractor Predicting: 300it [03:16,  1.46it/s]Extractor Predicting: 301it [03:16,  1.49it/s]Extractor Predicting: 302it [03:17,  1.46it/s]Extractor Predicting: 303it [03:18,  1.48it/s]Extractor Predicting: 304it [03:18,  1.50it/s]Extractor Predicting: 305it [03:19,  1.53it/s]Extractor Predicting: 306it [03:20,  1.55it/s]Extractor Predicting: 307it [03:20,  1.52it/s]Extractor Predicting: 308it [03:21,  1.54it/s]Extractor Predicting: 309it [03:22,  1.51it/s]Extractor Predicting: 310it [03:22,  1.51it/s]Extractor Predicting: 311it [03:23,  1.47it/s]Extractor Predicting: 312it [03:24,  1.49it/s]Extractor Predicting: 313it [03:24,  1.44it/s]Extractor Predicting: 314it [03:25,  1.44it/s]Extractor Predicting: 315it [03:26,  1.47it/s]Extractor Predicting: 316it [03:26,  1.51it/s]Extractor Predicting: 317it [03:27,  1.52it/s]Extractor Predicting: 318it [03:28,  1.55it/s]Extractor Predicting: 319it [03:28,  1.51it/s]Extractor Predicting: 320it [03:29,  1.49it/s]Extractor Predicting: 321it [03:30,  1.48it/s]Extractor Predicting: 322it [03:30,  1.50it/s]Extractor Predicting: 323it [03:31,  1.45it/s]Extractor Predicting: 324it [03:32,  1.47it/s]Extractor Predicting: 325it [03:32,  1.50it/s]Extractor Predicting: 326it [03:33,  1.50it/s]Extractor Predicting: 327it [03:34,  1.51it/s]Extractor Predicting: 328it [03:34,  1.48it/s]Extractor Predicting: 329it [03:35,  1.49it/s]Extractor Predicting: 330it [03:36,  1.48it/s]Extractor Predicting: 331it [03:36,  1.49it/s]Extractor Predicting: 332it [03:37,  1.49it/s]Extractor Predicting: 333it [03:38,  1.47it/s]Extractor Predicting: 334it [03:38,  1.49it/s]Extractor Predicting: 335it [03:39,  1.48it/s]Extractor Predicting: 336it [03:40,  1.41it/s]Extractor Predicting: 337it [03:41,  1.42it/s]Extractor Predicting: 338it [03:41,  1.42it/s]Extractor Predicting: 339it [03:42,  1.44it/s]Extractor Predicting: 340it [03:43,  1.45it/s]Extractor Predicting: 341it [03:43,  1.43it/s]Extractor Predicting: 342it [03:44,  1.44it/s]Extractor Predicting: 343it [03:45,  1.47it/s]Extractor Predicting: 344it [03:45,  1.47it/s]Extractor Predicting: 345it [03:46,  1.43it/s]Extractor Predicting: 346it [03:47,  1.44it/s]Extractor Predicting: 347it [03:47,  1.45it/s]Extractor Predicting: 348it [03:48,  1.47it/s]Extractor Predicting: 349it [03:49,  1.48it/s]Extractor Predicting: 350it [03:50,  1.32it/s]Extractor Predicting: 351it [03:50,  1.39it/s]Extractor Predicting: 352it [03:51,  1.38it/s]Extractor Predicting: 353it [03:52,  1.38it/s]Extractor Predicting: 354it [03:53,  1.42it/s]Extractor Predicting: 355it [03:53,  1.45it/s]Extractor Predicting: 356it [03:54,  1.46it/s]Extractor Predicting: 357it [03:54,  1.47it/s]Extractor Predicting: 358it [03:55,  1.48it/s]Extractor Predicting: 359it [03:56,  1.47it/s]Extractor Predicting: 360it [03:57,  1.45it/s]Extractor Predicting: 361it [03:57,  1.45it/s]Extractor Predicting: 362it [03:58,  1.46it/s]Extractor Predicting: 363it [03:59,  1.50it/s]Extractor Predicting: 364it [03:59,  1.51it/s]Extractor Predicting: 365it [04:00,  1.48it/s]Extractor Predicting: 366it [04:01,  1.44it/s]Extractor Predicting: 367it [04:01,  1.44it/s]Extractor Predicting: 368it [04:02,  1.42it/s]Extractor Predicting: 369it [04:03,  1.38it/s]Extractor Predicting: 370it [04:04,  1.39it/s]Extractor Predicting: 371it [04:04,  1.42it/s]Extractor Predicting: 372it [04:05,  1.67it/s]Extractor Predicting: 372it [04:05,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:24:48,545 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:24:48,550 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:24:48,550 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:24:48,551 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:24:48,551 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:24:49,150 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:24:49,151 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:24:49,714 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:24:50,775 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:24:50,775 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:24:53,612 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:24:53,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:24:53,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:24:53,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:24:53,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:24:54,286 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:24:54,287 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:24:54,849 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:24:55,007 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:24:55,007 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3567567567567568,
  "recall": 0.051840215439856376,
  "score": 0.09052610953267365,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.52it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.47it/s]Extractor Predicting: 25it [00:16,  1.49it/s]Extractor Predicting: 26it [00:16,  1.47it/s]Extractor Predicting: 27it [00:17,  1.47it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:18,  1.49it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.49it/s]Extractor Predicting: 38it [00:25,  1.45it/s]Extractor Predicting: 39it [00:25,  1.42it/s]Extractor Predicting: 40it [00:26,  1.41it/s]Extractor Predicting: 41it [00:27,  1.41it/s]Extractor Predicting: 42it [00:27,  1.43it/s]Extractor Predicting: 43it [00:28,  1.43it/s]Extractor Predicting: 44it [00:29,  1.44it/s]Extractor Predicting: 45it [00:29,  1.42it/s]Extractor Predicting: 46it [00:30,  1.45it/s]Extractor Predicting: 47it [00:31,  1.34it/s]Extractor Predicting: 48it [00:32,  1.37it/s]Extractor Predicting: 49it [00:32,  1.38it/s]Extractor Predicting: 50it [00:33,  1.39it/s]Extractor Predicting: 51it [00:34,  1.38it/s]Extractor Predicting: 52it [00:35,  1.38it/s]Extractor Predicting: 53it [00:35,  1.40it/s]Extractor Predicting: 54it [00:36,  1.39it/s]Extractor Predicting: 55it [00:37,  1.43it/s]Extractor Predicting: 56it [00:37,  1.44it/s]Extractor Predicting: 57it [00:38,  1.43it/s]Extractor Predicting: 58it [00:39,  1.41it/s]Extractor Predicting: 59it [00:39,  1.41it/s]Extractor Predicting: 60it [00:40,  1.41it/s]Extractor Predicting: 61it [00:41,  1.37it/s]Extractor Predicting: 62it [00:42,  1.37it/s]Extractor Predicting: 63it [00:42,  1.49it/s]Extractor Predicting: 63it [00:42,  1.47it/s]
[INFO|configuration_utils.py:515] 2023-08-29 03:25:38,735 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:25:38,736 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:25:38,744 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:25:38,745 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 03:25:38,749 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:25:42,082 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 03:25:42,082 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 03:25:42,146 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:25:42,147 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:25:42,175 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:25:42,180 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:25:42,181 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:25:42,181 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:25:42,181 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:25:42,181 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:25:42,181 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5463917525773195,
  "recall": 0.03176505843572071,
  "score": 0.06003964882469555,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 03:25:42,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:43,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:43,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:44,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:45,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:46,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:46,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:47,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:48,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:48,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:49,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:50,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:50,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:51,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:52,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:52,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:53,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:54,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:55,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:55,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:56,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:57,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:58,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:58,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:56, 16.92s/it][WARNING|generation_utils.py:914] 2023-08-29 03:25:59,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:59,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:00,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:01,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:01,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:02,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:03,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:03,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:04,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:05,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:05,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:06,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:07,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:07,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:08,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:09,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:09,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:10,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:11,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:11,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:12,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:13,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:22, 15.55s/it][WARNING|generation_utils.py:914] 2023-08-29 03:26:13,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:14,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:15,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:15,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:16,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:16,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:17,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:18,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:18,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:19,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:19,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:20,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:21,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:21,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:22,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:22,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:23,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:23,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:24,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:41, 13.46s/it][WARNING|generation_utils.py:914] 2023-08-29 03:26:24,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:25,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:26,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:26,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:27,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:28,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:29,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:29,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:30,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:31,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:32,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:32,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:33,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:34,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:35,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:36,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:36,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:37,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:38,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:38,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:39,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:57<02:36, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-29 03:26:40,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:40,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:41,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:42,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:43,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:43,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:44,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:45,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:45,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:46,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:47,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:47,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:48,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:49,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:49,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:50,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:51,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:51,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:52,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:53,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:53,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:54,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:55,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:55,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:56,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:14<02:31, 15.19s/it][WARNING|generation_utils.py:914] 2023-08-29 03:26:57,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:57,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:58,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:59,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:59,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:00,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:01,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:02,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:02,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:03,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:04,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:04,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:05,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:06,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:07,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:07,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:08,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:09,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:09,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:10,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:11,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:29<02:15, 15.08s/it][WARNING|generation_utils.py:914] 2023-08-29 03:27:12,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:12,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:13,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:14,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:14,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:15,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:16,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:16,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:17,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:18,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:19,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:19,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:20,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:21,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:21,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:22,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:23,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:23,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:24,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:25,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:43<01:57, 14.69s/it][WARNING|generation_utils.py:914] 2023-08-29 03:27:25,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:26,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:27,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:28,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:28,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:29,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:30,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:31,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:31,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:32,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:33,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:33,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:34,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:35,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:36,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:36,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:37,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:38,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:38,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:39,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:40,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:58<01:43, 14.84s/it][WARNING|generation_utils.py:914] 2023-08-29 03:27:41,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:41,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:42,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:43,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:44,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:44,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:45,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:46,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:47,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:47,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:48,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:49,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:50,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:50,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:51,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:52,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:53,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:53,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:54,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:55,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:56,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:56,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:57,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:58,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:27:59,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:17<01:36, 16.10s/it][WARNING|generation_utils.py:914] 2023-08-29 03:28:00,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:00,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:01,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:02,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:02,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:03,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:04,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:04,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:05,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:06,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:07,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:07,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:08,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:09,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:10,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:10,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:11,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:12,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:12,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:13,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:14,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:32<01:18, 15.76s/it][WARNING|generation_utils.py:914] 2023-08-29 03:28:15,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:15,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:16,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:16,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:17,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:18,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:18,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:19,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:19,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:20,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:20,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:21,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:22,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:22,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:23,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:23,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:24,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:25,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:25,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:43<00:57, 14.39s/it][WARNING|generation_utils.py:914] 2023-08-29 03:28:26,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:26,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:27,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:28,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:29,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:29,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:30,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:31,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:32,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:32,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:33,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:34,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:35,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:35,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:36,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:37,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:37,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:38,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:39,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:40,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:40,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:41,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:59<00:44, 14.85s/it][WARNING|generation_utils.py:914] 2023-08-29 03:28:42,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:42,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:43,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:44,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:45,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:45,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:46,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:47,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:48,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:48,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:49,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:50,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:51,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:52,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:52,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:53,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:54,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:55,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:55,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:56,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:57,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:58,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:28:59,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:17<00:31, 15.66s/it][WARNING|generation_utils.py:914] 2023-08-29 03:28:59,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:00,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:00,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:01,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:02,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:02,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:03,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:03,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:04,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:05,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:05,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:06,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:06,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:07,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:08,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:08,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:09,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:10,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:10,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:28<00:14, 14.43s/it][WARNING|generation_utils.py:914] 2023-08-29 03:29:11,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:12,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:13,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:13,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:14,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:15,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:16,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:17,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:18,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:18,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:19,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:20,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:20,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:21,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:22,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:23,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:24,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:25,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:25,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:26,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:27,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:29:27,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:46<00:00, 15.33s/it]Generating: 100%|██████████| 15/15 [03:46<00:00, 15.08s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:29:34,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:29:34,582 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:29:34,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:29:34,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:29:34,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:29:35,304 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:29:35,305 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:29:35,980 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:29:37,121 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:29:37,122 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:29:39,988 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:29:39,991 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:29:39,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:29:39,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:29:39,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:29:40,648 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:29:40,649 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:29:41,249 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:29:41,406 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:29:41,406 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : conflict .', 'success_rate': 0.8046875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 319, 'raw': 320}
{'target': 600, 'success': 350, 'raw': 352}
{'target': 600, 'success': 382, 'raw': 384}
{'target': 600, 'success': 414, 'raw': 416}
{'target': 600, 'success': 446, 'raw': 448}
{'target': 600, 'success': 478, 'raw': 480}
{'target': 600, 'success': 510, 'raw': 512}
{'target': 600, 'success': 542, 'raw': 544}
{'target': 600, 'success': 574, 'raw': 576}
{'target': 600, 'success': 606, 'raw': 608}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9967105263157895, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 456, 'raw': 608}
{'target': 600, 'success': 481, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 531, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.75375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : composer . Context : Later in 2008 , he composed what is now called a score to the soundtrack to a 2011 BBC series of animated musicals for BBC One , based on a novel by George Petter . Head Entity : The soundtrack , Tail Entity : George Petter .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9703125, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : creator .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The composer William Arthur Greenblatt created the songs The Lion and the Witch of the Seven Fuen visited New York City from 1907 until the death of his wife , Dorothy Greenblatt . Head Entity : William Arthur Greenblatt , Tail Entity : music .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 537, 'raw': 704}
{'target': 600, 'success': 564, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 319, 'raw': 320}
{'target': 600, 'success': 351, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 415, 'raw': 416}
{'target': 600, 'success': 447, 'raw': 448}
{'target': 600, 'success': 479, 'raw': 480}
{'target': 600, 'success': 511, 'raw': 512}
{'target': 600, 'success': 543, 'raw': 544}
{'target': 600, 'success': 575, 'raw': 576}
{'target': 600, 'success': 607, 'raw': 608}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9983552631578947, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 319, 'raw': 320}
{'target': 600, 'success': 351, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 414, 'raw': 416}
{'target': 600, 'success': 446, 'raw': 448}
{'target': 600, 'success': 478, 'raw': 480}
{'target': 600, 'success': 510, 'raw': 512}
{'target': 600, 'success': 542, 'raw': 544}
{'target': 600, 'success': 574, 'raw': 576}
{'target': 600, 'success': 605, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9950657894736842, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8664772727272727, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 9758
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9858, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.64it/s]Extractor Estimating: 2it [00:01,  1.51it/s]Extractor Estimating: 3it [00:01,  1.51it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.59it/s]Extractor Estimating: 6it [00:03,  1.54it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.53it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:07,  1.59it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.55it/s]Extractor Estimating: 14it [00:08,  1.56it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:10,  1.58it/s]Extractor Estimating: 17it [00:10,  1.59it/s]Extractor Estimating: 18it [00:11,  1.59it/s]Extractor Estimating: 19it [00:12,  1.53it/s]Extractor Estimating: 20it [00:12,  1.55it/s]Extractor Estimating: 21it [00:13,  1.56it/s]Extractor Estimating: 22it [00:14,  1.58it/s]Extractor Estimating: 23it [00:14,  1.59it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:15,  1.60it/s]Extractor Estimating: 26it [00:16,  1.61it/s]Extractor Estimating: 27it [00:17,  1.60it/s]Extractor Estimating: 28it [00:17,  1.60it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:19,  1.60it/s]Extractor Estimating: 31it [00:19,  1.57it/s]Extractor Estimating: 32it [00:20,  1.61it/s]Extractor Estimating: 33it [00:21,  1.54it/s]Extractor Estimating: 34it [00:21,  1.52it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:22,  1.56it/s]Extractor Estimating: 37it [00:23,  1.58it/s]Extractor Estimating: 38it [00:24,  1.57it/s]Extractor Estimating: 39it [00:24,  1.60it/s]Extractor Estimating: 40it [00:25,  1.60it/s]Extractor Estimating: 41it [00:26,  1.62it/s]Extractor Estimating: 42it [00:26,  1.64it/s]Extractor Estimating: 43it [00:27,  1.63it/s]Extractor Estimating: 44it [00:27,  1.62it/s]Extractor Estimating: 45it [00:28,  1.60it/s]Extractor Estimating: 46it [00:29,  1.53it/s]Extractor Estimating: 47it [00:29,  1.57it/s]Extractor Estimating: 48it [00:30,  1.58it/s]Extractor Estimating: 49it [00:31,  1.61it/s]Extractor Estimating: 50it [00:31,  1.56it/s]Extractor Estimating: 51it [00:32,  1.69it/s]Extractor Estimating: 52it [00:32,  1.78it/s]Extractor Estimating: 53it [00:33,  1.85it/s]Extractor Estimating: 54it [00:33,  1.86it/s]Extractor Estimating: 55it [00:34,  1.94it/s]Extractor Estimating: 56it [00:34,  2.01it/s]Extractor Estimating: 57it [00:35,  2.04it/s]Extractor Estimating: 58it [00:35,  2.08it/s]Extractor Estimating: 59it [00:36,  2.07it/s]Extractor Estimating: 60it [00:36,  2.10it/s]Extractor Estimating: 61it [00:37,  2.10it/s]Extractor Estimating: 62it [00:37,  2.12it/s]Extractor Estimating: 63it [00:37,  2.08it/s]Extractor Estimating: 64it [00:38,  2.08it/s]Extractor Estimating: 65it [00:38,  2.10it/s]Extractor Estimating: 66it [00:39,  2.13it/s]Extractor Estimating: 67it [00:39,  2.15it/s]Extractor Estimating: 68it [00:40,  2.13it/s]Extractor Estimating: 69it [00:40,  2.12it/s]Extractor Estimating: 70it [00:41,  2.15it/s]Extractor Estimating: 71it [00:41,  2.15it/s]Extractor Estimating: 72it [00:42,  2.08it/s]Extractor Estimating: 73it [00:42,  2.05it/s]Extractor Estimating: 74it [00:43,  2.10it/s]Extractor Estimating: 75it [00:43,  2.05it/s]Extractor Estimating: 76it [00:44,  1.87it/s]Extractor Estimating: 77it [00:44,  1.78it/s]Extractor Estimating: 78it [00:45,  1.73it/s]Extractor Estimating: 79it [00:46,  1.73it/s]Extractor Estimating: 80it [00:46,  1.70it/s]Extractor Estimating: 81it [00:47,  1.66it/s]Extractor Estimating: 82it [00:48,  1.56it/s]Extractor Estimating: 83it [00:48,  1.60it/s]Extractor Estimating: 84it [00:49,  1.51it/s]Extractor Estimating: 85it [00:50,  1.48it/s]Extractor Estimating: 86it [00:50,  1.53it/s]Extractor Estimating: 87it [00:51,  1.52it/s]Extractor Estimating: 88it [00:52,  1.55it/s]Extractor Estimating: 89it [00:52,  1.54it/s]Extractor Estimating: 90it [00:53,  1.48it/s]Extractor Estimating: 91it [00:54,  1.42it/s]Extractor Estimating: 92it [00:54,  1.47it/s]Extractor Estimating: 93it [00:55,  1.49it/s]Extractor Estimating: 94it [00:56,  1.51it/s]Extractor Estimating: 95it [00:56,  1.56it/s]Extractor Estimating: 96it [00:57,  1.56it/s]Extractor Estimating: 97it [00:58,  1.56it/s]Extractor Estimating: 98it [00:58,  1.55it/s]Extractor Estimating: 99it [00:59,  1.56it/s]Extractor Estimating: 100it [00:59,  1.57it/s]Extractor Estimating: 101it [01:00,  1.59it/s]Extractor Estimating: 102it [01:01,  1.59it/s]Extractor Estimating: 103it [01:01,  1.58it/s]Extractor Estimating: 104it [01:02,  1.53it/s]Extractor Estimating: 105it [01:03,  1.58it/s]Extractor Estimating: 106it [01:03,  1.59it/s]Extractor Estimating: 107it [01:04,  1.61it/s]Extractor Estimating: 108it [01:04,  1.59it/s]Extractor Estimating: 109it [01:05,  1.60it/s]Extractor Estimating: 110it [01:06,  1.62it/s]Extractor Estimating: 111it [01:06,  1.58it/s]Extractor Estimating: 112it [01:07,  1.56it/s]Extractor Estimating: 113it [01:08,  1.55it/s]Extractor Estimating: 114it [01:08,  1.56it/s]Extractor Estimating: 115it [01:09,  1.62it/s]Extractor Estimating: 116it [01:09,  1.63it/s]Extractor Estimating: 117it [01:10,  1.64it/s]Extractor Estimating: 118it [01:11,  1.64it/s]Extractor Estimating: 119it [01:11,  1.66it/s]Extractor Estimating: 120it [01:12,  1.65it/s]Extractor Estimating: 121it [01:12,  1.65it/s]Extractor Estimating: 122it [01:13,  1.63it/s]Extractor Estimating: 123it [01:14,  1.65it/s]Extractor Estimating: 124it [01:14,  1.62it/s]Extractor Estimating: 125it [01:15,  1.63it/s]Extractor Estimating: 126it [01:16,  1.63it/s]Extractor Estimating: 127it [01:16,  1.62it/s]Extractor Estimating: 128it [01:17,  1.61it/s]Extractor Estimating: 129it [01:17,  1.58it/s]Extractor Estimating: 130it [01:18,  1.59it/s]Extractor Estimating: 131it [01:19,  1.60it/s]Extractor Estimating: 132it [01:19,  1.57it/s]Extractor Estimating: 133it [01:20,  1.54it/s]Extractor Estimating: 134it [01:21,  1.56it/s]Extractor Estimating: 135it [01:21,  1.52it/s]Extractor Estimating: 136it [01:22,  1.53it/s]Extractor Estimating: 137it [01:23,  1.54it/s]Extractor Estimating: 138it [01:23,  1.57it/s]Extractor Estimating: 139it [01:24,  1.54it/s]Extractor Estimating: 140it [01:25,  1.56it/s]Extractor Estimating: 141it [01:25,  1.53it/s]Extractor Estimating: 142it [01:26,  1.54it/s]Extractor Estimating: 143it [01:27,  1.51it/s]Extractor Estimating: 144it [01:27,  1.52it/s]Extractor Estimating: 145it [01:28,  1.51it/s]Extractor Estimating: 146it [01:29,  1.55it/s]Extractor Estimating: 147it [01:29,  1.49it/s]Extractor Estimating: 148it [01:30,  1.51it/s]Extractor Estimating: 149it [01:31,  1.51it/s]Extractor Estimating: 150it [01:31,  1.48it/s]Extractor Estimating: 151it [01:32,  1.52it/s]Extractor Estimating: 152it [01:33,  1.53it/s]Extractor Estimating: 153it [01:33,  1.52it/s]Extractor Estimating: 154it [01:34,  1.54it/s]Extractor Estimating: 155it [01:35,  1.50it/s]Extractor Estimating: 156it [01:35,  1.49it/s]Extractor Estimating: 157it [01:36,  1.46it/s]Extractor Estimating: 158it [01:37,  1.51it/s]Extractor Estimating: 159it [01:37,  1.52it/s]Extractor Estimating: 160it [01:38,  1.55it/s]Extractor Estimating: 161it [01:39,  1.39it/s]Extractor Estimating: 162it [01:39,  1.46it/s]Extractor Estimating: 163it [01:40,  1.44it/s]Extractor Estimating: 164it [01:41,  1.48it/s]Extractor Estimating: 165it [01:41,  1.51it/s]Extractor Estimating: 166it [01:42,  1.52it/s]Extractor Estimating: 167it [01:43,  1.49it/s]Extractor Estimating: 168it [01:43,  1.50it/s]Extractor Estimating: 169it [01:44,  1.51it/s]Extractor Estimating: 170it [01:45,  1.51it/s]Extractor Estimating: 171it [01:45,  1.50it/s]Extractor Estimating: 172it [01:46,  1.52it/s]Extractor Estimating: 173it [01:47,  1.55it/s]Extractor Estimating: 174it [01:47,  1.55it/s]Extractor Estimating: 175it [01:48,  1.56it/s]Extractor Estimating: 176it [01:48,  1.55it/s]Extractor Estimating: 177it [01:49,  1.59it/s]Extractor Estimating: 178it [01:50,  1.59it/s]Extractor Estimating: 179it [01:50,  1.59it/s]Extractor Estimating: 180it [01:51,  1.56it/s]Extractor Estimating: 181it [01:52,  1.58it/s]Extractor Estimating: 182it [01:52,  1.56it/s]Extractor Estimating: 183it [01:53,  1.56it/s]Extractor Estimating: 184it [01:54,  1.56it/s]Extractor Estimating: 185it [01:54,  1.57it/s]Extractor Estimating: 186it [01:55,  1.60it/s]Extractor Estimating: 187it [01:55,  1.59it/s]Extractor Estimating: 188it [01:56,  1.62it/s]Extractor Estimating: 189it [01:57,  1.62it/s]Extractor Estimating: 190it [01:57,  1.60it/s]Extractor Estimating: 191it [01:58,  1.65it/s]Extractor Estimating: 192it [01:58,  1.63it/s]Extractor Estimating: 193it [01:59,  1.63it/s]Extractor Estimating: 194it [02:00,  1.61it/s]Extractor Estimating: 195it [02:00,  1.61it/s]Extractor Estimating: 196it [02:01,  1.61it/s]Extractor Estimating: 197it [02:02,  1.57it/s]Extractor Estimating: 198it [02:02,  1.53it/s]Extractor Estimating: 199it [02:03,  1.52it/s]Extractor Estimating: 200it [02:04,  1.53it/s]Extractor Estimating: 201it [02:04,  1.53it/s]Extractor Estimating: 202it [02:05,  1.54it/s]Extractor Estimating: 203it [02:06,  1.58it/s]Extractor Estimating: 204it [02:06,  1.60it/s]Extractor Estimating: 205it [02:07,  1.57it/s]Extractor Estimating: 206it [02:07,  1.58it/s]Extractor Estimating: 207it [02:08,  1.58it/s]Extractor Estimating: 208it [02:09,  1.56it/s]Extractor Estimating: 209it [02:09,  1.55it/s]Extractor Estimating: 210it [02:10,  1.53it/s]Extractor Estimating: 211it [02:11,  1.53it/s]Extractor Estimating: 212it [02:11,  1.48it/s]Extractor Estimating: 213it [02:12,  1.51it/s]Extractor Estimating: 214it [02:13,  1.52it/s]Extractor Estimating: 215it [02:13,  1.48it/s]Extractor Estimating: 216it [02:14,  1.52it/s]Extractor Estimating: 217it [02:15,  1.54it/s]Extractor Estimating: 218it [02:15,  1.53it/s]Extractor Estimating: 219it [02:16,  1.48it/s]Extractor Estimating: 220it [02:17,  1.50it/s]Extractor Estimating: 221it [02:17,  1.55it/s]Extractor Estimating: 222it [02:18,  1.52it/s]Extractor Estimating: 223it [02:19,  1.51it/s]Extractor Estimating: 224it [02:19,  1.51it/s]Extractor Estimating: 225it [02:20,  1.53it/s]Extractor Estimating: 226it [02:21,  1.53it/s]Extractor Estimating: 227it [02:21,  1.51it/s]Extractor Estimating: 228it [02:22,  1.51it/s]Extractor Estimating: 229it [02:23,  1.52it/s]Extractor Estimating: 230it [02:23,  1.52it/s]Extractor Estimating: 231it [02:24,  1.48it/s]Extractor Estimating: 232it [02:25,  1.48it/s]Extractor Estimating: 233it [02:25,  1.47it/s]Extractor Estimating: 234it [02:26,  1.46it/s]Extractor Estimating: 235it [02:27,  1.47it/s]Extractor Estimating: 236it [02:27,  1.44it/s]Extractor Estimating: 237it [02:28,  1.46it/s]Extractor Estimating: 238it [02:29,  1.42it/s]Extractor Estimating: 239it [02:29,  1.46it/s]Extractor Estimating: 240it [02:30,  1.45it/s]Extractor Estimating: 241it [02:31,  1.45it/s]Extractor Estimating: 242it [02:32,  1.41it/s]Extractor Estimating: 243it [02:32,  1.45it/s]Extractor Estimating: 244it [02:33,  1.48it/s]Extractor Estimating: 245it [02:34,  1.52it/s]Extractor Estimating: 246it [02:34,  1.51it/s]Extractor Estimating: 247it [02:35,  1.53it/s]Extractor Estimating: 248it [02:36,  1.41it/s]Extractor Estimating: 249it [02:36,  1.41it/s]Extractor Estimating: 250it [02:37,  1.44it/s]Extractor Estimating: 251it [02:38,  1.47it/s]Extractor Estimating: 252it [02:38,  1.48it/s]Extractor Estimating: 253it [02:39,  1.49it/s]Extractor Estimating: 254it [02:40,  1.50it/s]Extractor Estimating: 255it [02:40,  1.50it/s]Extractor Estimating: 256it [02:41,  1.52it/s]Extractor Estimating: 257it [02:42,  1.51it/s]Extractor Estimating: 258it [02:42,  1.53it/s]Extractor Estimating: 259it [02:43,  1.53it/s]Extractor Estimating: 260it [02:44,  1.53it/s]Extractor Estimating: 261it [02:44,  1.53it/s]Extractor Estimating: 262it [02:45,  1.54it/s]Extractor Estimating: 263it [02:46,  1.54it/s]Extractor Estimating: 264it [02:46,  1.55it/s]Extractor Estimating: 265it [02:47,  1.55it/s]Extractor Estimating: 266it [02:47,  1.53it/s]Extractor Estimating: 267it [02:48,  1.54it/s]Extractor Estimating: 268it [02:49,  1.52it/s]Extractor Estimating: 269it [02:49,  1.54it/s]Extractor Estimating: 270it [02:50,  1.55it/s]Extractor Estimating: 271it [02:51,  1.54it/s]Extractor Estimating: 272it [02:51,  1.53it/s]Extractor Estimating: 273it [02:52,  1.53it/s]Extractor Estimating: 274it [02:53,  1.54it/s]Extractor Estimating: 275it [02:53,  1.56it/s]Extractor Estimating: 276it [02:54,  1.54it/s]Extractor Estimating: 277it [02:55,  1.53it/s]Extractor Estimating: 278it [02:55,  1.53it/s]Extractor Estimating: 279it [02:56,  1.50it/s]Extractor Estimating: 280it [02:57,  1.45it/s]Extractor Estimating: 281it [02:57,  1.45it/s]Extractor Estimating: 282it [02:58,  1.45it/s]Extractor Estimating: 283it [02:59,  1.49it/s]Extractor Estimating: 284it [02:59,  1.51it/s]Extractor Estimating: 285it [03:00,  1.48it/s]Extractor Estimating: 286it [03:01,  1.47it/s]Extractor Estimating: 287it [03:01,  1.48it/s]Extractor Estimating: 288it [03:02,  1.51it/s]Extractor Estimating: 289it [03:03,  1.50it/s]Extractor Estimating: 290it [03:03,  1.49it/s]Extractor Estimating: 291it [03:04,  1.47it/s]Extractor Estimating: 292it [03:05,  1.48it/s]Extractor Estimating: 293it [03:05,  1.46it/s]Extractor Estimating: 294it [03:06,  1.49it/s]Extractor Estimating: 295it [03:07,  1.51it/s]Extractor Estimating: 296it [03:07,  1.49it/s]Extractor Estimating: 297it [03:08,  1.50it/s]Extractor Estimating: 298it [03:09,  1.49it/s]Extractor Estimating: 299it [03:09,  1.51it/s]Extractor Estimating: 300it [03:10,  1.53it/s]Extractor Estimating: 301it [03:11,  1.54it/s]Extractor Estimating: 302it [03:11,  1.54it/s]Extractor Estimating: 303it [03:12,  1.55it/s]Extractor Estimating: 304it [03:13,  1.53it/s]Extractor Estimating: 305it [03:13,  1.55it/s]Extractor Estimating: 306it [03:14,  1.52it/s]Extractor Estimating: 307it [03:15,  1.53it/s]Extractor Estimating: 308it [03:15,  1.51it/s]Extractor Estimating: 309it [03:16,  1.56it/s]Extractor Estimating: 310it [03:17,  1.54it/s]Extractor Estimating: 311it [03:17,  1.47it/s]Extractor Estimating: 312it [03:18,  1.46it/s]Extractor Estimating: 313it [03:19,  1.51it/s]Extractor Estimating: 314it [03:19,  1.48it/s]Extractor Estimating: 315it [03:20,  1.52it/s]Extractor Estimating: 316it [03:21,  1.53it/s]Extractor Estimating: 317it [03:21,  1.51it/s]Extractor Estimating: 318it [03:22,  1.53it/s]Extractor Estimating: 319it [03:23,  1.42it/s]Extractor Estimating: 320it [03:23,  1.40it/s]Extractor Estimating: 321it [03:24,  1.43it/s]Extractor Estimating: 322it [03:25,  1.48it/s]Extractor Estimating: 323it [03:25,  1.51it/s]Extractor Estimating: 324it [03:26,  1.52it/s]Extractor Estimating: 325it [03:27,  1.51it/s]Extractor Estimating: 326it [03:27,  1.51it/s]Extractor Estimating: 327it [03:28,  1.50it/s]Extractor Estimating: 328it [03:29,  1.49it/s]Extractor Estimating: 329it [03:29,  1.50it/s]Extractor Estimating: 330it [03:30,  1.50it/s]Extractor Estimating: 331it [03:31,  1.50it/s]Extractor Estimating: 332it [03:31,  1.50it/s]Extractor Estimating: 333it [03:32,  1.50it/s]Extractor Estimating: 334it [03:33,  1.50it/s]Extractor Estimating: 335it [03:33,  1.50it/s]Extractor Estimating: 336it [03:34,  1.51it/s]Extractor Estimating: 337it [03:35,  1.49it/s]Extractor Estimating: 338it [03:35,  1.52it/s]Extractor Estimating: 339it [03:36,  1.52it/s]Extractor Estimating: 340it [03:37,  1.53it/s]Extractor Estimating: 341it [03:37,  1.52it/s]Extractor Estimating: 342it [03:38,  1.49it/s]Extractor Estimating: 343it [03:39,  1.50it/s]Extractor Estimating: 344it [03:39,  1.50it/s]Extractor Estimating: 345it [03:40,  1.49it/s]Extractor Estimating: 346it [03:41,  1.50it/s]Extractor Estimating: 347it [03:41,  1.51it/s]Extractor Estimating: 348it [03:42,  1.52it/s]Extractor Estimating: 349it [03:43,  1.49it/s]Extractor Estimating: 350it [03:43,  1.51it/s]Extractor Estimating: 351it [03:44,  1.57it/s]Extractor Estimating: 352it [03:44,  1.62it/s]Extractor Estimating: 353it [03:45,  1.67it/s]Extractor Estimating: 354it [03:46,  1.74it/s]Extractor Estimating: 355it [03:46,  1.71it/s]Extractor Estimating: 356it [03:47,  1.69it/s]Extractor Estimating: 357it [03:47,  1.73it/s]Extractor Estimating: 358it [03:48,  1.70it/s]Extractor Estimating: 359it [03:48,  1.74it/s]Extractor Estimating: 360it [03:49,  1.74it/s]Extractor Estimating: 361it [03:50,  1.71it/s]Extractor Estimating: 362it [03:50,  1.66it/s]Extractor Estimating: 363it [03:51,  1.66it/s]Extractor Estimating: 364it [03:51,  1.69it/s]Extractor Estimating: 365it [03:52,  1.64it/s]Extractor Estimating: 366it [03:53,  1.64it/s]Extractor Estimating: 367it [03:53,  1.65it/s]Extractor Estimating: 368it [03:54,  1.66it/s]Extractor Estimating: 369it [03:55,  1.68it/s]Extractor Estimating: 370it [03:55,  1.68it/s]Extractor Estimating: 371it [03:56,  1.64it/s]Extractor Estimating: 372it [03:56,  1.66it/s]Extractor Estimating: 373it [03:57,  1.68it/s]Extractor Estimating: 374it [03:58,  1.67it/s]Extractor Estimating: 375it [03:58,  1.88it/s]Extractor Estimating: 375it [03:58,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:33:57,219 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:33:57,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:33:57,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:33:57,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:33:57,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:33:57,847 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:33:57,848 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:33:58,420 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:33:59,473 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:33:59,473 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:34:02,381 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:34:02,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:34:02,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:34:02,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:34:02,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:34:03,049 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:34:03,050 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:34:03,629 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:34:03,787 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:34:03,787 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 06:02:24,681 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 06:02:24,715 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7476 mean pseudo reward: 0.9553246468528637
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 19952
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20052, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20052, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.097, loss:508.6240
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.097, loss:489.9599
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.093, loss:512.6257
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.087, loss:489.2975
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.090, loss:472.8827
>> valid entity prec:0.5842, rec:0.6119, f1:0.5978
>> valid relation prec:0.1996, rec:0.0394, f1:0.0657
>> valid relation with NER prec:0.1996, rec:0.0394, f1:0.0657
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.855, loss:495.8643
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.089, loss:470.5590
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.089, loss:496.4253
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.091, loss:502.4035
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.115, loss:480.7338
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5634, rec:0.5116, f1:0.5362
>> valid relation prec:0.2831, rec:0.0623, f1:0.1021
>> valid relation with NER prec:0.2831, rec:0.0623, f1:0.1021
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.843, loss:488.6214
g_step 1200, step 264, avg_time 1.097, loss:492.4293
g_step 1300, step 52, avg_time 1.078, loss:464.4115
g_step 1400, step 152, avg_time 1.099, loss:463.2526
g_step 1500, step 252, avg_time 1.087, loss:477.7814
>> valid entity prec:0.5807, rec:0.5732, f1:0.5769
>> valid relation prec:0.2353, rec:0.0385, f1:0.0662
>> valid relation with NER prec:0.2353, rec:0.0385, f1:0.0662
g_step 1600, step 40, avg_time 2.853, loss:460.4859
g_step 1700, step 140, avg_time 1.081, loss:451.3650
g_step 1800, step 240, avg_time 1.096, loss:435.8172
g_step 1900, step 28, avg_time 1.090, loss:461.8448
g_step 2000, step 128, avg_time 1.088, loss:429.7486
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5703, rec:0.5450, f1:0.5574
>> valid relation prec:0.2711, rec:0.0607, f1:0.0991
>> valid relation with NER prec:0.2711, rec:0.0607, f1:0.0991
g_step 2100, step 228, avg_time 2.839, loss:417.6374
g_step 2200, step 16, avg_time 1.083, loss:404.6049
g_step 2300, step 116, avg_time 1.083, loss:397.3810
g_step 2400, step 216, avg_time 1.092, loss:411.6143
g_step 2500, step 4, avg_time 1.095, loss:407.1236
>> valid entity prec:0.5370, rec:0.4989, f1:0.5173
>> valid relation prec:0.2819, rec:0.0439, f1:0.0759
>> valid relation with NER prec:0.2819, rec:0.0439, f1:0.0759
g_step 2600, step 104, avg_time 2.843, loss:381.4018
g_step 2700, step 204, avg_time 1.092, loss:387.1386
g_step 2800, step 304, avg_time 1.088, loss:407.3568
g_step 2900, step 92, avg_time 1.096, loss:360.4932
g_step 3000, step 192, avg_time 1.091, loss:385.2699
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5881, rec:0.4095, f1:0.4828
>> valid relation prec:0.1948, rec:0.0215, f1:0.0388
>> valid relation with NER prec:0.1948, rec:0.0215, f1:0.0388
g_step 3100, step 292, avg_time 2.827, loss:394.4603
g_step 3200, step 80, avg_time 1.091, loss:360.5118
g_step 3300, step 180, avg_time 1.090, loss:382.0360
g_step 3400, step 280, avg_time 1.091, loss:350.4868
g_step 3500, step 68, avg_time 1.091, loss:348.6193
>> valid entity prec:0.5794, rec:0.5339, f1:0.5557
>> valid relation prec:0.1823, rec:0.0326, f1:0.0553
>> valid relation with NER prec:0.1823, rec:0.0326, f1:0.0553
g_step 3600, step 168, avg_time 2.837, loss:340.2708
g_step 3700, step 268, avg_time 1.089, loss:342.8930
g_step 3800, step 56, avg_time 1.078, loss:344.7153
g_step 3900, step 156, avg_time 1.094, loss:343.2686
g_step 4000, step 256, avg_time 1.099, loss:344.4866
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5829, rec:0.4041, f1:0.4773
>> valid relation prec:0.1901, rec:0.0221, f1:0.0397
>> valid relation with NER prec:0.1901, rec:0.0221, f1:0.0397
g_step 4100, step 44, avg_time 2.813, loss:326.7132
g_step 4200, step 144, avg_time 1.096, loss:316.1123
g_step 4300, step 244, avg_time 1.088, loss:341.1170
g_step 4400, step 32, avg_time 1.101, loss:312.6362
g_step 4500, step 132, avg_time 1.074, loss:301.5413
>> valid entity prec:0.5493, rec:0.5322, f1:0.5406
>> valid relation prec:0.2475, rec:0.0664, f1:0.1047
>> valid relation with NER prec:0.2475, rec:0.0664, f1:0.1047
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 232, avg_time 2.835, loss:300.7295
g_step 4700, step 20, avg_time 1.078, loss:334.7020
g_step 4800, step 120, avg_time 1.101, loss:293.7586
g_step 4900, step 220, avg_time 1.089, loss:326.5451
g_step 5000, step 8, avg_time 1.090, loss:308.9273
learning rate was adjusted to 0.0008
>> valid entity prec:0.5364, rec:0.4921, f1:0.5133
>> valid relation prec:0.1735, rec:0.0340, f1:0.0569
>> valid relation with NER prec:0.1735, rec:0.0340, f1:0.0569
g_step 5100, step 108, avg_time 2.833, loss:289.1311
g_step 5200, step 208, avg_time 1.095, loss:286.2965
g_step 5300, step 308, avg_time 1.080, loss:304.4237
g_step 5400, step 96, avg_time 1.076, loss:269.6845
g_step 5500, step 196, avg_time 1.083, loss:268.3133
>> valid entity prec:0.5460, rec:0.5012, f1:0.5226
>> valid relation prec:0.1893, rec:0.0521, f1:0.0817
>> valid relation with NER prec:0.1893, rec:0.0521, f1:0.0817
g_step 5600, step 296, avg_time 2.843, loss:284.2683
g_step 5700, step 84, avg_time 1.083, loss:283.8932
g_step 5800, step 184, avg_time 1.088, loss:261.5747
g_step 5900, step 284, avg_time 1.087, loss:282.8308
g_step 6000, step 72, avg_time 1.077, loss:268.7861
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5702, rec:0.4884, f1:0.5262
>> valid relation prec:0.2306, rec:0.0498, f1:0.0819
>> valid relation with NER prec:0.2306, rec:0.0498, f1:0.0819
g_step 6100, step 172, avg_time 2.841, loss:263.5917
g_step 6200, step 272, avg_time 1.086, loss:265.6580
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 06:02:24 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 06:02:24 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_06-02-24_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 06:02:25 - WARNING - datasets.builder -   Using custom data configuration default-1c5be62d717405d3
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-1c5be62d717405d3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 06:02:25,940 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:02:25,941 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 06:02:25,941 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:02:25,942 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 06:02:25,949 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:02:25,952 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:02:25,952 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:02:25,952 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:02:25,952 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:02:25,952 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:02:25,952 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 06:02:26,091 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 06:02:29,196 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 06:02:29,202 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-1c5be62d717405d3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.18ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.01ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.36ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.56ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.69ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.75ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.78ba/s]100%|██████████| 8/8 [00:01<00:00,  5.70ba/s]100%|██████████| 8/8 [00:01<00:00,  4.86ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.22ba/s] 40%|████      | 2/5 [00:00<00:00,  3.37ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.86ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.13ba/s]100%|██████████| 5/5 [00:01<00:00,  4.47ba/s]100%|██████████| 5/5 [00:01<00:00,  4.18ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.85ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.06ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.26ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.40ba/s]100%|██████████| 8/8 [00:00<00:00, 10.96ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.77ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.08ba/s]100%|██████████| 5/5 [00:00<00:00, 10.59ba/s]100%|██████████| 5/5 [00:00<00:00, 10.37ba/s]
[INFO|trainer.py:414] 2023-08-29 06:02:33,623 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 06:02:33,634 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 06:02:33,634 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-29 06:02:33,634 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 06:02:33,634 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 06:02:33,634 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 06:02:33,634 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 06:02:33,634 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:52,  3.39it/s]  0%|          | 2/585 [00:00<02:48,  3.45it/s]  1%|          | 3/585 [00:00<02:48,  3.46it/s]  1%|          | 4/585 [00:01<02:47,  3.47it/s]  1%|          | 5/585 [00:01<02:46,  3.48it/s]  1%|          | 6/585 [00:01<02:46,  3.49it/s]  1%|          | 7/585 [00:02<02:45,  3.50it/s]  1%|▏         | 8/585 [00:02<02:45,  3.49it/s]  2%|▏         | 9/585 [00:02<02:44,  3.50it/s]  2%|▏         | 10/585 [00:02<02:44,  3.49it/s]  2%|▏         | 11/585 [00:03<02:44,  3.50it/s]  2%|▏         | 12/585 [00:03<02:43,  3.49it/s]  2%|▏         | 13/585 [00:03<02:43,  3.49it/s]  2%|▏         | 14/585 [00:04<02:43,  3.50it/s]  3%|▎         | 15/585 [00:04<02:43,  3.49it/s]  3%|▎         | 16/585 [00:04<02:42,  3.49it/s]  3%|▎         | 17/585 [00:04<02:42,  3.49it/s]  3%|▎         | 18/585 [00:05<02:42,  3.49it/s]  3%|▎         | 19/585 [00:05<02:42,  3.49it/s]  3%|▎         | 20/585 [00:05<02:41,  3.49it/s]  4%|▎         | 21/585 [00:06<02:42,  3.48it/s]  4%|▍         | 22/585 [00:06<02:41,  3.48it/s]  4%|▍         | 23/585 [00:06<02:41,  3.49it/s]  4%|▍         | 24/585 [00:06<02:41,  3.48it/s]  4%|▍         | 25/585 [00:07<02:40,  3.49it/s]  4%|▍         | 26/585 [00:07<02:40,  3.49it/s]  5%|▍         | 27/585 [00:07<02:39,  3.49it/s]  5%|▍         | 28/585 [00:08<02:39,  3.49it/s]  5%|▍         | 29/585 [00:08<02:39,  3.49it/s]  5%|▌         | 30/585 [00:08<02:38,  3.49it/s]  5%|▌         | 31/585 [00:08<02:38,  3.49it/s]  5%|▌         | 32/585 [00:09<02:38,  3.49it/s]  6%|▌         | 33/585 [00:09<02:38,  3.49it/s]  6%|▌         | 34/585 [00:09<02:37,  3.49it/s]  6%|▌         | 35/585 [00:10<02:37,  3.49it/s]  6%|▌         | 36/585 [00:10<02:37,  3.49it/s]  6%|▋         | 37/585 [00:10<02:37,  3.48it/s]  6%|▋         | 38/585 [00:10<02:37,  3.48it/s]  7%|▋         | 39/585 [00:11<02:37,  3.47it/s]  7%|▋         | 40/585 [00:11<02:36,  3.48it/s]  7%|▋         | 41/585 [00:11<02:36,  3.48it/s]  7%|▋         | 42/585 [00:12<02:36,  3.48it/s]  7%|▋         | 43/585 [00:12<02:35,  3.48it/s]  8%|▊         | 44/585 [00:12<02:35,  3.49it/s]  8%|▊         | 45/585 [00:12<02:34,  3.49it/s]  8%|▊         | 46/585 [00:13<02:34,  3.49it/s]  8%|▊         | 47/585 [00:13<02:34,  3.48it/s]  8%|▊         | 48/585 [00:13<02:33,  3.49it/s]  8%|▊         | 49/585 [00:14<02:33,  3.48it/s]  9%|▊         | 50/585 [00:14<02:33,  3.49it/s]  9%|▊         | 51/585 [00:14<02:33,  3.49it/s]  9%|▉         | 52/585 [00:14<02:32,  3.49it/s]  9%|▉         | 53/585 [00:15<02:32,  3.48it/s]  9%|▉         | 54/585 [00:15<02:32,  3.49it/s]  9%|▉         | 55/585 [00:15<02:32,  3.48it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:31,  3.47it/s] 10%|█         | 59/585 [00:16<02:31,  3.48it/s] 10%|█         | 60/585 [00:17<02:31,  3.48it/s] 10%|█         | 61/585 [00:17<02:30,  3.48it/s] 11%|█         | 62/585 [00:17<02:30,  3.48it/s] 11%|█         | 63/585 [00:18<02:30,  3.48it/s] 11%|█         | 64/585 [00:18<02:30,  3.47it/s] 11%|█         | 65/585 [00:18<02:29,  3.48it/s] 11%|█▏        | 66/585 [00:18<02:29,  3.48it/s] 11%|█▏        | 67/585 [00:19<02:28,  3.49it/s] 12%|█▏        | 68/585 [00:19<02:28,  3.49it/s] 12%|█▏        | 69/585 [00:19<02:27,  3.49it/s] 12%|█▏        | 70/585 [00:20<02:27,  3.49it/s] 12%|█▏        | 71/585 [00:20<02:27,  3.49it/s] 12%|█▏        | 72/585 [00:20<02:26,  3.49it/s] 12%|█▏        | 73/585 [00:20<02:26,  3.49it/s] 13%|█▎        | 74/585 [00:21<02:26,  3.48it/s] 13%|█▎        | 75/585 [00:21<02:26,  3.49it/s] 13%|█▎        | 76/585 [00:21<02:25,  3.49it/s] 13%|█▎        | 77/585 [00:22<02:25,  3.49it/s] 13%|█▎        | 78/585 [00:22<02:25,  3.49it/s] 14%|█▎        | 79/585 [00:22<02:24,  3.49it/s] 14%|█▎        | 80/585 [00:22<02:24,  3.49it/s] 14%|█▍        | 81/585 [00:23<02:24,  3.49it/s] 14%|█▍        | 82/585 [00:23<02:24,  3.49it/s] 14%|█▍        | 83/585 [00:23<02:23,  3.49it/s] 14%|█▍        | 84/585 [00:24<02:23,  3.49it/s] 15%|█▍        | 85/585 [00:24<02:23,  3.48it/s] 15%|█▍        | 86/585 [00:24<02:23,  3.48it/s] 15%|█▍        | 87/585 [00:24<02:23,  3.48it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 89/585 [00:25<02:22,  3.48it/s] 15%|█▌        | 90/585 [00:25<02:22,  3.48it/s] 16%|█▌        | 91/585 [00:26<02:21,  3.48it/s] 16%|█▌        | 92/585 [00:26<02:21,  3.49it/s] 16%|█▌        | 93/585 [00:26<02:20,  3.49it/s] 16%|█▌        | 94/585 [00:26<02:20,  3.49it/s] 16%|█▌        | 95/585 [00:27<02:20,  3.49it/s] 16%|█▋        | 96/585 [00:27<02:20,  3.48it/s] 17%|█▋        | 97/585 [00:27<02:20,  3.48it/s] 17%|█▋        | 98/585 [00:28<02:19,  3.49it/s] 17%|█▋        | 99/585 [00:28<02:19,  3.48it/s] 17%|█▋        | 100/585 [00:28<02:19,  3.48it/s] 17%|█▋        | 101/585 [00:28<02:18,  3.48it/s] 17%|█▋        | 102/585 [00:29<02:18,  3.49it/s] 18%|█▊        | 103/585 [00:29<02:18,  3.48it/s] 18%|█▊        | 104/585 [00:29<02:18,  3.48it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 106/585 [00:30<02:17,  3.48it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.45it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.45it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 111/585 [00:31<02:16,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:32<02:15,  3.47it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.47it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.47it/s] 20%|██        | 117/585 [00:33<02:14,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 06:03:07,284 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:03:07,284 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 06:03:07,284 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.58it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.68it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.55it/s][A
  4%|▍         | 23/611 [00:00<00:12, 48.15it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.60it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.26it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.94it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.60it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.48it/s][A
  9%|▊         | 53/611 [00:01<00:11, 46.59it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.69it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.78it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.88it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.91it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.67it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.54it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.33it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.40it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.53it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.68it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.69it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.82it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.84it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.77it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.64it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.54it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.32it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.47it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.57it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.56it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.75it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.77it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.71it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.63it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.50it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.41it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.46it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.40it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.62it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.77it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.71it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.81it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.71it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.55it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.51it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.51it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.52it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.56it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.58it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.66it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.78it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.65it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.68it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.60it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.56it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.41it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.47it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.60it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.63it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.68it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.71it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.65it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.54it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.52it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.50it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.53it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.61it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.50it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.61it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.63it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.66it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.62it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.58it/s][A
 61%|██████    | 373/611 [00:07<00:05, 46.42it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.41it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.46it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.59it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.67it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.66it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.55it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.52it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.56it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.53it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.50it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.44it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.60it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.67it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.54it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.62it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.63it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.40it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.48it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.52it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.51it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.55it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.64it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.45it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.64it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.66it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.64it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.61it/s][A
 84%|████████▍ | 513/611 [00:10<00:02, 46.19it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.47it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.50it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.55it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.58it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.63it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.66it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.51it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.57it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.55it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.59it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.55it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.58it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.52it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.44it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.58it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.60it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.57it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.56it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.52it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 46.52it/s][A 20%|██        | 117/585 [00:46<02:14,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:03:20,417 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 06:03:20,437 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:03:22,649 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:03:22,666 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:03:22,681 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:55<51:37,  6.63s/it] 20%|██        | 119/585 [00:55<36:44,  4.73s/it] 21%|██        | 120/585 [00:55<26:19,  3.40s/it] 21%|██        | 121/585 [00:55<19:03,  2.46s/it] 21%|██        | 122/585 [00:56<13:58,  1.81s/it] 21%|██        | 123/585 [00:56<10:25,  1.35s/it] 21%|██        | 124/585 [00:56<07:56,  1.03s/it] 21%|██▏       | 125/585 [00:57<06:12,  1.24it/s] 22%|██▏       | 126/585 [00:57<04:59,  1.53it/s] 22%|██▏       | 127/585 [00:57<04:08,  1.84it/s] 22%|██▏       | 128/585 [00:57<03:33,  2.14it/s] 22%|██▏       | 129/585 [00:58<03:07,  2.43it/s] 22%|██▏       | 130/585 [00:58<02:50,  2.66it/s] 22%|██▏       | 131/585 [00:58<02:38,  2.87it/s] 23%|██▎       | 132/585 [00:59<02:29,  3.02it/s] 23%|██▎       | 133/585 [00:59<02:23,  3.15it/s] 23%|██▎       | 134/585 [00:59<02:18,  3.24it/s] 23%|██▎       | 135/585 [00:59<02:15,  3.31it/s] 23%|██▎       | 136/585 [01:00<02:13,  3.36it/s] 23%|██▎       | 137/585 [01:00<02:11,  3.39it/s] 24%|██▎       | 138/585 [01:00<02:10,  3.42it/s] 24%|██▍       | 139/585 [01:01<02:09,  3.44it/s] 24%|██▍       | 140/585 [01:01<02:09,  3.45it/s] 24%|██▍       | 141/585 [01:01<02:09,  3.43it/s] 24%|██▍       | 142/585 [01:01<02:08,  3.45it/s] 24%|██▍       | 143/585 [01:02<02:07,  3.45it/s] 25%|██▍       | 144/585 [01:02<02:07,  3.46it/s] 25%|██▍       | 145/585 [01:02<02:07,  3.46it/s] 25%|██▍       | 146/585 [01:03<02:06,  3.47it/s] 25%|██▌       | 147/585 [01:03<02:06,  3.47it/s] 25%|██▌       | 148/585 [01:03<02:05,  3.48it/s] 25%|██▌       | 149/585 [01:03<02:05,  3.47it/s] 26%|██▌       | 150/585 [01:04<02:05,  3.47it/s] 26%|██▌       | 151/585 [01:04<02:04,  3.47it/s] 26%|██▌       | 152/585 [01:04<02:05,  3.46it/s] 26%|██▌       | 153/585 [01:05<02:04,  3.47it/s] 26%|██▋       | 154/585 [01:05<02:04,  3.47it/s] 26%|██▋       | 155/585 [01:05<02:03,  3.47it/s] 27%|██▋       | 156/585 [01:05<02:03,  3.47it/s] 27%|██▋       | 157/585 [01:06<02:03,  3.47it/s] 27%|██▋       | 158/585 [01:06<02:02,  3.48it/s] 27%|██▋       | 159/585 [01:06<02:02,  3.47it/s] 27%|██▋       | 160/585 [01:07<02:02,  3.48it/s] 28%|██▊       | 161/585 [01:07<02:02,  3.47it/s] 28%|██▊       | 162/585 [01:07<02:01,  3.48it/s] 28%|██▊       | 163/585 [01:07<02:01,  3.46it/s] 28%|██▊       | 164/585 [01:08<02:01,  3.47it/s] 28%|██▊       | 165/585 [01:08<02:01,  3.47it/s] 28%|██▊       | 166/585 [01:08<02:00,  3.47it/s] 29%|██▊       | 167/585 [01:09<02:00,  3.47it/s] 29%|██▊       | 168/585 [01:09<02:00,  3.47it/s] 29%|██▉       | 169/585 [01:09<01:59,  3.47it/s] 29%|██▉       | 170/585 [01:10<01:59,  3.47it/s] 29%|██▉       | 171/585 [01:10<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:10<01:58,  3.48it/s] 30%|██▉       | 173/585 [01:10<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:11<01:58,  3.47it/s] 30%|██▉       | 175/585 [01:11<01:58,  3.47it/s] 30%|███       | 176/585 [01:11<01:57,  3.47it/s] 30%|███       | 177/585 [01:12<01:57,  3.47it/s] 30%|███       | 178/585 [01:12<01:57,  3.47it/s] 31%|███       | 179/585 [01:12<01:56,  3.47it/s] 31%|███       | 180/585 [01:12<01:56,  3.47it/s] 31%|███       | 181/585 [01:13<01:56,  3.47it/s] 31%|███       | 182/585 [01:13<01:56,  3.47it/s] 31%|███▏      | 183/585 [01:13<01:55,  3.47it/s] 31%|███▏      | 184/585 [01:14<01:55,  3.47it/s] 32%|███▏      | 185/585 [01:14<01:55,  3.46it/s] 32%|███▏      | 186/585 [01:14<01:55,  3.46it/s] 32%|███▏      | 187/585 [01:14<01:54,  3.46it/s] 32%|███▏      | 188/585 [01:15<01:54,  3.46it/s] 32%|███▏      | 189/585 [01:15<01:54,  3.46it/s] 32%|███▏      | 190/585 [01:15<01:53,  3.47it/s] 33%|███▎      | 191/585 [01:16<01:53,  3.47it/s] 33%|███▎      | 192/585 [01:16<01:53,  3.47it/s] 33%|███▎      | 193/585 [01:16<01:53,  3.46it/s] 33%|███▎      | 194/585 [01:16<01:52,  3.47it/s] 33%|███▎      | 195/585 [01:17<01:52,  3.47it/s] 34%|███▎      | 196/585 [01:17<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:17<01:52,  3.46it/s] 34%|███▍      | 198/585 [01:18<01:51,  3.46it/s] 34%|███▍      | 199/585 [01:18<01:51,  3.46it/s] 34%|███▍      | 200/585 [01:18<01:51,  3.46it/s] 34%|███▍      | 201/585 [01:18<01:50,  3.47it/s] 35%|███▍      | 202/585 [01:19<01:50,  3.47it/s] 35%|███▍      | 203/585 [01:19<01:50,  3.47it/s] 35%|███▍      | 204/585 [01:19<01:49,  3.47it/s] 35%|███▌      | 205/585 [01:20<01:49,  3.47it/s] 35%|███▌      | 206/585 [01:20<01:49,  3.47it/s] 35%|███▌      | 207/585 [01:20<01:49,  3.45it/s] 36%|███▌      | 208/585 [01:20<01:48,  3.46it/s] 36%|███▌      | 209/585 [01:21<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:21<01:48,  3.46it/s] 36%|███▌      | 211/585 [01:21<01:47,  3.47it/s] 36%|███▌      | 212/585 [01:22<01:47,  3.46it/s] 36%|███▋      | 213/585 [01:22<01:47,  3.47it/s] 37%|███▋      | 214/585 [01:22<01:47,  3.46it/s] 37%|███▋      | 215/585 [01:22<01:46,  3.47it/s] 37%|███▋      | 216/585 [01:23<01:46,  3.47it/s] 37%|███▋      | 217/585 [01:23<01:46,  3.47it/s] 37%|███▋      | 218/585 [01:23<01:46,  3.46it/s] 37%|███▋      | 219/585 [01:24<01:45,  3.47it/s] 38%|███▊      | 220/585 [01:24<01:45,  3.47it/s] 38%|███▊      | 221/585 [01:24<01:45,  3.46it/s] 38%|███▊      | 222/585 [01:25<01:44,  3.47it/s] 38%|███▊      | 223/585 [01:25<01:44,  3.47it/s] 38%|███▊      | 224/585 [01:25<01:44,  3.47it/s] 38%|███▊      | 225/585 [01:25<01:43,  3.47it/s] 39%|███▊      | 226/585 [01:26<01:43,  3.47it/s] 39%|███▉      | 227/585 [01:26<01:43,  3.47it/s] 39%|███▉      | 228/585 [01:26<01:43,  3.46it/s] 39%|███▉      | 229/585 [01:27<01:42,  3.46it/s] 39%|███▉      | 230/585 [01:27<01:42,  3.46it/s] 39%|███▉      | 231/585 [01:27<01:42,  3.46it/s] 40%|███▉      | 232/585 [01:27<01:41,  3.47it/s] 40%|███▉      | 233/585 [01:28<01:41,  3.46it/s] 40%|████      | 234/585 [01:28<01:41,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 06:04:02,151 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:04:02,151 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 06:04:02,151 >>   Batch size = 8
{'eval_loss': 0.9303779006004333, 'eval_runtime': 13.1206, 'eval_samples_per_second': 372.088, 'eval_steps_per_second': 46.568, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.10it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.47it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.71it/s][A
  4%|▍         | 23/611 [00:00<00:12, 48.02it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.44it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.07it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.62it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.45it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.45it/s][A
  9%|▊         | 53/611 [00:01<00:11, 46.51it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.47it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.61it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.53it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.61it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.58it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.53it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.40it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.37it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.32it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.40it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.50it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.64it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.50it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.62it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.47it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.42it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.44it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.34it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.43it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.41it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.42it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.54it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.62it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.60it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.53it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.48it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.38it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.37it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.45it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.47it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.48it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.30it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.54it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.52it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.57it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.47it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.35it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.43it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.29it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.44it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.46it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.51it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.59it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.60it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.37it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.44it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.44it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.48it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.49it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.54it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.35it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.50it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.51it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.53it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.47it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.52it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.48it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.38it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.31it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.43it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.54it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.53it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.36it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.34it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.37it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.44it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.44it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.46it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.48it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.32it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.45it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.44it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.47it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.50it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.51it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.37it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.36it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.38it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.45it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.46it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.51it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.46it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.44it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.44it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.34it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.37it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.36it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.48it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.37it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.39it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.30it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.45it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.47it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.43it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.34it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.38it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.45it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.46it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.49it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.52it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.41it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.47it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.41it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.42it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.48it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.50it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.46it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.39it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.39it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.50it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.51it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 46.51it/s][A 40%|████      | 234/585 [01:41<01:41,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:04:15,318 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 06:04:15,336 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:04:17,531 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:04:17,546 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:04:17,560 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:48<36:38,  6.28s/it] 40%|████      | 236/585 [01:49<26:05,  4.49s/it] 41%|████      | 237/585 [01:49<18:42,  3.23s/it] 41%|████      | 238/585 [01:49<13:33,  2.34s/it] 41%|████      | 239/585 [01:49<09:57,  1.73s/it] 41%|████      | 240/585 [01:50<07:26,  1.29s/it] 41%|████      | 241/585 [01:50<05:41,  1.01it/s] 41%|████▏     | 242/585 [01:50<04:27,  1.28it/s] 42%|████▏     | 243/585 [01:51<03:36,  1.58it/s] 42%|████▏     | 244/585 [01:51<03:00,  1.89it/s] 42%|████▏     | 245/585 [01:51<02:35,  2.19it/s] 42%|████▏     | 246/585 [01:51<02:17,  2.46it/s] 42%|████▏     | 247/585 [01:52<02:05,  2.69it/s] 42%|████▏     | 248/585 [01:52<01:56,  2.89it/s] 43%|████▎     | 249/585 [01:52<01:50,  3.04it/s] 43%|████▎     | 250/585 [01:53<01:46,  3.16it/s] 43%|████▎     | 251/585 [01:53<01:42,  3.25it/s] 43%|████▎     | 252/585 [01:53<01:40,  3.31it/s] 43%|████▎     | 253/585 [01:53<01:38,  3.36it/s] 43%|████▎     | 254/585 [01:54<01:37,  3.39it/s] 44%|████▎     | 255/585 [01:54<01:36,  3.41it/s] 44%|████▍     | 256/585 [01:54<01:38,  3.34it/s] 44%|████▍     | 257/585 [01:55<01:37,  3.38it/s] 44%|████▍     | 258/585 [01:55<01:36,  3.39it/s] 44%|████▍     | 259/585 [01:55<01:35,  3.42it/s] 44%|████▍     | 260/585 [01:55<01:34,  3.44it/s] 45%|████▍     | 261/585 [01:56<01:34,  3.44it/s] 45%|████▍     | 262/585 [01:56<01:33,  3.46it/s] 45%|████▍     | 263/585 [01:56<01:33,  3.45it/s] 45%|████▌     | 264/585 [01:57<01:32,  3.46it/s] 45%|████▌     | 265/585 [01:57<01:32,  3.46it/s] 45%|████▌     | 266/585 [01:57<01:32,  3.47it/s] 46%|████▌     | 267/585 [01:57<01:31,  3.47it/s] 46%|████▌     | 268/585 [01:58<01:31,  3.47it/s] 46%|████▌     | 269/585 [01:58<01:31,  3.46it/s] 46%|████▌     | 270/585 [01:58<01:31,  3.46it/s] 46%|████▋     | 271/585 [01:59<01:30,  3.47it/s] 46%|████▋     | 272/585 [01:59<01:30,  3.46it/s] 47%|████▋     | 273/585 [01:59<01:29,  3.47it/s] 47%|████▋     | 274/585 [02:00<01:29,  3.47it/s] 47%|████▋     | 275/585 [02:00<01:29,  3.47it/s] 47%|████▋     | 276/585 [02:00<01:29,  3.47it/s] 47%|████▋     | 277/585 [02:00<01:28,  3.47it/s] 48%|████▊     | 278/585 [02:01<01:28,  3.47it/s] 48%|████▊     | 279/585 [02:01<01:28,  3.47it/s] 48%|████▊     | 280/585 [02:01<01:28,  3.46it/s] 48%|████▊     | 281/585 [02:02<01:27,  3.46it/s] 48%|████▊     | 282/585 [02:02<01:27,  3.47it/s] 48%|████▊     | 283/585 [02:02<01:27,  3.47it/s] 49%|████▊     | 284/585 [02:02<01:26,  3.47it/s] 49%|████▊     | 285/585 [02:03<01:26,  3.47it/s] 49%|████▉     | 286/585 [02:03<01:26,  3.47it/s] 49%|████▉     | 287/585 [02:03<01:25,  3.47it/s] 49%|████▉     | 288/585 [02:04<01:25,  3.47it/s] 49%|████▉     | 289/585 [02:04<01:25,  3.47it/s] 50%|████▉     | 290/585 [02:04<01:25,  3.46it/s] 50%|████▉     | 291/585 [02:04<01:25,  3.46it/s] 50%|████▉     | 292/585 [02:05<01:24,  3.46it/s] 50%|█████     | 293/585 [02:05<01:24,  3.46it/s] 50%|█████     | 294/585 [02:05<01:23,  3.47it/s] 50%|█████     | 295/585 [02:06<01:23,  3.46it/s] 51%|█████     | 296/585 [02:06<01:23,  3.46it/s] 51%|█████     | 297/585 [02:06<01:23,  3.47it/s] 51%|█████     | 298/585 [02:06<01:22,  3.47it/s] 51%|█████     | 299/585 [02:07<01:22,  3.47it/s] 51%|█████▏    | 300/585 [02:07<01:22,  3.47it/s] 51%|█████▏    | 301/585 [02:07<01:21,  3.47it/s] 52%|█████▏    | 302/585 [02:08<01:22,  3.43it/s] 52%|█████▏    | 303/585 [02:08<01:21,  3.44it/s] 52%|█████▏    | 304/585 [02:08<01:21,  3.45it/s] 52%|█████▏    | 305/585 [02:08<01:20,  3.46it/s] 52%|█████▏    | 306/585 [02:09<01:20,  3.46it/s] 52%|█████▏    | 307/585 [02:09<01:20,  3.46it/s] 53%|█████▎    | 308/585 [02:09<01:20,  3.46it/s] 53%|█████▎    | 309/585 [02:10<01:19,  3.47it/s] 53%|█████▎    | 310/585 [02:10<01:19,  3.47it/s] 53%|█████▎    | 311/585 [02:10<01:19,  3.47it/s] 53%|█████▎    | 312/585 [02:10<01:18,  3.46it/s] 54%|█████▎    | 313/585 [02:11<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:11<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:11<01:17,  3.46it/s] 54%|█████▍    | 316/585 [02:12<01:17,  3.46it/s] 54%|█████▍    | 317/585 [02:12<01:17,  3.46it/s] 54%|█████▍    | 318/585 [02:12<01:17,  3.46it/s] 55%|█████▍    | 319/585 [02:13<01:16,  3.46it/s] 55%|█████▍    | 320/585 [02:13<01:16,  3.46it/s] 55%|█████▍    | 321/585 [02:13<01:16,  3.46it/s] 55%|█████▌    | 322/585 [02:13<01:16,  3.44it/s] 55%|█████▌    | 323/585 [02:14<01:15,  3.45it/s] 55%|█████▌    | 324/585 [02:14<01:15,  3.46it/s] 56%|█████▌    | 325/585 [02:14<01:15,  3.46it/s] 56%|█████▌    | 326/585 [02:15<01:14,  3.46it/s] 56%|█████▌    | 327/585 [02:15<01:14,  3.46it/s] 56%|█████▌    | 328/585 [02:15<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:15<01:13,  3.46it/s] 56%|█████▋    | 330/585 [02:16<01:13,  3.46it/s] 57%|█████▋    | 331/585 [02:16<01:13,  3.47it/s] 57%|█████▋    | 332/585 [02:16<01:13,  3.46it/s] 57%|█████▋    | 333/585 [02:17<01:13,  3.45it/s] 57%|█████▋    | 334/585 [02:17<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:17<01:12,  3.45it/s] 57%|█████▋    | 336/585 [02:17<01:12,  3.46it/s] 58%|█████▊    | 337/585 [02:18<01:11,  3.46it/s] 58%|█████▊    | 338/585 [02:18<01:11,  3.46it/s] 58%|█████▊    | 339/585 [02:18<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:19<01:10,  3.46it/s] 58%|█████▊    | 341/585 [02:19<01:10,  3.46it/s] 58%|█████▊    | 342/585 [02:19<01:10,  3.46it/s] 59%|█████▊    | 343/585 [02:19<01:09,  3.46it/s] 59%|█████▉    | 344/585 [02:20<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:20<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:20<01:09,  3.46it/s] 59%|█████▉    | 347/585 [02:21<01:08,  3.46it/s] 59%|█████▉    | 348/585 [02:21<01:08,  3.46it/s] 60%|█████▉    | 349/585 [02:21<01:08,  3.46it/s] 60%|█████▉    | 350/585 [02:21<01:07,  3.46it/s] 60%|██████    | 351/585 [02:22<01:07,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 06:04:55,932 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:04:55,932 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 06:04:55,932 >>   Batch size = 8
{'eval_loss': 0.9512670636177063, 'eval_runtime': 13.1556, 'eval_samples_per_second': 371.097, 'eval_steps_per_second': 46.444, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.32it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.60it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.75it/s][A
  4%|▍         | 23/611 [00:00<00:12, 48.07it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.21it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.01it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.70it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.40it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.39it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.44it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.12it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.42it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.51it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.59it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.65it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.40it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.30it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.29it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.39it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.42it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.50it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.56it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.52it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.58it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.58it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.53it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.51it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.50it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.36it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.39it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.39it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.53it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.59it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.60it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.39it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.48it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.44it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.38it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.39it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.46it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.32it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.29it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.52it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.56it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.55it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.43it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.28it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.44it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.42it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.46it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.48it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.50it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.57it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.37it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.43it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.29it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.19it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.45it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.46it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.34it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.44it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.52it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.54it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.45it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.32it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.43it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.29it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.43it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.50it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.50it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.41it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.46it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.51it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.43it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.40it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.50it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.36it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.47it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.52it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.42it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.51it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.51it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.29it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.39it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.43it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.47it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.50it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.50it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.45it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.50it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.40it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.43it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.40it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.46it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.46it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.36it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.38it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.29it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.42it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.51it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.34it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.44it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.44it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.43it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.42it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.35it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.33it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.35it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.40it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.41it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.39it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.50it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.53it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.43it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.48it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.46it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.38it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.41it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.40it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.33it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.36it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 46.36it/s][A 60%|██████    | 351/585 [02:35<01:07,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:05:09,109 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 06:05:09,127 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:05:11,250 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:05:11,271 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:05:11,281 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:42<24:07,  6.21s/it] 60%|██████    | 353/585 [02:42<17:08,  4.43s/it] 61%|██████    | 354/585 [02:42<12:16,  3.19s/it] 61%|██████    | 355/585 [02:43<08:53,  2.32s/it] 61%|██████    | 356/585 [02:43<06:31,  1.71s/it] 61%|██████    | 357/585 [02:43<04:52,  1.28s/it] 61%|██████    | 358/585 [02:44<03:43,  1.02it/s] 61%|██████▏   | 359/585 [02:44<02:55,  1.29it/s] 62%|██████▏   | 360/585 [02:44<02:21,  1.59it/s] 62%|██████▏   | 361/585 [02:44<01:57,  1.90it/s] 62%|██████▏   | 362/585 [02:45<01:41,  2.20it/s] 62%|██████▏   | 363/585 [02:45<01:29,  2.47it/s] 62%|██████▏   | 364/585 [02:45<01:22,  2.69it/s] 62%|██████▏   | 365/585 [02:46<01:16,  2.88it/s] 63%|██████▎   | 366/585 [02:46<01:12,  3.04it/s] 63%|██████▎   | 367/585 [02:46<01:09,  3.15it/s] 63%|██████▎   | 368/585 [02:46<01:06,  3.25it/s] 63%|██████▎   | 369/585 [02:47<01:05,  3.31it/s] 63%|██████▎   | 370/585 [02:47<01:04,  3.36it/s] 63%|██████▎   | 371/585 [02:47<01:03,  3.39it/s] 64%|██████▎   | 372/585 [02:48<01:02,  3.41it/s] 64%|██████▍   | 373/585 [02:48<01:01,  3.44it/s] 64%|██████▍   | 374/585 [02:48<01:01,  3.44it/s] 64%|██████▍   | 375/585 [02:48<01:00,  3.44it/s] 64%|██████▍   | 376/585 [02:49<01:00,  3.45it/s] 64%|██████▍   | 377/585 [02:49<01:00,  3.46it/s] 65%|██████▍   | 378/585 [02:49<00:59,  3.46it/s] 65%|██████▍   | 379/585 [02:50<00:59,  3.46it/s] 65%|██████▍   | 380/585 [02:50<00:59,  3.47it/s] 65%|██████▌   | 381/585 [02:50<00:58,  3.47it/s] 65%|██████▌   | 382/585 [02:50<00:58,  3.47it/s] 65%|██████▌   | 383/585 [02:51<00:58,  3.46it/s] 66%|██████▌   | 384/585 [02:51<00:57,  3.47it/s] 66%|██████▌   | 385/585 [02:51<00:57,  3.47it/s] 66%|██████▌   | 386/585 [02:52<00:57,  3.47it/s] 66%|██████▌   | 387/585 [02:52<00:57,  3.47it/s] 66%|██████▋   | 388/585 [02:52<00:56,  3.47it/s] 66%|██████▋   | 389/585 [02:52<00:56,  3.47it/s] 67%|██████▋   | 390/585 [02:53<00:56,  3.47it/s] 67%|██████▋   | 391/585 [02:53<00:55,  3.47it/s] 67%|██████▋   | 392/585 [02:53<00:55,  3.47it/s] 67%|██████▋   | 393/585 [02:54<00:55,  3.47it/s] 67%|██████▋   | 394/585 [02:54<00:55,  3.47it/s] 68%|██████▊   | 395/585 [02:54<00:54,  3.47it/s] 68%|██████▊   | 396/585 [02:54<00:55,  3.40it/s] 68%|██████▊   | 397/585 [02:55<00:55,  3.40it/s] 68%|██████▊   | 398/585 [02:55<00:54,  3.43it/s] 68%|██████▊   | 399/585 [02:55<00:54,  3.44it/s] 68%|██████▊   | 400/585 [02:56<00:53,  3.45it/s] 69%|██████▊   | 401/585 [02:56<00:53,  3.45it/s] 69%|██████▊   | 402/585 [02:56<00:52,  3.46it/s] 69%|██████▉   | 403/585 [02:57<00:52,  3.46it/s] 69%|██████▉   | 404/585 [02:57<00:52,  3.46it/s] 69%|██████▉   | 405/585 [02:57<00:51,  3.47it/s] 69%|██████▉   | 406/585 [02:57<00:51,  3.47it/s] 70%|██████▉   | 407/585 [02:58<00:51,  3.47it/s] 70%|██████▉   | 408/585 [02:58<00:51,  3.46it/s] 70%|██████▉   | 409/585 [02:58<00:50,  3.47it/s] 70%|███████   | 410/585 [02:59<00:50,  3.47it/s] 70%|███████   | 411/585 [02:59<00:50,  3.47it/s] 70%|███████   | 412/585 [02:59<00:49,  3.47it/s] 71%|███████   | 413/585 [02:59<00:49,  3.47it/s] 71%|███████   | 414/585 [03:00<00:49,  3.47it/s] 71%|███████   | 415/585 [03:00<00:49,  3.47it/s] 71%|███████   | 416/585 [03:00<00:48,  3.47it/s] 71%|███████▏  | 417/585 [03:01<00:48,  3.47it/s] 71%|███████▏  | 418/585 [03:01<00:48,  3.47it/s] 72%|███████▏  | 419/585 [03:01<00:47,  3.46it/s] 72%|███████▏  | 420/585 [03:01<00:47,  3.46it/s] 72%|███████▏  | 421/585 [03:02<00:47,  3.47it/s] 72%|███████▏  | 422/585 [03:02<00:46,  3.47it/s] 72%|███████▏  | 423/585 [03:02<00:46,  3.46it/s] 72%|███████▏  | 424/585 [03:03<00:46,  3.47it/s] 73%|███████▎  | 425/585 [03:03<00:46,  3.46it/s] 73%|███████▎  | 426/585 [03:03<00:45,  3.47it/s] 73%|███████▎  | 427/585 [03:03<00:45,  3.47it/s] 73%|███████▎  | 428/585 [03:04<00:45,  3.47it/s] 73%|███████▎  | 429/585 [03:04<00:44,  3.47it/s] 74%|███████▎  | 430/585 [03:04<00:44,  3.46it/s] 74%|███████▎  | 431/585 [03:05<00:44,  3.46it/s] 74%|███████▍  | 432/585 [03:05<00:44,  3.46it/s] 74%|███████▍  | 433/585 [03:05<00:43,  3.46it/s] 74%|███████▍  | 434/585 [03:05<00:43,  3.47it/s] 74%|███████▍  | 435/585 [03:06<00:43,  3.46it/s] 75%|███████▍  | 436/585 [03:06<00:42,  3.47it/s] 75%|███████▍  | 437/585 [03:06<00:42,  3.46it/s] 75%|███████▍  | 438/585 [03:07<00:42,  3.47it/s] 75%|███████▌  | 439/585 [03:07<00:42,  3.47it/s] 75%|███████▌  | 440/585 [03:07<00:41,  3.47it/s] 75%|███████▌  | 441/585 [03:07<00:41,  3.47it/s] 76%|███████▌  | 442/585 [03:08<00:41,  3.45it/s] 76%|███████▌  | 443/585 [03:08<00:41,  3.46it/s] 76%|███████▌  | 444/585 [03:08<00:40,  3.46it/s] 76%|███████▌  | 445/585 [03:09<00:40,  3.47it/s] 76%|███████▌  | 446/585 [03:09<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:09<00:39,  3.47it/s] 77%|███████▋  | 448/585 [03:09<00:39,  3.47it/s] 77%|███████▋  | 449/585 [03:10<00:39,  3.46it/s] 77%|███████▋  | 450/585 [03:10<00:39,  3.46it/s] 77%|███████▋  | 451/585 [03:10<00:38,  3.46it/s] 77%|███████▋  | 452/585 [03:11<00:38,  3.46it/s] 77%|███████▋  | 453/585 [03:11<00:38,  3.45it/s] 78%|███████▊  | 454/585 [03:11<00:37,  3.46it/s] 78%|███████▊  | 455/585 [03:12<00:37,  3.46it/s] 78%|███████▊  | 456/585 [03:12<00:37,  3.47it/s] 78%|███████▊  | 457/585 [03:12<00:36,  3.46it/s] 78%|███████▊  | 458/585 [03:12<00:36,  3.47it/s] 78%|███████▊  | 459/585 [03:13<00:36,  3.46it/s] 79%|███████▊  | 460/585 [03:13<00:36,  3.47it/s] 79%|███████▉  | 461/585 [03:13<00:35,  3.46it/s] 79%|███████▉  | 462/585 [03:14<00:35,  3.46it/s] 79%|███████▉  | 463/585 [03:14<00:35,  3.46it/s] 79%|███████▉  | 464/585 [03:14<00:34,  3.46it/s] 79%|███████▉  | 465/585 [03:14<00:34,  3.46it/s] 80%|███████▉  | 466/585 [03:15<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:15<00:34,  3.46it/s] 80%|████████  | 468/585 [03:15<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 06:05:49,446 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:05:49,446 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 06:05:49,446 >>   Batch size = 8
{'eval_loss': 0.9625447392463684, 'eval_runtime': 13.1597, 'eval_samples_per_second': 370.98, 'eval_steps_per_second': 46.43, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.20it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.27it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.61it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.94it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.44it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.09it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.66it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.41it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.43it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.50it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.60it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.42it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.61it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.59it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.61it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.42it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.30it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.15it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.30it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.43it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.56it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.58it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.42it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.55it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.48it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.31it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.19it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.35it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.31it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.41it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.42it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.58it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.63it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.58it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.32it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.38it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.42it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.41it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.44it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.46it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.45it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.53it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.55it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.41it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.22it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.34it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.37it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.32it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.30it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.39it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.50it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.55it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.47it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.49it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.43it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.41it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.42it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.39it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.48it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.39it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.34it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.37it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.34it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.40it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.42it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.44it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.36it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.37it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.48it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.50it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.47it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.43it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.37it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.42it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.42it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.41it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.44it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.43it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.41it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.48it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.49it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.54it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.52it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.29it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.41it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.40it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.41it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.44it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.46it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.36it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.44it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.44it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.50it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.48it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.47it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.36it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.37it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.43it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.45it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.51it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.36it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.41it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.49it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.49it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.47it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.41it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.25it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.31it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.38it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.45it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.46it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.48it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.35it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.41it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.42it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.41it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.45it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.52it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.39it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.41it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.33it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 46.33it/s][A 80%|████████  | 468/585 [03:28<00:33,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:06:02,618 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 06:06:02,638 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:06:04,788 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:06:04,811 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:06:04,822 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:36<12:07,  6.27s/it] 80%|████████  | 470/585 [03:36<08:35,  4.48s/it] 81%|████████  | 471/585 [03:36<06:07,  3.22s/it] 81%|████████  | 472/585 [03:36<04:24,  2.34s/it] 81%|████████  | 473/585 [03:37<03:13,  1.73s/it] 81%|████████  | 474/585 [03:37<02:23,  1.29s/it] 81%|████████  | 475/585 [03:37<01:49,  1.01it/s] 81%|████████▏ | 476/585 [03:38<01:25,  1.28it/s] 82%|████████▏ | 477/585 [03:38<01:08,  1.58it/s] 82%|████████▏ | 478/585 [03:38<00:56,  1.89it/s] 82%|████████▏ | 479/585 [03:38<00:48,  2.19it/s] 82%|████████▏ | 480/585 [03:39<00:42,  2.46it/s] 82%|████████▏ | 481/585 [03:39<00:38,  2.69it/s] 82%|████████▏ | 482/585 [03:39<00:35,  2.89it/s] 83%|████████▎ | 483/585 [03:40<00:33,  3.04it/s] 83%|████████▎ | 484/585 [03:40<00:31,  3.16it/s] 83%|████████▎ | 485/585 [03:40<00:30,  3.24it/s] 83%|████████▎ | 486/585 [03:40<00:29,  3.31it/s] 83%|████████▎ | 487/585 [03:41<00:29,  3.36it/s] 83%|████████▎ | 488/585 [03:41<00:28,  3.39it/s] 84%|████████▎ | 489/585 [03:41<00:28,  3.41it/s] 84%|████████▍ | 490/585 [03:42<00:27,  3.43it/s] 84%|████████▍ | 491/585 [03:42<00:27,  3.44it/s] 84%|████████▍ | 492/585 [03:42<00:27,  3.43it/s] 84%|████████▍ | 493/585 [03:42<00:26,  3.44it/s] 84%|████████▍ | 494/585 [03:43<00:26,  3.45it/s] 85%|████████▍ | 495/585 [03:43<00:26,  3.46it/s] 85%|████████▍ | 496/585 [03:43<00:25,  3.46it/s] 85%|████████▍ | 497/585 [03:44<00:25,  3.46it/s] 85%|████████▌ | 498/585 [03:44<00:25,  3.46it/s] 85%|████████▌ | 499/585 [03:44<00:24,  3.46it/s] 85%|████████▌ | 500/585 [03:44<00:24,  3.47it/s]                                                  85%|████████▌ | 500/585 [03:44<00:24,  3.47it/s] 86%|████████▌ | 501/585 [03:45<00:24,  3.47it/s] 86%|████████▌ | 502/585 [03:45<00:23,  3.47it/s] 86%|████████▌ | 503/585 [03:45<00:23,  3.46it/s] 86%|████████▌ | 504/585 [03:46<00:23,  3.46it/s] 86%|████████▋ | 505/585 [03:46<00:23,  3.46it/s] 86%|████████▋ | 506/585 [03:46<00:22,  3.46it/s] 87%|████████▋ | 507/585 [03:46<00:22,  3.47it/s] 87%|████████▋ | 508/585 [03:47<00:22,  3.46it/s] 87%|████████▋ | 509/585 [03:47<00:21,  3.47it/s] 87%|████████▋ | 510/585 [03:47<00:21,  3.46it/s] 87%|████████▋ | 511/585 [03:48<00:21,  3.47it/s] 88%|████████▊ | 512/585 [03:48<00:21,  3.46it/s] 88%|████████▊ | 513/585 [03:48<00:20,  3.47it/s] 88%|████████▊ | 514/585 [03:48<00:20,  3.46it/s] 88%|████████▊ | 515/585 [03:49<00:20,  3.46it/s] 88%|████████▊ | 516/585 [03:49<00:19,  3.47it/s] 88%|████████▊ | 517/585 [03:49<00:19,  3.47it/s] 89%|████████▊ | 518/585 [03:50<00:19,  3.47it/s] 89%|████████▊ | 519/585 [03:50<00:19,  3.47it/s] 89%|████████▉ | 520/585 [03:50<00:18,  3.47it/s] 89%|████████▉ | 521/585 [03:51<00:18,  3.47it/s] 89%|████████▉ | 522/585 [03:51<00:18,  3.47it/s] 89%|████████▉ | 523/585 [03:51<00:17,  3.47it/s] 90%|████████▉ | 524/585 [03:51<00:17,  3.47it/s] 90%|████████▉ | 525/585 [03:52<00:17,  3.46it/s] 90%|████████▉ | 526/585 [03:52<00:17,  3.46it/s] 90%|█████████ | 527/585 [03:52<00:16,  3.46it/s] 90%|█████████ | 528/585 [03:53<00:16,  3.46it/s] 90%|█████████ | 529/585 [03:53<00:16,  3.46it/s] 91%|█████████ | 530/585 [03:53<00:15,  3.47it/s] 91%|█████████ | 531/585 [03:53<00:15,  3.47it/s] 91%|█████████ | 532/585 [03:54<00:15,  3.47it/s] 91%|█████████ | 533/585 [03:54<00:14,  3.47it/s] 91%|█████████▏| 534/585 [03:54<00:14,  3.47it/s] 91%|█████████▏| 535/585 [03:55<00:15,  3.31it/s] 92%|█████████▏| 536/585 [03:55<00:14,  3.34it/s] 92%|█████████▏| 537/585 [03:55<00:14,  3.38it/s] 92%|█████████▏| 538/585 [03:55<00:13,  3.40it/s] 92%|█████████▏| 539/585 [03:56<00:13,  3.42it/s] 92%|█████████▏| 540/585 [03:56<00:13,  3.43it/s] 92%|█████████▏| 541/585 [03:56<00:12,  3.44it/s] 93%|█████████▎| 542/585 [03:57<00:12,  3.45it/s] 93%|█████████▎| 543/585 [03:57<00:12,  3.46it/s] 93%|█████████▎| 544/585 [03:57<00:11,  3.46it/s] 93%|█████████▎| 545/585 [03:57<00:11,  3.46it/s] 93%|█████████▎| 546/585 [03:58<00:11,  3.47it/s] 94%|█████████▎| 547/585 [03:58<00:10,  3.46it/s] 94%|█████████▎| 548/585 [03:58<00:10,  3.47it/s] 94%|█████████▍| 549/585 [03:59<00:10,  3.47it/s] 94%|█████████▍| 550/585 [03:59<00:10,  3.47it/s] 94%|█████████▍| 551/585 [03:59<00:09,  3.47it/s] 94%|█████████▍| 552/585 [04:00<00:09,  3.47it/s] 95%|█████████▍| 553/585 [04:00<00:09,  3.47it/s] 95%|█████████▍| 554/585 [04:00<00:08,  3.46it/s] 95%|█████████▍| 555/585 [04:00<00:08,  3.47it/s] 95%|█████████▌| 556/585 [04:01<00:08,  3.47it/s] 95%|█████████▌| 557/585 [04:01<00:08,  3.45it/s] 95%|█████████▌| 558/585 [04:01<00:07,  3.46it/s] 96%|█████████▌| 559/585 [04:02<00:07,  3.46it/s] 96%|█████████▌| 560/585 [04:02<00:07,  3.46it/s] 96%|█████████▌| 561/585 [04:02<00:06,  3.46it/s] 96%|█████████▌| 562/585 [04:02<00:06,  3.47it/s] 96%|█████████▌| 563/585 [04:03<00:06,  3.47it/s] 96%|█████████▋| 564/585 [04:03<00:06,  3.47it/s] 97%|█████████▋| 565/585 [04:03<00:05,  3.46it/s] 97%|█████████▋| 566/585 [04:04<00:05,  3.46it/s] 97%|█████████▋| 567/585 [04:04<00:05,  3.47it/s] 97%|█████████▋| 568/585 [04:04<00:04,  3.46it/s] 97%|█████████▋| 569/585 [04:04<00:04,  3.47it/s] 97%|█████████▋| 570/585 [04:05<00:04,  3.46it/s] 98%|█████████▊| 571/585 [04:05<00:04,  3.47it/s] 98%|█████████▊| 572/585 [04:05<00:03,  3.46it/s] 98%|█████████▊| 573/585 [04:06<00:03,  3.46it/s] 98%|█████████▊| 574/585 [04:06<00:03,  3.47it/s] 98%|█████████▊| 575/585 [04:06<00:02,  3.46it/s] 98%|█████████▊| 576/585 [04:06<00:02,  3.47it/s] 99%|█████████▊| 577/585 [04:07<00:02,  3.46it/s] 99%|█████████▉| 578/585 [04:07<00:02,  3.46it/s] 99%|█████████▉| 579/585 [04:07<00:01,  3.46it/s] 99%|█████████▉| 580/585 [04:08<00:01,  3.46it/s] 99%|█████████▉| 581/585 [04:08<00:01,  3.46it/s] 99%|█████████▉| 582/585 [04:08<00:00,  3.46it/s]100%|█████████▉| 583/585 [04:08<00:00,  3.45it/s]100%|█████████▉| 584/585 [04:09<00:00,  3.45it/s]100%|██████████| 585/585 [04:09<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 06:06:43,171 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:06:43,171 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 06:06:43,171 >>   Batch size = 8
{'eval_loss': 0.9708476662635803, 'eval_runtime': 13.1632, 'eval_samples_per_second': 370.883, 'eval_steps_per_second': 46.417, 'epoch': 4.0}
{'loss': 0.4739, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.52it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.26it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.69it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.95it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.51it/s][A
  5%|▌         | 33/611 [00:00<00:12, 46.94it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.56it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.40it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.40it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.44it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.41it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.55it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.60it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.62it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.55it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.36it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.13it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.19it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.33it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.38it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.49it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.58it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.62it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.61it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.44it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.30it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.26it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.30it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.41it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.43it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.39it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.52it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.55it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.59it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.47it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.38it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.39it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.42it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.44it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.45it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.46it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.42it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.56it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.51it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.47it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.38it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.44it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.45it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.36it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.42it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.29it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.42it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.49it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.40it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.39it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.43it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.42it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.43it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.41it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.43it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.43it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.46it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.38it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.34it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.36it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.35it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.35it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.43it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.44it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.48it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.49it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.46it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.44it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.44it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.47it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.49it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.50it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.44it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.40it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.42it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.48it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.49it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.44it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.44it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.44it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.47it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.49it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.43it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.46it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.47it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.35it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.40it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.42it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.44it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.49it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.45it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.41it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.35it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.40it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.43it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.40it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.43it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.46it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.42it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.44it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.37it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.38it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.31it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.44it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.36it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.32it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.12it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.22it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.32it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.30it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.11it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.27it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.23it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.34it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.38it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.43it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 46.43it/s][A100%|██████████| 585/585 [04:22<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:06:56,342 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 06:06:56,359 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:06:58,660 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:06:58,716 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:06:58,731 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 06:07:03,066 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 06:07:03,070 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117 (score: 0.9303779006004333).
                                                 100%|██████████| 585/585 [04:31<00:00,  3.45it/s]100%|██████████| 585/585 [04:31<00:00,  2.16it/s]
[INFO|trainer.py:1894] 2023-08-29 06:07:04,670 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 06:07:04,684 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:07:06,930 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:07:06,949 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:07:06,964 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 06:07:07,140 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:07,140 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:07,140 >>   train_loss               =     0.4703
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:07,140 >>   train_runtime            = 0:04:31.03
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:07,140 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:07,140 >>   train_samples_per_second =    138.342
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:07,140 >>   train_steps_per_second   =      2.158
{'eval_loss': 0.9754431247711182, 'eval_runtime': 13.1564, 'eval_samples_per_second': 371.074, 'eval_steps_per_second': 46.441, 'epoch': 5.0}
{'train_runtime': 271.0321, 'train_samples_per_second': 138.342, 'train_steps_per_second': 2.158, 'train_loss': 0.4703227768596421, 'epoch': 5.0}
08/29/2023 06:07:07 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 06:07:07,186 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:07:07,186 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 06:07:07,186 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 58.50it/s]  2%|▏         | 12/611 [00:00<00:11, 51.22it/s]  3%|▎         | 18/611 [00:00<00:12, 49.26it/s]  4%|▍         | 23/611 [00:00<00:12, 48.36it/s]  5%|▍         | 28/611 [00:00<00:12, 47.88it/s]  5%|▌         | 33/611 [00:00<00:12, 47.46it/s]  6%|▌         | 38/611 [00:00<00:12, 47.31it/s]  7%|▋         | 43/611 [00:00<00:12, 47.16it/s]  8%|▊         | 48/611 [00:00<00:12, 46.84it/s]  9%|▊         | 53/611 [00:01<00:11, 46.77it/s]  9%|▉         | 58/611 [00:01<00:11, 46.84it/s] 10%|█         | 63/611 [00:01<00:11, 46.77it/s] 11%|█         | 68/611 [00:01<00:11, 46.84it/s] 12%|█▏        | 73/611 [00:01<00:11, 46.89it/s] 13%|█▎        | 78/611 [00:01<00:11, 46.87it/s] 14%|█▎        | 83/611 [00:01<00:11, 46.81it/s] 14%|█▍        | 88/611 [00:01<00:11, 46.76it/s] 15%|█▌        | 93/611 [00:01<00:11, 46.68it/s] 16%|█▌        | 98/611 [00:02<00:11, 46.48it/s] 17%|█▋        | 103/611 [00:02<00:10, 46.63it/s] 18%|█▊        | 108/611 [00:02<00:10, 46.64it/s] 18%|█▊        | 113/611 [00:02<00:10, 46.74it/s] 19%|█▉        | 118/611 [00:02<00:10, 46.68it/s] 20%|██        | 123/611 [00:02<00:10, 46.86it/s] 21%|██        | 128/611 [00:02<00:10, 46.89it/s] 22%|██▏       | 133/611 [00:02<00:10, 46.73it/s] 23%|██▎       | 138/611 [00:02<00:10, 46.70it/s] 23%|██▎       | 143/611 [00:03<00:10, 46.69it/s] 24%|██▍       | 148/611 [00:03<00:09, 46.71it/s] 25%|██▌       | 153/611 [00:03<00:09, 46.70it/s] 26%|██▌       | 158/611 [00:03<00:09, 46.71it/s] 27%|██▋       | 163/611 [00:03<00:09, 46.73it/s] 27%|██▋       | 168/611 [00:03<00:09, 46.65it/s] 28%|██▊       | 173/611 [00:03<00:09, 46.65it/s] 29%|██▉       | 178/611 [00:03<00:09, 46.79it/s] 30%|██▉       | 183/611 [00:03<00:09, 46.73it/s] 31%|███       | 188/611 [00:03<00:09, 46.74it/s] 32%|███▏      | 193/611 [00:04<00:08, 46.75it/s] 32%|███▏      | 198/611 [00:04<00:08, 46.69it/s] 33%|███▎      | 203/611 [00:04<00:08, 46.59it/s] 34%|███▍      | 208/611 [00:04<00:08, 46.62it/s] 35%|███▍      | 213/611 [00:04<00:08, 46.72it/s] 36%|███▌      | 218/611 [00:04<00:08, 46.76it/s] 36%|███▋      | 223/611 [00:04<00:08, 46.75it/s] 37%|███▋      | 228/611 [00:04<00:08, 46.70it/s] 38%|███▊      | 233/611 [00:04<00:08, 46.51it/s] 39%|███▉      | 238/611 [00:05<00:08, 46.61it/s] 40%|███▉      | 243/611 [00:05<00:07, 46.64it/s] 41%|████      | 248/611 [00:05<00:07, 46.69it/s] 41%|████▏     | 253/611 [00:05<00:07, 46.68it/s] 42%|████▏     | 258/611 [00:05<00:07, 46.71it/s] 43%|████▎     | 263/611 [00:05<00:07, 46.73it/s] 44%|████▍     | 268/611 [00:05<00:07, 46.65it/s] 45%|████▍     | 273/611 [00:05<00:07, 46.75it/s] 45%|████▌     | 278/611 [00:05<00:07, 46.75it/s] 46%|████▋     | 283/611 [00:06<00:07, 46.72it/s] 47%|████▋     | 288/611 [00:06<00:06, 46.74it/s] 48%|████▊     | 293/611 [00:06<00:06, 46.58it/s] 49%|████▉     | 298/611 [00:06<00:06, 46.51it/s] 50%|████▉     | 303/611 [00:06<00:06, 46.59it/s] 50%|█████     | 308/611 [00:06<00:06, 46.56it/s] 51%|█████     | 313/611 [00:06<00:06, 46.61it/s] 52%|█████▏    | 318/611 [00:06<00:06, 46.60it/s] 53%|█████▎    | 323/611 [00:06<00:06, 46.66it/s] 54%|█████▎    | 328/611 [00:06<00:06, 46.59it/s] 55%|█████▍    | 333/611 [00:07<00:05, 46.67it/s] 55%|█████▌    | 338/611 [00:07<00:05, 46.70it/s] 56%|█████▌    | 343/611 [00:07<00:05, 46.73it/s] 57%|█████▋    | 348/611 [00:07<00:05, 46.66it/s] 58%|█████▊    | 353/611 [00:07<00:05, 46.47it/s] 59%|█████▊    | 358/611 [00:07<00:05, 46.50it/s] 59%|█████▉    | 363/611 [00:07<00:05, 46.44it/s] 60%|██████    | 368/611 [00:07<00:05, 46.56it/s] 61%|██████    | 373/611 [00:07<00:05, 46.56it/s] 62%|██████▏   | 378/611 [00:08<00:04, 46.61it/s] 63%|██████▎   | 383/611 [00:08<00:04, 46.61it/s] 64%|██████▎   | 388/611 [00:08<00:04, 46.64it/s] 64%|██████▍   | 393/611 [00:08<00:04, 46.60it/s] 65%|██████▌   | 398/611 [00:08<00:04, 46.54it/s] 66%|██████▌   | 403/611 [00:08<00:04, 46.48it/s] 67%|██████▋   | 408/611 [00:08<00:04, 46.50it/s] 68%|██████▊   | 413/611 [00:08<00:04, 46.58it/s] 68%|██████▊   | 418/611 [00:08<00:04, 46.61it/s] 69%|██████▉   | 423/611 [00:09<00:04, 46.43it/s] 70%|███████   | 428/611 [00:09<00:03, 46.43it/s] 71%|███████   | 433/611 [00:09<00:03, 46.55it/s] 72%|███████▏  | 438/611 [00:09<00:03, 46.55it/s] 73%|███████▎  | 443/611 [00:09<00:03, 46.51it/s] 73%|███████▎  | 448/611 [00:09<00:03, 46.51it/s] 74%|███████▍  | 453/611 [00:09<00:03, 46.50it/s] 75%|███████▍  | 458/611 [00:09<00:03, 46.43it/s] 76%|███████▌  | 463/611 [00:09<00:03, 46.52it/s] 77%|███████▋  | 468/611 [00:10<00:03, 46.52it/s] 77%|███████▋  | 473/611 [00:10<00:02, 46.61it/s] 78%|███████▊  | 478/611 [00:10<00:02, 46.66it/s] 79%|███████▉  | 483/611 [00:10<00:02, 46.63it/s] 80%|███████▉  | 488/611 [00:10<00:02, 46.57it/s] 81%|████████  | 493/611 [00:10<00:02, 46.47it/s] 82%|████████▏ | 498/611 [00:10<00:02, 46.47it/s] 82%|████████▏ | 503/611 [00:10<00:02, 46.54it/s] 83%|████████▎ | 508/611 [00:10<00:02, 46.53it/s] 84%|████████▍ | 513/611 [00:10<00:02, 46.44it/s] 85%|████████▍ | 518/611 [00:11<00:01, 46.58it/s] 86%|████████▌ | 523/611 [00:11<00:01, 46.50it/s] 86%|████████▋ | 528/611 [00:11<00:01, 46.52it/s] 87%|████████▋ | 533/611 [00:11<00:01, 46.51it/s] 88%|████████▊ | 538/611 [00:11<00:01, 46.47it/s] 89%|████████▉ | 543/611 [00:11<00:01, 46.47it/s] 90%|████████▉ | 548/611 [00:11<00:01, 46.46it/s] 91%|█████████ | 553/611 [00:11<00:01, 46.44it/s] 91%|█████████▏| 558/611 [00:11<00:01, 46.48it/s] 92%|█████████▏| 563/611 [00:12<00:01, 46.52it/s] 93%|█████████▎| 568/611 [00:12<00:00, 46.59it/s] 94%|█████████▍| 573/611 [00:12<00:00, 46.48it/s] 95%|█████████▍| 578/611 [00:12<00:00, 46.55it/s] 95%|█████████▌| 583/611 [00:12<00:00, 46.35it/s] 96%|█████████▌| 588/611 [00:12<00:00, 46.37it/s] 97%|█████████▋| 593/611 [00:12<00:00, 46.45it/s] 98%|█████████▊| 598/611 [00:12<00:00, 46.54it/s] 99%|█████████▊| 603/611 [00:12<00:00, 46.53it/s]100%|█████████▉| 608/611 [00:13<00:00, 46.53it/s]100%|██████████| 611/611 [00:13<00:00, 46.71it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 06:07:20,291 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,291 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,291 >>   eval_loss               =     0.9304
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,291 >>   eval_runtime            = 0:00:13.10
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,291 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,291 >>   eval_samples_per_second =    372.542
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,291 >>   eval_steps_per_second   =     46.625
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,291 >>   perplexity              =     2.5355
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:27,057 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:27,061 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:27,061 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:27,061 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:27,061 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:07:27,663 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:07:27,664 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:07:28,228 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:07:29,227 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:07:29,227 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:32,249 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:32,253 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:32,253 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:32,253 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:32,254 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:07:32,921 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:07:32,922 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:07:33,488 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:07:33,639 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:07:33,639 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.59it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.62it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:13,  1.63it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:14,  1.67it/s]Extractor Predicting: 24it [00:15,  1.67it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:17,  1.63it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.63it/s]Extractor Predicting: 32it [00:20,  1.67it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:21,  1.62it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:23,  1.62it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.63it/s]Extractor Predicting: 41it [00:25,  1.59it/s]Extractor Predicting: 42it [00:26,  1.61it/s]Extractor Predicting: 43it [00:26,  1.60it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:28,  1.63it/s]Extractor Predicting: 46it [00:28,  1.62it/s]Extractor Predicting: 47it [00:29,  1.61it/s]Extractor Predicting: 48it [00:30,  1.60it/s]Extractor Predicting: 49it [00:30,  1.60it/s]Extractor Predicting: 50it [00:31,  1.56it/s]Extractor Predicting: 51it [00:31,  1.57it/s]Extractor Predicting: 52it [00:32,  1.60it/s]Extractor Predicting: 53it [00:33,  1.57it/s]Extractor Predicting: 54it [00:33,  1.57it/s]Extractor Predicting: 55it [00:34,  1.52it/s]Extractor Predicting: 56it [00:35,  1.49it/s]Extractor Predicting: 57it [00:35,  1.51it/s]Extractor Predicting: 58it [00:36,  1.51it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:37,  1.52it/s]Extractor Predicting: 61it [00:38,  1.51it/s]Extractor Predicting: 62it [00:39,  1.49it/s]Extractor Predicting: 63it [00:39,  1.50it/s]Extractor Predicting: 64it [00:40,  1.50it/s]Extractor Predicting: 65it [00:41,  1.49it/s]Extractor Predicting: 66it [00:41,  1.47it/s]Extractor Predicting: 67it [00:42,  1.49it/s]Extractor Predicting: 68it [00:43,  1.52it/s]Extractor Predicting: 69it [00:43,  1.51it/s]Extractor Predicting: 70it [00:44,  1.49it/s]Extractor Predicting: 71it [00:45,  1.50it/s]Extractor Predicting: 72it [00:45,  1.47it/s]Extractor Predicting: 73it [00:46,  1.49it/s]Extractor Predicting: 74it [00:47,  1.50it/s]Extractor Predicting: 75it [00:47,  1.50it/s]Extractor Predicting: 76it [00:48,  1.51it/s]Extractor Predicting: 77it [00:49,  1.50it/s]Extractor Predicting: 78it [00:49,  1.53it/s]Extractor Predicting: 79it [00:50,  1.55it/s]Extractor Predicting: 80it [00:51,  1.54it/s]Extractor Predicting: 81it [00:51,  1.54it/s]Extractor Predicting: 82it [00:52,  1.51it/s]Extractor Predicting: 83it [00:53,  1.51it/s]Extractor Predicting: 84it [00:53,  1.51it/s]Extractor Predicting: 85it [00:54,  1.50it/s]Extractor Predicting: 86it [00:55,  1.49it/s]Extractor Predicting: 87it [00:55,  1.48it/s]Extractor Predicting: 88it [00:56,  1.46it/s]Extractor Predicting: 89it [00:57,  1.46it/s]Extractor Predicting: 90it [00:57,  1.46it/s]Extractor Predicting: 91it [00:58,  1.46it/s]Extractor Predicting: 92it [00:59,  1.52it/s]Extractor Predicting: 93it [00:59,  1.58it/s]Extractor Predicting: 94it [01:00,  1.57it/s]Extractor Predicting: 95it [01:01,  1.57it/s]Extractor Predicting: 96it [01:01,  1.45it/s]Extractor Predicting: 97it [01:02,  1.49it/s]Extractor Predicting: 98it [01:03,  1.49it/s]Extractor Predicting: 99it [01:03,  1.44it/s]Extractor Predicting: 100it [01:04,  1.48it/s]Extractor Predicting: 101it [01:05,  1.42it/s]Extractor Predicting: 102it [01:06,  1.43it/s]Extractor Predicting: 103it [01:06,  1.45it/s]Extractor Predicting: 104it [01:07,  1.47it/s]Extractor Predicting: 105it [01:08,  1.49it/s]Extractor Predicting: 106it [01:08,  1.53it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:09,  1.57it/s]Extractor Predicting: 109it [01:10,  1.55it/s]Extractor Predicting: 110it [01:11,  1.55it/s]Extractor Predicting: 111it [01:11,  1.58it/s]Extractor Predicting: 112it [01:12,  1.59it/s]Extractor Predicting: 113it [01:13,  1.54it/s]Extractor Predicting: 114it [01:13,  1.52it/s]Extractor Predicting: 115it [01:14,  1.54it/s]Extractor Predicting: 116it [01:15,  1.50it/s]Extractor Predicting: 117it [01:15,  1.49it/s]Extractor Predicting: 118it [01:16,  1.49it/s]Extractor Predicting: 119it [01:17,  1.47it/s]Extractor Predicting: 120it [01:17,  1.44it/s]Extractor Predicting: 121it [01:18,  1.46it/s]Extractor Predicting: 122it [01:19,  1.46it/s]Extractor Predicting: 123it [01:19,  1.49it/s]Extractor Predicting: 124it [01:20,  1.49it/s]Extractor Predicting: 125it [01:21,  1.50it/s]Extractor Predicting: 126it [01:21,  1.50it/s]Extractor Predicting: 127it [01:22,  1.50it/s]Extractor Predicting: 128it [01:23,  1.50it/s]Extractor Predicting: 129it [01:23,  1.52it/s]Extractor Predicting: 130it [01:24,  1.47it/s]Extractor Predicting: 131it [01:25,  1.48it/s]Extractor Predicting: 132it [01:25,  1.52it/s]Extractor Predicting: 133it [01:26,  1.49it/s]Extractor Predicting: 134it [01:27,  1.49it/s]Extractor Predicting: 135it [01:27,  1.48it/s]Extractor Predicting: 136it [01:28,  1.50it/s]Extractor Predicting: 137it [01:29,  1.47it/s]Extractor Predicting: 138it [01:29,  1.48it/s]Extractor Predicting: 139it [01:30,  1.47it/s]Extractor Predicting: 140it [01:31,  1.46it/s]Extractor Predicting: 141it [01:32,  1.48it/s]Extractor Predicting: 142it [01:32,  1.48it/s]Extractor Predicting: 143it [01:33,  1.47it/s]Extractor Predicting: 144it [01:34,  1.50it/s]Extractor Predicting: 145it [01:34,  1.54it/s]Extractor Predicting: 146it [01:35,  1.52it/s]Extractor Predicting: 147it [01:36,  1.49it/s]Extractor Predicting: 148it [01:36,  1.49it/s]Extractor Predicting: 149it [01:37,  1.48it/s]Extractor Predicting: 150it [01:38,  1.47it/s]Extractor Predicting: 151it [01:38,  1.46it/s]Extractor Predicting: 152it [01:39,  1.46it/s]Extractor Predicting: 153it [01:40,  1.46it/s]Extractor Predicting: 154it [01:40,  1.47it/s]Extractor Predicting: 155it [01:41,  1.47it/s]Extractor Predicting: 156it [01:42,  1.42it/s]Extractor Predicting: 157it [01:42,  1.38it/s]Extractor Predicting: 158it [01:43,  1.35it/s]Extractor Predicting: 159it [01:44,  1.39it/s]Extractor Predicting: 160it [01:45,  1.41it/s]Extractor Predicting: 161it [01:45,  1.43it/s]Extractor Predicting: 162it [01:46,  1.44it/s]Extractor Predicting: 163it [01:47,  1.44it/s]Extractor Predicting: 164it [01:47,  1.47it/s]Extractor Predicting: 165it [01:48,  1.46it/s]Extractor Predicting: 166it [01:49,  1.49it/s]Extractor Predicting: 167it [01:49,  1.48it/s]Extractor Predicting: 168it [01:50,  1.48it/s]Extractor Predicting: 169it [01:51,  1.49it/s]Extractor Predicting: 170it [01:51,  1.49it/s]Extractor Predicting: 171it [01:52,  1.51it/s]Extractor Predicting: 172it [01:53,  1.53it/s]Extractor Predicting: 173it [01:53,  1.48it/s]Extractor Predicting: 174it [01:54,  1.50it/s]Extractor Predicting: 175it [01:55,  1.47it/s]Extractor Predicting: 176it [01:55,  1.49it/s]Extractor Predicting: 177it [01:56,  1.47it/s]Extractor Predicting: 178it [01:57,  1.47it/s]Extractor Predicting: 179it [01:57,  1.47it/s]Extractor Predicting: 180it [01:58,  1.36it/s]Extractor Predicting: 181it [01:59,  1.38it/s]Extractor Predicting: 182it [02:00,  1.40it/s]Extractor Predicting: 183it [02:00,  1.38it/s]Extractor Predicting: 184it [02:01,  1.40it/s]Extractor Predicting: 185it [02:02,  1.51it/s]Extractor Predicting: 185it [02:02,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:09:44,032 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:09:44,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:09:44,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:09:44,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:09:44,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:09:44,758 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:09:44,759 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:09:45,433 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:09:46,470 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:09:46,470 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:09:48,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:09:48,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:09:48,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:09:48,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:09:48,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:09:49,318 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:09:49,319 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:09:49,580 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:09:49,731 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:09:49,731 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.40160642570281124,
  "recall": 0.04096681687832855,
  "score": 0.07434944237918216,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:12,  1.51it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.60it/s]Extractor Predicting: 31it [00:19,  1.65it/s]Extractor Predicting: 32it [00:20,  1.64it/s]Extractor Predicting: 33it [00:21,  1.64it/s]Extractor Predicting: 34it [00:21,  1.64it/s]Extractor Predicting: 35it [00:22,  1.64it/s]Extractor Predicting: 36it [00:23,  1.62it/s]Extractor Predicting: 37it [00:23,  1.64it/s]Extractor Predicting: 38it [00:24,  1.62it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:25,  1.59it/s]Extractor Predicting: 41it [00:26,  1.60it/s]Extractor Predicting: 42it [00:26,  1.61it/s]Extractor Predicting: 43it [00:27,  1.62it/s]Extractor Predicting: 44it [00:28,  1.64it/s]Extractor Predicting: 45it [00:28,  1.65it/s]Extractor Predicting: 46it [00:29,  1.55it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:30,  1.60it/s]Extractor Predicting: 49it [00:31,  1.60it/s]Extractor Predicting: 50it [00:31,  1.60it/s]Extractor Predicting: 51it [00:32,  1.62it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:33,  1.61it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:34,  1.57it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.60it/s]Extractor Predicting: 58it [00:36,  1.66it/s]Extractor Predicting: 59it [00:37,  1.63it/s]Extractor Predicting: 60it [00:38,  1.60it/s]Extractor Predicting: 61it [00:38,  1.56it/s]Extractor Predicting: 62it [00:39,  1.53it/s]Extractor Predicting: 63it [00:40,  1.49it/s]Extractor Predicting: 64it [00:40,  1.46it/s]Extractor Predicting: 65it [00:41,  1.45it/s]Extractor Predicting: 66it [00:42,  1.45it/s]Extractor Predicting: 67it [00:42,  1.43it/s]Extractor Predicting: 68it [00:43,  1.44it/s]Extractor Predicting: 69it [00:44,  1.46it/s]Extractor Predicting: 70it [00:44,  1.47it/s]Extractor Predicting: 71it [00:45,  1.48it/s]Extractor Predicting: 72it [00:46,  1.50it/s]Extractor Predicting: 73it [00:46,  1.54it/s]Extractor Predicting: 74it [00:47,  1.54it/s]Extractor Predicting: 75it [00:48,  1.55it/s]Extractor Predicting: 76it [00:48,  1.56it/s]Extractor Predicting: 77it [00:49,  1.58it/s]Extractor Predicting: 78it [00:50,  1.57it/s]Extractor Predicting: 79it [00:50,  1.62it/s]Extractor Predicting: 80it [00:51,  1.66it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:52,  1.63it/s]Extractor Predicting: 83it [00:53,  1.59it/s]Extractor Predicting: 84it [00:53,  1.57it/s]Extractor Predicting: 85it [00:54,  1.53it/s]Extractor Predicting: 86it [00:55,  1.51it/s]Extractor Predicting: 87it [00:55,  1.51it/s]Extractor Predicting: 88it [00:56,  1.53it/s]Extractor Predicting: 89it [00:57,  1.52it/s]Extractor Predicting: 90it [00:57,  1.53it/s]Extractor Predicting: 91it [00:58,  1.52it/s]Extractor Predicting: 92it [00:59,  1.51it/s]Extractor Predicting: 93it [00:59,  1.53it/s]Extractor Predicting: 94it [01:00,  1.54it/s]Extractor Predicting: 95it [01:01,  1.53it/s]Extractor Predicting: 96it [01:01,  1.52it/s]Extractor Predicting: 97it [01:02,  1.54it/s]Extractor Predicting: 98it [01:02,  1.53it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.51it/s]Extractor Predicting: 101it [01:04,  1.53it/s]Extractor Predicting: 102it [01:05,  1.54it/s]Extractor Predicting: 103it [01:06,  1.51it/s]Extractor Predicting: 104it [01:06,  1.53it/s]Extractor Predicting: 105it [01:07,  1.55it/s]Extractor Predicting: 106it [01:08,  1.56it/s]Extractor Predicting: 107it [01:08,  1.55it/s]Extractor Predicting: 108it [01:09,  1.56it/s]Extractor Predicting: 109it [01:10,  1.55it/s]Extractor Predicting: 110it [01:10,  1.56it/s]Extractor Predicting: 111it [01:11,  1.56it/s]Extractor Predicting: 112it [01:12,  1.54it/s]Extractor Predicting: 113it [01:12,  1.53it/s]Extractor Predicting: 114it [01:13,  1.52it/s]Extractor Predicting: 115it [01:14,  1.52it/s]Extractor Predicting: 116it [01:14,  1.52it/s]Extractor Predicting: 117it [01:15,  1.55it/s]Extractor Predicting: 118it [01:16,  1.53it/s]Extractor Predicting: 119it [01:16,  1.53it/s]Extractor Predicting: 120it [01:17,  1.55it/s]Extractor Predicting: 121it [01:17,  1.58it/s]Extractor Predicting: 122it [01:18,  1.57it/s]Extractor Predicting: 123it [01:19,  1.54it/s]Extractor Predicting: 124it [01:19,  1.52it/s]Extractor Predicting: 125it [01:20,  1.53it/s]Extractor Predicting: 126it [01:21,  1.52it/s]Extractor Predicting: 127it [01:21,  1.55it/s]Extractor Predicting: 128it [01:22,  1.50it/s]Extractor Predicting: 129it [01:23,  1.52it/s]Extractor Predicting: 130it [01:23,  1.54it/s]Extractor Predicting: 131it [01:24,  1.52it/s]Extractor Predicting: 132it [01:25,  1.52it/s]Extractor Predicting: 133it [01:25,  1.52it/s]Extractor Predicting: 134it [01:26,  1.51it/s]Extractor Predicting: 135it [01:27,  1.52it/s]Extractor Predicting: 136it [01:27,  1.56it/s]Extractor Predicting: 137it [01:28,  1.51it/s]Extractor Predicting: 138it [01:29,  1.52it/s]Extractor Predicting: 139it [01:29,  1.54it/s]Extractor Predicting: 140it [01:30,  1.54it/s]Extractor Predicting: 141it [01:31,  1.52it/s]Extractor Predicting: 142it [01:31,  1.55it/s]Extractor Predicting: 143it [01:32,  1.45it/s]Extractor Predicting: 144it [01:33,  1.52it/s]Extractor Predicting: 145it [01:33,  1.49it/s]Extractor Predicting: 146it [01:34,  1.53it/s]Extractor Predicting: 147it [01:34,  1.57it/s]Extractor Predicting: 148it [01:35,  1.55it/s]Extractor Predicting: 149it [01:36,  1.58it/s]Extractor Predicting: 150it [01:36,  1.59it/s]Extractor Predicting: 151it [01:37,  1.62it/s]Extractor Predicting: 152it [01:38,  1.57it/s]Extractor Predicting: 153it [01:38,  1.57it/s]Extractor Predicting: 154it [01:39,  1.54it/s]Extractor Predicting: 155it [01:40,  1.54it/s]Extractor Predicting: 156it [01:40,  1.58it/s]Extractor Predicting: 157it [01:41,  1.54it/s]Extractor Predicting: 158it [01:42,  1.53it/s]Extractor Predicting: 159it [01:42,  1.54it/s]Extractor Predicting: 160it [01:43,  1.54it/s]Extractor Predicting: 161it [01:43,  1.57it/s]Extractor Predicting: 162it [01:44,  1.56it/s]Extractor Predicting: 163it [01:45,  1.55it/s]Extractor Predicting: 164it [01:45,  1.54it/s]Extractor Predicting: 165it [01:46,  1.50it/s]Extractor Predicting: 166it [01:47,  1.47it/s]Extractor Predicting: 167it [01:47,  1.47it/s]Extractor Predicting: 168it [01:48,  1.50it/s]Extractor Predicting: 169it [01:49,  1.53it/s]Extractor Predicting: 170it [01:49,  1.54it/s]Extractor Predicting: 171it [01:50,  1.55it/s]Extractor Predicting: 172it [01:51,  1.58it/s]Extractor Predicting: 173it [01:51,  1.57it/s]Extractor Predicting: 174it [01:52,  1.56it/s]Extractor Predicting: 175it [01:53,  1.40it/s]Extractor Predicting: 176it [01:53,  1.44it/s]Extractor Predicting: 177it [01:54,  1.45it/s]Extractor Predicting: 178it [01:55,  1.47it/s]Extractor Predicting: 179it [01:55,  1.45it/s]Extractor Predicting: 180it [01:56,  1.50it/s]Extractor Predicting: 181it [01:57,  1.51it/s]Extractor Predicting: 182it [01:57,  1.53it/s]Extractor Predicting: 183it [01:58,  1.56it/s]Extractor Predicting: 184it [01:59,  1.55it/s]Extractor Predicting: 185it [01:59,  1.56it/s]Extractor Predicting: 186it [02:00,  1.52it/s]Extractor Predicting: 187it [02:01,  1.53it/s]Extractor Predicting: 188it [02:01,  1.52it/s]Extractor Predicting: 189it [02:02,  1.52it/s]Extractor Predicting: 190it [02:03,  1.54it/s]Extractor Predicting: 191it [02:03,  1.55it/s]Extractor Predicting: 192it [02:04,  1.58it/s]Extractor Predicting: 193it [02:04,  1.57it/s]Extractor Predicting: 194it [02:05,  1.57it/s]Extractor Predicting: 195it [02:06,  1.55it/s]Extractor Predicting: 196it [02:06,  1.55it/s]Extractor Predicting: 197it [02:07,  1.54it/s]Extractor Predicting: 198it [02:08,  1.53it/s]Extractor Predicting: 199it [02:08,  1.52it/s]Extractor Predicting: 200it [02:09,  1.52it/s]Extractor Predicting: 201it [02:10,  1.54it/s]Extractor Predicting: 202it [02:10,  1.54it/s]Extractor Predicting: 203it [02:11,  1.57it/s]Extractor Predicting: 204it [02:12,  1.56it/s]Extractor Predicting: 205it [02:12,  1.57it/s]Extractor Predicting: 206it [02:13,  1.56it/s]Extractor Predicting: 207it [02:14,  1.57it/s]Extractor Predicting: 208it [02:14,  1.54it/s]Extractor Predicting: 209it [02:15,  1.51it/s]Extractor Predicting: 210it [02:15,  1.57it/s]Extractor Predicting: 211it [02:16,  1.55it/s]Extractor Predicting: 212it [02:17,  1.56it/s]Extractor Predicting: 213it [02:17,  1.57it/s]Extractor Predicting: 214it [02:18,  1.60it/s]Extractor Predicting: 215it [02:19,  1.58it/s]Extractor Predicting: 216it [02:19,  1.55it/s]Extractor Predicting: 217it [02:20,  1.56it/s]Extractor Predicting: 218it [02:21,  1.54it/s]Extractor Predicting: 219it [02:21,  1.54it/s]Extractor Predicting: 220it [02:22,  1.55it/s]Extractor Predicting: 221it [02:23,  1.56it/s]Extractor Predicting: 222it [02:23,  1.51it/s]Extractor Predicting: 223it [02:24,  1.46it/s]Extractor Predicting: 224it [02:25,  1.48it/s]Extractor Predicting: 225it [02:25,  1.50it/s]Extractor Predicting: 226it [02:26,  1.54it/s]Extractor Predicting: 227it [02:27,  1.54it/s]Extractor Predicting: 228it [02:27,  1.56it/s]Extractor Predicting: 229it [02:28,  1.59it/s]Extractor Predicting: 230it [02:28,  1.60it/s]Extractor Predicting: 231it [02:29,  1.60it/s]Extractor Predicting: 232it [02:30,  1.59it/s]Extractor Predicting: 233it [02:30,  1.58it/s]Extractor Predicting: 234it [02:31,  1.58it/s]Extractor Predicting: 235it [02:32,  1.56it/s]Extractor Predicting: 236it [02:32,  1.55it/s]Extractor Predicting: 237it [02:33,  1.56it/s]Extractor Predicting: 238it [02:34,  1.53it/s]Extractor Predicting: 239it [02:34,  1.55it/s]Extractor Predicting: 240it [02:35,  1.54it/s]Extractor Predicting: 241it [02:35,  1.56it/s]Extractor Predicting: 242it [02:36,  1.53it/s]Extractor Predicting: 243it [02:37,  1.49it/s]Extractor Predicting: 244it [02:37,  1.50it/s]Extractor Predicting: 245it [02:38,  1.54it/s]Extractor Predicting: 246it [02:39,  1.53it/s]Extractor Predicting: 247it [02:39,  1.56it/s]Extractor Predicting: 248it [02:40,  1.50it/s]Extractor Predicting: 249it [02:41,  1.50it/s]Extractor Predicting: 250it [02:41,  1.52it/s]Extractor Predicting: 251it [02:42,  1.51it/s]Extractor Predicting: 252it [02:43,  1.51it/s]Extractor Predicting: 253it [02:43,  1.47it/s]Extractor Predicting: 254it [02:44,  1.45it/s]Extractor Predicting: 255it [02:45,  1.47it/s]Extractor Predicting: 256it [02:45,  1.48it/s]Extractor Predicting: 257it [02:46,  1.50it/s]Extractor Predicting: 258it [02:47,  1.50it/s]Extractor Predicting: 259it [02:47,  1.50it/s]Extractor Predicting: 260it [02:48,  1.47it/s]Extractor Predicting: 261it [02:49,  1.49it/s]Extractor Predicting: 262it [02:50,  1.48it/s]Extractor Predicting: 263it [02:50,  1.49it/s]Extractor Predicting: 264it [02:51,  1.49it/s]Extractor Predicting: 265it [02:51,  1.52it/s]Extractor Predicting: 266it [02:52,  1.46it/s]Extractor Predicting: 267it [02:53,  1.46it/s]Extractor Predicting: 268it [02:54,  1.45it/s]Extractor Predicting: 269it [02:54,  1.47it/s]Extractor Predicting: 270it [02:55,  1.45it/s]Extractor Predicting: 271it [02:56,  1.45it/s]Extractor Predicting: 272it [02:56,  1.46it/s]Extractor Predicting: 273it [02:57,  1.47it/s]Extractor Predicting: 274it [02:58,  1.43it/s]Extractor Predicting: 275it [02:58,  1.45it/s]Extractor Predicting: 276it [02:59,  1.46it/s]Extractor Predicting: 277it [03:00,  1.47it/s]Extractor Predicting: 278it [03:00,  1.49it/s]Extractor Predicting: 279it [03:01,  1.47it/s]Extractor Predicting: 280it [03:02,  1.33it/s]Extractor Predicting: 281it [03:03,  1.33it/s]Extractor Predicting: 282it [03:03,  1.36it/s]Extractor Predicting: 283it [03:04,  1.39it/s]Extractor Predicting: 284it [03:05,  1.44it/s]Extractor Predicting: 285it [03:05,  1.44it/s]Extractor Predicting: 286it [03:06,  1.47it/s]Extractor Predicting: 287it [03:07,  1.43it/s]Extractor Predicting: 288it [03:08,  1.46it/s]Extractor Predicting: 289it [03:08,  1.48it/s]Extractor Predicting: 290it [03:09,  1.45it/s]Extractor Predicting: 291it [03:10,  1.42it/s]Extractor Predicting: 292it [03:10,  1.46it/s]Extractor Predicting: 293it [03:11,  1.46it/s]Extractor Predicting: 294it [03:12,  1.45it/s]Extractor Predicting: 295it [03:12,  1.45it/s]Extractor Predicting: 296it [03:13,  1.49it/s]Extractor Predicting: 297it [03:14,  1.49it/s]Extractor Predicting: 298it [03:14,  1.48it/s]Extractor Predicting: 299it [03:15,  1.48it/s]Extractor Predicting: 300it [03:16,  1.46it/s]Extractor Predicting: 301it [03:16,  1.48it/s]Extractor Predicting: 302it [03:17,  1.46it/s]Extractor Predicting: 303it [03:18,  1.49it/s]Extractor Predicting: 304it [03:18,  1.50it/s]Extractor Predicting: 305it [03:19,  1.53it/s]Extractor Predicting: 306it [03:20,  1.55it/s]Extractor Predicting: 307it [03:20,  1.52it/s]Extractor Predicting: 308it [03:21,  1.53it/s]Extractor Predicting: 309it [03:22,  1.50it/s]Extractor Predicting: 310it [03:22,  1.50it/s]Extractor Predicting: 311it [03:23,  1.47it/s]Extractor Predicting: 312it [03:24,  1.49it/s]Extractor Predicting: 313it [03:24,  1.43it/s]Extractor Predicting: 314it [03:25,  1.43it/s]Extractor Predicting: 315it [03:26,  1.47it/s]Extractor Predicting: 316it [03:26,  1.50it/s]Extractor Predicting: 317it [03:27,  1.51it/s]Extractor Predicting: 318it [03:28,  1.54it/s]Extractor Predicting: 319it [03:28,  1.51it/s]Extractor Predicting: 320it [03:29,  1.48it/s]Extractor Predicting: 321it [03:30,  1.47it/s]Extractor Predicting: 322it [03:30,  1.49it/s]Extractor Predicting: 323it [03:31,  1.45it/s]Extractor Predicting: 324it [03:32,  1.46it/s]Extractor Predicting: 325it [03:32,  1.49it/s]Extractor Predicting: 326it [03:33,  1.49it/s]Extractor Predicting: 327it [03:34,  1.50it/s]Extractor Predicting: 328it [03:35,  1.47it/s]Extractor Predicting: 329it [03:35,  1.49it/s]Extractor Predicting: 330it [03:36,  1.47it/s]Extractor Predicting: 331it [03:37,  1.49it/s]Extractor Predicting: 332it [03:37,  1.48it/s]Extractor Predicting: 333it [03:38,  1.46it/s]Extractor Predicting: 334it [03:39,  1.48it/s]Extractor Predicting: 335it [03:39,  1.47it/s]Extractor Predicting: 336it [03:40,  1.40it/s]Extractor Predicting: 337it [03:41,  1.42it/s]Extractor Predicting: 338it [03:41,  1.42it/s]Extractor Predicting: 339it [03:42,  1.44it/s]Extractor Predicting: 340it [03:43,  1.44it/s]Extractor Predicting: 341it [03:44,  1.40it/s]Extractor Predicting: 342it [03:44,  1.42it/s]Extractor Predicting: 343it [03:45,  1.45it/s]Extractor Predicting: 344it [03:46,  1.46it/s]Extractor Predicting: 345it [03:46,  1.42it/s]Extractor Predicting: 346it [03:47,  1.43it/s]Extractor Predicting: 347it [03:48,  1.45it/s]Extractor Predicting: 348it [03:48,  1.47it/s]Extractor Predicting: 349it [03:49,  1.47it/s]Extractor Predicting: 350it [03:50,  1.46it/s]Extractor Predicting: 351it [03:50,  1.49it/s]Extractor Predicting: 352it [03:51,  1.45it/s]Extractor Predicting: 353it [03:52,  1.43it/s]Extractor Predicting: 354it [03:52,  1.46it/s]Extractor Predicting: 355it [03:53,  1.48it/s]Extractor Predicting: 356it [03:54,  1.48it/s]Extractor Predicting: 357it [03:54,  1.48it/s]Extractor Predicting: 358it [03:55,  1.49it/s]Extractor Predicting: 359it [03:56,  1.48it/s]Extractor Predicting: 360it [03:57,  1.45it/s]Extractor Predicting: 361it [03:57,  1.44it/s]Extractor Predicting: 362it [03:58,  1.46it/s]Extractor Predicting: 363it [03:59,  1.50it/s]Extractor Predicting: 364it [03:59,  1.51it/s]Extractor Predicting: 365it [04:00,  1.48it/s]Extractor Predicting: 366it [04:01,  1.45it/s]Extractor Predicting: 367it [04:01,  1.44it/s]Extractor Predicting: 368it [04:02,  1.43it/s]Extractor Predicting: 369it [04:03,  1.38it/s]Extractor Predicting: 370it [04:04,  1.38it/s]Extractor Predicting: 371it [04:04,  1.41it/s]Extractor Predicting: 372it [04:05,  1.67it/s]Extractor Predicting: 372it [04:05,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:03,835 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:03,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:03,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:03,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:03,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:14:04,443 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:14:04,445 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:14:05,024 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:14:06,069 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:14:06,069 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:08,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:08,935 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:08,935 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:08,935 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:08,935 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:14:09,575 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:14:09,576 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:14:10,157 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:14:10,315 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:14:10,315 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3673678809645972,
  "recall": 0.0803411131059246,
  "score": 0.13184789614216003,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.47it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:17,  1.47it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:21,  1.40it/s]Extractor Predicting: 33it [00:21,  1.45it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:23,  1.47it/s]Extractor Predicting: 37it [00:24,  1.46it/s]Extractor Predicting: 38it [00:25,  1.42it/s]Extractor Predicting: 39it [00:26,  1.39it/s]Extractor Predicting: 40it [00:26,  1.39it/s]Extractor Predicting: 41it [00:27,  1.39it/s]Extractor Predicting: 42it [00:28,  1.41it/s]Extractor Predicting: 43it [00:28,  1.41it/s]Extractor Predicting: 44it [00:29,  1.42it/s]Extractor Predicting: 45it [00:30,  1.40it/s]Extractor Predicting: 46it [00:30,  1.43it/s]Extractor Predicting: 47it [00:31,  1.42it/s]Extractor Predicting: 48it [00:32,  1.41it/s]Extractor Predicting: 49it [00:33,  1.41it/s]Extractor Predicting: 50it [00:33,  1.41it/s]Extractor Predicting: 51it [00:34,  1.39it/s]Extractor Predicting: 52it [00:35,  1.38it/s]Extractor Predicting: 53it [00:36,  1.40it/s]Extractor Predicting: 54it [00:36,  1.39it/s]Extractor Predicting: 55it [00:37,  1.43it/s]Extractor Predicting: 56it [00:38,  1.43it/s]Extractor Predicting: 57it [00:38,  1.42it/s]Extractor Predicting: 58it [00:39,  1.40it/s]Extractor Predicting: 59it [00:40,  1.40it/s]Extractor Predicting: 60it [00:40,  1.40it/s]Extractor Predicting: 61it [00:41,  1.36it/s]Extractor Predicting: 62it [00:42,  1.36it/s]Extractor Predicting: 63it [00:43,  1.48it/s]Extractor Predicting: 63it [00:43,  1.46it/s]
[INFO|configuration_utils.py:515] 2023-08-29 06:14:54,499 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:14:54,501 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 06:14:54,506 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:14:54,507 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 06:14:54,509 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 06:14:57,483 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 06:14:57,486 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 06:14:57,506 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:14:57,507 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 06:14:57,513 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:14:57,517 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:14:57,517 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:14:57,517 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:14:57,517 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:14:57,517 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:14:57,517 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6074766355140186,
  "recall": 0.058435720707222055,
  "score": 0.1066156369600875,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 06:14:57,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:14:58,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:14:59,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:14:59,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:00,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:01,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:02,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:02,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:03,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:04,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:04,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:05,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:06,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:06,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:07,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:08,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:09,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:09,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:10,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:11,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:11,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:12,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:13,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:13,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:57, 16.93s/it][WARNING|generation_utils.py:914] 2023-08-29 06:15:14,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:15,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:16,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:16,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:17,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:18,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:18,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:19,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:20,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:20,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:21,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:21,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:22,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:23,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:24,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:24,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:25,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:26,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:27,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:27,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:28,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:20, 15.40s/it][WARNING|generation_utils.py:914] 2023-08-29 06:15:29,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:29,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:30,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:30,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:31,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:31,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:32,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:33,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:33,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:34,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:34,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:35,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:35,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:36,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:36,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:37,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:38,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:38,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:39,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:39, 13.31s/it][WARNING|generation_utils.py:914] 2023-08-29 06:15:39,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:40,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:41,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:41,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:42,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:43,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:43,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:44,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:45,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:46,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:46,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:47,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:48,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:49,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:49,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:50,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:51,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:51,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:52,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:53,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:56<02:30, 13.72s/it][WARNING|generation_utils.py:914] 2023-08-29 06:15:54,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:54,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:55,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:56,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:56,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:57,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:58,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:58,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:59,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:00,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:00,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:01,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:02,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:02,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:03,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:04,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:04,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:05,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:06,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:06,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:07,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:08,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:08,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:09,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:12<02:24, 14.50s/it][WARNING|generation_utils.py:914] 2023-08-29 06:16:10,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:10,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:11,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:12,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:12,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:13,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:14,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:14,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:15,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:16,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:17,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:17,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:18,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:19,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:19,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:20,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:21,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:21,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:22,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:23,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:26<02:08, 14.30s/it][WARNING|generation_utils.py:914] 2023-08-29 06:16:23,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:24,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:25,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:25,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:26,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:27,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:27,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:28,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:29,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:29,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:30,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:31,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:31,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:32,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:33,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:33,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:34,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:35,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:35,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:38<01:49, 13.74s/it][WARNING|generation_utils.py:914] 2023-08-29 06:16:36,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:37,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:37,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:38,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:39,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:39,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:40,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:41,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:41,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:42,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:43,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:44,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:44,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:45,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:45,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:46,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:47,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:48,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:48,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:49,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:50,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:53<01:37, 13.91s/it][WARNING|generation_utils.py:914] 2023-08-29 06:16:50,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:51,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:52,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:52,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:53,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:54,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:54,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:55,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:56,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:57,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:57,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:58,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:59,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:00,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:00,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:01,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:02,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:03,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:03,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:04,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:05,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:06,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:09<01:27, 14.54s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:06,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:07,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:08,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:08,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:09,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:10,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:10,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:12,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:12,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:13,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:14,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:14,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:15,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:16,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:17,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:17,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:18,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:19,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:19,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:20,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:23<01:12, 14.55s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:21,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:21,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:22,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:23,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:23,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:24,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:24,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:25,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:25,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:26,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:27,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:27,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:28,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:28,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:29,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:29,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:30,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:31,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:31,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:34<00:53, 13.42s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:32,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:32,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:33,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:34,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:34,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:35,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:36,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:37,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:37,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:38,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:39,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:39,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:40,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:41,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:41,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:42,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:43,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:44,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:44,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:45,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:46,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:49<00:41, 13.80s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:46,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:47,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:48,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:49,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:49,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:50,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:51,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:51,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:52,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:53,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:54,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:54,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:55,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:56,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:56,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:57,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:58,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:58,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:59,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:00,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:01,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:01,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:04<00:28, 14.36s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:02,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:03,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:03,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:04,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:04,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:05,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:05,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:06,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:07,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:07,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:08,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:08,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:09,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:09,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:10,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:11,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:11,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:12,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:12,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:15<00:13, 13.32s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:13,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:14,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:15,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:15,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:16,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:17,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:18,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:19,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:19,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:20,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:21,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:22,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:23,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:23,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:24,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:25,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:26,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:26,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:27,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:28,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:29,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:32<00:00, 14.33s/it]Generating: 100%|██████████| 15/15 [03:32<00:00, 14.16s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:18:36,579 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:18:36,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:18:36,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:18:36,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:18:36,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:18:37,216 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:18:37,217 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:18:37,794 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:18:38,880 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:18:38,880 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:18:41,733 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:18:41,738 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:18:41,738 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:18:41,738 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:18:41,738 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:18:42,358 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:18:42,360 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:18:42,927 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:18:43,099 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:18:43,099 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 565, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7981770833333334, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.8973214285714286, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 285, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 348, 'raw': 352}
{'target': 600, 'success': 380, 'raw': 384}
{'target': 600, 'success': 412, 'raw': 416}
{'target': 600, 'success': 444, 'raw': 448}
{'target': 600, 'success': 476, 'raw': 480}
{'target': 600, 'success': 507, 'raw': 512}
{'target': 600, 'success': 537, 'raw': 544}
{'target': 600, 'success': 568, 'raw': 576}
{'target': 600, 'success': 600, 'raw': 608}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9868421052631579, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : work location .', 'success_rate': 0.7903645833333334, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 254, 'raw': 256}
{'target': 600, 'success': 285, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 348, 'raw': 352}
{'target': 600, 'success': 380, 'raw': 384}
{'target': 600, 'success': 412, 'raw': 416}
{'target': 600, 'success': 443, 'raw': 448}
{'target': 600, 'success': 475, 'raw': 480}
{'target': 600, 'success': 507, 'raw': 512}
{'target': 600, 'success': 539, 'raw': 544}
{'target': 600, 'success': 570, 'raw': 576}
{'target': 600, 'success': 602, 'raw': 608}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9901315789473685, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : creator .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8636363636363636, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 256, 'raw': 256}
{'target': 600, 'success': 288, 'raw': 288}
{'target': 600, 'success': 320, 'raw': 320}
{'target': 600, 'success': 352, 'raw': 352}
{'target': 600, 'success': 384, 'raw': 384}
{'target': 600, 'success': 416, 'raw': 416}
{'target': 600, 'success': 448, 'raw': 448}
{'target': 600, 'success': 480, 'raw': 480}
{'target': 600, 'success': 512, 'raw': 512}
{'target': 600, 'success': 544, 'raw': 544}
{'target': 600, 'success': 576, 'raw': 576}
{'target': 600, 'success': 608, 'raw': 608}
{'prompt': 'Relation : member of sports team .', 'success_rate': 1.0, 'errors': set()}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8764204545454546, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 256, 'raw': 256}
{'target': 600, 'success': 288, 'raw': 288}
{'target': 600, 'success': 320, 'raw': 320}
{'target': 600, 'success': 352, 'raw': 352}
{'target': 600, 'success': 384, 'raw': 384}
{'target': 600, 'success': 416, 'raw': 416}
{'target': 600, 'success': 447, 'raw': 448}
{'target': 600, 'success': 479, 'raw': 480}
{'target': 600, 'success': 511, 'raw': 512}
{'target': 600, 'success': 543, 'raw': 544}
{'target': 600, 'success': 575, 'raw': 576}
{'target': 600, 'success': 607, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9983552631578947, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 8304
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8404, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.48it/s]Extractor Estimating: 3it [00:01,  1.52it/s]Extractor Estimating: 4it [00:02,  1.58it/s]Extractor Estimating: 5it [00:03,  1.60it/s]Extractor Estimating: 6it [00:03,  1.63it/s]Extractor Estimating: 7it [00:04,  1.60it/s]Extractor Estimating: 8it [00:05,  1.63it/s]Extractor Estimating: 9it [00:05,  1.63it/s]Extractor Estimating: 10it [00:06,  1.61it/s]Extractor Estimating: 11it [00:06,  1.66it/s]Extractor Estimating: 12it [00:07,  1.54it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.63it/s]Extractor Estimating: 16it [00:10,  1.61it/s]Extractor Estimating: 17it [00:10,  1.62it/s]Extractor Estimating: 18it [00:11,  1.62it/s]Extractor Estimating: 19it [00:11,  1.64it/s]Extractor Estimating: 20it [00:12,  1.60it/s]Extractor Estimating: 21it [00:13,  1.62it/s]Extractor Estimating: 22it [00:13,  1.65it/s]Extractor Estimating: 23it [00:14,  1.63it/s]Extractor Estimating: 24it [00:14,  1.62it/s]Extractor Estimating: 25it [00:15,  1.64it/s]Extractor Estimating: 26it [00:16,  1.57it/s]Extractor Estimating: 27it [00:17,  1.43it/s]Extractor Estimating: 28it [00:17,  1.48it/s]Extractor Estimating: 29it [00:18,  1.52it/s]Extractor Estimating: 30it [00:19,  1.47it/s]Extractor Estimating: 31it [00:19,  1.55it/s]Extractor Estimating: 32it [00:20,  1.58it/s]Extractor Estimating: 33it [00:20,  1.59it/s]Extractor Estimating: 34it [00:21,  1.53it/s]Extractor Estimating: 35it [00:22,  1.57it/s]Extractor Estimating: 36it [00:22,  1.58it/s]Extractor Estimating: 37it [00:23,  1.61it/s]Extractor Estimating: 38it [00:23,  1.65it/s]Extractor Estimating: 39it [00:24,  1.57it/s]Extractor Estimating: 40it [00:25,  1.58it/s]Extractor Estimating: 41it [00:25,  1.59it/s]Extractor Estimating: 42it [00:26,  1.50it/s]Extractor Estimating: 43it [00:27,  1.50it/s]Extractor Estimating: 44it [00:27,  1.47it/s]Extractor Estimating: 45it [00:28,  1.52it/s]Extractor Estimating: 46it [00:29,  1.56it/s]Extractor Estimating: 47it [00:29,  1.59it/s]Extractor Estimating: 48it [00:30,  1.62it/s]Extractor Estimating: 49it [00:30,  1.64it/s]Extractor Estimating: 50it [00:31,  1.58it/s]Extractor Estimating: 51it [00:32,  1.71it/s]Extractor Estimating: 52it [00:32,  1.84it/s]Extractor Estimating: 53it [00:33,  1.91it/s]Extractor Estimating: 54it [00:33,  1.98it/s]Extractor Estimating: 55it [00:33,  2.04it/s]Extractor Estimating: 56it [00:34,  2.07it/s]Extractor Estimating: 57it [00:34,  2.02it/s]Extractor Estimating: 58it [00:35,  2.07it/s]Extractor Estimating: 59it [00:35,  2.08it/s]Extractor Estimating: 60it [00:36,  2.08it/s]Extractor Estimating: 61it [00:36,  2.12it/s]Extractor Estimating: 62it [00:37,  2.08it/s]Extractor Estimating: 63it [00:37,  2.11it/s]Extractor Estimating: 64it [00:38,  2.15it/s]Extractor Estimating: 65it [00:38,  2.17it/s]Extractor Estimating: 66it [00:39,  2.16it/s]Extractor Estimating: 67it [00:39,  2.14it/s]Extractor Estimating: 68it [00:40,  2.19it/s]Extractor Estimating: 69it [00:40,  2.15it/s]Extractor Estimating: 70it [00:41,  2.17it/s]Extractor Estimating: 71it [00:41,  2.13it/s]Extractor Estimating: 72it [00:41,  2.10it/s]Extractor Estimating: 73it [00:42,  2.14it/s]Extractor Estimating: 74it [00:42,  2.18it/s]Extractor Estimating: 75it [00:43,  2.18it/s]Extractor Estimating: 76it [00:43,  1.97it/s]Extractor Estimating: 77it [00:44,  1.78it/s]Extractor Estimating: 78it [00:45,  1.74it/s]Extractor Estimating: 79it [00:45,  1.69it/s]Extractor Estimating: 80it [00:46,  1.66it/s]Extractor Estimating: 81it [00:47,  1.65it/s]Extractor Estimating: 82it [00:47,  1.68it/s]Extractor Estimating: 83it [00:48,  1.61it/s]Extractor Estimating: 84it [00:48,  1.64it/s]Extractor Estimating: 85it [00:49,  1.58it/s]Extractor Estimating: 86it [00:50,  1.47it/s]Extractor Estimating: 87it [00:51,  1.50it/s]Extractor Estimating: 88it [00:51,  1.53it/s]Extractor Estimating: 89it [00:52,  1.54it/s]Extractor Estimating: 90it [00:52,  1.56it/s]Extractor Estimating: 91it [00:53,  1.59it/s]Extractor Estimating: 92it [00:54,  1.56it/s]Extractor Estimating: 93it [00:54,  1.59it/s]Extractor Estimating: 94it [00:55,  1.63it/s]Extractor Estimating: 95it [00:55,  1.64it/s]Extractor Estimating: 96it [00:56,  1.62it/s]Extractor Estimating: 97it [00:57,  1.60it/s]Extractor Estimating: 98it [00:57,  1.61it/s]Extractor Estimating: 99it [00:58,  1.56it/s]Extractor Estimating: 100it [00:59,  1.55it/s]Extractor Estimating: 101it [00:59,  1.59it/s]Extractor Estimating: 102it [01:00,  1.59it/s]Extractor Estimating: 103it [01:01,  1.61it/s]Extractor Estimating: 104it [01:01,  1.63it/s]Extractor Estimating: 105it [01:02,  1.49it/s]Extractor Estimating: 106it [01:03,  1.50it/s]Extractor Estimating: 107it [01:03,  1.54it/s]Extractor Estimating: 108it [01:04,  1.58it/s]Extractor Estimating: 109it [01:04,  1.56it/s]Extractor Estimating: 110it [01:05,  1.59it/s]Extractor Estimating: 111it [01:06,  1.61it/s]Extractor Estimating: 112it [01:06,  1.60it/s]Extractor Estimating: 113it [01:07,  1.64it/s]Extractor Estimating: 114it [01:07,  1.66it/s]Extractor Estimating: 115it [01:08,  1.66it/s]Extractor Estimating: 116it [01:09,  1.67it/s]Extractor Estimating: 117it [01:09,  1.67it/s]Extractor Estimating: 118it [01:10,  1.65it/s]Extractor Estimating: 119it [01:11,  1.58it/s]Extractor Estimating: 120it [01:11,  1.61it/s]Extractor Estimating: 121it [01:12,  1.60it/s]Extractor Estimating: 122it [01:12,  1.60it/s]Extractor Estimating: 123it [01:13,  1.61it/s]Extractor Estimating: 124it [01:14,  1.62it/s]Extractor Estimating: 125it [01:14,  1.62it/s]Extractor Estimating: 126it [01:15,  1.60it/s]Extractor Estimating: 127it [01:16,  1.55it/s]Extractor Estimating: 128it [01:16,  1.58it/s]Extractor Estimating: 129it [01:17,  1.58it/s]Extractor Estimating: 130it [01:18,  1.48it/s]Extractor Estimating: 131it [01:18,  1.47it/s]Extractor Estimating: 132it [01:19,  1.53it/s]Extractor Estimating: 133it [01:20,  1.55it/s]Extractor Estimating: 134it [01:20,  1.58it/s]Extractor Estimating: 135it [01:21,  1.57it/s]Extractor Estimating: 136it [01:21,  1.54it/s]Extractor Estimating: 137it [01:22,  1.55it/s]Extractor Estimating: 138it [01:23,  1.53it/s]Extractor Estimating: 139it [01:23,  1.55it/s]Extractor Estimating: 140it [01:24,  1.56it/s]Extractor Estimating: 141it [01:25,  1.54it/s]Extractor Estimating: 142it [01:25,  1.53it/s]Extractor Estimating: 143it [01:26,  1.54it/s]Extractor Estimating: 144it [01:27,  1.56it/s]Extractor Estimating: 145it [01:27,  1.55it/s]Extractor Estimating: 146it [01:28,  1.55it/s]Extractor Estimating: 147it [01:29,  1.54it/s]Extractor Estimating: 148it [01:29,  1.53it/s]Extractor Estimating: 149it [01:30,  1.53it/s]Extractor Estimating: 150it [01:31,  1.55it/s]Extractor Estimating: 151it [01:31,  1.54it/s]Extractor Estimating: 152it [01:32,  1.55it/s]Extractor Estimating: 153it [01:32,  1.54it/s]Extractor Estimating: 154it [01:33,  1.54it/s]Extractor Estimating: 155it [01:34,  1.53it/s]Extractor Estimating: 156it [01:34,  1.51it/s]Extractor Estimating: 157it [01:35,  1.49it/s]Extractor Estimating: 158it [01:36,  1.54it/s]Extractor Estimating: 159it [01:36,  1.52it/s]Extractor Estimating: 160it [01:37,  1.54it/s]Extractor Estimating: 161it [01:38,  1.55it/s]Extractor Estimating: 162it [01:38,  1.55it/s]Extractor Estimating: 163it [01:39,  1.59it/s]Extractor Estimating: 164it [01:40,  1.60it/s]Extractor Estimating: 165it [01:40,  1.58it/s]Extractor Estimating: 166it [01:41,  1.57it/s]Extractor Estimating: 167it [01:42,  1.55it/s]Extractor Estimating: 168it [01:42,  1.45it/s]Extractor Estimating: 169it [01:43,  1.48it/s]Extractor Estimating: 170it [01:44,  1.47it/s]Extractor Estimating: 171it [01:44,  1.47it/s]Extractor Estimating: 172it [01:45,  1.50it/s]Extractor Estimating: 173it [01:46,  1.50it/s]Extractor Estimating: 174it [01:46,  1.49it/s]Extractor Estimating: 175it [01:47,  1.50it/s]Extractor Estimating: 176it [01:48,  1.57it/s]Extractor Estimating: 177it [01:48,  1.55it/s]Extractor Estimating: 178it [01:49,  1.56it/s]Extractor Estimating: 179it [01:49,  1.60it/s]Extractor Estimating: 180it [01:50,  1.68it/s]Extractor Estimating: 181it [01:51,  1.71it/s]Extractor Estimating: 182it [01:51,  1.67it/s]Extractor Estimating: 183it [01:52,  1.69it/s]Extractor Estimating: 184it [01:52,  1.65it/s]Extractor Estimating: 185it [01:53,  1.64it/s]Extractor Estimating: 186it [01:54,  1.60it/s]Extractor Estimating: 187it [01:54,  1.54it/s]Extractor Estimating: 188it [01:55,  1.59it/s]Extractor Estimating: 189it [01:56,  1.59it/s]Extractor Estimating: 190it [01:56,  1.61it/s]Extractor Estimating: 191it [01:57,  1.65it/s]Extractor Estimating: 192it [01:57,  1.66it/s]Extractor Estimating: 193it [01:58,  1.67it/s]Extractor Estimating: 194it [01:59,  1.63it/s]Extractor Estimating: 195it [01:59,  1.64it/s]Extractor Estimating: 196it [02:00,  1.60it/s]Extractor Estimating: 197it [02:00,  1.62it/s]Extractor Estimating: 198it [02:01,  1.60it/s]Extractor Estimating: 199it [02:02,  1.65it/s]Extractor Estimating: 200it [02:02,  1.64it/s]Extractor Estimating: 201it [02:03,  1.59it/s]Extractor Estimating: 202it [02:04,  1.56it/s]Extractor Estimating: 203it [02:04,  1.56it/s]Extractor Estimating: 204it [02:05,  1.54it/s]Extractor Estimating: 205it [02:06,  1.54it/s]Extractor Estimating: 206it [02:06,  1.54it/s]Extractor Estimating: 207it [02:07,  1.53it/s]Extractor Estimating: 208it [02:08,  1.52it/s]Extractor Estimating: 209it [02:08,  1.54it/s]Extractor Estimating: 210it [02:09,  1.54it/s]Extractor Estimating: 211it [02:09,  1.55it/s]Extractor Estimating: 212it [02:10,  1.54it/s]Extractor Estimating: 213it [02:11,  1.54it/s]Extractor Estimating: 214it [02:11,  1.50it/s]Extractor Estimating: 215it [02:12,  1.52it/s]Extractor Estimating: 216it [02:13,  1.52it/s]Extractor Estimating: 217it [02:13,  1.52it/s]Extractor Estimating: 218it [02:14,  1.53it/s]Extractor Estimating: 219it [02:15,  1.55it/s]Extractor Estimating: 220it [02:15,  1.47it/s]Extractor Estimating: 221it [02:16,  1.50it/s]Extractor Estimating: 222it [02:17,  1.51it/s]Extractor Estimating: 223it [02:17,  1.50it/s]Extractor Estimating: 224it [02:18,  1.51it/s]Extractor Estimating: 225it [02:19,  1.52it/s]Extractor Estimating: 226it [02:19,  1.52it/s]Extractor Estimating: 227it [02:20,  1.49it/s]Extractor Estimating: 228it [02:21,  1.50it/s]Extractor Estimating: 229it [02:21,  1.48it/s]Extractor Estimating: 230it [02:22,  1.49it/s]Extractor Estimating: 231it [02:23,  1.53it/s]Extractor Estimating: 232it [02:23,  1.56it/s]Extractor Estimating: 233it [02:24,  1.56it/s]Extractor Estimating: 234it [02:25,  1.52it/s]Extractor Estimating: 235it [02:25,  1.47it/s]Extractor Estimating: 236it [02:26,  1.48it/s]Extractor Estimating: 237it [02:27,  1.49it/s]Extractor Estimating: 238it [02:27,  1.52it/s]Extractor Estimating: 239it [02:28,  1.53it/s]Extractor Estimating: 240it [02:29,  1.49it/s]Extractor Estimating: 241it [02:29,  1.51it/s]Extractor Estimating: 242it [02:30,  1.52it/s]Extractor Estimating: 243it [02:31,  1.50it/s]Extractor Estimating: 244it [02:31,  1.45it/s]Extractor Estimating: 245it [02:32,  1.43it/s]Extractor Estimating: 246it [02:33,  1.44it/s]Extractor Estimating: 247it [02:33,  1.49it/s]Extractor Estimating: 248it [02:34,  1.37it/s]Extractor Estimating: 249it [02:35,  1.43it/s]Extractor Estimating: 250it [02:36,  1.44it/s]Extractor Estimating: 251it [02:36,  1.47it/s]Extractor Estimating: 252it [02:37,  1.46it/s]Extractor Estimating: 253it [02:38,  1.48it/s]Extractor Estimating: 254it [02:38,  1.49it/s]Extractor Estimating: 255it [02:39,  1.50it/s]Extractor Estimating: 256it [02:40,  1.51it/s]Extractor Estimating: 257it [02:40,  1.52it/s]Extractor Estimating: 258it [02:41,  1.53it/s]Extractor Estimating: 259it [02:42,  1.53it/s]Extractor Estimating: 260it [02:42,  1.54it/s]Extractor Estimating: 261it [02:43,  1.54it/s]Extractor Estimating: 262it [02:43,  1.55it/s]Extractor Estimating: 263it [02:44,  1.52it/s]Extractor Estimating: 264it [02:45,  1.52it/s]Extractor Estimating: 265it [02:45,  1.53it/s]Extractor Estimating: 266it [02:46,  1.54it/s]Extractor Estimating: 267it [02:47,  1.54it/s]Extractor Estimating: 268it [02:47,  1.55it/s]Extractor Estimating: 269it [02:48,  1.54it/s]Extractor Estimating: 270it [02:49,  1.54it/s]Extractor Estimating: 271it [02:49,  1.54it/s]Extractor Estimating: 272it [02:50,  1.55it/s]Extractor Estimating: 273it [02:51,  1.55it/s]Extractor Estimating: 274it [02:51,  1.55it/s]Extractor Estimating: 275it [02:52,  1.54it/s]Extractor Estimating: 276it [02:53,  1.53it/s]Extractor Estimating: 277it [02:53,  1.51it/s]Extractor Estimating: 278it [02:54,  1.52it/s]Extractor Estimating: 279it [02:55,  1.54it/s]Extractor Estimating: 280it [02:55,  1.52it/s]Extractor Estimating: 281it [02:56,  1.53it/s]Extractor Estimating: 282it [02:56,  1.53it/s]Extractor Estimating: 283it [02:57,  1.53it/s]Extractor Estimating: 284it [02:58,  1.53it/s]Extractor Estimating: 285it [02:58,  1.53it/s]Extractor Estimating: 286it [02:59,  1.55it/s]Extractor Estimating: 287it [03:00,  1.53it/s]Extractor Estimating: 288it [03:00,  1.55it/s]Extractor Estimating: 289it [03:01,  1.53it/s]Extractor Estimating: 290it [03:02,  1.52it/s]Extractor Estimating: 291it [03:02,  1.47it/s]Extractor Estimating: 292it [03:03,  1.44it/s]Extractor Estimating: 293it [03:04,  1.49it/s]Extractor Estimating: 294it [03:04,  1.50it/s]Extractor Estimating: 295it [03:05,  1.55it/s]Extractor Estimating: 296it [03:06,  1.53it/s]Extractor Estimating: 297it [03:06,  1.49it/s]Extractor Estimating: 298it [03:07,  1.50it/s]Extractor Estimating: 299it [03:08,  1.51it/s]Extractor Estimating: 300it [03:08,  1.53it/s]Extractor Estimating: 301it [03:09,  1.57it/s]Extractor Estimating: 302it [03:10,  1.57it/s]Extractor Estimating: 303it [03:10,  1.55it/s]Extractor Estimating: 304it [03:11,  1.60it/s]Extractor Estimating: 305it [03:12,  1.59it/s]Extractor Estimating: 306it [03:12,  1.58it/s]Extractor Estimating: 307it [03:13,  1.63it/s]Extractor Estimating: 308it [03:13,  1.62it/s]Extractor Estimating: 309it [03:14,  1.62it/s]Extractor Estimating: 310it [03:15,  1.59it/s]Extractor Estimating: 311it [03:15,  1.53it/s]Extractor Estimating: 312it [03:16,  1.57it/s]Extractor Estimating: 313it [03:17,  1.57it/s]Extractor Estimating: 314it [03:17,  1.56it/s]Extractor Estimating: 315it [03:18,  1.55it/s]Extractor Estimating: 316it [03:19,  1.55it/s]Extractor Estimating: 317it [03:19,  1.56it/s]Extractor Estimating: 318it [03:20,  1.56it/s]Extractor Estimating: 319it [03:20,  1.57it/s]Extractor Estimating: 320it [03:21,  1.58it/s]Extractor Estimating: 321it [03:22,  1.59it/s]Extractor Estimating: 322it [03:22,  1.47it/s]Extractor Estimating: 323it [03:23,  1.54it/s]Extractor Estimating: 324it [03:24,  1.55it/s]Extractor Estimating: 325it [03:24,  1.59it/s]Extractor Estimating: 326it [03:25,  1.55it/s]Extractor Estimating: 327it [03:26,  1.54it/s]Extractor Estimating: 328it [03:26,  1.53it/s]Extractor Estimating: 329it [03:27,  1.53it/s]Extractor Estimating: 330it [03:28,  1.52it/s]Extractor Estimating: 331it [03:28,  1.53it/s]Extractor Estimating: 332it [03:29,  1.51it/s]Extractor Estimating: 333it [03:30,  1.50it/s]Extractor Estimating: 334it [03:30,  1.52it/s]Extractor Estimating: 335it [03:31,  1.53it/s]Extractor Estimating: 336it [03:32,  1.54it/s]Extractor Estimating: 337it [03:32,  1.53it/s]Extractor Estimating: 338it [03:33,  1.52it/s]Extractor Estimating: 339it [03:34,  1.51it/s]Extractor Estimating: 340it [03:34,  1.52it/s]Extractor Estimating: 341it [03:35,  1.53it/s]Extractor Estimating: 342it [03:35,  1.53it/s]Extractor Estimating: 343it [03:36,  1.53it/s]Extractor Estimating: 344it [03:37,  1.53it/s]Extractor Estimating: 345it [03:37,  1.53it/s]Extractor Estimating: 346it [03:38,  1.54it/s]Extractor Estimating: 347it [03:39,  1.53it/s]Extractor Estimating: 348it [03:39,  1.54it/s]Extractor Estimating: 349it [03:40,  1.56it/s]Extractor Estimating: 350it [03:41,  1.60it/s]Extractor Estimating: 351it [03:41,  1.63it/s]Extractor Estimating: 352it [03:42,  1.65it/s]Extractor Estimating: 353it [03:42,  1.70it/s]Extractor Estimating: 354it [03:43,  1.58it/s]Extractor Estimating: 355it [03:44,  1.64it/s]Extractor Estimating: 356it [03:44,  1.66it/s]Extractor Estimating: 357it [03:45,  1.69it/s]Extractor Estimating: 358it [03:45,  1.67it/s]Extractor Estimating: 359it [03:46,  1.68it/s]Extractor Estimating: 360it [03:47,  1.70it/s]Extractor Estimating: 361it [03:47,  1.69it/s]Extractor Estimating: 362it [03:48,  1.64it/s]Extractor Estimating: 363it [03:48,  1.68it/s]Extractor Estimating: 364it [03:49,  1.70it/s]Extractor Estimating: 365it [03:49,  1.77it/s]Extractor Estimating: 366it [03:50,  1.74it/s]Extractor Estimating: 367it [03:51,  1.76it/s]Extractor Estimating: 368it [03:51,  1.74it/s]Extractor Estimating: 369it [03:52,  1.73it/s]Extractor Estimating: 370it [03:52,  1.69it/s]Extractor Estimating: 371it [03:53,  1.71it/s]Extractor Estimating: 372it [03:53,  1.75it/s]Extractor Estimating: 373it [03:54,  1.66it/s]Extractor Estimating: 374it [03:55,  1.65it/s]Extractor Estimating: 374it [03:55,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:22:55,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:22:55,372 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:22:55,373 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:22:55,373 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:22:55,373 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:22:55,999 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:22:56,000 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:22:56,588 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:22:57,637 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:22:57,637 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:23:00,489 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:23:00,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:23:00,497 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:23:00,497 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:23:00,497 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:23:01,149 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:23:01,150 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:23:01,708 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:23:01,873 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:23:01,873 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 08:53:05,746 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 08:53:05,771 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7474 mean pseudo reward: 0.9435213418692309
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 16609
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16709, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16709, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.104, loss:387.2553
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.119, loss:358.3335
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.100, loss:347.2648
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.093, loss:328.1405
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.111, loss:343.3082
>> valid entity prec:0.5354, rec:0.5742, f1:0.5541
>> valid relation prec:0.2569, rec:0.0574, f1:0.0938
>> valid relation with NER prec:0.2569, rec:0.0574, f1:0.0938
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.871, loss:341.6864
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.096, loss:332.4174
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.114, loss:327.8609
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.105, loss:324.2852
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.111, loss:331.7121
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5885, rec:0.5485, f1:0.5678
>> valid relation prec:0.2493, rec:0.0978, f1:0.1405
>> valid relation with NER prec:0.2493, rec:0.0978, f1:0.1405
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.850, loss:330.4211
g_step 1200, step 264, avg_time 1.104, loss:323.6453
g_step 1300, step 52, avg_time 1.098, loss:303.2398
g_step 1400, step 152, avg_time 1.105, loss:313.0898
g_step 1500, step 252, avg_time 1.102, loss:325.8897
>> valid entity prec:0.5694, rec:0.5667, f1:0.5680
>> valid relation prec:0.2083, rec:0.0898, f1:0.1255
>> valid relation with NER prec:0.2083, rec:0.0898, f1:0.1255
new max entity f1 on valid!
g_step 1600, step 40, avg_time 2.848, loss:303.5395
g_step 1700, step 140, avg_time 1.097, loss:297.4566
g_step 1800, step 240, avg_time 1.112, loss:291.0773
g_step 1900, step 28, avg_time 1.109, loss:296.5721
g_step 2000, step 128, avg_time 1.113, loss:277.2676
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5586, rec:0.6647, f1:0.6070
>> valid relation prec:0.1741, rec:0.0820, f1:0.1115
>> valid relation with NER prec:0.1741, rec:0.0820, f1:0.1115
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.848, loss:300.2710
g_step 2200, step 16, avg_time 1.098, loss:289.3082
g_step 2300, step 116, avg_time 1.107, loss:274.2083
g_step 2400, step 216, avg_time 1.100, loss:276.6068
g_step 2500, step 4, avg_time 1.119, loss:271.2985
>> valid entity prec:0.5487, rec:0.5801, f1:0.5640
>> valid relation prec:0.2228, rec:0.1058, f1:0.1434
>> valid relation with NER prec:0.2228, rec:0.1058, f1:0.1434
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 104, avg_time 2.858, loss:251.0596
g_step 2700, step 204, avg_time 1.106, loss:274.5612
g_step 2800, step 304, avg_time 1.103, loss:266.7707
g_step 2900, step 92, avg_time 1.100, loss:252.8897
g_step 3000, step 192, avg_time 1.099, loss:248.1147
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5743, rec:0.5291, f1:0.5508
>> valid relation prec:0.1282, rec:0.0646, f1:0.0859
>> valid relation with NER prec:0.1282, rec:0.0646, f1:0.0859
g_step 3100, step 292, avg_time 2.861, loss:267.0871
g_step 3200, step 80, avg_time 1.093, loss:238.3988
g_step 3300, step 180, avg_time 1.111, loss:248.4298
g_step 3400, step 280, avg_time 1.095, loss:251.9123
g_step 3500, step 68, avg_time 1.100, loss:230.2376
>> valid entity prec:0.5593, rec:0.5875, f1:0.5730
>> valid relation prec:0.1951, rec:0.0732, f1:0.1064
>> valid relation with NER prec:0.1951, rec:0.0732, f1:0.1064
g_step 3600, step 168, avg_time 2.871, loss:240.6877
g_step 3700, step 268, avg_time 1.117, loss:246.0933
g_step 3800, step 56, avg_time 1.099, loss:229.7119
g_step 3900, step 156, avg_time 1.112, loss:225.1322
g_step 4000, step 256, avg_time 1.109, loss:228.4326
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5681, rec:0.5194, f1:0.5427
>> valid relation prec:0.2027, rec:0.0974, f1:0.1315
>> valid relation with NER prec:0.2027, rec:0.0974, f1:0.1315
g_step 4100, step 44, avg_time 2.845, loss:220.6369
g_step 4200, step 144, avg_time 1.101, loss:215.2405
g_step 4300, step 244, avg_time 1.106, loss:215.5960
g_step 4400, step 32, avg_time 1.127, loss:213.4001
g_step 4500, step 132, avg_time 1.113, loss:215.8099
>> valid entity prec:0.5647, rec:0.5148, f1:0.5386
>> valid relation prec:0.2018, rec:0.0818, f1:0.1164
>> valid relation with NER prec:0.2018, rec:0.0818, f1:0.1164
g_step 4600, step 232, avg_time 2.853, loss:218.6611
g_step 4700, step 20, avg_time 1.096, loss:218.1440
g_step 4800, step 120, avg_time 1.113, loss:197.7598
g_step 4900, step 220, avg_time 1.105, loss:220.4862
g_step 5000, step 8, avg_time 1.115, loss:204.9056
learning rate was adjusted to 0.0008
>> valid entity prec:0.5599, rec:0.4699, f1:0.5110
>> valid relation prec:0.1994, rec:0.0916, f1:0.1255
>> valid relation with NER prec:0.1994, rec:0.0916, f1:0.1255
g_step 5100, step 108, avg_time 2.860, loss:189.6061
g_step 5200, step 208, avg_time 1.109, loss:204.2230
g_step 5300, step 308, avg_time 1.105, loss:207.1563
g_step 5400, step 96, avg_time 1.097, loss:179.5843
g_step 5500, step 196, avg_time 1.104, loss:182.9630
>> valid entity prec:0.5608, rec:0.5241, f1:0.5418
>> valid relation prec:0.1939, rec:0.0818, f1:0.1150
>> valid relation with NER prec:0.1939, rec:0.0818, f1:0.1150
g_step 5600, step 296, avg_time 2.868, loss:202.7747
g_step 5700, step 84, avg_time 1.104, loss:182.5121
g_step 5800, step 184, avg_time 1.097, loss:189.9603
g_step 5900, step 284, avg_time 1.112, loss:186.9582
g_step 6000, step 72, avg_time 1.106, loss:183.3964
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5693, rec:0.5296, f1:0.5487
>> valid relation prec:0.1669, rec:0.0877, f1:0.1150
>> valid relation with NER prec:0.1669, rec:0.0877, f1:0.1150
g_step 6100, step 172, avg_time 2.855, loss:176.8258
g_step 6200, step 272, avg_time 1.107, loss:193.0923
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:53:05 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:53:05 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-53-05_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:53:06 - WARNING - datasets.builder -   Using custom data configuration default-0d7d16ce33d09181
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-0d7d16ce33d09181/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:53:07,046 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:53:07,047 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:53:07,048 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:53:07,049 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:53:07,058 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:53:07,061 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:53:07,061 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:53:07,061 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:53:07,061 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:53:07,061 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:53:07,061 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:53:07,191 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:53:10,273 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:53:10,276 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-0d7d16ce33d09181/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.21ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.04ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.42ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.78ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.12ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.38ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.55ba/s]100%|██████████| 8/8 [00:01<00:00,  5.47ba/s]100%|██████████| 8/8 [00:01<00:00,  4.57ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.20ba/s] 40%|████      | 2/5 [00:00<00:00,  4.51ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.59ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.62ba/s]100%|██████████| 5/5 [00:01<00:00,  4.84ba/s]100%|██████████| 5/5 [00:01<00:00,  4.69ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.75ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.04ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.15ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.47ba/s]100%|██████████| 8/8 [00:00<00:00, 10.96ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.87ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.97ba/s]100%|██████████| 5/5 [00:00<00:00, 10.55ba/s]100%|██████████| 5/5 [00:00<00:00, 10.33ba/s]
[INFO|trainer.py:414] 2023-08-29 08:53:14,676 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:53:14,685 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:53:14,686 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 08:53:14,686 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:53:14,686 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:53:14,686 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:53:14,686 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:53:14,686 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:52,  3.39it/s]  0%|          | 2/585 [00:00<02:49,  3.45it/s]  1%|          | 3/585 [00:00<02:47,  3.47it/s]  1%|          | 4/585 [00:01<02:47,  3.48it/s]  1%|          | 5/585 [00:01<02:46,  3.48it/s]  1%|          | 6/585 [00:01<02:46,  3.48it/s]  1%|          | 7/585 [00:02<02:48,  3.42it/s]  1%|▏         | 8/585 [00:02<02:47,  3.44it/s]  2%|▏         | 9/585 [00:02<02:46,  3.46it/s]  2%|▏         | 10/585 [00:02<02:45,  3.47it/s]  2%|▏         | 11/585 [00:03<02:45,  3.47it/s]  2%|▏         | 12/585 [00:03<02:44,  3.48it/s]  2%|▏         | 13/585 [00:03<02:44,  3.48it/s]  2%|▏         | 14/585 [00:04<02:44,  3.48it/s]  3%|▎         | 15/585 [00:04<02:43,  3.48it/s]  3%|▎         | 16/585 [00:04<02:43,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.47it/s]  3%|▎         | 18/585 [00:05<02:43,  3.46it/s]  3%|▎         | 19/585 [00:05<02:43,  3.47it/s]  3%|▎         | 20/585 [00:05<02:42,  3.47it/s]  4%|▎         | 21/585 [00:06<02:42,  3.48it/s]  4%|▍         | 22/585 [00:06<02:41,  3.48it/s]  4%|▍         | 23/585 [00:06<02:41,  3.48it/s]  4%|▍         | 24/585 [00:06<02:40,  3.49it/s]  4%|▍         | 25/585 [00:07<02:40,  3.49it/s]  4%|▍         | 26/585 [00:07<02:40,  3.48it/s]  5%|▍         | 27/585 [00:07<02:40,  3.48it/s]  5%|▍         | 28/585 [00:08<02:40,  3.47it/s]  5%|▍         | 29/585 [00:08<02:40,  3.46it/s]  5%|▌         | 30/585 [00:08<02:40,  3.47it/s]  5%|▌         | 31/585 [00:08<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.47it/s]  6%|▌         | 33/585 [00:09<02:39,  3.47it/s]  6%|▌         | 34/585 [00:09<02:39,  3.47it/s]  6%|▌         | 35/585 [00:10<02:38,  3.47it/s]  6%|▌         | 36/585 [00:10<02:38,  3.47it/s]  6%|▋         | 37/585 [00:10<02:37,  3.47it/s]  6%|▋         | 38/585 [00:10<02:37,  3.47it/s]  7%|▋         | 39/585 [00:11<02:37,  3.47it/s]  7%|▋         | 40/585 [00:11<02:37,  3.46it/s]  7%|▋         | 41/585 [00:11<02:37,  3.46it/s]  7%|▋         | 42/585 [00:12<02:36,  3.47it/s]  7%|▋         | 43/585 [00:12<02:36,  3.46it/s]  8%|▊         | 44/585 [00:12<02:35,  3.47it/s]  8%|▊         | 45/585 [00:12<02:35,  3.47it/s]  8%|▊         | 46/585 [00:13<02:35,  3.47it/s]  8%|▊         | 47/585 [00:13<02:34,  3.47it/s]  8%|▊         | 48/585 [00:13<02:34,  3.47it/s]  8%|▊         | 49/585 [00:14<02:34,  3.47it/s]  9%|▊         | 50/585 [00:14<02:34,  3.47it/s]  9%|▊         | 51/585 [00:14<02:34,  3.46it/s]  9%|▉         | 52/585 [00:14<02:34,  3.46it/s]  9%|▉         | 53/585 [00:15<02:33,  3.45it/s]  9%|▉         | 54/585 [00:15<02:33,  3.46it/s]  9%|▉         | 55/585 [00:15<02:33,  3.46it/s] 10%|▉         | 56/585 [00:16<02:32,  3.46it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:31,  3.47it/s] 10%|█         | 59/585 [00:17<02:31,  3.47it/s] 10%|█         | 60/585 [00:17<02:31,  3.47it/s] 10%|█         | 61/585 [00:17<02:31,  3.47it/s] 11%|█         | 62/585 [00:17<02:31,  3.46it/s] 11%|█         | 63/585 [00:18<02:30,  3.46it/s] 11%|█         | 64/585 [00:18<02:30,  3.46it/s] 11%|█         | 65/585 [00:18<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.46it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 69/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.46it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 76/585 [00:21<02:26,  3.47it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.46it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 79/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:23<02:24,  3.47it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:25<02:23,  3.46it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.47it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.45it/s] 17%|█▋        | 97/585 [00:27<02:21,  3.45it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.45it/s] 17%|█▋        | 101/585 [00:29<02:20,  3.45it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.46it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.45it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:32<02:15,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.47it/s] 20%|██        | 117/585 [00:33<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 08:53:48,510 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:53:48,510 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 08:53:48,510 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.31it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.36it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.73it/s][A
  4%|▍         | 23/611 [00:00<00:12, 48.01it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.48it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.29it/s][A
  6%|▌         | 38/611 [00:00<00:12, 47.08it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.58it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.66it/s][A
  9%|▊         | 53/611 [00:01<00:11, 46.73it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.74it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.65it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.64it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.76it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.70it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.68it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.53it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.37it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.56it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.61it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.61it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.64it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.70it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.72it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.61it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.49it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.41it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.55it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.55it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.61it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.55it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.60it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.71it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.72it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.63it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.46it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.36it/s][A
 32%|███▏      | 193/611 [00:04<00:08, 46.50it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.60it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.64it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.66it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.65it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.58it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.57it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.52it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.51it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.61it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.63it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.59it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.48it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.62it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.57it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.55it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.52it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.57it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.50it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.55it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.44it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.48it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.55it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.58it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.50it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.54it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.46it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.50it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.58it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.48it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.52it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.62it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.56it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.55it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.52it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.42it/s][A
 61%|██████    | 373/611 [00:07<00:05, 46.56it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.57it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.45it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.46it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.58it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.65it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.64it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.49it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.45it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.54it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.57it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.54it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.50it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.55it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.46it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.56it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.56it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.55it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.56it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.60it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.47it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.60it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.57it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.60it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.59it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.57it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.54it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.35it/s][A
 84%|████████▍ | 513/611 [00:10<00:02, 46.45it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.50it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.53it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.54it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.59it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.61it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.55it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.47it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.40it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.51it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.54it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.50it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.54it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.58it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.50it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.58it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.53it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.44it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.44it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.49it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 46.49it/s][A 20%|██        | 117/585 [00:46<02:15,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:54:01,654 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 08:54:01,672 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:54:03,839 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:54:03,860 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:54:03,871 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:54<49:08,  6.31s/it] 20%|██        | 119/585 [00:54<35:00,  4.51s/it] 21%|██        | 120/585 [00:54<25:07,  3.24s/it] 21%|██        | 121/585 [00:55<18:13,  2.36s/it] 21%|██        | 122/585 [00:55<13:23,  1.74s/it] 21%|██        | 123/585 [00:55<10:01,  1.30s/it] 21%|██        | 124/585 [00:55<07:39,  1.00it/s] 21%|██▏       | 125/585 [00:56<06:01,  1.27it/s] 22%|██▏       | 126/585 [00:56<04:51,  1.57it/s] 22%|██▏       | 127/585 [00:56<04:03,  1.88it/s] 22%|██▏       | 128/585 [00:57<03:29,  2.18it/s] 22%|██▏       | 129/585 [00:57<03:05,  2.45it/s] 22%|██▏       | 130/585 [00:57<02:49,  2.69it/s] 22%|██▏       | 131/585 [00:57<02:37,  2.88it/s] 23%|██▎       | 132/585 [00:58<02:29,  3.03it/s] 23%|██▎       | 133/585 [00:58<02:23,  3.15it/s] 23%|██▎       | 134/585 [00:58<02:19,  3.24it/s] 23%|██▎       | 135/585 [00:59<02:16,  3.30it/s] 23%|██▎       | 136/585 [00:59<02:14,  3.34it/s] 23%|██▎       | 137/585 [00:59<02:12,  3.38it/s] 24%|██▎       | 138/585 [00:59<02:11,  3.40it/s] 24%|██▍       | 139/585 [01:00<02:10,  3.42it/s] 24%|██▍       | 140/585 [01:00<02:09,  3.43it/s] 24%|██▍       | 141/585 [01:00<02:09,  3.44it/s] 24%|██▍       | 142/585 [01:01<02:08,  3.44it/s] 24%|██▍       | 143/585 [01:01<02:08,  3.45it/s] 25%|██▍       | 144/585 [01:01<02:07,  3.45it/s] 25%|██▍       | 145/585 [01:01<02:07,  3.46it/s] 25%|██▍       | 146/585 [01:02<02:07,  3.45it/s] 25%|██▌       | 147/585 [01:02<02:06,  3.46it/s] 25%|██▌       | 148/585 [01:02<02:06,  3.46it/s] 25%|██▌       | 149/585 [01:03<02:05,  3.46it/s] 26%|██▌       | 150/585 [01:03<02:05,  3.46it/s] 26%|██▌       | 151/585 [01:03<02:05,  3.47it/s] 26%|██▌       | 152/585 [01:03<02:05,  3.46it/s] 26%|██▌       | 153/585 [01:04<02:04,  3.46it/s] 26%|██▋       | 154/585 [01:04<02:04,  3.46it/s] 26%|██▋       | 155/585 [01:04<02:04,  3.46it/s] 27%|██▋       | 156/585 [01:05<02:03,  3.46it/s] 27%|██▋       | 157/585 [01:05<02:04,  3.44it/s] 27%|██▋       | 158/585 [01:05<02:03,  3.44it/s] 27%|██▋       | 159/585 [01:06<02:03,  3.45it/s] 27%|██▋       | 160/585 [01:06<02:03,  3.45it/s] 28%|██▊       | 161/585 [01:06<02:02,  3.45it/s] 28%|██▊       | 162/585 [01:06<02:02,  3.46it/s] 28%|██▊       | 163/585 [01:07<02:02,  3.46it/s] 28%|██▊       | 164/585 [01:07<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:07<02:01,  3.46it/s] 28%|██▊       | 166/585 [01:08<02:01,  3.46it/s] 29%|██▊       | 167/585 [01:08<02:00,  3.46it/s] 29%|██▊       | 168/585 [01:08<02:00,  3.45it/s] 29%|██▉       | 169/585 [01:08<02:00,  3.45it/s] 29%|██▉       | 170/585 [01:09<02:00,  3.45it/s] 29%|██▉       | 171/585 [01:09<01:59,  3.45it/s] 29%|██▉       | 172/585 [01:09<01:59,  3.45it/s] 30%|██▉       | 173/585 [01:10<01:59,  3.46it/s] 30%|██▉       | 174/585 [01:10<01:58,  3.45it/s] 30%|██▉       | 175/585 [01:10<01:58,  3.46it/s] 30%|███       | 176/585 [01:10<01:58,  3.45it/s] 30%|███       | 177/585 [01:11<01:58,  3.46it/s] 30%|███       | 178/585 [01:11<01:57,  3.46it/s] 31%|███       | 179/585 [01:11<01:57,  3.45it/s] 31%|███       | 180/585 [01:12<01:57,  3.46it/s] 31%|███       | 181/585 [01:12<01:56,  3.46it/s] 31%|███       | 182/585 [01:12<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:12<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:13<01:55,  3.46it/s] 32%|███▏      | 185/585 [01:13<01:55,  3.46it/s] 32%|███▏      | 186/585 [01:13<01:55,  3.46it/s] 32%|███▏      | 187/585 [01:14<01:54,  3.46it/s] 32%|███▏      | 188/585 [01:14<01:54,  3.46it/s] 32%|███▏      | 189/585 [01:14<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:14<01:55,  3.43it/s] 33%|███▎      | 191/585 [01:15<01:54,  3.45it/s] 33%|███▎      | 192/585 [01:15<01:53,  3.45it/s] 33%|███▎      | 193/585 [01:15<01:53,  3.46it/s] 33%|███▎      | 194/585 [01:16<01:53,  3.46it/s] 33%|███▎      | 195/585 [01:16<01:52,  3.45it/s] 34%|███▎      | 196/585 [01:16<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:17<01:52,  3.46it/s] 34%|███▍      | 198/585 [01:17<01:51,  3.46it/s] 34%|███▍      | 199/585 [01:17<01:51,  3.46it/s] 34%|███▍      | 200/585 [01:17<01:51,  3.46it/s] 34%|███▍      | 201/585 [01:18<01:51,  3.44it/s] 35%|███▍      | 202/585 [01:18<01:51,  3.45it/s] 35%|███▍      | 203/585 [01:18<01:50,  3.44it/s] 35%|███▍      | 204/585 [01:19<01:50,  3.44it/s] 35%|███▌      | 205/585 [01:19<01:50,  3.45it/s] 35%|███▌      | 206/585 [01:19<01:49,  3.45it/s] 35%|███▌      | 207/585 [01:19<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:20<01:48,  3.46it/s] 36%|███▌      | 209/585 [01:20<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:20<01:48,  3.46it/s] 36%|███▌      | 211/585 [01:21<01:47,  3.47it/s] 36%|███▌      | 212/585 [01:21<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:21<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:21<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:22<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:22<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:23<02:34,  2.38it/s] 37%|███▋      | 218/585 [01:23<02:25,  2.53it/s] 37%|███▋      | 219/585 [01:23<02:12,  2.76it/s] 38%|███▊      | 220/585 [01:24<02:04,  2.94it/s] 38%|███▊      | 221/585 [01:24<01:58,  3.06it/s] 38%|███▊      | 222/585 [01:24<01:54,  3.17it/s] 38%|███▊      | 223/585 [01:25<01:51,  3.26it/s] 38%|███▊      | 224/585 [01:25<01:48,  3.31it/s] 38%|███▊      | 225/585 [01:25<01:47,  3.36it/s] 39%|███▊      | 226/585 [01:25<01:46,  3.39it/s] 39%|███▉      | 227/585 [01:26<01:44,  3.41it/s] 39%|███▉      | 228/585 [01:26<01:44,  3.43it/s] 39%|███▉      | 229/585 [01:26<01:43,  3.43it/s] 39%|███▉      | 230/585 [01:27<01:43,  3.44it/s] 39%|███▉      | 231/585 [01:27<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:27<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:27<01:41,  3.46it/s] 40%|████      | 234/585 [01:28<01:41,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 08:54:42,934 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:54:42,937 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 08:54:42,937 >>   Batch size = 8
{'eval_loss': 0.9761479496955872, 'eval_runtime': 13.1288, 'eval_samples_per_second': 371.853, 'eval_steps_per_second': 46.539, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.15it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.40it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.70it/s][A
  4%|▍         | 23/611 [00:00<00:12, 48.01it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.51it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.08it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.85it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.15it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.31it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.41it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.45it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.60it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.61it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.70it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.68it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.39it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.29it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.21it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.37it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.34it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.47it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.56it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.62it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.56it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.47it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.34it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.30it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.35it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.48it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.48it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.40it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.57it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.58it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.52it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.36it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.27it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.37it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.36it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.46it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.57it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.53it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.55it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.51it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.40it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.24it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.33it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.42it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.49it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.49it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.54it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.52it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.47it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.33it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.31it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.35it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.41it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.46it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.54it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.54it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.38it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.39it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.33it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.33it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.32it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.40it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.52it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.53it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.54it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.45it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.34it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.35it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.29it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.37it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.42it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.29it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.50it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.54it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.52it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.42it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.32it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.32it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.34it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.25it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.48it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.49it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.53it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.50it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.41it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.35it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.31it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.32it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.32it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.40it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.51it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.59it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.54it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.48it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.28it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.33it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.30it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.36it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.38it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.43it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.52it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.50it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.45it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.42it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.29it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.35it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.31it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.37it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.41it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.45it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.53it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.45it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.38it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.33it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.30it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.29it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.36it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.40it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 46.40it/s][A 40%|████      | 234/585 [01:41<01:41,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:54:56,129 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 08:54:56,150 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:54:58,214 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:54:58,228 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:54:58,237 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:48<36:30,  6.26s/it] 40%|████      | 236/585 [01:48<25:59,  4.47s/it] 41%|████      | 237/585 [01:48<18:38,  3.21s/it] 41%|████      | 238/585 [01:49<13:30,  2.34s/it] 41%|████      | 239/585 [01:49<09:55,  1.72s/it] 41%|████      | 240/585 [01:49<07:25,  1.29s/it] 41%|████      | 241/585 [01:50<05:40,  1.01it/s] 41%|████▏     | 242/585 [01:50<04:27,  1.28it/s] 42%|████▏     | 243/585 [01:50<03:36,  1.58it/s] 42%|████▏     | 244/585 [01:50<03:00,  1.89it/s] 42%|████▏     | 245/585 [01:51<02:35,  2.19it/s] 42%|████▏     | 246/585 [01:51<02:17,  2.46it/s] 42%|████▏     | 247/585 [01:51<02:05,  2.69it/s] 42%|████▏     | 248/585 [01:52<01:56,  2.89it/s] 43%|████▎     | 249/585 [01:52<01:50,  3.04it/s] 43%|████▎     | 250/585 [01:52<01:46,  3.16it/s] 43%|████▎     | 251/585 [01:52<01:42,  3.25it/s] 43%|████▎     | 252/585 [01:53<01:40,  3.31it/s] 43%|████▎     | 253/585 [01:53<01:38,  3.36it/s] 43%|████▎     | 254/585 [01:53<01:39,  3.32it/s] 44%|████▎     | 255/585 [01:54<01:38,  3.36it/s] 44%|████▍     | 256/585 [01:54<01:36,  3.39it/s] 44%|████▍     | 257/585 [01:54<01:36,  3.41it/s] 44%|████▍     | 258/585 [01:55<01:35,  3.42it/s] 44%|████▍     | 259/585 [01:55<01:34,  3.44it/s] 44%|████▍     | 260/585 [01:55<01:34,  3.45it/s] 45%|████▍     | 261/585 [01:55<01:33,  3.45it/s] 45%|████▍     | 262/585 [01:56<01:33,  3.46it/s] 45%|████▍     | 263/585 [01:56<01:33,  3.46it/s] 45%|████▌     | 264/585 [01:56<01:32,  3.46it/s] 45%|████▌     | 265/585 [01:57<01:32,  3.47it/s] 45%|████▌     | 266/585 [01:57<01:32,  3.46it/s] 46%|████▌     | 267/585 [01:57<01:31,  3.47it/s] 46%|████▌     | 268/585 [01:57<01:31,  3.47it/s] 46%|████▌     | 269/585 [01:58<01:31,  3.46it/s] 46%|████▌     | 270/585 [01:58<01:30,  3.46it/s] 46%|████▋     | 271/585 [01:58<01:30,  3.46it/s] 46%|████▋     | 272/585 [01:59<01:30,  3.46it/s] 47%|████▋     | 273/585 [01:59<01:30,  3.46it/s] 47%|████▋     | 274/585 [01:59<01:29,  3.46it/s] 47%|████▋     | 275/585 [01:59<01:29,  3.47it/s] 47%|████▋     | 276/585 [02:00<01:29,  3.47it/s] 47%|████▋     | 277/585 [02:00<01:28,  3.47it/s] 48%|████▊     | 278/585 [02:00<01:28,  3.47it/s] 48%|████▊     | 279/585 [02:01<01:28,  3.47it/s] 48%|████▊     | 280/585 [02:01<01:28,  3.46it/s] 48%|████▊     | 281/585 [02:01<01:27,  3.46it/s] 48%|████▊     | 282/585 [02:01<01:27,  3.46it/s] 48%|████▊     | 283/585 [02:02<01:27,  3.46it/s] 49%|████▊     | 284/585 [02:02<01:26,  3.46it/s] 49%|████▊     | 285/585 [02:02<01:26,  3.47it/s] 49%|████▉     | 286/585 [02:03<01:26,  3.47it/s] 49%|████▉     | 287/585 [02:03<01:25,  3.47it/s] 49%|████▉     | 288/585 [02:03<01:25,  3.47it/s] 49%|████▉     | 289/585 [02:03<01:25,  3.47it/s] 50%|████▉     | 290/585 [02:04<01:25,  3.47it/s] 50%|████▉     | 291/585 [02:04<01:24,  3.46it/s] 50%|████▉     | 292/585 [02:04<01:24,  3.47it/s] 50%|█████     | 293/585 [02:05<01:24,  3.46it/s] 50%|█████     | 294/585 [02:05<01:23,  3.47it/s] 50%|█████     | 295/585 [02:05<01:23,  3.47it/s] 51%|█████     | 296/585 [02:05<01:23,  3.47it/s] 51%|█████     | 297/585 [02:06<01:23,  3.47it/s] 51%|█████     | 298/585 [02:06<01:22,  3.46it/s] 51%|█████     | 299/585 [02:06<01:22,  3.47it/s] 51%|█████▏    | 300/585 [02:07<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:07<01:21,  3.47it/s] 52%|█████▏    | 302/585 [02:07<01:21,  3.46it/s] 52%|█████▏    | 303/585 [02:08<01:21,  3.46it/s] 52%|█████▏    | 304/585 [02:08<01:21,  3.46it/s] 52%|█████▏    | 305/585 [02:08<01:20,  3.46it/s] 52%|█████▏    | 306/585 [02:08<01:20,  3.47it/s] 52%|█████▏    | 307/585 [02:09<01:20,  3.47it/s] 53%|█████▎    | 308/585 [02:09<01:19,  3.47it/s] 53%|█████▎    | 309/585 [02:09<01:19,  3.47it/s] 53%|█████▎    | 310/585 [02:10<01:19,  3.47it/s] 53%|█████▎    | 311/585 [02:10<01:19,  3.47it/s] 53%|█████▎    | 312/585 [02:10<01:18,  3.47it/s] 54%|█████▎    | 313/585 [02:10<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:11<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:11<01:18,  3.46it/s] 54%|█████▍    | 316/585 [02:11<01:17,  3.46it/s] 54%|█████▍    | 317/585 [02:12<01:17,  3.47it/s] 54%|█████▍    | 318/585 [02:12<01:17,  3.46it/s] 55%|█████▍    | 319/585 [02:12<01:16,  3.46it/s] 55%|█████▍    | 320/585 [02:12<01:16,  3.47it/s] 55%|█████▍    | 321/585 [02:13<01:16,  3.46it/s] 55%|█████▌    | 322/585 [02:13<01:15,  3.46it/s] 55%|█████▌    | 323/585 [02:13<01:15,  3.46it/s] 55%|█████▌    | 324/585 [02:14<01:15,  3.46it/s] 56%|█████▌    | 325/585 [02:14<01:14,  3.47it/s] 56%|█████▌    | 326/585 [02:14<01:14,  3.46it/s] 56%|█████▌    | 327/585 [02:14<01:14,  3.47it/s] 56%|█████▌    | 328/585 [02:15<01:14,  3.47it/s] 56%|█████▌    | 329/585 [02:15<01:13,  3.47it/s] 56%|█████▋    | 330/585 [02:15<01:13,  3.47it/s] 57%|█████▋    | 331/585 [02:16<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:16<01:13,  3.46it/s] 57%|█████▋    | 333/585 [02:16<01:12,  3.46it/s] 57%|█████▋    | 334/585 [02:16<01:12,  3.46it/s] 57%|█████▋    | 335/585 [02:17<01:12,  3.46it/s] 57%|█████▋    | 336/585 [02:17<01:11,  3.47it/s] 58%|█████▊    | 337/585 [02:17<01:11,  3.46it/s] 58%|█████▊    | 338/585 [02:18<01:11,  3.46it/s] 58%|█████▊    | 339/585 [02:18<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:18<01:10,  3.46it/s] 58%|█████▊    | 341/585 [02:18<01:10,  3.46it/s] 58%|█████▊    | 342/585 [02:19<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:19<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:19<01:09,  3.46it/s] 59%|█████▉    | 345/585 [02:20<01:09,  3.46it/s] 59%|█████▉    | 346/585 [02:20<01:09,  3.46it/s] 59%|█████▉    | 347/585 [02:20<01:08,  3.46it/s] 59%|█████▉    | 348/585 [02:21<01:08,  3.46it/s] 60%|█████▉    | 349/585 [02:21<01:08,  3.46it/s] 60%|█████▉    | 350/585 [02:21<01:07,  3.46it/s] 60%|██████    | 351/585 [02:21<01:07,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 08:55:36,601 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:55:36,601 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 08:55:36,601 >>   Batch size = 8
{'eval_loss': 0.9907234311103821, 'eval_runtime': 13.1713, 'eval_samples_per_second': 370.655, 'eval_steps_per_second': 46.389, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.88it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.39it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.70it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.99it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.52it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.07it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.59it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.47it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.45it/s][A
  9%|▊         | 53/611 [00:01<00:11, 46.51it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.60it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.70it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.66it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.66it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.53it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.38it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.24it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.26it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.38it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.24it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.49it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.58it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.59it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.54it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.42it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.39it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.22it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.37it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.47it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.49it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.54it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.49it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.47it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.30it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.37it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.34it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.42it/s][A
 32%|███▏      | 193/611 [00:04<00:08, 46.44it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.46it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.49it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.46it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.48it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.41it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.37it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.42it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.44it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.35it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.47it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.45it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.54it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.41it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.38it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.35it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.25it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.41it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.44it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.47it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.49it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.46it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.48it/s][A
 50%|█████     | 308/611 [00:06<00:06, 45.98it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.18it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.23it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.35it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.43it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.46it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.43it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.44it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.42it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.38it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.34it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.39it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.38it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.44it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.49it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.51it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.46it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.42it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.29it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.30it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.40it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.40it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.48it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.45it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.42it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.46it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.34it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.34it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.32it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.37it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.42it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.43it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.37it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.46it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.45it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.42it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.35it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.41it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.45it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.46it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.36it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.43it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.45it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.49it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.40it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.37it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.35it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.34it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.44it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.38it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.39it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.43it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.43it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.41it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.29it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.34it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.40it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.41it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.44it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.43it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.34it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 46.34it/s][A 60%|██████    | 351/585 [02:35<01:07,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:55:49,789 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 08:55:49,807 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:55:51,961 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:55:51,987 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:55:51,996 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:42<24:14,  6.24s/it] 60%|██████    | 353/585 [02:42<17:14,  4.46s/it] 61%|██████    | 354/585 [02:42<12:20,  3.21s/it] 61%|██████    | 355/585 [02:42<08:56,  2.33s/it] 61%|██████    | 356/585 [02:43<06:33,  1.72s/it] 61%|██████    | 357/585 [02:43<04:53,  1.29s/it] 61%|██████    | 358/585 [02:43<03:44,  1.01it/s] 61%|██████▏   | 359/585 [02:44<02:55,  1.28it/s] 62%|██████▏   | 360/585 [02:44<02:22,  1.58it/s] 62%|██████▏   | 361/585 [02:44<01:58,  1.89it/s] 62%|██████▏   | 362/585 [02:44<01:41,  2.19it/s] 62%|██████▏   | 363/585 [02:45<01:30,  2.46it/s] 62%|██████▏   | 364/585 [02:45<01:21,  2.70it/s] 62%|██████▏   | 365/585 [02:45<01:16,  2.89it/s] 63%|██████▎   | 366/585 [02:46<01:11,  3.05it/s] 63%|██████▎   | 367/585 [02:46<01:08,  3.16it/s] 63%|██████▎   | 368/585 [02:46<01:06,  3.24it/s] 63%|██████▎   | 369/585 [02:46<01:05,  3.31it/s] 63%|██████▎   | 370/585 [02:47<01:04,  3.35it/s] 63%|██████▎   | 371/585 [02:47<01:03,  3.39it/s] 64%|██████▎   | 372/585 [02:47<01:02,  3.42it/s] 64%|██████▍   | 373/585 [02:48<01:01,  3.43it/s] 64%|██████▍   | 374/585 [02:48<01:01,  3.45it/s] 64%|██████▍   | 375/585 [02:48<01:00,  3.45it/s] 64%|██████▍   | 376/585 [02:48<01:00,  3.46it/s] 64%|██████▍   | 377/585 [02:49<01:00,  3.46it/s] 65%|██████▍   | 378/585 [02:49<00:59,  3.46it/s] 65%|██████▍   | 379/585 [02:49<00:59,  3.46it/s] 65%|██████▍   | 380/585 [02:50<00:59,  3.46it/s] 65%|██████▌   | 381/585 [02:50<00:58,  3.46it/s] 65%|██████▌   | 382/585 [02:50<00:58,  3.47it/s] 65%|██████▌   | 383/585 [02:50<00:58,  3.47it/s] 66%|██████▌   | 384/585 [02:51<00:57,  3.47it/s] 66%|██████▌   | 385/585 [02:51<00:57,  3.47it/s] 66%|██████▌   | 386/585 [02:51<00:57,  3.47it/s] 66%|██████▌   | 387/585 [02:52<00:57,  3.47it/s] 66%|██████▋   | 388/585 [02:52<00:56,  3.47it/s] 66%|██████▋   | 389/585 [02:52<00:56,  3.47it/s] 67%|██████▋   | 390/585 [02:52<00:56,  3.46it/s] 67%|██████▋   | 391/585 [02:53<00:56,  3.46it/s] 67%|██████▋   | 392/585 [02:53<00:55,  3.47it/s] 67%|██████▋   | 393/585 [02:53<00:55,  3.46it/s] 67%|██████▋   | 394/585 [02:54<00:56,  3.41it/s] 68%|██████▊   | 395/585 [02:54<00:55,  3.42it/s] 68%|██████▊   | 396/585 [02:54<00:55,  3.44it/s] 68%|██████▊   | 397/585 [02:55<00:54,  3.45it/s] 68%|██████▊   | 398/585 [02:55<00:54,  3.45it/s] 68%|██████▊   | 399/585 [02:55<00:53,  3.46it/s] 68%|██████▊   | 400/585 [02:55<00:53,  3.46it/s] 69%|██████▊   | 401/585 [02:56<00:53,  3.45it/s] 69%|██████▊   | 402/585 [02:56<00:52,  3.46it/s] 69%|██████▉   | 403/585 [02:56<00:52,  3.46it/s] 69%|██████▉   | 404/585 [02:57<00:52,  3.46it/s] 69%|██████▉   | 405/585 [02:57<00:51,  3.46it/s] 69%|██████▉   | 406/585 [02:57<00:51,  3.46it/s] 70%|██████▉   | 407/585 [02:57<00:51,  3.46it/s] 70%|██████▉   | 408/585 [02:58<00:51,  3.46it/s] 70%|██████▉   | 409/585 [02:58<00:50,  3.46it/s] 70%|███████   | 410/585 [02:58<00:50,  3.46it/s] 70%|███████   | 411/585 [02:59<00:50,  3.46it/s] 70%|███████   | 412/585 [02:59<00:50,  3.46it/s] 71%|███████   | 413/585 [02:59<00:49,  3.46it/s] 71%|███████   | 414/585 [02:59<00:49,  3.46it/s] 71%|███████   | 415/585 [03:00<00:49,  3.47it/s] 71%|███████   | 416/585 [03:00<00:48,  3.47it/s] 71%|███████▏  | 417/585 [03:00<00:48,  3.47it/s] 71%|███████▏  | 418/585 [03:01<00:48,  3.46it/s] 72%|███████▏  | 419/585 [03:01<00:47,  3.46it/s] 72%|███████▏  | 420/585 [03:01<00:47,  3.47it/s] 72%|███████▏  | 421/585 [03:01<00:47,  3.47it/s] 72%|███████▏  | 422/585 [03:02<00:46,  3.47it/s] 72%|███████▏  | 423/585 [03:02<00:46,  3.45it/s] 72%|███████▏  | 424/585 [03:02<00:46,  3.45it/s] 73%|███████▎  | 425/585 [03:03<00:46,  3.46it/s] 73%|███████▎  | 426/585 [03:03<00:45,  3.46it/s] 73%|███████▎  | 427/585 [03:03<00:45,  3.46it/s] 73%|███████▎  | 428/585 [03:03<00:45,  3.46it/s] 73%|███████▎  | 429/585 [03:04<00:45,  3.46it/s] 74%|███████▎  | 430/585 [03:04<00:44,  3.47it/s] 74%|███████▎  | 431/585 [03:04<00:44,  3.46it/s] 74%|███████▍  | 432/585 [03:05<00:44,  3.46it/s] 74%|███████▍  | 433/585 [03:05<00:43,  3.46it/s] 74%|███████▍  | 434/585 [03:05<00:44,  3.41it/s] 74%|███████▍  | 435/585 [03:05<00:43,  3.43it/s] 75%|███████▍  | 436/585 [03:06<00:43,  3.44it/s] 75%|███████▍  | 437/585 [03:06<00:42,  3.45it/s] 75%|███████▍  | 438/585 [03:06<00:42,  3.46it/s] 75%|███████▌  | 439/585 [03:07<00:42,  3.46it/s] 75%|███████▌  | 440/585 [03:07<00:41,  3.46it/s] 75%|███████▌  | 441/585 [03:07<00:41,  3.46it/s] 76%|███████▌  | 442/585 [03:08<00:41,  3.46it/s] 76%|███████▌  | 443/585 [03:08<00:40,  3.47it/s] 76%|███████▌  | 444/585 [03:08<00:40,  3.46it/s] 76%|███████▌  | 445/585 [03:08<00:40,  3.44it/s] 76%|███████▌  | 446/585 [03:09<00:40,  3.45it/s] 76%|███████▋  | 447/585 [03:09<00:39,  3.45it/s] 77%|███████▋  | 448/585 [03:09<00:39,  3.46it/s] 77%|███████▋  | 449/585 [03:10<00:39,  3.46it/s] 77%|███████▋  | 450/585 [03:10<00:38,  3.47it/s] 77%|███████▋  | 451/585 [03:10<00:38,  3.46it/s] 77%|███████▋  | 452/585 [03:10<00:38,  3.46it/s] 77%|███████▋  | 453/585 [03:11<00:38,  3.47it/s] 78%|███████▊  | 454/585 [03:11<00:37,  3.47it/s] 78%|███████▊  | 455/585 [03:11<00:37,  3.47it/s] 78%|███████▊  | 456/585 [03:12<00:37,  3.41it/s] 78%|███████▊  | 457/585 [03:12<00:37,  3.42it/s] 78%|███████▊  | 458/585 [03:12<00:36,  3.44it/s] 78%|███████▊  | 459/585 [03:12<00:36,  3.44it/s] 79%|███████▊  | 460/585 [03:13<00:36,  3.45it/s] 79%|███████▉  | 461/585 [03:13<00:35,  3.46it/s] 79%|███████▉  | 462/585 [03:13<00:35,  3.46it/s] 79%|███████▉  | 463/585 [03:14<00:35,  3.46it/s] 79%|███████▉  | 464/585 [03:14<00:34,  3.46it/s] 79%|███████▉  | 465/585 [03:14<00:34,  3.46it/s] 80%|███████▉  | 466/585 [03:14<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:15<00:34,  3.47it/s] 80%|████████  | 468/585 [03:15<00:33,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 08:56:30,264 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:56:30,264 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 08:56:30,264 >>   Batch size = 8
{'eval_loss': 1.004693627357483, 'eval_runtime': 13.1687, 'eval_samples_per_second': 370.726, 'eval_steps_per_second': 46.398, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.37it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.42it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.76it/s][A
  4%|▍         | 23/611 [00:00<00:12, 48.08it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.64it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.22it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.92it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.49it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.59it/s][A
  9%|▊         | 53/611 [00:01<00:11, 46.65it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.66it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.55it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.64it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.63it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.59it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.44it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.34it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.31it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.39it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.19it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.30it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.37it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.44it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.44it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.31it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.27it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.29it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.36it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.46it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.50it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.51it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.48it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.48it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.42it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.37it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.30it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.40it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.41it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.47it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.52it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.52it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.46it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.47it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.33it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.31it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.32it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.40it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.53it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.57it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.54it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.40it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.39it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.38it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.34it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.34it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.40it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.41it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.48it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.50it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.54it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.48it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.37it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.36it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.28it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.37it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.47it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.48it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.53it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.52it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.46it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.30it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.28it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.33it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.38it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.39it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.43it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.56it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.52it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.49it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.39it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.36it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.33it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.31it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.41it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.44it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.51it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.51it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.47it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.41it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.29it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.32it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.38it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.41it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.47it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.49it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.40it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.46it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.39it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.34it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.41it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.41it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.49it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.36it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.43it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.42it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.42it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.42it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.29it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.38it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.36it/s][A
 91%|█████████▏| 558/611 [00:11<00:01, 46.41it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.46it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.48it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.45it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.50it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.42it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.33it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.35it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.37it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.44it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.46it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 46.46it/s][A 80%|████████  | 468/585 [03:28<00:33,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:56:43,440 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 08:56:43,460 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:56:45,642 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:56:45,662 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:56:45,673 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:35<12:12,  6.31s/it] 80%|████████  | 470/585 [03:36<08:38,  4.51s/it] 81%|████████  | 471/585 [03:36<06:09,  3.24s/it] 81%|████████  | 472/585 [03:36<04:26,  2.35s/it] 81%|████████  | 473/585 [03:37<03:14,  1.73s/it] 81%|████████  | 474/585 [03:37<02:24,  1.30s/it] 81%|████████  | 475/585 [03:37<01:49,  1.00it/s] 81%|████████▏ | 476/585 [03:37<01:25,  1.28it/s] 82%|████████▏ | 477/585 [03:38<01:08,  1.57it/s] 82%|████████▏ | 478/585 [03:38<00:56,  1.88it/s] 82%|████████▏ | 479/585 [03:38<00:48,  2.18it/s] 82%|████████▏ | 480/585 [03:39<00:42,  2.46it/s] 82%|████████▏ | 481/585 [03:39<00:38,  2.69it/s] 82%|████████▏ | 482/585 [03:39<00:35,  2.88it/s] 83%|████████▎ | 483/585 [03:39<00:33,  3.04it/s] 83%|████████▎ | 484/585 [03:40<00:31,  3.16it/s] 83%|████████▎ | 485/585 [03:40<00:30,  3.25it/s] 83%|████████▎ | 486/585 [03:40<00:29,  3.31it/s] 83%|████████▎ | 487/585 [03:41<00:29,  3.36it/s] 83%|████████▎ | 488/585 [03:41<00:28,  3.39it/s] 84%|████████▎ | 489/585 [03:41<00:28,  3.42it/s] 84%|████████▍ | 490/585 [03:41<00:27,  3.44it/s] 84%|████████▍ | 491/585 [03:42<00:27,  3.45it/s] 84%|████████▍ | 492/585 [03:42<00:27,  3.44it/s] 84%|████████▍ | 493/585 [03:42<00:26,  3.45it/s] 84%|████████▍ | 494/585 [03:43<00:26,  3.46it/s] 85%|████████▍ | 495/585 [03:43<00:26,  3.46it/s] 85%|████████▍ | 496/585 [03:43<00:25,  3.47it/s] 85%|████████▍ | 497/585 [03:43<00:25,  3.47it/s] 85%|████████▌ | 498/585 [03:44<00:25,  3.47it/s] 85%|████████▌ | 499/585 [03:44<00:24,  3.47it/s] 85%|████████▌ | 500/585 [03:44<00:24,  3.47it/s]                                                  85%|████████▌ | 500/585 [03:44<00:24,  3.47it/s] 86%|████████▌ | 501/585 [03:45<00:24,  3.47it/s] 86%|████████▌ | 502/585 [03:45<00:23,  3.47it/s] 86%|████████▌ | 503/585 [03:45<00:23,  3.47it/s] 86%|████████▌ | 504/585 [03:45<00:23,  3.47it/s] 86%|████████▋ | 505/585 [03:46<00:23,  3.47it/s] 86%|████████▋ | 506/585 [03:46<00:22,  3.47it/s] 87%|████████▋ | 507/585 [03:46<00:22,  3.47it/s] 87%|████████▋ | 508/585 [03:47<00:22,  3.48it/s] 87%|████████▋ | 509/585 [03:47<00:21,  3.47it/s] 87%|████████▋ | 510/585 [03:47<00:21,  3.48it/s] 87%|████████▋ | 511/585 [03:47<00:21,  3.46it/s] 88%|████████▊ | 512/585 [03:48<00:21,  3.46it/s] 88%|████████▊ | 513/585 [03:48<00:20,  3.47it/s] 88%|████████▊ | 514/585 [03:48<00:20,  3.47it/s] 88%|████████▊ | 515/585 [03:49<00:20,  3.47it/s] 88%|████████▊ | 516/585 [03:49<00:19,  3.47it/s] 88%|████████▊ | 517/585 [03:49<00:19,  3.47it/s] 89%|████████▊ | 518/585 [03:50<00:19,  3.46it/s] 89%|████████▊ | 519/585 [03:50<00:19,  3.47it/s] 89%|████████▉ | 520/585 [03:50<00:18,  3.47it/s] 89%|████████▉ | 521/585 [03:50<00:18,  3.47it/s] 89%|████████▉ | 522/585 [03:51<00:18,  3.45it/s] 89%|████████▉ | 523/585 [03:51<00:17,  3.46it/s] 90%|████████▉ | 524/585 [03:51<00:17,  3.46it/s] 90%|████████▉ | 525/585 [03:52<00:17,  3.47it/s] 90%|████████▉ | 526/585 [03:52<00:17,  3.47it/s] 90%|█████████ | 527/585 [03:52<00:16,  3.47it/s] 90%|█████████ | 528/585 [03:52<00:16,  3.47it/s] 90%|█████████ | 529/585 [03:53<00:16,  3.47it/s] 91%|█████████ | 530/585 [03:53<00:15,  3.47it/s] 91%|█████████ | 531/585 [03:53<00:15,  3.47it/s] 91%|█████████ | 532/585 [03:54<00:15,  3.47it/s] 91%|█████████ | 533/585 [03:54<00:15,  3.37it/s] 91%|█████████▏| 534/585 [03:54<00:15,  3.39it/s] 91%|█████████▏| 535/585 [03:54<00:14,  3.42it/s] 92%|█████████▏| 536/585 [03:55<00:14,  3.43it/s] 92%|█████████▏| 537/585 [03:55<00:13,  3.44it/s] 92%|█████████▏| 538/585 [03:55<00:13,  3.45it/s] 92%|█████████▏| 539/585 [03:56<00:13,  3.45it/s] 92%|█████████▏| 540/585 [03:56<00:13,  3.46it/s] 92%|█████████▏| 541/585 [03:56<00:12,  3.46it/s] 93%|█████████▎| 542/585 [03:56<00:12,  3.46it/s] 93%|█████████▎| 543/585 [03:57<00:12,  3.46it/s] 93%|█████████▎| 544/585 [03:57<00:11,  3.46it/s] 93%|█████████▎| 545/585 [03:57<00:11,  3.46it/s] 93%|█████████▎| 546/585 [03:58<00:11,  3.46it/s] 94%|█████████▎| 547/585 [03:58<00:10,  3.47it/s] 94%|█████████▎| 548/585 [03:58<00:10,  3.47it/s] 94%|█████████▍| 549/585 [03:58<00:10,  3.47it/s] 94%|█████████▍| 550/585 [03:59<00:10,  3.47it/s] 94%|█████████▍| 551/585 [03:59<00:09,  3.47it/s] 94%|█████████▍| 552/585 [03:59<00:09,  3.47it/s] 95%|█████████▍| 553/585 [04:00<00:09,  3.47it/s] 95%|█████████▍| 554/585 [04:00<00:08,  3.47it/s] 95%|█████████▍| 555/585 [04:00<00:08,  3.46it/s] 95%|█████████▌| 556/585 [04:01<00:08,  3.46it/s] 95%|█████████▌| 557/585 [04:01<00:08,  3.46it/s] 95%|█████████▌| 558/585 [04:01<00:07,  3.47it/s] 96%|█████████▌| 559/585 [04:01<00:07,  3.47it/s] 96%|█████████▌| 560/585 [04:02<00:07,  3.47it/s] 96%|█████████▌| 561/585 [04:02<00:06,  3.46it/s] 96%|█████████▌| 562/585 [04:02<00:06,  3.47it/s] 96%|█████████▌| 563/585 [04:03<00:06,  3.46it/s] 96%|█████████▋| 564/585 [04:03<00:06,  3.46it/s] 97%|█████████▋| 565/585 [04:03<00:05,  3.47it/s] 97%|█████████▋| 566/585 [04:03<00:05,  3.46it/s] 97%|█████████▋| 567/585 [04:04<00:05,  3.45it/s] 97%|█████████▋| 568/585 [04:04<00:04,  3.46it/s] 97%|█████████▋| 569/585 [04:04<00:04,  3.46it/s] 97%|█████████▋| 570/585 [04:05<00:04,  3.46it/s] 98%|█████████▊| 571/585 [04:05<00:04,  3.46it/s] 98%|█████████▊| 572/585 [04:05<00:03,  3.47it/s] 98%|█████████▊| 573/585 [04:05<00:03,  3.46it/s] 98%|█████████▊| 574/585 [04:06<00:03,  3.46it/s] 98%|█████████▊| 575/585 [04:06<00:02,  3.47it/s] 98%|█████████▊| 576/585 [04:06<00:02,  3.46it/s] 99%|█████████▊| 577/585 [04:07<00:02,  3.45it/s] 99%|█████████▉| 578/585 [04:07<00:02,  3.45it/s] 99%|█████████▉| 579/585 [04:07<00:01,  3.45it/s] 99%|█████████▉| 580/585 [04:07<00:01,  3.45it/s] 99%|█████████▉| 581/585 [04:08<00:01,  3.45it/s] 99%|█████████▉| 582/585 [04:08<00:00,  3.46it/s]100%|█████████▉| 583/585 [04:08<00:00,  3.45it/s]100%|█████████▉| 584/585 [04:09<00:00,  3.46it/s]100%|██████████| 585/585 [04:09<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 08:57:24,075 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:57:24,075 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 08:57:24,075 >>   Batch size = 8
{'eval_loss': 1.0196139812469482, 'eval_runtime': 13.1621, 'eval_samples_per_second': 370.914, 'eval_steps_per_second': 46.421, 'epoch': 4.0}
{'loss': 0.3819, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.73it/s][A
  2%|▏         | 12/611 [00:00<00:11, 50.36it/s][A
  3%|▎         | 18/611 [00:00<00:12, 48.66it/s][A
  4%|▍         | 23/611 [00:00<00:12, 47.89it/s][A
  5%|▍         | 28/611 [00:00<00:12, 47.54it/s][A
  5%|▌         | 33/611 [00:00<00:12, 47.14it/s][A
  6%|▌         | 38/611 [00:00<00:12, 46.83it/s][A
  7%|▋         | 43/611 [00:00<00:12, 46.50it/s][A
  8%|▊         | 48/611 [00:01<00:12, 46.45it/s][A
  9%|▊         | 53/611 [00:01<00:12, 46.45it/s][A
  9%|▉         | 58/611 [00:01<00:11, 46.47it/s][A
 10%|█         | 63/611 [00:01<00:11, 46.58it/s][A
 11%|█         | 68/611 [00:01<00:11, 46.64it/s][A
 12%|█▏        | 73/611 [00:01<00:11, 46.66it/s][A
 13%|█▎        | 78/611 [00:01<00:11, 46.59it/s][A
 14%|█▎        | 83/611 [00:01<00:11, 46.41it/s][A
 14%|█▍        | 88/611 [00:01<00:11, 46.33it/s][A
 15%|█▌        | 93/611 [00:01<00:11, 46.23it/s][A
 16%|█▌        | 98/611 [00:02<00:11, 46.36it/s][A
 17%|█▋        | 103/611 [00:02<00:10, 46.42it/s][A
 18%|█▊        | 108/611 [00:02<00:10, 46.46it/s][A
 18%|█▊        | 113/611 [00:02<00:10, 46.51it/s][A
 19%|█▉        | 118/611 [00:02<00:10, 46.57it/s][A
 20%|██        | 123/611 [00:02<00:10, 46.54it/s][A
 21%|██        | 128/611 [00:02<00:10, 46.45it/s][A
 22%|██▏       | 133/611 [00:02<00:10, 46.27it/s][A
 23%|██▎       | 138/611 [00:02<00:10, 46.20it/s][A
 23%|██▎       | 143/611 [00:03<00:10, 46.29it/s][A
 24%|██▍       | 148/611 [00:03<00:09, 46.34it/s][A
 25%|██▌       | 153/611 [00:03<00:09, 46.48it/s][A
 26%|██▌       | 158/611 [00:03<00:09, 46.57it/s][A
 27%|██▋       | 163/611 [00:03<00:09, 46.60it/s][A
 27%|██▋       | 168/611 [00:03<00:09, 46.33it/s][A
 28%|██▊       | 173/611 [00:03<00:09, 46.18it/s][A
 29%|██▉       | 178/611 [00:03<00:09, 46.22it/s][A
 30%|██▉       | 183/611 [00:03<00:09, 46.22it/s][A
 31%|███       | 188/611 [00:04<00:09, 46.16it/s][A
 32%|███▏      | 193/611 [00:04<00:09, 46.29it/s][A
 32%|███▏      | 198/611 [00:04<00:08, 46.37it/s][A
 33%|███▎      | 203/611 [00:04<00:08, 46.43it/s][A
 34%|███▍      | 208/611 [00:04<00:08, 46.57it/s][A
 35%|███▍      | 213/611 [00:04<00:08, 46.53it/s][A
 36%|███▌      | 218/611 [00:04<00:08, 46.45it/s][A
 36%|███▋      | 223/611 [00:04<00:08, 46.37it/s][A
 37%|███▋      | 228/611 [00:04<00:08, 46.37it/s][A
 38%|███▊      | 233/611 [00:04<00:08, 46.23it/s][A
 39%|███▉      | 238/611 [00:05<00:08, 46.37it/s][A
 40%|███▉      | 243/611 [00:05<00:07, 46.42it/s][A
 41%|████      | 248/611 [00:05<00:07, 46.41it/s][A
 41%|████▏     | 253/611 [00:05<00:07, 46.51it/s][A
 42%|████▏     | 258/611 [00:05<00:07, 46.47it/s][A
 43%|████▎     | 263/611 [00:05<00:07, 46.44it/s][A
 44%|████▍     | 268/611 [00:05<00:07, 46.35it/s][A
 45%|████▍     | 273/611 [00:05<00:07, 46.30it/s][A
 45%|████▌     | 278/611 [00:05<00:07, 46.32it/s][A
 46%|████▋     | 283/611 [00:06<00:07, 46.30it/s][A
 47%|████▋     | 288/611 [00:06<00:06, 46.34it/s][A
 48%|████▊     | 293/611 [00:06<00:06, 46.49it/s][A
 49%|████▉     | 298/611 [00:06<00:06, 46.39it/s][A
 50%|████▉     | 303/611 [00:06<00:06, 46.48it/s][A
 50%|█████     | 308/611 [00:06<00:06, 46.48it/s][A
 51%|█████     | 313/611 [00:06<00:06, 46.35it/s][A
 52%|█████▏    | 318/611 [00:06<00:06, 46.33it/s][A
 53%|█████▎    | 323/611 [00:06<00:06, 46.30it/s][A
 54%|█████▎    | 328/611 [00:07<00:06, 46.33it/s][A
 55%|█████▍    | 333/611 [00:07<00:05, 46.41it/s][A
 55%|█████▌    | 338/611 [00:07<00:05, 46.32it/s][A
 56%|█████▌    | 343/611 [00:07<00:05, 46.38it/s][A
 57%|█████▋    | 348/611 [00:07<00:05, 46.42it/s][A
 58%|█████▊    | 353/611 [00:07<00:05, 46.43it/s][A
 59%|█████▊    | 358/611 [00:07<00:05, 46.38it/s][A
 59%|█████▉    | 363/611 [00:07<00:05, 46.26it/s][A
 60%|██████    | 368/611 [00:07<00:05, 46.34it/s][A
 61%|██████    | 373/611 [00:08<00:05, 46.28it/s][A
 62%|██████▏   | 378/611 [00:08<00:05, 46.39it/s][A
 63%|██████▎   | 383/611 [00:08<00:04, 46.47it/s][A
 64%|██████▎   | 388/611 [00:08<00:04, 46.44it/s][A
 64%|██████▍   | 393/611 [00:08<00:04, 46.36it/s][A
 65%|██████▌   | 398/611 [00:08<00:04, 46.43it/s][A
 66%|██████▌   | 403/611 [00:08<00:04, 46.30it/s][A
 67%|██████▋   | 408/611 [00:08<00:04, 46.32it/s][A
 68%|██████▊   | 413/611 [00:08<00:04, 46.29it/s][A
 68%|██████▊   | 418/611 [00:08<00:04, 46.26it/s][A
 69%|██████▉   | 423/611 [00:09<00:04, 46.34it/s][A
 70%|███████   | 428/611 [00:09<00:03, 46.35it/s][A
 71%|███████   | 433/611 [00:09<00:03, 46.44it/s][A
 72%|███████▏  | 438/611 [00:09<00:03, 46.47it/s][A
 73%|███████▎  | 443/611 [00:09<00:03, 46.44it/s][A
 73%|███████▎  | 448/611 [00:09<00:03, 46.37it/s][A
 74%|███████▍  | 453/611 [00:09<00:03, 46.30it/s][A
 75%|███████▍  | 458/611 [00:09<00:03, 46.29it/s][A
 76%|███████▌  | 463/611 [00:09<00:03, 46.31it/s][A
 77%|███████▋  | 468/611 [00:10<00:03, 46.35it/s][A
 77%|███████▋  | 473/611 [00:10<00:02, 46.38it/s][A
 78%|███████▊  | 478/611 [00:10<00:02, 46.43it/s][A
 79%|███████▉  | 483/611 [00:10<00:02, 46.50it/s][A
 80%|███████▉  | 488/611 [00:10<00:02, 46.46it/s][A
 81%|████████  | 493/611 [00:10<00:02, 46.35it/s][A
 82%|████████▏ | 498/611 [00:10<00:02, 46.30it/s][A
 82%|████████▏ | 503/611 [00:10<00:02, 46.26it/s][A
 83%|████████▎ | 508/611 [00:10<00:02, 46.38it/s][A
 84%|████████▍ | 513/611 [00:11<00:02, 46.38it/s][A
 85%|████████▍ | 518/611 [00:11<00:02, 46.38it/s][A
 86%|████████▌ | 523/611 [00:11<00:01, 46.36it/s][A
 86%|████████▋ | 528/611 [00:11<00:01, 46.46it/s][A
 87%|████████▋ | 533/611 [00:11<00:01, 46.45it/s][A
 88%|████████▊ | 538/611 [00:11<00:01, 46.38it/s][A
 89%|████████▉ | 543/611 [00:11<00:01, 46.27it/s][A
 90%|████████▉ | 548/611 [00:11<00:01, 46.30it/s][A
 91%|█████████ | 553/611 [00:11<00:01, 46.29it/s][A
 91%|█████████▏| 558/611 [00:12<00:01, 46.34it/s][A
 92%|█████████▏| 563/611 [00:12<00:01, 46.41it/s][A
 93%|█████████▎| 568/611 [00:12<00:00, 46.39it/s][A
 94%|█████████▍| 573/611 [00:12<00:00, 46.38it/s][A
 95%|█████████▍| 578/611 [00:12<00:00, 46.42it/s][A
 95%|█████████▌| 583/611 [00:12<00:00, 46.32it/s][A
 96%|█████████▌| 588/611 [00:12<00:00, 46.24it/s][A
 97%|█████████▋| 593/611 [00:12<00:00, 46.27it/s][A
 98%|█████████▊| 598/611 [00:12<00:00, 46.38it/s][A
 99%|█████████▊| 603/611 [00:12<00:00, 46.37it/s][A
100%|█████████▉| 608/611 [00:13<00:00, 46.45it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 46.45it/s][A100%|██████████| 585/585 [04:22<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:57:37,256 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 08:57:37,274 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:57:39,657 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:57:39,673 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:57:39,683 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 08:57:44,080 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 08:57:44,084 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117 (score: 0.9761479496955872).
                                                 100%|██████████| 585/585 [04:31<00:00,  3.46it/s]100%|██████████| 585/585 [04:31<00:00,  2.16it/s]
[INFO|trainer.py:1894] 2023-08-29 08:57:45,783 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 08:57:45,798 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:57:48,022 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:57:48,040 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:57:48,048 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:57:48,235 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:57:48,235 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:57:48,235 >>   train_loss               =     0.3794
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:57:48,235 >>   train_runtime            = 0:04:31.09
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:57:48,235 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:57:48,235 >>   train_samples_per_second =     138.33
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:57:48,235 >>   train_steps_per_second   =      2.158
{'eval_loss': 1.0246129035949707, 'eval_runtime': 13.164, 'eval_samples_per_second': 370.859, 'eval_steps_per_second': 46.414, 'epoch': 5.0}
{'train_runtime': 271.0913, 'train_samples_per_second': 138.33, 'train_steps_per_second': 2.158, 'train_loss': 0.379410161727514, 'epoch': 5.0}
08/29/2023 08:57:48 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 08:57:48,275 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:57:48,275 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 08:57:48,275 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 58.61it/s]  2%|▏         | 12/611 [00:00<00:11, 51.15it/s]  3%|▎         | 18/611 [00:00<00:12, 49.21it/s]  4%|▍         | 23/611 [00:00<00:12, 48.46it/s]  5%|▍         | 28/611 [00:00<00:12, 47.89it/s]  5%|▌         | 33/611 [00:00<00:12, 47.67it/s]  6%|▌         | 38/611 [00:00<00:12, 47.46it/s]  7%|▋         | 43/611 [00:00<00:12, 46.94it/s]  8%|▊         | 48/611 [00:01<00:12, 46.58it/s]  9%|▊         | 53/611 [00:01<00:11, 46.58it/s]  9%|▉         | 58/611 [00:01<00:11, 46.62it/s] 10%|█         | 63/611 [00:01<00:11, 46.70it/s] 11%|█         | 68/611 [00:01<00:11, 46.77it/s] 12%|█▏        | 73/611 [00:01<00:11, 46.56it/s] 13%|█▎        | 78/611 [00:01<00:11, 46.66it/s] 14%|█▎        | 83/611 [00:01<00:11, 46.76it/s] 14%|█▍        | 88/611 [00:01<00:11, 46.78it/s] 15%|█▌        | 93/611 [00:01<00:11, 46.69it/s] 16%|█▌        | 98/611 [00:02<00:11, 46.58it/s] 17%|█▋        | 103/611 [00:02<00:10, 46.57it/s] 18%|█▊        | 108/611 [00:02<00:10, 46.54it/s] 18%|█▊        | 113/611 [00:02<00:10, 46.66it/s] 19%|█▉        | 118/611 [00:02<00:10, 46.72it/s] 20%|██        | 123/611 [00:02<00:10, 46.76it/s] 21%|██        | 128/611 [00:02<00:10, 46.81it/s] 22%|██▏       | 133/611 [00:02<00:10, 46.87it/s] 23%|██▎       | 138/611 [00:02<00:10, 46.77it/s] 23%|██▎       | 143/611 [00:03<00:10, 46.59it/s] 24%|██▍       | 148/611 [00:03<00:09, 46.56it/s] 25%|██▌       | 153/611 [00:03<00:09, 46.61it/s] 26%|██▌       | 158/611 [00:03<00:09, 46.63it/s] 27%|██▋       | 163/611 [00:03<00:09, 46.66it/s] 27%|██▋       | 168/611 [00:03<00:09, 46.72it/s] 28%|██▊       | 173/611 [00:03<00:09, 46.76it/s] 29%|██▉       | 178/611 [00:03<00:09, 46.84it/s] 30%|██▉       | 183/611 [00:03<00:09, 46.84it/s] 31%|███       | 188/611 [00:03<00:09, 46.71it/s] 32%|███▏      | 193/611 [00:04<00:08, 46.64it/s] 32%|███▏      | 198/611 [00:04<00:08, 46.66it/s] 33%|███▎      | 203/611 [00:04<00:08, 46.63it/s] 34%|███▍      | 208/611 [00:04<00:08, 46.68it/s] 35%|███▍      | 213/611 [00:04<00:08, 46.70it/s] 36%|███▌      | 218/611 [00:04<00:08, 46.76it/s] 36%|███▋      | 223/611 [00:04<00:08, 46.80it/s] 37%|███▋      | 228/611 [00:04<00:08, 46.80it/s] 38%|███▊      | 233/611 [00:04<00:08, 46.78it/s] 39%|███▉      | 238/611 [00:05<00:07, 46.66it/s] 40%|███▉      | 243/611 [00:05<00:07, 46.68it/s] 41%|████      | 248/611 [00:05<00:07, 46.67it/s] 41%|████▏     | 253/611 [00:05<00:07, 46.66it/s] 42%|████▏     | 258/611 [00:05<00:07, 46.67it/s] 43%|████▎     | 263/611 [00:05<00:07, 46.64it/s] 44%|████▍     | 268/611 [00:05<00:07, 46.79it/s] 45%|████▍     | 273/611 [00:05<00:07, 46.76it/s] 45%|████▌     | 278/611 [00:05<00:07, 46.68it/s] 46%|████▋     | 283/611 [00:06<00:07, 46.72it/s] 47%|████▋     | 288/611 [00:06<00:06, 46.73it/s] 48%|████▊     | 293/611 [00:06<00:06, 46.69it/s] 49%|████▉     | 298/611 [00:06<00:06, 46.65it/s] 50%|████▉     | 303/611 [00:06<00:06, 46.71it/s] 50%|█████     | 308/611 [00:06<00:06, 46.71it/s] 51%|█████     | 313/611 [00:06<00:06, 46.75it/s] 52%|█████▏    | 318/611 [00:06<00:06, 46.73it/s] 53%|█████▎    | 323/611 [00:06<00:06, 46.67it/s] 54%|█████▎    | 328/611 [00:06<00:06, 46.71it/s] 55%|█████▍    | 333/611 [00:07<00:05, 46.62it/s] 55%|█████▌    | 338/611 [00:07<00:05, 46.66it/s] 56%|█████▌    | 343/611 [00:07<00:05, 46.71it/s] 57%|█████▋    | 348/611 [00:07<00:05, 46.64it/s] 58%|█████▊    | 353/611 [00:07<00:05, 46.69it/s] 59%|█████▊    | 358/611 [00:07<00:05, 46.72it/s] 59%|█████▉    | 363/611 [00:07<00:05, 46.59it/s] 60%|██████    | 368/611 [00:07<00:05, 46.70it/s] 61%|██████    | 373/611 [00:07<00:05, 46.67it/s] 62%|██████▏   | 378/611 [00:08<00:04, 46.71it/s] 63%|██████▎   | 383/611 [00:08<00:04, 46.74it/s] 64%|██████▎   | 388/611 [00:08<00:04, 46.67it/s] 64%|██████▍   | 393/611 [00:08<00:04, 46.66it/s] 65%|██████▌   | 398/611 [00:08<00:04, 46.67it/s] 66%|██████▌   | 403/611 [00:08<00:04, 46.66it/s] 67%|██████▋   | 408/611 [00:08<00:04, 46.71it/s] 68%|██████▊   | 413/611 [00:08<00:04, 46.74it/s] 68%|██████▊   | 418/611 [00:08<00:04, 46.67it/s] 69%|██████▉   | 423/611 [00:09<00:04, 46.71it/s] 70%|███████   | 428/611 [00:09<00:03, 46.48it/s] 71%|███████   | 433/611 [00:09<00:03, 46.54it/s] 72%|███████▏  | 438/611 [00:09<00:03, 46.61it/s] 73%|███████▎  | 443/611 [00:09<00:03, 46.57it/s] 73%|███████▎  | 448/611 [00:09<00:03, 46.60it/s] 74%|███████▍  | 453/611 [00:09<00:03, 46.68it/s] 75%|███████▍  | 458/611 [00:09<00:03, 46.60it/s] 76%|███████▌  | 463/611 [00:09<00:03, 46.63it/s] 77%|███████▋  | 468/611 [00:09<00:03, 46.67it/s] 77%|███████▋  | 473/611 [00:10<00:02, 46.67it/s] 78%|███████▊  | 478/611 [00:10<00:02, 46.68it/s] 79%|███████▉  | 483/611 [00:10<00:02, 46.71it/s] 80%|███████▉  | 488/611 [00:10<00:02, 46.51it/s] 81%|████████  | 493/611 [00:10<00:02, 46.61it/s] 82%|████████▏ | 498/611 [00:10<00:02, 46.67it/s] 82%|████████▏ | 503/611 [00:10<00:02, 46.77it/s] 83%|████████▎ | 508/611 [00:10<00:02, 46.69it/s] 84%|████████▍ | 513/611 [00:10<00:02, 46.64it/s] 85%|████████▍ | 518/611 [00:11<00:01, 46.67it/s] 86%|████████▌ | 523/611 [00:11<00:01, 46.64it/s] 86%|████████▋ | 528/611 [00:11<00:01, 46.65it/s] 87%|████████▋ | 533/611 [00:11<00:01, 46.67it/s] 88%|████████▊ | 538/611 [00:11<00:01, 46.58it/s] 89%|████████▉ | 543/611 [00:11<00:01, 46.59it/s] 90%|████████▉ | 548/611 [00:11<00:01, 46.66it/s] 91%|█████████ | 553/611 [00:11<00:01, 46.63it/s] 91%|█████████▏| 558/611 [00:11<00:01, 46.63it/s] 92%|█████████▏| 563/611 [00:12<00:01, 46.65it/s] 93%|█████████▎| 568/611 [00:12<00:00, 46.71it/s] 94%|█████████▍| 573/611 [00:12<00:00, 46.69it/s] 95%|█████████▍| 578/611 [00:12<00:00, 46.68it/s] 95%|█████████▌| 583/611 [00:12<00:00, 46.55it/s] 96%|█████████▌| 588/611 [00:12<00:00, 46.62it/s] 97%|█████████▋| 593/611 [00:12<00:00, 46.66it/s] 98%|█████████▊| 598/611 [00:12<00:00, 46.66it/s] 99%|█████████▊| 603/611 [00:12<00:00, 46.69it/s]100%|█████████▉| 608/611 [00:12<00:00, 46.62it/s]100%|██████████| 611/611 [00:13<00:00, 46.76it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:58:01,364 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:58:01,364 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:58:01,364 >>   eval_loss               =     0.9761
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:58:01,364 >>   eval_runtime            = 0:00:13.08
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:58:01,364 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:58:01,364 >>   eval_samples_per_second =    372.984
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:58:01,364 >>   eval_steps_per_second   =      46.68
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:58:01,364 >>   perplexity              =     2.6542
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:58:07,135 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:58:07,139 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:58:07,139 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:58:07,139 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:58:07,139 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:58:07,454 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:58:07,455 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:58:07,722 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:58:08,762 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:58:08,762 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:58:11,320 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:58:11,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:58:11,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:58:11,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:58:11,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:58:11,657 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:58:11,658 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:58:11,922 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:58:12,085 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:58:12,085 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:11,  1.57it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:14,  1.66it/s]Extractor Predicting: 23it [00:14,  1.65it/s]Extractor Predicting: 24it [00:15,  1.65it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:17,  1.51it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.62it/s]Extractor Predicting: 33it [00:21,  1.60it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.61it/s]Extractor Predicting: 38it [00:24,  1.60it/s]Extractor Predicting: 39it [00:24,  1.58it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:26,  1.58it/s]Extractor Predicting: 43it [00:27,  1.58it/s]Extractor Predicting: 44it [00:28,  1.57it/s]Extractor Predicting: 45it [00:28,  1.61it/s]Extractor Predicting: 46it [00:29,  1.60it/s]Extractor Predicting: 47it [00:29,  1.59it/s]Extractor Predicting: 48it [00:30,  1.58it/s]Extractor Predicting: 49it [00:31,  1.58it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:32,  1.54it/s]Extractor Predicting: 52it [00:33,  1.58it/s]Extractor Predicting: 53it [00:33,  1.56it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:35,  1.50it/s]Extractor Predicting: 56it [00:35,  1.47it/s]Extractor Predicting: 57it [00:36,  1.49it/s]Extractor Predicting: 58it [00:37,  1.49it/s]Extractor Predicting: 59it [00:37,  1.52it/s]Extractor Predicting: 60it [00:38,  1.51it/s]Extractor Predicting: 61it [00:39,  1.50it/s]Extractor Predicting: 62it [00:39,  1.47it/s]Extractor Predicting: 63it [00:40,  1.48it/s]Extractor Predicting: 64it [00:41,  1.49it/s]Extractor Predicting: 65it [00:41,  1.47it/s]Extractor Predicting: 66it [00:42,  1.45it/s]Extractor Predicting: 67it [00:43,  1.46it/s]Extractor Predicting: 68it [00:43,  1.50it/s]Extractor Predicting: 69it [00:44,  1.49it/s]Extractor Predicting: 70it [00:45,  1.47it/s]Extractor Predicting: 71it [00:45,  1.48it/s]Extractor Predicting: 72it [00:46,  1.45it/s]Extractor Predicting: 73it [00:47,  1.48it/s]Extractor Predicting: 74it [00:47,  1.48it/s]Extractor Predicting: 75it [00:48,  1.49it/s]Extractor Predicting: 76it [00:49,  1.49it/s]Extractor Predicting: 77it [00:49,  1.48it/s]Extractor Predicting: 78it [00:50,  1.51it/s]Extractor Predicting: 79it [00:51,  1.53it/s]Extractor Predicting: 80it [00:51,  1.52it/s]Extractor Predicting: 81it [00:52,  1.52it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:53,  1.50it/s]Extractor Predicting: 84it [00:54,  1.49it/s]Extractor Predicting: 85it [00:55,  1.49it/s]Extractor Predicting: 86it [00:55,  1.48it/s]Extractor Predicting: 87it [00:56,  1.47it/s]Extractor Predicting: 88it [00:57,  1.44it/s]Extractor Predicting: 89it [00:58,  1.44it/s]Extractor Predicting: 90it [00:58,  1.44it/s]Extractor Predicting: 91it [00:59,  1.45it/s]Extractor Predicting: 92it [01:00,  1.50it/s]Extractor Predicting: 93it [01:00,  1.57it/s]Extractor Predicting: 94it [01:01,  1.56it/s]Extractor Predicting: 95it [01:01,  1.56it/s]Extractor Predicting: 96it [01:02,  1.56it/s]Extractor Predicting: 97it [01:03,  1.58it/s]Extractor Predicting: 98it [01:03,  1.55it/s]Extractor Predicting: 99it [01:04,  1.49it/s]Extractor Predicting: 100it [01:05,  1.52it/s]Extractor Predicting: 101it [01:05,  1.43it/s]Extractor Predicting: 102it [01:06,  1.43it/s]Extractor Predicting: 103it [01:07,  1.45it/s]Extractor Predicting: 104it [01:08,  1.48it/s]Extractor Predicting: 105it [01:08,  1.49it/s]Extractor Predicting: 106it [01:09,  1.55it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:10,  1.57it/s]Extractor Predicting: 109it [01:11,  1.56it/s]Extractor Predicting: 110it [01:11,  1.55it/s]Extractor Predicting: 111it [01:12,  1.58it/s]Extractor Predicting: 112it [01:13,  1.58it/s]Extractor Predicting: 113it [01:13,  1.40it/s]Extractor Predicting: 114it [01:14,  1.42it/s]Extractor Predicting: 115it [01:15,  1.46it/s]Extractor Predicting: 116it [01:15,  1.45it/s]Extractor Predicting: 117it [01:16,  1.44it/s]Extractor Predicting: 118it [01:17,  1.45it/s]Extractor Predicting: 119it [01:18,  1.44it/s]Extractor Predicting: 120it [01:18,  1.42it/s]Extractor Predicting: 121it [01:19,  1.43it/s]Extractor Predicting: 122it [01:20,  1.44it/s]Extractor Predicting: 123it [01:20,  1.47it/s]Extractor Predicting: 124it [01:21,  1.47it/s]Extractor Predicting: 125it [01:22,  1.49it/s]Extractor Predicting: 126it [01:22,  1.48it/s]Extractor Predicting: 127it [01:23,  1.47it/s]Extractor Predicting: 128it [01:24,  1.47it/s]Extractor Predicting: 129it [01:24,  1.50it/s]Extractor Predicting: 130it [01:25,  1.45it/s]Extractor Predicting: 131it [01:26,  1.46it/s]Extractor Predicting: 132it [01:26,  1.49it/s]Extractor Predicting: 133it [01:27,  1.47it/s]Extractor Predicting: 134it [01:28,  1.47it/s]Extractor Predicting: 135it [01:28,  1.46it/s]Extractor Predicting: 136it [01:29,  1.47it/s]Extractor Predicting: 137it [01:30,  1.45it/s]Extractor Predicting: 138it [01:31,  1.46it/s]Extractor Predicting: 139it [01:31,  1.45it/s]Extractor Predicting: 140it [01:32,  1.44it/s]Extractor Predicting: 141it [01:33,  1.46it/s]Extractor Predicting: 142it [01:33,  1.47it/s]Extractor Predicting: 143it [01:34,  1.45it/s]Extractor Predicting: 144it [01:35,  1.49it/s]Extractor Predicting: 145it [01:35,  1.52it/s]Extractor Predicting: 146it [01:36,  1.50it/s]Extractor Predicting: 147it [01:37,  1.46it/s]Extractor Predicting: 148it [01:37,  1.48it/s]Extractor Predicting: 149it [01:38,  1.47it/s]Extractor Predicting: 150it [01:39,  1.46it/s]Extractor Predicting: 151it [01:39,  1.45it/s]Extractor Predicting: 152it [01:40,  1.45it/s]Extractor Predicting: 153it [01:41,  1.45it/s]Extractor Predicting: 154it [01:41,  1.46it/s]Extractor Predicting: 155it [01:42,  1.46it/s]Extractor Predicting: 156it [01:43,  1.40it/s]Extractor Predicting: 157it [01:44,  1.36it/s]Extractor Predicting: 158it [01:44,  1.34it/s]Extractor Predicting: 159it [01:45,  1.37it/s]Extractor Predicting: 160it [01:46,  1.40it/s]Extractor Predicting: 161it [01:47,  1.41it/s]Extractor Predicting: 162it [01:47,  1.42it/s]Extractor Predicting: 163it [01:48,  1.43it/s]Extractor Predicting: 164it [01:49,  1.46it/s]Extractor Predicting: 165it [01:49,  1.45it/s]Extractor Predicting: 166it [01:50,  1.49it/s]Extractor Predicting: 167it [01:51,  1.48it/s]Extractor Predicting: 168it [01:51,  1.48it/s]Extractor Predicting: 169it [01:52,  1.49it/s]Extractor Predicting: 170it [01:53,  1.49it/s]Extractor Predicting: 171it [01:53,  1.50it/s]Extractor Predicting: 172it [01:54,  1.52it/s]Extractor Predicting: 173it [01:55,  1.47it/s]Extractor Predicting: 174it [01:55,  1.49it/s]Extractor Predicting: 175it [01:56,  1.46it/s]Extractor Predicting: 176it [01:57,  1.49it/s]Extractor Predicting: 177it [01:57,  1.46it/s]Extractor Predicting: 178it [01:58,  1.47it/s]Extractor Predicting: 179it [01:59,  1.47it/s]Extractor Predicting: 180it [01:59,  1.47it/s]Extractor Predicting: 181it [02:00,  1.47it/s]Extractor Predicting: 182it [02:01,  1.47it/s]Extractor Predicting: 183it [02:01,  1.43it/s]Extractor Predicting: 184it [02:02,  1.44it/s]Extractor Predicting: 185it [02:03,  1.53it/s]Extractor Predicting: 185it [02:03,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:00:23,353 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:00:23,358 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:00:23,359 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:00:23,359 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:00:23,359 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:00:24,057 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:00:24,058 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:00:24,320 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:00:25,322 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:00:25,322 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:00:27,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:00:27,039 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:00:27,040 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:00:27,040 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:00:27,040 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:00:27,353 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:00:27,354 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:00:28,021 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:00:28,171 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:00:28,171 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.25372279495990835,
  "recall": 0.09074149938549775,
  "score": 0.1336753168376584,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.48it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.50it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.59it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.56it/s]Extractor Predicting: 34it [00:22,  1.58it/s]Extractor Predicting: 35it [00:22,  1.60it/s]Extractor Predicting: 36it [00:23,  1.59it/s]Extractor Predicting: 37it [00:23,  1.61it/s]Extractor Predicting: 38it [00:24,  1.60it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.58it/s]Extractor Predicting: 42it [00:27,  1.58it/s]Extractor Predicting: 43it [00:27,  1.59it/s]Extractor Predicting: 44it [00:28,  1.60it/s]Extractor Predicting: 45it [00:28,  1.62it/s]Extractor Predicting: 46it [00:29,  1.65it/s]Extractor Predicting: 47it [00:30,  1.65it/s]Extractor Predicting: 48it [00:30,  1.65it/s]Extractor Predicting: 49it [00:31,  1.62it/s]Extractor Predicting: 50it [00:32,  1.61it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:33,  1.60it/s]Extractor Predicting: 54it [00:34,  1.57it/s]Extractor Predicting: 55it [00:35,  1.56it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:36,  1.58it/s]Extractor Predicting: 58it [00:37,  1.65it/s]Extractor Predicting: 59it [00:37,  1.62it/s]Extractor Predicting: 60it [00:38,  1.59it/s]Extractor Predicting: 61it [00:39,  1.55it/s]Extractor Predicting: 62it [00:39,  1.52it/s]Extractor Predicting: 63it [00:40,  1.48it/s]Extractor Predicting: 64it [00:41,  1.46it/s]Extractor Predicting: 65it [00:41,  1.45it/s]Extractor Predicting: 66it [00:42,  1.44it/s]Extractor Predicting: 67it [00:43,  1.42it/s]Extractor Predicting: 68it [00:43,  1.44it/s]Extractor Predicting: 69it [00:44,  1.45it/s]Extractor Predicting: 70it [00:45,  1.47it/s]Extractor Predicting: 71it [00:45,  1.48it/s]Extractor Predicting: 72it [00:46,  1.49it/s]Extractor Predicting: 73it [00:47,  1.53it/s]Extractor Predicting: 74it [00:47,  1.53it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:49,  1.55it/s]Extractor Predicting: 77it [00:49,  1.57it/s]Extractor Predicting: 78it [00:50,  1.55it/s]Extractor Predicting: 79it [00:50,  1.61it/s]Extractor Predicting: 80it [00:51,  1.65it/s]Extractor Predicting: 81it [00:52,  1.60it/s]Extractor Predicting: 82it [00:52,  1.61it/s]Extractor Predicting: 83it [00:53,  1.57it/s]Extractor Predicting: 84it [00:54,  1.55it/s]Extractor Predicting: 85it [00:54,  1.51it/s]Extractor Predicting: 86it [00:55,  1.49it/s]Extractor Predicting: 87it [00:56,  1.49it/s]Extractor Predicting: 88it [00:56,  1.52it/s]Extractor Predicting: 89it [00:57,  1.51it/s]Extractor Predicting: 90it [00:58,  1.52it/s]Extractor Predicting: 91it [00:58,  1.51it/s]Extractor Predicting: 92it [00:59,  1.50it/s]Extractor Predicting: 93it [01:00,  1.52it/s]Extractor Predicting: 94it [01:00,  1.53it/s]Extractor Predicting: 95it [01:01,  1.52it/s]Extractor Predicting: 96it [01:02,  1.51it/s]Extractor Predicting: 97it [01:02,  1.52it/s]Extractor Predicting: 98it [01:03,  1.52it/s]Extractor Predicting: 99it [01:04,  1.52it/s]Extractor Predicting: 100it [01:04,  1.50it/s]Extractor Predicting: 101it [01:05,  1.52it/s]Extractor Predicting: 102it [01:06,  1.53it/s]Extractor Predicting: 103it [01:06,  1.50it/s]Extractor Predicting: 104it [01:07,  1.53it/s]Extractor Predicting: 105it [01:08,  1.54it/s]Extractor Predicting: 106it [01:08,  1.55it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:09,  1.56it/s]Extractor Predicting: 109it [01:10,  1.54it/s]Extractor Predicting: 110it [01:11,  1.56it/s]Extractor Predicting: 111it [01:11,  1.56it/s]Extractor Predicting: 112it [01:12,  1.53it/s]Extractor Predicting: 113it [01:13,  1.51it/s]Extractor Predicting: 114it [01:13,  1.51it/s]Extractor Predicting: 115it [01:14,  1.51it/s]Extractor Predicting: 116it [01:15,  1.51it/s]Extractor Predicting: 117it [01:15,  1.54it/s]Extractor Predicting: 118it [01:16,  1.52it/s]Extractor Predicting: 119it [01:17,  1.52it/s]Extractor Predicting: 120it [01:17,  1.54it/s]Extractor Predicting: 121it [01:18,  1.57it/s]Extractor Predicting: 122it [01:19,  1.56it/s]Extractor Predicting: 123it [01:19,  1.53it/s]Extractor Predicting: 124it [01:20,  1.51it/s]Extractor Predicting: 125it [01:21,  1.52it/s]Extractor Predicting: 126it [01:21,  1.51it/s]Extractor Predicting: 127it [01:22,  1.54it/s]Extractor Predicting: 128it [01:23,  1.49it/s]Extractor Predicting: 129it [01:23,  1.51it/s]Extractor Predicting: 130it [01:24,  1.53it/s]Extractor Predicting: 131it [01:25,  1.51it/s]Extractor Predicting: 132it [01:25,  1.51it/s]Extractor Predicting: 133it [01:26,  1.51it/s]Extractor Predicting: 134it [01:27,  1.50it/s]Extractor Predicting: 135it [01:27,  1.51it/s]Extractor Predicting: 136it [01:28,  1.55it/s]Extractor Predicting: 137it [01:29,  1.50it/s]Extractor Predicting: 138it [01:29,  1.51it/s]Extractor Predicting: 139it [01:30,  1.53it/s]Extractor Predicting: 140it [01:30,  1.54it/s]Extractor Predicting: 141it [01:31,  1.52it/s]Extractor Predicting: 142it [01:32,  1.38it/s]Extractor Predicting: 143it [01:33,  1.34it/s]Extractor Predicting: 144it [01:33,  1.42it/s]Extractor Predicting: 145it [01:34,  1.42it/s]Extractor Predicting: 146it [01:35,  1.47it/s]Extractor Predicting: 147it [01:35,  1.52it/s]Extractor Predicting: 148it [01:36,  1.51it/s]Extractor Predicting: 149it [01:37,  1.55it/s]Extractor Predicting: 150it [01:37,  1.57it/s]Extractor Predicting: 151it [01:38,  1.59it/s]Extractor Predicting: 152it [01:39,  1.55it/s]Extractor Predicting: 153it [01:39,  1.56it/s]Extractor Predicting: 154it [01:40,  1.52it/s]Extractor Predicting: 155it [01:41,  1.51it/s]Extractor Predicting: 156it [01:41,  1.56it/s]Extractor Predicting: 157it [01:42,  1.52it/s]Extractor Predicting: 158it [01:43,  1.51it/s]Extractor Predicting: 159it [01:43,  1.52it/s]Extractor Predicting: 160it [01:44,  1.52it/s]Extractor Predicting: 161it [01:44,  1.56it/s]Extractor Predicting: 162it [01:45,  1.54it/s]Extractor Predicting: 163it [01:46,  1.53it/s]Extractor Predicting: 164it [01:46,  1.52it/s]Extractor Predicting: 165it [01:47,  1.48it/s]Extractor Predicting: 166it [01:48,  1.45it/s]Extractor Predicting: 167it [01:49,  1.46it/s]Extractor Predicting: 168it [01:49,  1.47it/s]Extractor Predicting: 169it [01:50,  1.50it/s]Extractor Predicting: 170it [01:50,  1.52it/s]Extractor Predicting: 171it [01:51,  1.53it/s]Extractor Predicting: 172it [01:52,  1.56it/s]Extractor Predicting: 173it [01:52,  1.55it/s]Extractor Predicting: 174it [01:53,  1.54it/s]Extractor Predicting: 175it [01:54,  1.54it/s]Extractor Predicting: 176it [01:54,  1.55it/s]Extractor Predicting: 177it [01:55,  1.53it/s]Extractor Predicting: 178it [01:56,  1.52it/s]Extractor Predicting: 179it [01:56,  1.49it/s]Extractor Predicting: 180it [01:57,  1.53it/s]Extractor Predicting: 181it [01:58,  1.53it/s]Extractor Predicting: 182it [01:58,  1.55it/s]Extractor Predicting: 183it [01:59,  1.57it/s]Extractor Predicting: 184it [02:00,  1.56it/s]Extractor Predicting: 185it [02:00,  1.56it/s]Extractor Predicting: 186it [02:01,  1.52it/s]Extractor Predicting: 187it [02:02,  1.53it/s]Extractor Predicting: 188it [02:02,  1.53it/s]Extractor Predicting: 189it [02:03,  1.53it/s]Extractor Predicting: 190it [02:03,  1.55it/s]Extractor Predicting: 191it [02:04,  1.55it/s]Extractor Predicting: 192it [02:05,  1.58it/s]Extractor Predicting: 193it [02:05,  1.57it/s]Extractor Predicting: 194it [02:06,  1.57it/s]Extractor Predicting: 195it [02:07,  1.55it/s]Extractor Predicting: 196it [02:07,  1.55it/s]Extractor Predicting: 197it [02:08,  1.54it/s]Extractor Predicting: 198it [02:09,  1.52it/s]Extractor Predicting: 199it [02:09,  1.52it/s]Extractor Predicting: 200it [02:10,  1.53it/s]Extractor Predicting: 201it [02:11,  1.54it/s]Extractor Predicting: 202it [02:11,  1.54it/s]Extractor Predicting: 203it [02:12,  1.56it/s]Extractor Predicting: 204it [02:12,  1.55it/s]Extractor Predicting: 205it [02:13,  1.56it/s]Extractor Predicting: 206it [02:14,  1.55it/s]Extractor Predicting: 207it [02:14,  1.56it/s]Extractor Predicting: 208it [02:15,  1.54it/s]Extractor Predicting: 209it [02:16,  1.51it/s]Extractor Predicting: 210it [02:16,  1.56it/s]Extractor Predicting: 211it [02:17,  1.54it/s]Extractor Predicting: 212it [02:18,  1.56it/s]Extractor Predicting: 213it [02:18,  1.56it/s]Extractor Predicting: 214it [02:19,  1.59it/s]Extractor Predicting: 215it [02:20,  1.57it/s]Extractor Predicting: 216it [02:20,  1.54it/s]Extractor Predicting: 217it [02:21,  1.56it/s]Extractor Predicting: 218it [02:22,  1.54it/s]Extractor Predicting: 219it [02:22,  1.53it/s]Extractor Predicting: 220it [02:23,  1.55it/s]Extractor Predicting: 221it [02:23,  1.55it/s]Extractor Predicting: 222it [02:24,  1.50it/s]Extractor Predicting: 223it [02:25,  1.45it/s]Extractor Predicting: 224it [02:26,  1.48it/s]Extractor Predicting: 225it [02:26,  1.50it/s]Extractor Predicting: 226it [02:27,  1.54it/s]Extractor Predicting: 227it [02:27,  1.54it/s]Extractor Predicting: 228it [02:28,  1.57it/s]Extractor Predicting: 229it [02:29,  1.59it/s]Extractor Predicting: 230it [02:29,  1.60it/s]Extractor Predicting: 231it [02:30,  1.61it/s]Extractor Predicting: 232it [02:31,  1.59it/s]Extractor Predicting: 233it [02:31,  1.59it/s]Extractor Predicting: 234it [02:32,  1.59it/s]Extractor Predicting: 235it [02:32,  1.56it/s]Extractor Predicting: 236it [02:33,  1.56it/s]Extractor Predicting: 237it [02:34,  1.57it/s]Extractor Predicting: 238it [02:34,  1.53it/s]Extractor Predicting: 239it [02:35,  1.56it/s]Extractor Predicting: 240it [02:36,  1.55it/s]Extractor Predicting: 241it [02:36,  1.56it/s]Extractor Predicting: 242it [02:37,  1.53it/s]Extractor Predicting: 243it [02:38,  1.50it/s]Extractor Predicting: 244it [02:38,  1.50it/s]Extractor Predicting: 245it [02:39,  1.53it/s]Extractor Predicting: 246it [02:40,  1.53it/s]Extractor Predicting: 247it [02:40,  1.56it/s]Extractor Predicting: 248it [02:41,  1.50it/s]Extractor Predicting: 249it [02:42,  1.49it/s]Extractor Predicting: 250it [02:42,  1.51it/s]Extractor Predicting: 251it [02:43,  1.52it/s]Extractor Predicting: 252it [02:44,  1.51it/s]Extractor Predicting: 253it [02:44,  1.47it/s]Extractor Predicting: 254it [02:45,  1.45it/s]Extractor Predicting: 255it [02:46,  1.47it/s]Extractor Predicting: 256it [02:46,  1.48it/s]Extractor Predicting: 257it [02:47,  1.50it/s]Extractor Predicting: 258it [02:48,  1.50it/s]Extractor Predicting: 259it [02:48,  1.49it/s]Extractor Predicting: 260it [02:49,  1.47it/s]Extractor Predicting: 261it [02:50,  1.49it/s]Extractor Predicting: 262it [02:50,  1.48it/s]Extractor Predicting: 263it [02:51,  1.49it/s]Extractor Predicting: 264it [02:52,  1.49it/s]Extractor Predicting: 265it [02:52,  1.51it/s]Extractor Predicting: 266it [02:53,  1.47it/s]Extractor Predicting: 267it [02:54,  1.47it/s]Extractor Predicting: 268it [02:55,  1.46it/s]Extractor Predicting: 269it [02:55,  1.32it/s]Extractor Predicting: 270it [02:56,  1.35it/s]Extractor Predicting: 271it [02:57,  1.38it/s]Extractor Predicting: 272it [02:58,  1.40it/s]Extractor Predicting: 273it [02:58,  1.43it/s]Extractor Predicting: 274it [02:59,  1.40it/s]Extractor Predicting: 275it [03:00,  1.42it/s]Extractor Predicting: 276it [03:00,  1.44it/s]Extractor Predicting: 277it [03:01,  1.45it/s]Extractor Predicting: 278it [03:02,  1.47it/s]Extractor Predicting: 279it [03:02,  1.45it/s]Extractor Predicting: 280it [03:03,  1.47it/s]Extractor Predicting: 281it [03:04,  1.43it/s]Extractor Predicting: 282it [03:04,  1.43it/s]Extractor Predicting: 283it [03:05,  1.44it/s]Extractor Predicting: 284it [03:06,  1.47it/s]Extractor Predicting: 285it [03:06,  1.47it/s]Extractor Predicting: 286it [03:07,  1.49it/s]Extractor Predicting: 287it [03:08,  1.44it/s]Extractor Predicting: 288it [03:08,  1.47it/s]Extractor Predicting: 289it [03:09,  1.49it/s]Extractor Predicting: 290it [03:10,  1.46it/s]Extractor Predicting: 291it [03:11,  1.43it/s]Extractor Predicting: 292it [03:11,  1.46it/s]Extractor Predicting: 293it [03:12,  1.47it/s]Extractor Predicting: 294it [03:13,  1.45it/s]Extractor Predicting: 295it [03:13,  1.45it/s]Extractor Predicting: 296it [03:14,  1.48it/s]Extractor Predicting: 297it [03:15,  1.49it/s]Extractor Predicting: 298it [03:15,  1.47it/s]Extractor Predicting: 299it [03:16,  1.47it/s]Extractor Predicting: 300it [03:17,  1.45it/s]Extractor Predicting: 301it [03:17,  1.48it/s]Extractor Predicting: 302it [03:18,  1.45it/s]Extractor Predicting: 303it [03:19,  1.49it/s]Extractor Predicting: 304it [03:19,  1.49it/s]Extractor Predicting: 305it [03:20,  1.53it/s]Extractor Predicting: 306it [03:21,  1.55it/s]Extractor Predicting: 307it [03:21,  1.52it/s]Extractor Predicting: 308it [03:22,  1.53it/s]Extractor Predicting: 309it [03:23,  1.50it/s]Extractor Predicting: 310it [03:23,  1.50it/s]Extractor Predicting: 311it [03:24,  1.47it/s]Extractor Predicting: 312it [03:25,  1.49it/s]Extractor Predicting: 313it [03:25,  1.43it/s]Extractor Predicting: 314it [03:26,  1.43it/s]Extractor Predicting: 315it [03:27,  1.46it/s]Extractor Predicting: 316it [03:27,  1.50it/s]Extractor Predicting: 317it [03:28,  1.51it/s]Extractor Predicting: 318it [03:29,  1.54it/s]Extractor Predicting: 319it [03:29,  1.50it/s]Extractor Predicting: 320it [03:30,  1.47it/s]Extractor Predicting: 321it [03:31,  1.47it/s]Extractor Predicting: 322it [03:31,  1.49it/s]Extractor Predicting: 323it [03:32,  1.45it/s]Extractor Predicting: 324it [03:33,  1.46it/s]Extractor Predicting: 325it [03:33,  1.49it/s]Extractor Predicting: 326it [03:34,  1.49it/s]Extractor Predicting: 327it [03:35,  1.50it/s]Extractor Predicting: 328it [03:36,  1.47it/s]Extractor Predicting: 329it [03:36,  1.48it/s]Extractor Predicting: 330it [03:37,  1.47it/s]Extractor Predicting: 331it [03:38,  1.48it/s]Extractor Predicting: 332it [03:38,  1.48it/s]Extractor Predicting: 333it [03:39,  1.46it/s]Extractor Predicting: 334it [03:40,  1.48it/s]Extractor Predicting: 335it [03:40,  1.47it/s]Extractor Predicting: 336it [03:41,  1.40it/s]Extractor Predicting: 337it [03:42,  1.41it/s]Extractor Predicting: 338it [03:42,  1.42it/s]Extractor Predicting: 339it [03:43,  1.44it/s]Extractor Predicting: 340it [03:44,  1.44it/s]Extractor Predicting: 341it [03:45,  1.43it/s]Extractor Predicting: 342it [03:45,  1.44it/s]Extractor Predicting: 343it [03:46,  1.46it/s]Extractor Predicting: 344it [03:47,  1.46it/s]Extractor Predicting: 345it [03:47,  1.42it/s]Extractor Predicting: 346it [03:48,  1.43it/s]Extractor Predicting: 347it [03:49,  1.44it/s]Extractor Predicting: 348it [03:49,  1.47it/s]Extractor Predicting: 349it [03:50,  1.47it/s]Extractor Predicting: 350it [03:51,  1.46it/s]Extractor Predicting: 351it [03:51,  1.49it/s]Extractor Predicting: 352it [03:52,  1.45it/s]Extractor Predicting: 353it [03:53,  1.43it/s]Extractor Predicting: 354it [03:53,  1.46it/s]Extractor Predicting: 355it [03:54,  1.48it/s]Extractor Predicting: 356it [03:55,  1.48it/s]Extractor Predicting: 357it [03:55,  1.48it/s]Extractor Predicting: 358it [03:56,  1.49it/s]Extractor Predicting: 359it [03:57,  1.48it/s]Extractor Predicting: 360it [03:58,  1.45it/s]Extractor Predicting: 361it [03:58,  1.44it/s]Extractor Predicting: 362it [03:59,  1.46it/s]Extractor Predicting: 363it [04:00,  1.50it/s]Extractor Predicting: 364it [04:00,  1.51it/s]Extractor Predicting: 365it [04:01,  1.33it/s]Extractor Predicting: 366it [04:02,  1.34it/s]Extractor Predicting: 367it [04:03,  1.36it/s]Extractor Predicting: 368it [04:03,  1.37it/s]Extractor Predicting: 369it [04:04,  1.34it/s]Extractor Predicting: 370it [04:05,  1.35it/s]Extractor Predicting: 371it [04:05,  1.39it/s]Extractor Predicting: 372it [04:06,  1.64it/s]Extractor Predicting: 372it [04:06,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:04:43,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:04:43,457 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:04:43,457 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:04:43,457 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:04:43,457 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:04:44,099 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:04:44,100 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:04:44,673 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:04:45,723 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:04:45,723 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:04:48,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:04:48,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:04:48,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:04:48,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:04:48,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:04:49,228 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:04:49,229 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:04:49,826 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:04:49,990 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:04:49,990 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.24951727097189444,
  "recall": 0.13049820466786355,
  "score": 0.1713696308848449,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.38it/s]Extractor Predicting: 25it [00:16,  1.42it/s]Extractor Predicting: 26it [00:17,  1.42it/s]Extractor Predicting: 27it [00:17,  1.43it/s]Extractor Predicting: 28it [00:18,  1.42it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:19,  1.47it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:23,  1.49it/s]Extractor Predicting: 37it [00:24,  1.47it/s]Extractor Predicting: 38it [00:25,  1.43it/s]Extractor Predicting: 39it [00:26,  1.40it/s]Extractor Predicting: 40it [00:26,  1.39it/s]Extractor Predicting: 41it [00:27,  1.39it/s]Extractor Predicting: 42it [00:28,  1.41it/s]Extractor Predicting: 43it [00:29,  1.41it/s]Extractor Predicting: 44it [00:29,  1.42it/s]Extractor Predicting: 45it [00:30,  1.40it/s]Extractor Predicting: 46it [00:31,  1.43it/s]Extractor Predicting: 47it [00:31,  1.41it/s]Extractor Predicting: 48it [00:32,  1.41it/s]Extractor Predicting: 49it [00:33,  1.42it/s]Extractor Predicting: 50it [00:33,  1.41it/s]Extractor Predicting: 51it [00:34,  1.39it/s]Extractor Predicting: 52it [00:35,  1.38it/s]Extractor Predicting: 53it [00:36,  1.40it/s]Extractor Predicting: 54it [00:36,  1.38it/s]Extractor Predicting: 55it [00:37,  1.43it/s]Extractor Predicting: 56it [00:38,  1.43it/s]Extractor Predicting: 57it [00:38,  1.41it/s]Extractor Predicting: 58it [00:39,  1.40it/s]Extractor Predicting: 59it [00:40,  1.40it/s]Extractor Predicting: 60it [00:41,  1.40it/s]Extractor Predicting: 61it [00:41,  1.36it/s]Extractor Predicting: 62it [00:42,  1.36it/s]Extractor Predicting: 63it [00:43,  1.47it/s]Extractor Predicting: 63it [00:43,  1.46it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.41384388807069217,
  "recall": 0.08420737189092,
  "score": 0.13994023904382472,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
