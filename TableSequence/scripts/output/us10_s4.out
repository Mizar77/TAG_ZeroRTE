/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_10_seed_4', 'type': 'train', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 67133
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 67233, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/model', pretrained_wv='outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=67233, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.312, loss:49726.8478
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.063, loss:2502.8688
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.058, loss:2156.4068
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.049, loss:2083.6232
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.062, loss:2033.2188
>> valid entity prec:0.4021, rec:0.5600, f1:0.4681
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 600, avg_time 2.530, loss:1859.2693
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 700, avg_time 1.039, loss:1741.7593
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 800, avg_time 1.032, loss:1647.4272
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 900, avg_time 1.030, loss:1529.2446
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1000, avg_time 1.038, loss:1433.4007
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4559, rec:0.4856, f1:0.4703
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 1100, avg_time 2.453, loss:1389.7945
g_step 1200, step 1200, avg_time 1.030, loss:1349.0508
g_step 1300, step 1300, avg_time 1.025, loss:1293.4559
g_step 1400, step 1400, avg_time 1.030, loss:1214.0549
g_step 1500, step 1500, avg_time 1.031, loss:1181.9206
>> valid entity prec:0.4846, rec:0.3963, f1:0.4361
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 1600, avg_time 2.431, loss:1172.1435
g_step 1700, step 1700, avg_time 1.023, loss:1129.8946
g_step 1800, step 1800, avg_time 1.024, loss:1090.2485
g_step 1900, step 75, avg_time 1.028, loss:1102.8492
g_step 2000, step 175, avg_time 1.030, loss:1091.9256
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4557, rec:0.4951, f1:0.4746
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 275, avg_time 2.439, loss:1038.4962
g_step 2200, step 375, avg_time 1.029, loss:1052.9514
g_step 2300, step 475, avg_time 1.032, loss:1028.0786
g_step 2400, step 575, avg_time 1.035, loss:1019.6671
g_step 2500, step 675, avg_time 1.033, loss:1000.3970
>> valid entity prec:0.5102, rec:0.5233, f1:0.5166
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 775, avg_time 2.448, loss:1001.2266
g_step 2700, step 875, avg_time 1.023, loss:1027.1828
g_step 2800, step 975, avg_time 1.026, loss:1006.0049
g_step 2900, step 1075, avg_time 1.032, loss:999.2817
g_step 3000, step 1175, avg_time 1.029, loss:976.1699
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5085, rec:0.4734, f1:0.4903
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 1275, avg_time 2.441, loss:967.4635
g_step 3200, step 1375, avg_time 1.027, loss:969.4868
g_step 3300, step 1475, avg_time 1.027, loss:939.0012
g_step 3400, step 1575, avg_time 1.031, loss:940.2990
g_step 3500, step 1675, avg_time 1.023, loss:930.6967
>> valid entity prec:0.5350, rec:0.5609, f1:0.5477
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 1775, avg_time 2.437, loss:975.2349
g_step 3700, step 50, avg_time 1.037, loss:923.6407
g_step 3800, step 150, avg_time 1.027, loss:925.1850
g_step 3900, step 250, avg_time 1.034, loss:869.6372
g_step 4000, step 350, avg_time 1.035, loss:885.5473
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5404, rec:0.5700, f1:0.5548
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 450, avg_time 2.432, loss:917.4046
g_step 4200, step 550, avg_time 1.023, loss:885.3194
g_step 4300, step 650, avg_time 1.026, loss:865.4966
g_step 4400, step 750, avg_time 1.029, loss:881.6204
g_step 4500, step 850, avg_time 1.026, loss:888.1142
>> valid entity prec:0.5816, rec:0.4953, f1:0.5350
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 950, avg_time 2.437, loss:841.5728
g_step 4700, step 1050, avg_time 1.019, loss:901.5537
g_step 4800, step 1150, avg_time 1.031, loss:893.4265
g_step 4900, step 1250, avg_time 1.032, loss:870.5568
g_step 5000, step 1350, avg_time 1.026, loss:889.2694
learning rate was adjusted to 0.0008
>> valid entity prec:0.5054, rec:0.5664, f1:0.5342
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 1450, avg_time 2.450, loss:844.1949
g_step 5200, step 1550, avg_time 1.026, loss:852.7643
g_step 5300, step 1650, avg_time 1.028, loss:867.0929
g_step 5400, step 1750, avg_time 1.023, loss:857.5427
g_step 5500, step 25, avg_time 1.030, loss:843.1702
>> valid entity prec:0.4817, rec:0.4636, f1:0.4725
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 125, avg_time 2.425, loss:825.6276
g_step 5700, step 225, avg_time 1.026, loss:801.1125
g_step 5800, step 325, avg_time 1.030, loss:817.3862
g_step 5900, step 425, avg_time 1.028, loss:833.6225
g_step 6000, step 525, avg_time 1.032, loss:833.3442
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4859, rec:0.5799, f1:0.5288
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 625, avg_time 2.440, loss:790.0875
g_step 6200, step 725, avg_time 1.034, loss:809.0008
g_step 6300, step 825, avg_time 1.024, loss:801.6311
g_step 6400, step 925, avg_time 1.031, loss:856.3965
g_step 6500, step 1025, avg_time 1.024, loss:819.0157
>> valid entity prec:0.5380, rec:0.5357, f1:0.5369
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 1125, avg_time 2.441, loss:809.9692
g_step 6700, step 1225, avg_time 1.025, loss:809.8018
g_step 6800, step 1325, avg_time 1.028, loss:792.2848
g_step 6900, step 1425, avg_time 1.036, loss:783.6603
g_step 7000, step 1525, avg_time 1.026, loss:811.6704
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5062, rec:0.5537, f1:0.5289
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 1625, avg_time 2.443, loss:789.5933
g_step 7200, step 1725, avg_time 1.034, loss:808.6882
g_step 7300, step 1825, avg_time 1.032, loss:806.3626
g_step 7400, step 100, avg_time 1.028, loss:749.8720
g_step 7500, step 200, avg_time 1.029, loss:751.2123
>> valid entity prec:0.5182, rec:0.5414, f1:0.5295
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 300, avg_time 2.436, loss:750.9601
g_step 7700, step 400, avg_time 1.030, loss:787.9062
g_step 7800, step 500, avg_time 1.027, loss:743.4077
g_step 7900, step 600, avg_time 1.025, loss:758.9484
g_step 8000, step 700, avg_time 1.024, loss:778.4104
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5069, rec:0.4876, f1:0.4971
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 800, avg_time 2.431, loss:762.3542
g_step 8200, step 900, avg_time 1.028, loss:750.7720
g_step 8300, step 1000, avg_time 1.029, loss:785.9106
g_step 8400, step 1100, avg_time 1.032, loss:790.8176
g_step 8500, step 1200, avg_time 1.028, loss:743.4680
>> valid entity prec:0.5405, rec:0.4668, f1:0.5009
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 1300, avg_time 2.435, loss:741.0009
g_step 8700, step 1400, avg_time 1.031, loss:779.3631
g_step 8800, step 1500, avg_time 1.028, loss:757.0300
g_step 8900, step 1600, avg_time 1.032, loss:790.4408
g_step 9000, step 1700, avg_time 1.026, loss:762.9315
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.5886, rec:0.3724, f1:0.4562
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9100, step 1800, avg_time 2.432, loss:768.9021
g_step 9200, step 75, avg_time 1.032, loss:720.6930
g_step 9300, step 175, avg_time 1.024, loss:708.7612
g_step 9400, step 275, avg_time 1.033, loss:725.4064
g_step 9500, step 375, avg_time 1.032, loss:711.7983
>> valid entity prec:0.5184, rec:0.4969, f1:0.5074
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9600, step 475, avg_time 2.429, loss:725.0765
g_step 9700, step 575, avg_time 1.027, loss:714.9355
g_step 9800, step 675, avg_time 1.025, loss:711.6866
g_step 9900, step 775, avg_time 1.032, loss:757.2909
g_step 10000, step 875, avg_time 1.035, loss:741.1801
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.5252, rec:0.5866, f1:0.5542
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
reach max_steps, stop training
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.20s/it]Extractor Predicting: 2it [00:07,  3.31s/it]Extractor Predicting: 3it [00:08,  2.10s/it]Extractor Predicting: 4it [00:09,  1.51s/it]Extractor Predicting: 5it [00:09,  1.21s/it]Extractor Predicting: 6it [00:10,  1.04s/it]Extractor Predicting: 7it [00:11,  1.10it/s]Extractor Predicting: 8it [00:11,  1.20it/s]Extractor Predicting: 9it [00:12,  1.29it/s]Extractor Predicting: 10it [00:13,  1.31it/s]Extractor Predicting: 11it [00:13,  1.36it/s]Extractor Predicting: 12it [00:14,  1.44it/s]Extractor Predicting: 13it [00:15,  1.47it/s]Extractor Predicting: 14it [00:15,  1.51it/s]Extractor Predicting: 15it [00:16,  1.50it/s]Extractor Predicting: 16it [00:16,  1.52it/s]Extractor Predicting: 17it [00:17,  1.54it/s]Extractor Predicting: 18it [00:18,  1.54it/s]Extractor Predicting: 19it [00:18,  1.54it/s]Extractor Predicting: 20it [00:19,  1.50it/s]Extractor Predicting: 21it [00:20,  1.49it/s]Extractor Predicting: 22it [00:21,  1.46it/s]Extractor Predicting: 23it [00:21,  1.50it/s]Extractor Predicting: 24it [00:22,  1.52it/s]Extractor Predicting: 25it [00:22,  1.52it/s]Extractor Predicting: 26it [00:23,  1.58it/s]Extractor Predicting: 27it [00:24,  1.57it/s]Extractor Predicting: 28it [00:24,  1.60it/s]Extractor Predicting: 29it [00:25,  1.57it/s]Extractor Predicting: 30it [00:26,  1.54it/s]Extractor Predicting: 31it [00:26,  1.52it/s]Extractor Predicting: 32it [00:27,  1.50it/s]Extractor Predicting: 33it [00:28,  1.46it/s]Extractor Predicting: 34it [00:28,  1.44it/s]Extractor Predicting: 35it [00:29,  1.46it/s]Extractor Predicting: 36it [00:30,  1.45it/s]Extractor Predicting: 37it [00:30,  1.44it/s]Extractor Predicting: 38it [00:31,  1.36it/s]Extractor Predicting: 39it [00:32,  1.40it/s]Extractor Predicting: 40it [00:33,  1.42it/s]Extractor Predicting: 41it [00:33,  1.42it/s]Extractor Predicting: 42it [00:34,  1.44it/s]Extractor Predicting: 43it [00:35,  1.44it/s]Extractor Predicting: 44it [00:35,  1.47it/s]Extractor Predicting: 45it [00:36,  1.48it/s]Extractor Predicting: 46it [00:37,  1.44it/s]Extractor Predicting: 47it [00:37,  1.43it/s]Extractor Predicting: 48it [00:38,  1.44it/s]Extractor Predicting: 49it [00:39,  1.42it/s]Extractor Predicting: 50it [00:40,  1.44it/s]Extractor Predicting: 51it [00:40,  1.45it/s]Extractor Predicting: 52it [00:41,  1.44it/s]Extractor Predicting: 53it [00:42,  1.47it/s]Extractor Predicting: 54it [00:42,  1.46it/s]Extractor Predicting: 55it [00:43,  1.43it/s]Extractor Predicting: 56it [00:44,  1.43it/s]Extractor Predicting: 57it [00:44,  1.41it/s]Extractor Predicting: 58it [00:45,  1.41it/s]Extractor Predicting: 59it [00:46,  1.40it/s]Extractor Predicting: 60it [00:47,  1.40it/s]Extractor Predicting: 61it [00:47,  1.41it/s]Extractor Predicting: 62it [00:48,  1.41it/s]Extractor Predicting: 63it [00:49,  1.44it/s]Extractor Predicting: 64it [00:49,  1.46it/s]Extractor Predicting: 65it [00:50,  1.44it/s]Extractor Predicting: 66it [00:51,  1.47it/s]Extractor Predicting: 67it [00:51,  1.45it/s]Extractor Predicting: 68it [00:52,  1.45it/s]Extractor Predicting: 69it [00:53,  1.44it/s]Extractor Predicting: 70it [00:54,  1.42it/s]Extractor Predicting: 71it [00:54,  1.43it/s]Extractor Predicting: 72it [00:55,  1.44it/s]Extractor Predicting: 73it [00:56,  1.44it/s]Extractor Predicting: 74it [00:56,  1.46it/s]Extractor Predicting: 75it [00:57,  1.47it/s]Extractor Predicting: 76it [00:58,  1.45it/s]Extractor Predicting: 77it [00:58,  1.44it/s]Extractor Predicting: 78it [00:59,  1.43it/s]Extractor Predicting: 79it [01:00,  1.43it/s]Extractor Predicting: 80it [01:00,  1.42it/s]Extractor Predicting: 81it [01:01,  1.47it/s]Extractor Predicting: 82it [01:02,  1.45it/s]Extractor Predicting: 83it [01:03,  1.43it/s]Extractor Predicting: 84it [01:03,  1.44it/s]Extractor Predicting: 85it [01:04,  1.46it/s]Extractor Predicting: 86it [01:05,  1.51it/s]Extractor Predicting: 87it [01:05,  1.55it/s]Extractor Predicting: 88it [01:06,  1.55it/s]Extractor Predicting: 89it [01:06,  1.54it/s]Extractor Predicting: 90it [01:07,  1.56it/s]Extractor Predicting: 91it [01:08,  1.54it/s]Extractor Predicting: 92it [01:08,  1.51it/s]Extractor Predicting: 93it [01:09,  1.54it/s]Extractor Predicting: 94it [01:10,  1.56it/s]Extractor Predicting: 95it [01:10,  1.55it/s]Extractor Predicting: 96it [01:11,  1.56it/s]Extractor Predicting: 97it [01:12,  1.54it/s]Extractor Predicting: 98it [01:12,  1.50it/s]Extractor Predicting: 99it [01:13,  1.48it/s]Extractor Predicting: 100it [01:14,  1.50it/s]Extractor Predicting: 101it [01:14,  1.54it/s]Extractor Predicting: 102it [01:15,  1.52it/s]Extractor Predicting: 103it [01:16,  1.52it/s]Extractor Predicting: 104it [01:16,  1.52it/s]Extractor Predicting: 105it [01:17,  1.54it/s]Extractor Predicting: 106it [01:18,  1.51it/s]Extractor Predicting: 107it [01:18,  1.53it/s]Extractor Predicting: 108it [01:19,  1.53it/s]Extractor Predicting: 109it [01:19,  1.56it/s]Extractor Predicting: 110it [01:20,  1.55it/s]Extractor Predicting: 111it [01:21,  1.53it/s]Extractor Predicting: 112it [01:21,  1.52it/s]Extractor Predicting: 113it [01:22,  1.52it/s]Extractor Predicting: 114it [01:23,  1.48it/s]Extractor Predicting: 115it [01:24,  1.49it/s]Extractor Predicting: 116it [01:24,  1.48it/s]Extractor Predicting: 117it [01:25,  1.47it/s]Extractor Predicting: 118it [01:26,  1.41it/s]Extractor Predicting: 119it [01:26,  1.43it/s]Extractor Predicting: 120it [01:27,  1.47it/s]Extractor Predicting: 121it [01:28,  1.47it/s]Extractor Predicting: 122it [01:28,  1.47it/s]Extractor Predicting: 123it [01:29,  1.47it/s]Extractor Predicting: 124it [01:30,  1.45it/s]Extractor Predicting: 125it [01:30,  1.45it/s]Extractor Predicting: 126it [01:31,  1.46it/s]Extractor Predicting: 127it [01:32,  1.45it/s]Extractor Predicting: 128it [01:33,  1.43it/s]Extractor Predicting: 129it [01:33,  1.40it/s]Extractor Predicting: 130it [01:34,  1.42it/s]Extractor Predicting: 131it [01:35,  1.43it/s]Extractor Predicting: 132it [01:35,  1.46it/s]Extractor Predicting: 133it [01:36,  1.46it/s]Extractor Predicting: 134it [01:37,  1.46it/s]Extractor Predicting: 135it [01:37,  1.46it/s]Extractor Predicting: 136it [01:38,  1.46it/s]Extractor Predicting: 137it [01:39,  1.45it/s]Extractor Predicting: 138it [01:39,  1.45it/s]Extractor Predicting: 139it [01:40,  1.44it/s]Extractor Predicting: 140it [01:41,  1.48it/s]Extractor Predicting: 141it [01:41,  1.47it/s]Extractor Predicting: 142it [01:42,  1.53it/s]Extractor Predicting: 142it [01:42,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.47it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.48it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.48it/s]Extractor Predicting: 25it [00:16,  1.48it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:17,  1.47it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.42it/s]Extractor Predicting: 31it [00:20,  1.41it/s]Extractor Predicting: 32it [00:21,  1.41it/s]Extractor Predicting: 33it [00:22,  1.43it/s]Extractor Predicting: 34it [00:22,  1.44it/s]Extractor Predicting: 35it [00:23,  1.44it/s]Extractor Predicting: 36it [00:24,  1.47it/s]Extractor Predicting: 37it [00:24,  1.49it/s]Extractor Predicting: 38it [00:25,  1.50it/s]Extractor Predicting: 39it [00:26,  1.48it/s]Extractor Predicting: 40it [00:26,  1.49it/s]Extractor Predicting: 41it [00:27,  1.49it/s]Extractor Predicting: 42it [00:28,  1.47it/s]Extractor Predicting: 43it [00:28,  1.48it/s]Extractor Predicting: 44it [00:29,  1.47it/s]Extractor Predicting: 45it [00:30,  1.46it/s]Extractor Predicting: 46it [00:30,  1.47it/s]Extractor Predicting: 47it [00:31,  1.48it/s]Extractor Predicting: 48it [00:32,  1.47it/s]Extractor Predicting: 49it [00:33,  1.47it/s]Extractor Predicting: 50it [00:33,  1.44it/s]Extractor Predicting: 51it [00:34,  1.46it/s]Extractor Predicting: 52it [00:35,  1.46it/s]Extractor Predicting: 53it [00:35,  1.46it/s]Extractor Predicting: 54it [00:36,  1.46it/s]Extractor Predicting: 55it [00:37,  1.43it/s]Extractor Predicting: 56it [00:37,  1.45it/s]Extractor Predicting: 57it [00:38,  1.48it/s]Extractor Predicting: 58it [00:39,  1.50it/s]Extractor Predicting: 59it [00:39,  1.49it/s]Extractor Predicting: 60it [00:40,  1.49it/s]Extractor Predicting: 61it [00:41,  1.50it/s]Extractor Predicting: 62it [00:41,  1.50it/s]Extractor Predicting: 63it [00:42,  1.50it/s]Extractor Predicting: 64it [00:43,  1.47it/s]Extractor Predicting: 65it [00:43,  1.47it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:45,  1.48it/s]Extractor Predicting: 68it [00:45,  1.52it/s]Extractor Predicting: 69it [00:46,  1.51it/s]Extractor Predicting: 70it [00:47,  1.50it/s]Extractor Predicting: 71it [00:47,  1.49it/s]Extractor Predicting: 72it [00:48,  1.50it/s]Extractor Predicting: 73it [00:49,  1.46it/s]Extractor Predicting: 74it [00:49,  1.46it/s]Extractor Predicting: 75it [00:50,  1.47it/s]Extractor Predicting: 76it [00:51,  1.47it/s]Extractor Predicting: 77it [00:52,  1.45it/s]Extractor Predicting: 78it [00:52,  1.45it/s]Extractor Predicting: 79it [00:53,  1.45it/s]Extractor Predicting: 80it [00:54,  1.45it/s]Extractor Predicting: 81it [00:54,  1.46it/s]Extractor Predicting: 82it [00:55,  1.44it/s]Extractor Predicting: 83it [00:56,  1.45it/s]Extractor Predicting: 84it [00:56,  1.46it/s]Extractor Predicting: 85it [00:57,  1.48it/s]Extractor Predicting: 86it [00:58,  1.44it/s]Extractor Predicting: 87it [00:58,  1.45it/s]Extractor Predicting: 88it [00:59,  1.42it/s]Extractor Predicting: 89it [01:00,  1.43it/s]Extractor Predicting: 90it [01:00,  1.46it/s]Extractor Predicting: 91it [01:01,  1.29it/s]Extractor Predicting: 92it [01:02,  1.33it/s]Extractor Predicting: 93it [01:03,  1.34it/s]Extractor Predicting: 94it [01:04,  1.35it/s]Extractor Predicting: 95it [01:04,  1.37it/s]Extractor Predicting: 96it [01:05,  1.38it/s]Extractor Predicting: 97it [01:06,  1.40it/s]Extractor Predicting: 98it [01:06,  1.40it/s]Extractor Predicting: 99it [01:07,  1.42it/s]Extractor Predicting: 100it [01:08,  1.42it/s]Extractor Predicting: 101it [01:09,  1.42it/s]Extractor Predicting: 102it [01:09,  1.42it/s]Extractor Predicting: 103it [01:10,  1.43it/s]Extractor Predicting: 104it [01:11,  1.43it/s]Extractor Predicting: 105it [01:11,  1.41it/s]Extractor Predicting: 106it [01:12,  1.42it/s]Extractor Predicting: 107it [01:13,  1.40it/s]Extractor Predicting: 108it [01:13,  1.40it/s]Extractor Predicting: 109it [01:14,  1.41it/s]Extractor Predicting: 110it [01:15,  1.39it/s]Extractor Predicting: 111it [01:16,  1.28it/s]Extractor Predicting: 112it [01:17,  1.34it/s]Extractor Predicting: 113it [01:17,  1.38it/s]Extractor Predicting: 114it [01:18,  1.41it/s]Extractor Predicting: 115it [01:19,  1.42it/s]Extractor Predicting: 116it [01:19,  1.43it/s]Extractor Predicting: 117it [01:20,  1.45it/s]Extractor Predicting: 118it [01:21,  1.47it/s]Extractor Predicting: 119it [01:21,  1.46it/s]Extractor Predicting: 120it [01:22,  1.48it/s]Extractor Predicting: 121it [01:23,  1.48it/s]Extractor Predicting: 122it [01:23,  1.52it/s]Extractor Predicting: 123it [01:24,  1.52it/s]Extractor Predicting: 124it [01:25,  1.51it/s]Extractor Predicting: 125it [01:25,  1.51it/s]Extractor Predicting: 126it [01:26,  1.49it/s]Extractor Predicting: 127it [01:27,  1.49it/s]Extractor Predicting: 128it [01:27,  1.50it/s]Extractor Predicting: 129it [01:28,  1.47it/s]Extractor Predicting: 130it [01:29,  1.48it/s]Extractor Predicting: 131it [01:29,  1.47it/s]Extractor Predicting: 132it [01:30,  1.48it/s]Extractor Predicting: 133it [01:31,  1.47it/s]Extractor Predicting: 134it [01:31,  1.47it/s]Extractor Predicting: 135it [01:32,  1.50it/s]Extractor Predicting: 136it [01:33,  1.50it/s]Extractor Predicting: 137it [01:33,  1.48it/s]Extractor Predicting: 138it [01:34,  1.48it/s]Extractor Predicting: 139it [01:35,  1.52it/s]Extractor Predicting: 140it [01:35,  1.49it/s]Extractor Predicting: 141it [01:36,  1.48it/s]Extractor Predicting: 142it [01:37,  1.52it/s]Extractor Predicting: 143it [01:37,  1.51it/s]Extractor Predicting: 144it [01:38,  1.48it/s]Extractor Predicting: 145it [01:39,  1.49it/s]Extractor Predicting: 146it [01:39,  1.48it/s]Extractor Predicting: 147it [01:40,  1.46it/s]Extractor Predicting: 148it [01:41,  1.46it/s]Extractor Predicting: 149it [01:41,  1.45it/s]Extractor Predicting: 150it [01:42,  1.47it/s]Extractor Predicting: 151it [01:43,  1.49it/s]Extractor Predicting: 152it [01:43,  1.50it/s]Extractor Predicting: 153it [01:44,  1.49it/s]Extractor Predicting: 154it [01:45,  1.47it/s]Extractor Predicting: 155it [01:45,  1.46it/s]Extractor Predicting: 156it [01:46,  1.45it/s]Extractor Predicting: 157it [01:47,  1.43it/s]Extractor Predicting: 158it [01:48,  1.42it/s]Extractor Predicting: 159it [01:48,  1.43it/s]Extractor Predicting: 160it [01:49,  1.43it/s]Extractor Predicting: 161it [01:50,  1.46it/s]Extractor Predicting: 162it [01:50,  1.48it/s]Extractor Predicting: 163it [01:51,  1.47it/s]Extractor Predicting: 164it [01:52,  1.48it/s]Extractor Predicting: 165it [01:52,  1.45it/s]Extractor Predicting: 166it [01:53,  1.46it/s]Extractor Predicting: 167it [01:54,  1.48it/s]Extractor Predicting: 168it [01:54,  1.47it/s]Extractor Predicting: 169it [01:55,  1.46it/s]Extractor Predicting: 170it [01:56,  1.45it/s]Extractor Predicting: 171it [01:57,  1.42it/s]Extractor Predicting: 172it [01:57,  1.42it/s]Extractor Predicting: 173it [01:58,  1.43it/s]Extractor Predicting: 174it [01:59,  1.39it/s]Extractor Predicting: 175it [01:59,  1.36it/s]Extractor Predicting: 176it [02:00,  1.38it/s]Extractor Predicting: 177it [02:01,  1.39it/s]Extractor Predicting: 178it [02:02,  1.43it/s]Extractor Predicting: 179it [02:02,  1.43it/s]Extractor Predicting: 180it [02:03,  1.47it/s]Extractor Predicting: 181it [02:04,  1.45it/s]Extractor Predicting: 182it [02:04,  1.47it/s]Extractor Predicting: 183it [02:05,  1.47it/s]Extractor Predicting: 184it [02:06,  1.50it/s]Extractor Predicting: 185it [02:06,  1.52it/s]Extractor Predicting: 186it [02:07,  1.52it/s]Extractor Predicting: 187it [02:08,  1.52it/s]Extractor Predicting: 188it [02:08,  1.51it/s]Extractor Predicting: 189it [02:09,  1.52it/s]Extractor Predicting: 190it [02:10,  1.49it/s]Extractor Predicting: 191it [02:10,  1.45it/s]Extractor Predicting: 192it [02:11,  1.47it/s]Extractor Predicting: 193it [02:12,  1.50it/s]Extractor Predicting: 194it [02:12,  1.50it/s]Extractor Predicting: 195it [02:13,  1.49it/s]Extractor Predicting: 196it [02:14,  1.51it/s]Extractor Predicting: 197it [02:14,  1.51it/s]Extractor Predicting: 198it [02:15,  1.49it/s]Extractor Predicting: 199it [02:16,  1.49it/s]Extractor Predicting: 200it [02:16,  1.49it/s]Extractor Predicting: 201it [02:17,  1.37it/s]Extractor Predicting: 202it [02:18,  1.43it/s]Extractor Predicting: 203it [02:18,  1.46it/s]Extractor Predicting: 204it [02:19,  1.47it/s]Extractor Predicting: 205it [02:20,  1.44it/s]Extractor Predicting: 206it [02:20,  1.45it/s]Extractor Predicting: 207it [02:21,  1.47it/s]Extractor Predicting: 208it [02:22,  1.49it/s]Extractor Predicting: 209it [02:23,  1.44it/s]Extractor Predicting: 210it [02:23,  1.43it/s]Extractor Predicting: 211it [02:24,  1.43it/s]Extractor Predicting: 212it [02:25,  1.45it/s]Extractor Predicting: 213it [02:25,  1.46it/s]Extractor Predicting: 214it [02:26,  1.43it/s]Extractor Predicting: 215it [02:27,  1.43it/s]Extractor Predicting: 216it [02:27,  1.46it/s]Extractor Predicting: 217it [02:28,  1.47it/s]Extractor Predicting: 218it [02:29,  1.42it/s]Extractor Predicting: 219it [02:29,  1.43it/s]Extractor Predicting: 220it [02:30,  1.43it/s]Extractor Predicting: 221it [02:31,  1.40it/s]Extractor Predicting: 222it [02:32,  1.41it/s]Extractor Predicting: 223it [02:32,  1.41it/s]Extractor Predicting: 224it [02:33,  1.45it/s]Extractor Predicting: 225it [02:34,  1.45it/s]Extractor Predicting: 226it [02:34,  1.49it/s]Extractor Predicting: 227it [02:35,  1.51it/s]Extractor Predicting: 228it [02:36,  1.48it/s]Extractor Predicting: 229it [02:36,  1.49it/s]Extractor Predicting: 230it [02:37,  1.46it/s]Extractor Predicting: 231it [02:38,  1.45it/s]Extractor Predicting: 232it [02:38,  1.46it/s]Extractor Predicting: 233it [02:39,  1.49it/s]Extractor Predicting: 234it [02:40,  1.47it/s]Extractor Predicting: 235it [02:40,  1.48it/s]Extractor Predicting: 236it [02:41,  1.45it/s]Extractor Predicting: 237it [02:42,  1.45it/s]Extractor Predicting: 238it [02:42,  1.47it/s]Extractor Predicting: 239it [02:43,  1.48it/s]Extractor Predicting: 240it [02:44,  1.48it/s]Extractor Predicting: 241it [02:45,  1.47it/s]Extractor Predicting: 242it [02:45,  1.45it/s]Extractor Predicting: 243it [02:46,  1.42it/s]Extractor Predicting: 244it [02:47,  1.44it/s]Extractor Predicting: 245it [02:47,  1.48it/s]Extractor Predicting: 246it [02:48,  1.47it/s]Extractor Predicting: 247it [02:49,  1.48it/s]Extractor Predicting: 248it [02:49,  1.49it/s]Extractor Predicting: 249it [02:50,  1.48it/s]Extractor Predicting: 250it [02:51,  1.48it/s]Extractor Predicting: 251it [02:51,  1.45it/s]Extractor Predicting: 252it [02:52,  1.45it/s]Extractor Predicting: 253it [02:53,  1.46it/s]Extractor Predicting: 254it [02:53,  1.48it/s]Extractor Predicting: 255it [02:54,  1.47it/s]Extractor Predicting: 256it [02:55,  1.46it/s]Extractor Predicting: 257it [02:55,  1.47it/s]Extractor Predicting: 258it [02:56,  1.47it/s]Extractor Predicting: 259it [02:57,  1.43it/s]Extractor Predicting: 260it [02:58,  1.45it/s]Extractor Predicting: 261it [02:58,  1.47it/s]Extractor Predicting: 262it [02:59,  1.45it/s]Extractor Predicting: 263it [03:00,  1.43it/s]Extractor Predicting: 264it [03:00,  1.43it/s]Extractor Predicting: 265it [03:01,  1.41it/s]Extractor Predicting: 266it [03:02,  1.40it/s]Extractor Predicting: 267it [03:03,  1.38it/s]Extractor Predicting: 268it [03:03,  1.40it/s]Extractor Predicting: 269it [03:04,  1.40it/s]Extractor Predicting: 270it [03:05,  1.39it/s]Extractor Predicting: 271it [03:05,  1.40it/s]Extractor Predicting: 272it [03:06,  1.40it/s]Extractor Predicting: 273it [03:07,  1.39it/s]Extractor Predicting: 274it [03:08,  1.39it/s]Extractor Predicting: 275it [03:08,  1.43it/s]Extractor Predicting: 276it [03:09,  1.43it/s]Extractor Predicting: 277it [03:10,  1.41it/s]Extractor Predicting: 278it [03:10,  1.41it/s]Extractor Predicting: 279it [03:11,  1.42it/s]Extractor Predicting: 280it [03:12,  1.41it/s]Extractor Predicting: 281it [03:12,  1.40it/s]Extractor Predicting: 282it [03:13,  1.42it/s]Extractor Predicting: 283it [03:14,  1.38it/s]Extractor Predicting: 284it [03:15,  1.36it/s]Extractor Predicting: 285it [03:15,  1.36it/s]Extractor Predicting: 286it [03:16,  1.36it/s]Extractor Predicting: 287it [03:17,  1.37it/s]Extractor Predicting: 288it [03:17,  1.79it/s]Extractor Predicting: 288it [03:17,  1.46it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.59it/s]
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_10_seed_4/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_10_seed_4', 'type': 'train', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl'}
train vocab size: 85917
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 86017, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_train_large/unseen_10_seed_4/extractor/model', pretrained_wv='outputs/wrapper/wiki_train_large/unseen_10_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=86017, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.332, loss:49937.8006
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.015, loss:2689.5925
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.053, loss:2457.3678
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.035, loss:2438.8402
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.034, loss:2325.4503
>> valid entity prec:0.3556, rec:0.3456, f1:0.3505
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 600, avg_time 3.026, loss:2255.3359
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 700, avg_time 1.050, loss:2117.5414
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 800, avg_time 1.038, loss:2018.7853
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 900, avg_time 1.067, loss:1868.9349
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1000, avg_time 1.040, loss:1794.7985
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4287, rec:0.4574, f1:0.4426
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 1100, avg_time 2.962, loss:1715.7380
g_step 1200, step 1200, avg_time 1.049, loss:1649.4132
g_step 1300, step 1300, avg_time 1.045, loss:1601.4803
g_step 1400, step 1400, avg_time 1.043, loss:1546.4082
g_step 1500, step 1500, avg_time 1.046, loss:1610.1872
>> valid entity prec:0.3973, rec:0.5021, f1:0.4436
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 1600, avg_time 2.974, loss:1482.8817
g_step 1700, step 1700, avg_time 1.050, loss:1465.9640
g_step 1800, step 1800, avg_time 1.045, loss:1427.5971
g_step 1900, step 1900, avg_time 1.054, loss:1390.3289
g_step 2000, step 2000, avg_time 1.041, loss:1438.7555
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4344, rec:0.5227, f1:0.4745
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 2100, avg_time 2.970, loss:1419.3116
g_step 2200, step 2200, avg_time 1.050, loss:1404.0147
g_step 2300, step 2300, avg_time 1.044, loss:1365.9802
g_step 2400, step 2400, avg_time 1.046, loss:1318.1506
g_step 2500, step 2500, avg_time 1.048, loss:1303.5912
>> valid entity prec:0.4503, rec:0.5173, f1:0.4815
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 2600, avg_time 2.968, loss:1330.7006
g_step 2700, step 2700, avg_time 1.041, loss:1305.9862
g_step 2800, step 2800, avg_time 1.050, loss:1332.0838
g_step 2900, step 2900, avg_time 1.039, loss:1310.5333
g_step 3000, step 3000, avg_time 1.049, loss:1245.5831
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4985, rec:0.4760, f1:0.4870
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 3100, avg_time 2.952, loss:1246.7343
g_step 3200, step 3200, avg_time 1.033, loss:1236.5216
g_step 3300, step 20, avg_time 1.046, loss:1231.3253
g_step 3400, step 120, avg_time 1.044, loss:1204.3893
g_step 3500, step 220, avg_time 1.043, loss:1182.6874
>> valid entity prec:0.4729, rec:0.5920, f1:0.5258
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 320, avg_time 2.970, loss:1185.9692
g_step 3700, step 420, avg_time 1.054, loss:1200.4269
g_step 3800, step 520, avg_time 1.032, loss:1239.9440
g_step 3900, step 620, avg_time 1.040, loss:1181.9294
g_step 4000, step 720, avg_time 1.050, loss:1182.1870
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4305, rec:0.5789, f1:0.4938
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 820, avg_time 2.973, loss:1257.9950
g_step 4200, step 920, avg_time 1.040, loss:1208.3244
g_step 4300, step 1020, avg_time 1.045, loss:1146.2811
g_step 4400, step 1120, avg_time 1.043, loss:1144.2236
g_step 4500, step 1220, avg_time 1.057, loss:1176.1144
>> valid entity prec:0.4716, rec:0.4793, f1:0.4754
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 1320, avg_time 2.966, loss:1139.6981
g_step 4700, step 1420, avg_time 1.046, loss:1165.8502
g_step 4800, step 1520, avg_time 1.051, loss:1149.6023
g_step 4900, step 1620, avg_time 1.054, loss:1107.5966
g_step 5000, step 1720, avg_time 1.028, loss:1130.5840
learning rate was adjusted to 0.0008
>> valid entity prec:0.5005, rec:0.4412, f1:0.4690
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 1820, avg_time 2.958, loss:1145.3186
g_step 5200, step 1920, avg_time 1.055, loss:1173.9677
g_step 5300, step 2020, avg_time 1.044, loss:1189.7419
g_step 5400, step 2120, avg_time 1.043, loss:1130.4829
g_step 5500, step 2220, avg_time 1.058, loss:1151.1147
>> valid entity prec:0.4851, rec:0.4937, f1:0.4894
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 2320, avg_time 2.953, loss:1110.9581
g_step 5700, step 2420, avg_time 1.044, loss:1105.6597
g_step 5800, step 2520, avg_time 1.042, loss:1134.5699
g_step 5900, step 2620, avg_time 1.034, loss:1105.5455
g_step 6000, step 2720, avg_time 1.051, loss:1118.5847
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4598, rec:0.5134, f1:0.4851
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 2820, avg_time 2.967, loss:1137.3444
g_step 6200, step 2920, avg_time 1.054, loss:1079.3929
g_step 6300, step 3020, avg_time 1.048, loss:1107.9070
g_step 6400, step 3120, avg_time 1.049, loss:1049.0748
g_step 6500, step 3220, avg_time 1.044, loss:1126.4256
>> valid entity prec:0.4266, rec:0.6008, f1:0.4989
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 40, avg_time 2.962, loss:1059.7815
g_step 6700, step 140, avg_time 1.044, loss:1109.7594
g_step 6800, step 240, avg_time 1.044, loss:1044.2046
g_step 6900, step 340, avg_time 1.050, loss:1103.2763
g_step 7000, step 440, avg_time 1.046, loss:1024.3403
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4739, rec:0.5275, f1:0.4993
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 540, avg_time 2.972, loss:1061.0461
g_step 7200, step 640, avg_time 1.042, loss:1043.6473
g_step 7300, step 740, avg_time 1.040, loss:1066.6284
g_step 7400, step 840, avg_time 1.050, loss:1065.1731
g_step 7500, step 940, avg_time 1.034, loss:1047.0107
>> valid entity prec:0.5214, rec:0.4218, f1:0.4664
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 1040, avg_time 2.952, loss:1025.6957
g_step 7700, step 1140, avg_time 1.044, loss:1097.3516
g_step 7800, step 1240, avg_time 1.047, loss:1046.0623
g_step 7900, step 1340, avg_time 1.046, loss:1040.4307
g_step 8000, step 1440, avg_time 1.043, loss:1038.5949
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4829, rec:0.4753, f1:0.4791
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 1540, avg_time 2.946, loss:1021.3873
g_step 8200, step 1640, avg_time 1.048, loss:1056.9145
g_step 8300, step 1740, avg_time 1.045, loss:1042.5371
g_step 8400, step 1840, avg_time 1.039, loss:1010.9195
g_step 8500, step 1940, avg_time 1.052, loss:1064.3917
>> valid entity prec:0.4514, rec:0.5244, f1:0.4851
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 2040, avg_time 2.966, loss:1048.1006
g_step 8700, step 2140, avg_time 1.048, loss:1020.5700
g_step 8800, step 2240, avg_time 1.036, loss:1009.5332
g_step 8900, step 2340, avg_time 1.044, loss:1055.6737
g_step 9000, step 2440, avg_time 1.052, loss:1001.5788
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.5113, rec:0.4640, f1:0.4865
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9100, step 2540, avg_time 2.959, loss:1076.4381
g_step 9200, step 2640, avg_time 1.044, loss:1002.8549
g_step 9300, step 2740, avg_time 1.052, loss:1034.8640
g_step 9400, step 2840, avg_time 1.051, loss:1024.4815
g_step 9500, step 2940, avg_time 1.046, loss:1068.4492
>> valid entity prec:0.4765, rec:0.5161, f1:0.4955
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9600, step 3040, avg_time 2.950, loss:1025.2668
g_step 9700, step 3140, avg_time 1.035, loss:1002.8014
g_step 9800, step 3240, avg_time 1.050, loss:1012.4494
g_step 9900, step 60, avg_time 1.035, loss:1007.5080
g_step 10000, step 160, avg_time 1.038, loss:955.2459
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.4357, rec:0.5345, f1:0.4801
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
reach max_steps, stop training
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'labels': ['headquarters location', 'licensed to broadcast to', 'member of political party', 'narrative location', 'notable work'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14287
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14387, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
Traceback (most recent call last):
  File "wrapper.py", line 821, in <module>
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 377, in main
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
  File "wrapper.py", line 762, in run_eval
    
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/extractor.py", line 230, in predict
    mode='predict'
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/extractor.py", line 127, in get_model
    model.load_ckpt(args.model_read_ckpt)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/joint_models.py", line 588, in load_ckpt
    state = torch.load(path+'.pt')
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/serialization.py", line 771, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/serialization.py", line 270, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/serialization.py", line 251, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_train_large/unseen_10_seed_4/extractor/model/joint.pt'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_10_seed_4', 'type': 'synthetic', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'outputs/wrapper/fewrel/unseen_10_seed_4/generator/synthetic.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 21468
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21568, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/model', pretrained_wv='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21568, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.519, loss:58968.7143
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.165, loss:2224.1055
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.170, loss:1901.1787
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 25, avg_time 1.161, loss:1802.2829
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 125, avg_time 1.164, loss:1720.3593
>> valid entity prec:0.2462, rec:0.2899, f1:0.2662
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 225, avg_time 2.637, loss:1582.7265
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 325, avg_time 1.154, loss:1457.1188
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 1.168, loss:1356.3141
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 1.151, loss:1382.8559
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 1.167, loss:1324.7357
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4458, rec:0.3596, f1:0.3980
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 350, avg_time 2.571, loss:1297.4951
g_step 1200, step 75, avg_time 1.157, loss:1244.0378
g_step 1300, step 175, avg_time 1.163, loss:1228.3203
g_step 1400, step 275, avg_time 1.151, loss:1183.7138
g_step 1500, step 375, avg_time 1.174, loss:1207.7489
>> valid entity prec:0.5844, rec:0.4300, f1:0.4954
>> valid relation prec:0.0248, rec:0.0014, f1:0.0027
>> valid relation with NER prec:0.0248, rec:0.0014, f1:0.0027
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 100, avg_time 1.151, loss:1145.4773
g_step 1700, step 200, avg_time 1.171, loss:1130.2230
g_step 1800, step 300, avg_time 1.163, loss:1175.2021
g_step 1900, step 25, avg_time 1.163, loss:1129.1183
g_step 2000, step 125, avg_time 1.174, loss:1101.6593
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4866, rec:0.4560, f1:0.4708
>> valid relation prec:0.2195, rec:0.0258, f1:0.0461
>> valid relation with NER prec:0.2195, rec:0.0258, f1:0.0461
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 225, avg_time 2.549, loss:1119.4704
g_step 2200, step 325, avg_time 1.158, loss:1101.7692
g_step 2300, step 50, avg_time 1.173, loss:1041.3454
g_step 2400, step 150, avg_time 1.163, loss:1092.4091
g_step 2500, step 250, avg_time 1.159, loss:1061.6938
>> valid entity prec:0.5145, rec:0.5212, f1:0.5178
>> valid relation prec:0.1738, rec:0.0421, f1:0.0678
>> valid relation with NER prec:0.1738, rec:0.0421, f1:0.0678
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 350, avg_time 2.542, loss:1035.4082
g_step 2700, step 75, avg_time 1.155, loss:1031.9904
g_step 2800, step 175, avg_time 1.157, loss:1047.2042
g_step 2900, step 275, avg_time 1.182, loss:1036.5579
g_step 3000, step 375, avg_time 1.164, loss:1013.2021
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4799, rec:0.4972, f1:0.4884
>> valid relation prec:0.3386, rec:0.0678, f1:0.1130
>> valid relation with NER prec:0.3386, rec:0.0678, f1:0.1130
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 100, avg_time 1.168, loss:985.0277
g_step 3200, step 200, avg_time 1.146, loss:1013.6880
g_step 3300, step 300, avg_time 1.166, loss:998.4518
g_step 3400, step 25, avg_time 1.164, loss:979.1384
g_step 3500, step 125, avg_time 1.163, loss:966.7695
>> valid entity prec:0.4796, rec:0.5805, f1:0.5252
>> valid relation prec:0.1990, rec:0.0584, f1:0.0903
>> valid relation with NER prec:0.1990, rec:0.0584, f1:0.0903
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 225, avg_time 2.565, loss:970.9024
g_step 3700, step 325, avg_time 1.167, loss:972.8389
g_step 3800, step 50, avg_time 1.149, loss:963.7510
g_step 3900, step 150, avg_time 1.185, loss:953.2478
g_step 4000, step 250, avg_time 1.162, loss:939.5856
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5616, rec:0.4252, f1:0.4840
>> valid relation prec:0.1395, rec:0.0220, f1:0.0381
>> valid relation with NER prec:0.1395, rec:0.0220, f1:0.0381
g_step 4100, step 350, avg_time 2.533, loss:985.8014
g_step 4200, step 75, avg_time 1.172, loss:924.8032
g_step 4300, step 175, avg_time 1.160, loss:932.3480
g_step 4400, step 275, avg_time 1.164, loss:920.9408
g_step 4500, step 375, avg_time 1.160, loss:928.7880
>> valid entity prec:0.5030, rec:0.5111, f1:0.5070
>> valid relation prec:0.2950, rec:0.0375, f1:0.0665
>> valid relation with NER prec:0.2950, rec:0.0375, f1:0.0665
g_step 4600, step 100, avg_time 1.158, loss:892.4085
g_step 4700, step 200, avg_time 1.152, loss:889.3083
g_step 4800, step 300, avg_time 1.166, loss:908.9632
g_step 4900, step 25, avg_time 1.188, loss:916.5309
g_step 5000, step 125, avg_time 1.156, loss:874.9968
learning rate was adjusted to 0.0008
>> valid entity prec:0.5079, rec:0.5669, f1:0.5357
>> valid relation prec:0.2068, rec:0.0676, f1:0.1019
>> valid relation with NER prec:0.2068, rec:0.0676, f1:0.1019
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5100, step 225, avg_time 2.569, loss:872.6934
g_step 5200, step 325, avg_time 1.173, loss:916.6557
g_step 5300, step 50, avg_time 1.164, loss:847.5737
g_step 5400, step 150, avg_time 1.150, loss:854.7376
g_step 5500, step 250, avg_time 1.173, loss:878.4224
>> valid entity prec:0.5052, rec:0.4608, f1:0.4820
>> valid relation prec:0.2306, rec:0.0621, f1:0.0979
>> valid relation with NER prec:0.2306, rec:0.0621, f1:0.0979
g_step 5600, step 350, avg_time 2.558, loss:872.5486
g_step 5700, step 75, avg_time 1.153, loss:833.6893
g_step 5800, step 175, avg_time 1.160, loss:815.0524
g_step 5900, step 275, avg_time 1.164, loss:862.6764
g_step 6000, step 375, avg_time 1.190, loss:835.0401
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5452, rec:0.4375, f1:0.4855
>> valid relation prec:0.2024, rec:0.0684, f1:0.1023
>> valid relation with NER prec:0.2024, rec:0.0684, f1:0.1023
g_step 6100, step 100, avg_time 1.170, loss:812.6855
g_step 6200, step 200, avg_time 1.183, loss:789.6006
g_step 6300, step 300, avg_time 1.152, loss:827.7636
g_step 6400, step 25, avg_time 1.176, loss:816.9224
g_step 6500, step 125, avg_time 1.172, loss:794.4188
>> valid entity prec:0.5176, rec:0.5219, f1:0.5198
>> valid relation prec:0.2045, rec:0.0799, f1:0.1149
>> valid relation with NER prec:0.2045, rec:0.0799, f1:0.1149
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6600, step 225, avg_time 2.533, loss:790.8116
g_step 6700, step 325, avg_time 1.180, loss:798.8363
g_step 6800, step 50, avg_time 1.179, loss:771.4626
g_step 6900, step 150, avg_time 1.164, loss:774.5167
g_step 7000, step 250, avg_time 1.170, loss:759.2616
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5337, rec:0.5076, f1:0.5203
>> valid relation prec:0.1951, rec:0.0747, f1:0.1081
>> valid relation with NER prec:0.1951, rec:0.0747, f1:0.1081
g_step 7100, step 350, avg_time 2.542, loss:756.0266
g_step 7200, step 75, avg_time 1.186, loss:749.5887
g_step 7300, step 175, avg_time 1.157, loss:755.2672
g_step 7400, step 275, avg_time 1.177, loss:729.5747
g_step 7500, step 375, avg_time 1.175, loss:762.8207
>> valid entity prec:0.5414, rec:0.4533, f1:0.4935
>> valid relation prec:0.2362, rec:0.0833, f1:0.1232
>> valid relation with NER prec:0.2362, rec:0.0833, f1:0.1232
new max relation f1 on valid!
new max relation f1 with NER on valid!
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.18s/it]Extractor Predicting: 2it [00:07,  3.30s/it]Extractor Predicting: 3it [00:08,  2.09s/it]Extractor Predicting: 4it [00:09,  1.50s/it]Extractor Predicting: 5it [00:09,  1.21s/it]Extractor Predicting: 6it [00:10,  1.04s/it]Extractor Predicting: 7it [00:11,  1.11it/s]Extractor Predicting: 8it [00:11,  1.22it/s]Extractor Predicting: 9it [00:12,  1.31it/s]Extractor Predicting: 10it [00:13,  1.33it/s]Extractor Predicting: 11it [00:13,  1.38it/s]Extractor Predicting: 12it [00:14,  1.45it/s]Extractor Predicting: 13it [00:14,  1.49it/s]Extractor Predicting: 14it [00:15,  1.52it/s]Extractor Predicting: 15it [00:16,  1.52it/s]Extractor Predicting: 16it [00:16,  1.54it/s]Extractor Predicting: 17it [00:17,  1.56it/s]Extractor Predicting: 18it [00:18,  1.55it/s]Extractor Predicting: 19it [00:18,  1.56it/s]Extractor Predicting: 20it [00:19,  1.52it/s]Extractor Predicting: 21it [00:20,  1.50it/s]Extractor Predicting: 22it [00:20,  1.47it/s]Extractor Predicting: 23it [00:21,  1.51it/s]Extractor Predicting: 24it [00:22,  1.54it/s]Extractor Predicting: 25it [00:22,  1.54it/s]Extractor Predicting: 26it [00:23,  1.60it/s]Extractor Predicting: 27it [00:23,  1.59it/s]Extractor Predicting: 28it [00:24,  1.62it/s]Extractor Predicting: 29it [00:25,  1.59it/s]Extractor Predicting: 30it [00:25,  1.55it/s]Extractor Predicting: 31it [00:26,  1.53it/s]Extractor Predicting: 32it [00:27,  1.51it/s]Extractor Predicting: 33it [00:27,  1.47it/s]Extractor Predicting: 34it [00:28,  1.45it/s]Extractor Predicting: 35it [00:29,  1.46it/s]Extractor Predicting: 36it [00:30,  1.45it/s]Extractor Predicting: 37it [00:30,  1.44it/s]Extractor Predicting: 38it [00:31,  1.43it/s]Extractor Predicting: 39it [00:32,  1.46it/s]Extractor Predicting: 40it [00:32,  1.46it/s]Extractor Predicting: 41it [00:33,  1.45it/s]Extractor Predicting: 42it [00:34,  1.46it/s]Extractor Predicting: 43it [00:34,  1.46it/s]Extractor Predicting: 44it [00:35,  1.48it/s]Extractor Predicting: 45it [00:36,  1.50it/s]Extractor Predicting: 46it [00:36,  1.46it/s]Extractor Predicting: 47it [00:37,  1.45it/s]Extractor Predicting: 48it [00:38,  1.45it/s]Extractor Predicting: 49it [00:38,  1.44it/s]Extractor Predicting: 50it [00:39,  1.45it/s]Extractor Predicting: 51it [00:40,  1.46it/s]Extractor Predicting: 52it [00:41,  1.44it/s]Extractor Predicting: 53it [00:41,  1.48it/s]Extractor Predicting: 54it [00:42,  1.46it/s]Extractor Predicting: 55it [00:43,  1.35it/s]Extractor Predicting: 56it [00:43,  1.37it/s]Extractor Predicting: 57it [00:44,  1.37it/s]Extractor Predicting: 58it [00:45,  1.39it/s]Extractor Predicting: 59it [00:46,  1.39it/s]Extractor Predicting: 60it [00:46,  1.39it/s]Extractor Predicting: 61it [00:47,  1.42it/s]Extractor Predicting: 62it [00:48,  1.42it/s]Extractor Predicting: 63it [00:48,  1.45it/s]Extractor Predicting: 64it [00:49,  1.48it/s]Extractor Predicting: 65it [00:50,  1.46it/s]Extractor Predicting: 66it [00:50,  1.49it/s]Extractor Predicting: 67it [00:51,  1.47it/s]Extractor Predicting: 68it [00:52,  1.46it/s]Extractor Predicting: 69it [00:52,  1.45it/s]Extractor Predicting: 70it [00:53,  1.44it/s]Extractor Predicting: 71it [00:54,  1.45it/s]Extractor Predicting: 72it [00:55,  1.46it/s]Extractor Predicting: 73it [00:55,  1.46it/s]Extractor Predicting: 74it [00:56,  1.48it/s]Extractor Predicting: 75it [00:57,  1.48it/s]Extractor Predicting: 76it [00:57,  1.46it/s]Extractor Predicting: 77it [00:58,  1.46it/s]Extractor Predicting: 78it [00:59,  1.45it/s]Extractor Predicting: 79it [00:59,  1.45it/s]Extractor Predicting: 80it [01:00,  1.44it/s]Extractor Predicting: 81it [01:01,  1.49it/s]Extractor Predicting: 82it [01:01,  1.47it/s]Extractor Predicting: 83it [01:02,  1.45it/s]Extractor Predicting: 84it [01:03,  1.46it/s]Extractor Predicting: 85it [01:03,  1.48it/s]Extractor Predicting: 86it [01:04,  1.53it/s]Extractor Predicting: 87it [01:05,  1.56it/s]Extractor Predicting: 88it [01:05,  1.56it/s]Extractor Predicting: 89it [01:06,  1.56it/s]Extractor Predicting: 90it [01:06,  1.57it/s]Extractor Predicting: 91it [01:07,  1.55it/s]Extractor Predicting: 92it [01:08,  1.53it/s]Extractor Predicting: 93it [01:08,  1.56it/s]Extractor Predicting: 94it [01:09,  1.58it/s]Extractor Predicting: 95it [01:10,  1.57it/s]Extractor Predicting: 96it [01:10,  1.58it/s]Extractor Predicting: 97it [01:11,  1.56it/s]Extractor Predicting: 98it [01:12,  1.53it/s]Extractor Predicting: 99it [01:12,  1.50it/s]Extractor Predicting: 100it [01:13,  1.53it/s]Extractor Predicting: 101it [01:14,  1.56it/s]Extractor Predicting: 102it [01:14,  1.55it/s]Extractor Predicting: 103it [01:15,  1.55it/s]Extractor Predicting: 104it [01:16,  1.55it/s]Extractor Predicting: 105it [01:16,  1.58it/s]Extractor Predicting: 106it [01:17,  1.55it/s]Extractor Predicting: 107it [01:17,  1.56it/s]Extractor Predicting: 108it [01:18,  1.55it/s]Extractor Predicting: 109it [01:19,  1.59it/s]Extractor Predicting: 110it [01:19,  1.57it/s]Extractor Predicting: 111it [01:20,  1.56it/s]Extractor Predicting: 112it [01:21,  1.54it/s]Extractor Predicting: 113it [01:21,  1.54it/s]Extractor Predicting: 114it [01:22,  1.50it/s]Extractor Predicting: 115it [01:23,  1.51it/s]Extractor Predicting: 116it [01:23,  1.50it/s]Extractor Predicting: 117it [01:24,  1.48it/s]Extractor Predicting: 118it [01:25,  1.55it/s]Extractor Predicting: 119it [01:25,  1.53it/s]Extractor Predicting: 120it [01:26,  1.54it/s]Extractor Predicting: 121it [01:27,  1.53it/s]Extractor Predicting: 122it [01:27,  1.51it/s]Extractor Predicting: 123it [01:28,  1.51it/s]Extractor Predicting: 124it [01:29,  1.47it/s]Extractor Predicting: 125it [01:29,  1.47it/s]Extractor Predicting: 126it [01:30,  1.48it/s]Extractor Predicting: 127it [01:31,  1.46it/s]Extractor Predicting: 128it [01:31,  1.46it/s]Extractor Predicting: 129it [01:32,  1.42it/s]Extractor Predicting: 130it [01:33,  1.33it/s]Extractor Predicting: 131it [01:34,  1.37it/s]Extractor Predicting: 132it [01:34,  1.42it/s]Extractor Predicting: 133it [01:35,  1.44it/s]Extractor Predicting: 134it [01:36,  1.45it/s]Extractor Predicting: 135it [01:36,  1.45it/s]Extractor Predicting: 136it [01:37,  1.45it/s]Extractor Predicting: 137it [01:38,  1.45it/s]Extractor Predicting: 138it [01:38,  1.46it/s]Extractor Predicting: 139it [01:39,  1.45it/s]Extractor Predicting: 140it [01:40,  1.49it/s]Extractor Predicting: 141it [01:40,  1.47it/s]Extractor Predicting: 142it [01:41,  1.54it/s]Extractor Predicting: 142it [01:41,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.23778195488721804,
  "recall": 0.07243057543658746,
  "score": 0.11103796357252578,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.51it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.48it/s]Extractor Predicting: 30it [00:19,  1.44it/s]Extractor Predicting: 31it [00:20,  1.44it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:21,  1.45it/s]Extractor Predicting: 34it [00:22,  1.46it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.39it/s]Extractor Predicting: 37it [00:24,  1.43it/s]Extractor Predicting: 38it [00:25,  1.46it/s]Extractor Predicting: 39it [00:26,  1.46it/s]Extractor Predicting: 40it [00:26,  1.47it/s]Extractor Predicting: 41it [00:27,  1.49it/s]Extractor Predicting: 42it [00:28,  1.48it/s]Extractor Predicting: 43it [00:28,  1.48it/s]Extractor Predicting: 44it [00:29,  1.48it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:30,  1.49it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:32,  1.49it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:33,  1.47it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:34,  1.49it/s]Extractor Predicting: 53it [00:35,  1.48it/s]Extractor Predicting: 54it [00:36,  1.48it/s]Extractor Predicting: 55it [00:36,  1.45it/s]Extractor Predicting: 56it [00:37,  1.47it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.51it/s]Extractor Predicting: 61it [00:40,  1.51it/s]Extractor Predicting: 62it [00:41,  1.52it/s]Extractor Predicting: 63it [00:42,  1.52it/s]Extractor Predicting: 64it [00:42,  1.47it/s]Extractor Predicting: 65it [00:43,  1.47it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:44,  1.48it/s]Extractor Predicting: 68it [00:45,  1.53it/s]Extractor Predicting: 69it [00:46,  1.52it/s]Extractor Predicting: 70it [00:46,  1.51it/s]Extractor Predicting: 71it [00:47,  1.50it/s]Extractor Predicting: 72it [00:48,  1.51it/s]Extractor Predicting: 73it [00:48,  1.46it/s]Extractor Predicting: 74it [00:49,  1.47it/s]Extractor Predicting: 75it [00:50,  1.48it/s]Extractor Predicting: 76it [00:50,  1.48it/s]Extractor Predicting: 77it [00:51,  1.45it/s]Extractor Predicting: 78it [00:52,  1.46it/s]Extractor Predicting: 79it [00:52,  1.47it/s]Extractor Predicting: 80it [00:53,  1.47it/s]Extractor Predicting: 81it [00:54,  1.48it/s]Extractor Predicting: 82it [00:54,  1.45it/s]Extractor Predicting: 83it [00:55,  1.46it/s]Extractor Predicting: 84it [00:56,  1.47it/s]Extractor Predicting: 85it [00:56,  1.49it/s]Extractor Predicting: 86it [00:57,  1.46it/s]Extractor Predicting: 87it [00:58,  1.47it/s]Extractor Predicting: 88it [00:59,  1.44it/s]Extractor Predicting: 89it [00:59,  1.46it/s]Extractor Predicting: 90it [01:00,  1.48it/s]Extractor Predicting: 91it [01:01,  1.46it/s]Extractor Predicting: 92it [01:01,  1.46it/s]Extractor Predicting: 93it [01:02,  1.44it/s]Extractor Predicting: 94it [01:03,  1.42it/s]Extractor Predicting: 95it [01:03,  1.43it/s]Extractor Predicting: 96it [01:04,  1.43it/s]Extractor Predicting: 97it [01:05,  1.44it/s]Extractor Predicting: 98it [01:06,  1.44it/s]Extractor Predicting: 99it [01:06,  1.45it/s]Extractor Predicting: 100it [01:07,  1.46it/s]Extractor Predicting: 101it [01:08,  1.45it/s]Extractor Predicting: 102it [01:08,  1.45it/s]Extractor Predicting: 103it [01:09,  1.46it/s]Extractor Predicting: 104it [01:10,  1.46it/s]Extractor Predicting: 105it [01:10,  1.45it/s]Extractor Predicting: 106it [01:11,  1.45it/s]Extractor Predicting: 107it [01:12,  1.43it/s]Extractor Predicting: 108it [01:12,  1.43it/s]Extractor Predicting: 109it [01:13,  1.44it/s]Extractor Predicting: 110it [01:14,  1.42it/s]Extractor Predicting: 111it [01:15,  1.42it/s]Extractor Predicting: 112it [01:15,  1.45it/s]Extractor Predicting: 113it [01:16,  1.48it/s]Extractor Predicting: 114it [01:17,  1.49it/s]Extractor Predicting: 115it [01:17,  1.48it/s]Extractor Predicting: 116it [01:18,  1.48it/s]Extractor Predicting: 117it [01:19,  1.50it/s]Extractor Predicting: 118it [01:19,  1.51it/s]Extractor Predicting: 119it [01:20,  1.50it/s]Extractor Predicting: 120it [01:20,  1.51it/s]Extractor Predicting: 121it [01:21,  1.50it/s]Extractor Predicting: 122it [01:22,  1.54it/s]Extractor Predicting: 123it [01:22,  1.53it/s]Extractor Predicting: 124it [01:23,  1.53it/s]Extractor Predicting: 125it [01:24,  1.53it/s]Extractor Predicting: 126it [01:24,  1.50it/s]Extractor Predicting: 127it [01:25,  1.50it/s]Extractor Predicting: 128it [01:26,  1.51it/s]Extractor Predicting: 129it [01:26,  1.49it/s]Extractor Predicting: 130it [01:27,  1.50it/s]Extractor Predicting: 131it [01:28,  1.48it/s]Extractor Predicting: 132it [01:28,  1.49it/s]Extractor Predicting: 133it [01:29,  1.48it/s]Extractor Predicting: 134it [01:30,  1.49it/s]Extractor Predicting: 135it [01:30,  1.51it/s]Extractor Predicting: 136it [01:31,  1.51it/s]Extractor Predicting: 137it [01:32,  1.50it/s]Extractor Predicting: 138it [01:32,  1.49it/s]Extractor Predicting: 139it [01:33,  1.39it/s]Extractor Predicting: 140it [01:34,  1.41it/s]Extractor Predicting: 141it [01:35,  1.42it/s]Extractor Predicting: 142it [01:35,  1.48it/s]Extractor Predicting: 143it [01:36,  1.48it/s]Extractor Predicting: 144it [01:37,  1.47it/s]Extractor Predicting: 145it [01:37,  1.48it/s]Extractor Predicting: 146it [01:38,  1.49it/s]Extractor Predicting: 147it [01:39,  1.47it/s]Extractor Predicting: 148it [01:39,  1.46it/s]Extractor Predicting: 149it [01:40,  1.45it/s]Extractor Predicting: 150it [01:41,  1.47it/s]Extractor Predicting: 151it [01:41,  1.49it/s]Extractor Predicting: 152it [01:42,  1.51it/s]Extractor Predicting: 153it [01:43,  1.50it/s]Extractor Predicting: 154it [01:43,  1.48it/s]Extractor Predicting: 155it [01:44,  1.47it/s]Extractor Predicting: 156it [01:45,  1.46it/s]Extractor Predicting: 157it [01:45,  1.44it/s]Extractor Predicting: 158it [01:46,  1.44it/s]Extractor Predicting: 159it [01:47,  1.45it/s]Extractor Predicting: 160it [01:48,  1.45it/s]Extractor Predicting: 161it [01:48,  1.48it/s]Extractor Predicting: 162it [01:49,  1.49it/s]Extractor Predicting: 163it [01:50,  1.48it/s]Extractor Predicting: 164it [01:50,  1.49it/s]Extractor Predicting: 165it [01:51,  1.46it/s]Extractor Predicting: 166it [01:52,  1.48it/s]Extractor Predicting: 167it [01:52,  1.50it/s]Extractor Predicting: 168it [01:53,  1.49it/s]Extractor Predicting: 169it [01:54,  1.48it/s]Extractor Predicting: 170it [01:54,  1.45it/s]Extractor Predicting: 171it [01:55,  1.42it/s]Extractor Predicting: 172it [01:56,  1.43it/s]Extractor Predicting: 173it [01:56,  1.44it/s]Extractor Predicting: 174it [01:57,  1.40it/s]Extractor Predicting: 175it [01:58,  1.37it/s]Extractor Predicting: 176it [01:59,  1.40it/s]Extractor Predicting: 177it [01:59,  1.40it/s]Extractor Predicting: 178it [02:00,  1.44it/s]Extractor Predicting: 179it [02:01,  1.44it/s]Extractor Predicting: 180it [02:01,  1.48it/s]Extractor Predicting: 181it [02:02,  1.47it/s]Extractor Predicting: 182it [02:03,  1.49it/s]Extractor Predicting: 183it [02:03,  1.48it/s]Extractor Predicting: 184it [02:04,  1.51it/s]Extractor Predicting: 185it [02:05,  1.53it/s]Extractor Predicting: 186it [02:05,  1.53it/s]Extractor Predicting: 187it [02:06,  1.54it/s]Extractor Predicting: 188it [02:07,  1.53it/s]Extractor Predicting: 189it [02:07,  1.53it/s]Extractor Predicting: 190it [02:08,  1.50it/s]Extractor Predicting: 191it [02:09,  1.46it/s]Extractor Predicting: 192it [02:09,  1.48it/s]Extractor Predicting: 193it [02:10,  1.51it/s]Extractor Predicting: 194it [02:11,  1.51it/s]Extractor Predicting: 195it [02:11,  1.51it/s]Extractor Predicting: 196it [02:12,  1.52it/s]Extractor Predicting: 197it [02:13,  1.53it/s]Extractor Predicting: 198it [02:13,  1.50it/s]Extractor Predicting: 199it [02:14,  1.51it/s]Extractor Predicting: 200it [02:15,  1.50it/s]Extractor Predicting: 201it [02:15,  1.51it/s]Extractor Predicting: 202it [02:16,  1.54it/s]Extractor Predicting: 203it [02:16,  1.54it/s]Extractor Predicting: 204it [02:17,  1.52it/s]Extractor Predicting: 205it [02:18,  1.48it/s]Extractor Predicting: 206it [02:19,  1.48it/s]Extractor Predicting: 207it [02:19,  1.51it/s]Extractor Predicting: 208it [02:20,  1.52it/s]Extractor Predicting: 209it [02:21,  1.47it/s]Extractor Predicting: 210it [02:21,  1.45it/s]Extractor Predicting: 211it [02:22,  1.45it/s]Extractor Predicting: 212it [02:23,  1.48it/s]Extractor Predicting: 213it [02:23,  1.49it/s]Extractor Predicting: 214it [02:24,  1.45it/s]Extractor Predicting: 215it [02:25,  1.46it/s]Extractor Predicting: 216it [02:25,  1.49it/s]Extractor Predicting: 217it [02:26,  1.50it/s]Extractor Predicting: 218it [02:27,  1.44it/s]Extractor Predicting: 219it [02:27,  1.46it/s]Extractor Predicting: 220it [02:28,  1.46it/s]Extractor Predicting: 221it [02:29,  1.43it/s]Extractor Predicting: 222it [02:30,  1.43it/s]Extractor Predicting: 223it [02:30,  1.44it/s]Extractor Predicting: 224it [02:31,  1.47it/s]Extractor Predicting: 225it [02:32,  1.46it/s]Extractor Predicting: 226it [02:32,  1.50it/s]Extractor Predicting: 227it [02:33,  1.53it/s]Extractor Predicting: 228it [02:33,  1.50it/s]Extractor Predicting: 229it [02:34,  1.50it/s]Extractor Predicting: 230it [02:35,  1.47it/s]Extractor Predicting: 231it [02:36,  1.47it/s]Extractor Predicting: 232it [02:36,  1.47it/s]Extractor Predicting: 233it [02:37,  1.51it/s]Extractor Predicting: 234it [02:38,  1.49it/s]Extractor Predicting: 235it [02:38,  1.50it/s]Extractor Predicting: 236it [02:39,  1.47it/s]Extractor Predicting: 237it [02:40,  1.47it/s]Extractor Predicting: 238it [02:40,  1.49it/s]Extractor Predicting: 239it [02:41,  1.50it/s]Extractor Predicting: 240it [02:42,  1.49it/s]Extractor Predicting: 241it [02:42,  1.48it/s]Extractor Predicting: 242it [02:43,  1.34it/s]Extractor Predicting: 243it [02:44,  1.35it/s]Extractor Predicting: 244it [02:45,  1.38it/s]Extractor Predicting: 245it [02:45,  1.44it/s]Extractor Predicting: 246it [02:46,  1.44it/s]Extractor Predicting: 247it [02:47,  1.47it/s]Extractor Predicting: 248it [02:47,  1.47it/s]Extractor Predicting: 249it [02:48,  1.48it/s]Extractor Predicting: 250it [02:49,  1.47it/s]Extractor Predicting: 251it [02:49,  1.45it/s]Extractor Predicting: 252it [02:50,  1.45it/s]Extractor Predicting: 253it [02:51,  1.46it/s]Extractor Predicting: 254it [02:51,  1.48it/s]Extractor Predicting: 255it [02:52,  1.48it/s]Extractor Predicting: 256it [02:53,  1.46it/s]Extractor Predicting: 257it [02:53,  1.48it/s]Extractor Predicting: 258it [02:54,  1.48it/s]Extractor Predicting: 259it [02:55,  1.44it/s]Extractor Predicting: 260it [02:55,  1.46it/s]Extractor Predicting: 261it [02:56,  1.48it/s]Extractor Predicting: 262it [02:57,  1.46it/s]Extractor Predicting: 263it [02:58,  1.45it/s]Extractor Predicting: 264it [02:58,  1.45it/s]Extractor Predicting: 265it [02:59,  1.44it/s]Extractor Predicting: 266it [03:00,  1.42it/s]Extractor Predicting: 267it [03:00,  1.40it/s]Extractor Predicting: 268it [03:01,  1.42it/s]Extractor Predicting: 269it [03:02,  1.42it/s]Extractor Predicting: 270it [03:02,  1.41it/s]Extractor Predicting: 271it [03:03,  1.42it/s]Extractor Predicting: 272it [03:04,  1.42it/s]Extractor Predicting: 273it [03:05,  1.41it/s]Extractor Predicting: 274it [03:05,  1.42it/s]Extractor Predicting: 275it [03:06,  1.45it/s]Extractor Predicting: 276it [03:07,  1.45it/s]Extractor Predicting: 277it [03:07,  1.43it/s]Extractor Predicting: 278it [03:08,  1.43it/s]Extractor Predicting: 279it [03:09,  1.43it/s]Extractor Predicting: 280it [03:09,  1.42it/s]Extractor Predicting: 281it [03:10,  1.41it/s]Extractor Predicting: 282it [03:11,  1.43it/s]Extractor Predicting: 283it [03:12,  1.39it/s]Extractor Predicting: 284it [03:12,  1.37it/s]Extractor Predicting: 285it [03:13,  1.38it/s]Extractor Predicting: 286it [03:14,  1.37it/s]Extractor Predicting: 287it [03:15,  1.39it/s]Extractor Predicting: 288it [03:15,  1.82it/s]Extractor Predicting: 288it [03:15,  1.48it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5064244572441293,
  "recall": 0.16591667876324576,
  "score": 0.24994533129236823,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.61it/s]
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.45454545454545453,
  "recall": 0.04504504504504504,
  "score": 0.0819672131147541,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_10_seed_4', 'type': 'synthetic', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'outputs/wrapper/wiki/unseen_10_seed_4/generator/synthetic.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl'}
train vocab size: 23047
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23147, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/model', pretrained_wv='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23147, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.562, loss:58138.6809
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.151, loss:1990.8599
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.182, loss:1765.2161
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 25, avg_time 1.169, loss:1585.4182
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 125, avg_time 1.193, loss:1584.2089
>> valid entity prec:0.4184, rec:0.3697, f1:0.3925
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 225, avg_time 3.091, loss:1421.4482
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 325, avg_time 1.155, loss:1413.2315
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 1.180, loss:1334.5693
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 1.163, loss:1313.0646
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 1.180, loss:1324.3334
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.3953, rec:0.3221, f1:0.3550
>> valid relation prec:1.0000, rec:0.0002, f1:0.0004
>> valid relation with NER prec:1.0000, rec:0.0002, f1:0.0004
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 350, avg_time 3.037, loss:1283.4346
g_step 1200, step 75, avg_time 1.212, loss:1238.1453
g_step 1300, step 175, avg_time 1.168, loss:1248.3696
g_step 1400, step 275, avg_time 1.146, loss:1189.3080
g_step 1500, step 375, avg_time 1.147, loss:1205.6631
>> valid entity prec:0.4623, rec:0.5711, f1:0.5109
>> valid relation prec:0.6558, rec:0.0498, f1:0.0926
>> valid relation with NER prec:0.6558, rec:0.0498, f1:0.0926
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 100, avg_time 1.138, loss:1176.4307
g_step 1700, step 200, avg_time 1.187, loss:1193.7363
g_step 1800, step 300, avg_time 1.161, loss:1154.8738
g_step 1900, step 25, avg_time 1.206, loss:1184.9404
g_step 2000, step 125, avg_time 1.144, loss:1099.2611
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5269, rec:0.3865, f1:0.4459
>> valid relation prec:0.5707, rec:0.0432, f1:0.0804
>> valid relation with NER prec:0.5707, rec:0.0432, f1:0.0804
g_step 2100, step 225, avg_time 3.039, loss:1132.6581
g_step 2200, step 325, avg_time 1.179, loss:1152.9509
g_step 2300, step 50, avg_time 1.186, loss:1096.3579
g_step 2400, step 150, avg_time 1.122, loss:1089.7652
g_step 2500, step 250, avg_time 1.190, loss:1108.8881
>> valid entity prec:0.3925, rec:0.4224, f1:0.4069
>> valid relation prec:0.6141, rec:0.0233, f1:0.0448
>> valid relation with NER prec:0.6141, rec:0.0233, f1:0.0448
g_step 2600, step 350, avg_time 3.069, loss:1130.7381
g_step 2700, step 75, avg_time 1.183, loss:1079.1760
g_step 2800, step 175, avg_time 1.150, loss:1076.7442
g_step 2900, step 275, avg_time 1.181, loss:1077.8256
g_step 3000, step 375, avg_time 1.165, loss:1099.5444
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4373, rec:0.5232, f1:0.4764
>> valid relation prec:0.6105, rec:0.0216, f1:0.0418
>> valid relation with NER prec:0.6105, rec:0.0216, f1:0.0418
g_step 3100, step 100, avg_time 1.165, loss:1045.2791
g_step 3200, step 200, avg_time 1.177, loss:1056.3625
g_step 3300, step 300, avg_time 1.195, loss:1058.1348
g_step 3400, step 25, avg_time 1.152, loss:1048.9218
g_step 3500, step 125, avg_time 1.170, loss:1018.2381
>> valid entity prec:0.5336, rec:0.4526, f1:0.4897
>> valid relation prec:0.6529, rec:0.0651, f1:0.1184
>> valid relation with NER prec:0.6529, rec:0.0651, f1:0.1184
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 225, avg_time 3.047, loss:1045.8589
g_step 3700, step 325, avg_time 1.157, loss:1028.3821
g_step 3800, step 50, avg_time 1.219, loss:1026.3908
g_step 3900, step 150, avg_time 1.136, loss:977.4024
g_step 4000, step 250, avg_time 1.144, loss:1011.1861
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5443, rec:0.4291, f1:0.4799
>> valid relation prec:0.5799, rec:0.0688, f1:0.1230
>> valid relation with NER prec:0.5799, rec:0.0688, f1:0.1230
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 350, avg_time 3.074, loss:1028.3158
g_step 4200, step 75, avg_time 1.174, loss:1008.8755
g_step 4300, step 175, avg_time 1.167, loss:988.3999
g_step 4400, step 275, avg_time 1.168, loss:991.7063
g_step 4500, step 375, avg_time 1.184, loss:986.7041
>> valid entity prec:0.5712, rec:0.4605, f1:0.5099
>> valid relation prec:0.6132, rec:0.0653, f1:0.1180
>> valid relation with NER prec:0.6132, rec:0.0653, f1:0.1180
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4600, step 100, avg_time 1.183, loss:963.2273
g_step 4700, step 200, avg_time 1.159, loss:929.5384
g_step 4800, step 300, avg_time 1.175, loss:993.9174
g_step 4900, step 25, avg_time 1.208, loss:964.9646
g_step 5000, step 125, avg_time 1.162, loss:911.4311
learning rate was adjusted to 0.0008
>> valid entity prec:0.5014, rec:0.5006, f1:0.5010
>> valid relation prec:0.5418, rec:0.0533, f1:0.0971
>> valid relation with NER prec:0.5418, rec:0.0533, f1:0.0971
g_step 5100, step 225, avg_time 3.044, loss:930.2505
g_step 5200, step 325, avg_time 1.165, loss:973.2882
g_step 5300, step 50, avg_time 1.146, loss:915.5932
g_step 5400, step 150, avg_time 1.183, loss:914.5848
g_step 5500, step 250, avg_time 1.181, loss:905.9289
>> valid entity prec:0.5354, rec:0.4523, f1:0.4904
>> valid relation prec:0.5376, rec:0.0795, f1:0.1385
>> valid relation with NER prec:0.5376, rec:0.0795, f1:0.1385
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5600, step 350, avg_time 3.024, loss:948.7269
g_step 5700, step 75, avg_time 1.138, loss:901.4929
g_step 5800, step 175, avg_time 1.202, loss:900.0201
g_step 5900, step 275, avg_time 1.146, loss:907.3803
g_step 6000, step 375, avg_time 1.186, loss:895.4364
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5694, rec:0.4863, f1:0.5246
>> valid relation prec:0.4987, rec:0.0762, f1:0.1322
>> valid relation with NER prec:0.4987, rec:0.0762, f1:0.1322
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 6100, step 100, avg_time 1.153, loss:865.4446
g_step 6200, step 200, avg_time 1.169, loss:870.7695
g_step 6300, step 300, avg_time 1.162, loss:870.6866
g_step 6400, step 25, avg_time 1.208, loss:873.6079
g_step 6500, step 125, avg_time 1.195, loss:828.1834
>> valid entity prec:0.5095, rec:0.4604, f1:0.4837
>> valid relation prec:0.5302, rec:0.0850, f1:0.1466
>> valid relation with NER prec:0.5302, rec:0.0850, f1:0.1466
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6600, step 225, avg_time 3.020, loss:844.5188
g_step 6700, step 325, avg_time 1.141, loss:862.3630
g_step 6800, step 50, avg_time 1.195, loss:842.8156
g_step 6900, step 150, avg_time 1.160, loss:808.1509
g_step 7000, step 250, avg_time 1.169, loss:828.3921
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5121, rec:0.4883, f1:0.4999
>> valid relation prec:0.4206, rec:0.0698, f1:0.1197
>> valid relation with NER prec:0.4206, rec:0.0698, f1:0.1197
g_step 7100, step 350, avg_time 3.055, loss:837.9522
g_step 7200, step 75, avg_time 1.150, loss:808.6210
g_step 7300, step 175, avg_time 1.138, loss:778.9804
g_step 7400, step 275, avg_time 1.194, loss:815.5367
g_step 7500, step 375, avg_time 1.208, loss:819.6264
>> valid entity prec:0.5487, rec:0.4389, f1:0.4877
>> valid relation prec:0.4173, rec:0.0525, f1:0.0933
>> valid relation with NER prec:0.4173, rec:0.0525, f1:0.0933
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'labels': ['headquarters location', 'licensed to broadcast to', 'member of political party', 'narrative location', 'notable work'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14287
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14387, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:05,  5.54s/it]Extractor Predicting: 2it [00:06,  2.64s/it]Extractor Predicting: 3it [00:07,  2.12s/it]Extractor Predicting: 4it [00:08,  1.54s/it]Extractor Predicting: 5it [00:10,  1.81s/it]Extractor Predicting: 6it [00:11,  1.42s/it]Extractor Predicting: 7it [00:11,  1.16s/it]Extractor Predicting: 8it [00:12,  1.02it/s]Extractor Predicting: 9it [00:13,  1.15it/s]Extractor Predicting: 10it [00:13,  1.25it/s]Extractor Predicting: 11it [00:14,  1.36it/s]Extractor Predicting: 12it [00:14,  1.41it/s]Extractor Predicting: 13it [00:15,  1.47it/s]Extractor Predicting: 14it [00:16,  1.48it/s]Extractor Predicting: 15it [00:16,  1.50it/s]Extractor Predicting: 16it [00:17,  1.51it/s]Extractor Predicting: 17it [00:18,  1.47it/s]Extractor Predicting: 18it [00:19,  1.42it/s]Extractor Predicting: 19it [00:19,  1.49it/s]Extractor Predicting: 20it [00:20,  1.50it/s]Extractor Predicting: 21it [00:20,  1.52it/s]Extractor Predicting: 22it [00:21,  1.51it/s]Extractor Predicting: 23it [00:22,  1.52it/s]Extractor Predicting: 24it [00:22,  1.52it/s]Extractor Predicting: 25it [00:23,  1.52it/s]Extractor Predicting: 26it [00:24,  1.53it/s]Extractor Predicting: 27it [00:24,  1.53it/s]Extractor Predicting: 28it [00:25,  1.55it/s]Extractor Predicting: 29it [00:26,  1.57it/s]Extractor Predicting: 30it [00:26,  1.56it/s]Extractor Predicting: 31it [00:27,  1.56it/s]Extractor Predicting: 32it [00:28,  1.57it/s]Extractor Predicting: 33it [00:28,  1.57it/s]Extractor Predicting: 34it [00:29,  1.57it/s]Extractor Predicting: 35it [00:29,  1.57it/s]Extractor Predicting: 36it [00:30,  1.56it/s]Extractor Predicting: 37it [00:31,  1.58it/s]Extractor Predicting: 38it [00:31,  1.55it/s]Extractor Predicting: 39it [00:32,  1.57it/s]Extractor Predicting: 40it [00:33,  1.54it/s]Extractor Predicting: 41it [00:33,  1.57it/s]Extractor Predicting: 42it [00:34,  1.59it/s]Extractor Predicting: 43it [00:35,  1.58it/s]Extractor Predicting: 44it [00:35,  1.53it/s]Extractor Predicting: 45it [00:36,  1.50it/s]Extractor Predicting: 46it [00:37,  1.51it/s]Extractor Predicting: 47it [00:37,  1.49it/s]Extractor Predicting: 48it [00:38,  1.51it/s]Extractor Predicting: 49it [00:39,  1.51it/s]Extractor Predicting: 50it [00:39,  1.51it/s]Extractor Predicting: 51it [00:40,  1.47it/s]Extractor Predicting: 52it [00:41,  1.46it/s]Extractor Predicting: 53it [00:41,  1.47it/s]Extractor Predicting: 54it [00:42,  1.50it/s]Extractor Predicting: 55it [00:43,  1.49it/s]Extractor Predicting: 56it [00:43,  1.49it/s]Extractor Predicting: 57it [00:44,  1.48it/s]Extractor Predicting: 58it [00:46,  1.06s/it]Extractor Predicting: 59it [00:47,  1.06it/s]Extractor Predicting: 60it [00:47,  1.15it/s]Extractor Predicting: 61it [00:48,  1.26it/s]Extractor Predicting: 62it [00:49,  1.33it/s]Extractor Predicting: 63it [00:49,  1.36it/s]Extractor Predicting: 64it [00:50,  1.41it/s]Extractor Predicting: 65it [00:51,  1.45it/s]Extractor Predicting: 66it [00:51,  1.47it/s]Extractor Predicting: 67it [00:52,  1.21it/s]Extractor Predicting: 68it [00:53,  1.32it/s]Extractor Predicting: 69it [00:54,  1.36it/s]Extractor Predicting: 70it [00:54,  1.40it/s]Extractor Predicting: 71it [00:55,  1.44it/s]Extractor Predicting: 72it [00:56,  1.48it/s]Extractor Predicting: 73it [00:56,  1.53it/s]Extractor Predicting: 74it [00:57,  1.54it/s]Extractor Predicting: 75it [00:58,  1.54it/s]Extractor Predicting: 76it [00:58,  1.57it/s]Extractor Predicting: 77it [00:59,  1.55it/s]Extractor Predicting: 78it [00:59,  1.56it/s]Extractor Predicting: 79it [01:00,  1.55it/s]Extractor Predicting: 80it [01:01,  1.54it/s]Extractor Predicting: 81it [01:01,  1.54it/s]Extractor Predicting: 82it [01:02,  1.56it/s]Extractor Predicting: 83it [01:03,  1.57it/s]Extractor Predicting: 84it [01:03,  1.57it/s]Extractor Predicting: 85it [01:04,  1.58it/s]Extractor Predicting: 86it [01:05,  1.59it/s]Extractor Predicting: 87it [01:05,  1.59it/s]Extractor Predicting: 88it [01:06,  1.57it/s]Extractor Predicting: 89it [01:06,  1.57it/s]Extractor Predicting: 90it [01:07,  1.58it/s]Extractor Predicting: 91it [01:08,  1.58it/s]Extractor Predicting: 92it [01:08,  1.55it/s]Extractor Predicting: 93it [01:09,  1.57it/s]Extractor Predicting: 94it [01:10,  1.58it/s]Extractor Predicting: 95it [01:10,  1.57it/s]Extractor Predicting: 96it [01:11,  1.56it/s]Extractor Predicting: 97it [01:12,  1.56it/s]Extractor Predicting: 98it [01:12,  1.61it/s]Extractor Predicting: 99it [01:13,  1.57it/s]Extractor Predicting: 100it [01:13,  1.57it/s]Extractor Predicting: 101it [01:14,  1.59it/s]Extractor Predicting: 102it [01:15,  1.57it/s]Extractor Predicting: 103it [01:15,  1.54it/s]Extractor Predicting: 104it [01:16,  1.55it/s]Extractor Predicting: 105it [01:17,  1.56it/s]Extractor Predicting: 106it [01:17,  1.58it/s]Extractor Predicting: 107it [01:18,  1.59it/s]Extractor Predicting: 108it [01:19,  1.56it/s]Extractor Predicting: 109it [01:19,  1.55it/s]Extractor Predicting: 110it [01:20,  1.57it/s]Extractor Predicting: 111it [01:20,  1.60it/s]Extractor Predicting: 112it [01:21,  1.60it/s]Extractor Predicting: 113it [01:22,  1.61it/s]Extractor Predicting: 114it [01:22,  1.60it/s]Extractor Predicting: 115it [01:23,  1.60it/s]Extractor Predicting: 116it [01:24,  1.58it/s]Extractor Predicting: 117it [01:24,  1.60it/s]Extractor Predicting: 118it [01:25,  1.61it/s]Extractor Predicting: 119it [01:25,  1.62it/s]Extractor Predicting: 120it [01:26,  1.60it/s]Extractor Predicting: 121it [01:27,  1.56it/s]Extractor Predicting: 122it [01:27,  1.57it/s]Extractor Predicting: 123it [01:28,  1.58it/s]Extractor Predicting: 124it [01:29,  1.57it/s]Extractor Predicting: 125it [01:29,  1.52it/s]Extractor Predicting: 126it [01:30,  1.51it/s]Extractor Predicting: 127it [01:31,  1.53it/s]Extractor Predicting: 128it [01:31,  1.53it/s]Extractor Predicting: 129it [01:32,  1.53it/s]Extractor Predicting: 130it [01:33,  1.57it/s]Extractor Predicting: 131it [01:33,  1.55it/s]Extractor Predicting: 132it [01:34,  1.56it/s]Extractor Predicting: 133it [01:35,  1.51it/s]Extractor Predicting: 134it [01:35,  1.39it/s]Extractor Predicting: 135it [01:36,  1.42it/s]Extractor Predicting: 136it [01:37,  1.45it/s]Extractor Predicting: 137it [01:37,  1.46it/s]Extractor Predicting: 138it [01:38,  1.47it/s]Extractor Predicting: 139it [01:39,  1.48it/s]Extractor Predicting: 140it [01:39,  1.49it/s]Extractor Predicting: 141it [01:40,  1.48it/s]Extractor Predicting: 142it [01:41,  1.51it/s]Extractor Predicting: 143it [01:41,  1.50it/s]Extractor Predicting: 144it [01:42,  1.51it/s]Extractor Predicting: 145it [01:43,  1.52it/s]Extractor Predicting: 146it [01:43,  1.55it/s]Extractor Predicting: 147it [01:44,  1.49it/s]Extractor Predicting: 148it [01:45,  1.49it/s]Extractor Predicting: 149it [01:45,  1.49it/s]Extractor Predicting: 150it [01:46,  1.51it/s]Extractor Predicting: 151it [01:47,  1.54it/s]Extractor Predicting: 152it [01:47,  1.55it/s]Extractor Predicting: 153it [01:48,  1.60it/s]Extractor Predicting: 154it [01:48,  1.58it/s]Extractor Predicting: 155it [01:49,  1.55it/s]Extractor Predicting: 156it [01:50,  1.56it/s]Extractor Predicting: 157it [01:50,  1.57it/s]Extractor Predicting: 158it [01:51,  1.57it/s]Extractor Predicting: 159it [01:52,  1.53it/s]Extractor Predicting: 160it [01:52,  1.57it/s]Extractor Predicting: 161it [01:53,  1.54it/s]Extractor Predicting: 162it [01:54,  1.53it/s]Extractor Predicting: 163it [01:54,  1.53it/s]Extractor Predicting: 164it [01:55,  1.55it/s]Extractor Predicting: 165it [01:56,  1.53it/s]Extractor Predicting: 166it [01:56,  1.54it/s]Extractor Predicting: 167it [01:57,  1.52it/s]Extractor Predicting: 168it [01:58,  1.54it/s]Extractor Predicting: 169it [01:58,  1.52it/s]Extractor Predicting: 170it [01:59,  1.54it/s]Extractor Predicting: 171it [02:00,  1.56it/s]Extractor Predicting: 172it [02:00,  1.53it/s]Extractor Predicting: 173it [02:01,  1.51it/s]Extractor Predicting: 174it [02:01,  1.55it/s]Extractor Predicting: 175it [02:02,  1.54it/s]Extractor Predicting: 176it [02:03,  1.53it/s]Extractor Predicting: 177it [02:03,  1.49it/s]Extractor Predicting: 178it [02:04,  1.47it/s]Extractor Predicting: 179it [02:05,  1.48it/s]Extractor Predicting: 180it [02:06,  1.47it/s]Extractor Predicting: 181it [02:06,  1.46it/s]Extractor Predicting: 182it [02:07,  1.50it/s]Extractor Predicting: 183it [02:08,  1.47it/s]Extractor Predicting: 184it [02:08,  1.46it/s]Extractor Predicting: 185it [02:09,  1.44it/s]Extractor Predicting: 186it [02:10,  1.45it/s]Extractor Predicting: 187it [02:10,  1.45it/s]Extractor Predicting: 188it [02:11,  1.46it/s]Extractor Predicting: 189it [02:12,  1.48it/s]Extractor Predicting: 190it [02:12,  1.47it/s]Extractor Predicting: 191it [02:13,  1.47it/s]Extractor Predicting: 192it [02:14,  1.49it/s]Extractor Predicting: 193it [02:14,  1.52it/s]Extractor Predicting: 194it [02:15,  1.52it/s]Extractor Predicting: 195it [02:15,  1.92it/s]Extractor Predicting: 195it [02:15,  1.44it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.511049723756906,
  "recall": 0.07613168724279835,
  "score": 0.13252148997134672,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21244
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21344, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:11,  1.55it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:18,  1.59it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:20,  1.57it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.47it/s]Extractor Predicting: 37it [00:23,  1.52it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.53it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:27,  1.49it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:29,  1.60it/s]Extractor Predicting: 47it [00:30,  1.58it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:31,  1.61it/s]Extractor Predicting: 50it [00:32,  1.61it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:34,  1.67it/s]Extractor Predicting: 54it [00:34,  1.68it/s]Extractor Predicting: 55it [00:35,  1.66it/s]Extractor Predicting: 56it [00:35,  1.62it/s]Extractor Predicting: 57it [00:36,  1.63it/s]Extractor Predicting: 58it [00:37,  1.68it/s]Extractor Predicting: 59it [00:37,  1.67it/s]Extractor Predicting: 60it [00:38,  1.66it/s]Extractor Predicting: 61it [00:38,  1.61it/s]Extractor Predicting: 62it [00:39,  1.65it/s]Extractor Predicting: 63it [00:40,  1.66it/s]Extractor Predicting: 64it [00:40,  1.61it/s]Extractor Predicting: 65it [00:41,  1.64it/s]Extractor Predicting: 66it [00:41,  1.63it/s]Extractor Predicting: 67it [00:42,  1.66it/s]Extractor Predicting: 68it [00:43,  1.67it/s]Extractor Predicting: 69it [00:43,  1.69it/s]Extractor Predicting: 70it [00:44,  1.67it/s]Extractor Predicting: 71it [00:45,  1.63it/s]Extractor Predicting: 72it [00:45,  1.45it/s]Extractor Predicting: 73it [00:46,  1.50it/s]Extractor Predicting: 74it [00:47,  1.51it/s]Extractor Predicting: 75it [00:47,  1.53it/s]Extractor Predicting: 76it [00:48,  1.52it/s]Extractor Predicting: 77it [00:49,  1.54it/s]Extractor Predicting: 78it [00:49,  1.54it/s]Extractor Predicting: 79it [00:50,  1.60it/s]Extractor Predicting: 80it [00:50,  1.63it/s]Extractor Predicting: 81it [00:51,  1.65it/s]Extractor Predicting: 82it [00:52,  1.65it/s]Extractor Predicting: 83it [00:52,  1.65it/s]Extractor Predicting: 84it [00:53,  1.58it/s]Extractor Predicting: 85it [00:53,  1.59it/s]Extractor Predicting: 86it [00:54,  1.64it/s]Extractor Predicting: 87it [00:55,  1.64it/s]Extractor Predicting: 88it [00:55,  1.70it/s]Extractor Predicting: 89it [00:56,  1.68it/s]Extractor Predicting: 90it [00:56,  1.68it/s]Extractor Predicting: 91it [00:57,  1.76it/s]Extractor Predicting: 92it [00:58,  1.71it/s]Extractor Predicting: 93it [00:58,  1.72it/s]Extractor Predicting: 94it [00:59,  1.72it/s]Extractor Predicting: 95it [00:59,  1.71it/s]Extractor Predicting: 96it [01:00,  1.67it/s]Extractor Predicting: 97it [01:01,  1.64it/s]Extractor Predicting: 98it [01:01,  1.68it/s]Extractor Predicting: 99it [01:02,  1.69it/s]Extractor Predicting: 100it [01:02,  1.68it/s]Extractor Predicting: 101it [01:03,  1.69it/s]Extractor Predicting: 102it [01:03,  1.68it/s]Extractor Predicting: 103it [01:04,  1.69it/s]Extractor Predicting: 104it [01:05,  1.67it/s]Extractor Predicting: 105it [01:05,  1.72it/s]Extractor Predicting: 106it [01:06,  1.71it/s]Extractor Predicting: 107it [01:06,  1.74it/s]Extractor Predicting: 108it [01:07,  1.71it/s]Extractor Predicting: 109it [01:08,  1.75it/s]Extractor Predicting: 110it [01:08,  1.70it/s]Extractor Predicting: 111it [01:09,  1.73it/s]Extractor Predicting: 112it [01:09,  1.67it/s]Extractor Predicting: 113it [01:10,  1.66it/s]Extractor Predicting: 114it [01:11,  1.67it/s]Extractor Predicting: 115it [01:11,  1.65it/s]Extractor Predicting: 116it [01:12,  1.65it/s]Extractor Predicting: 117it [01:12,  1.64it/s]Extractor Predicting: 118it [01:13,  1.62it/s]Extractor Predicting: 119it [01:14,  1.62it/s]Extractor Predicting: 120it [01:14,  1.63it/s]Extractor Predicting: 121it [01:15,  1.67it/s]Extractor Predicting: 122it [01:15,  1.64it/s]Extractor Predicting: 123it [01:17,  1.27it/s]Extractor Predicting: 124it [01:17,  1.34it/s]Extractor Predicting: 125it [01:18,  1.39it/s]Extractor Predicting: 126it [01:19,  1.47it/s]Extractor Predicting: 127it [01:19,  1.53it/s]Extractor Predicting: 128it [01:20,  1.54it/s]Extractor Predicting: 129it [01:20,  1.58it/s]Extractor Predicting: 130it [01:21,  1.57it/s]Extractor Predicting: 131it [01:22,  1.54it/s]Extractor Predicting: 132it [01:22,  1.56it/s]Extractor Predicting: 133it [01:23,  1.57it/s]Extractor Predicting: 134it [01:24,  1.57it/s]Extractor Predicting: 135it [01:24,  1.61it/s]Extractor Predicting: 136it [01:25,  1.60it/s]Extractor Predicting: 137it [01:25,  1.65it/s]Extractor Predicting: 138it [01:26,  1.62it/s]Extractor Predicting: 139it [01:27,  1.66it/s]Extractor Predicting: 140it [01:27,  1.66it/s]Extractor Predicting: 141it [01:28,  1.63it/s]Extractor Predicting: 142it [01:28,  1.66it/s]Extractor Predicting: 143it [01:29,  1.64it/s]Extractor Predicting: 144it [01:30,  1.59it/s]Extractor Predicting: 145it [01:30,  1.56it/s]Extractor Predicting: 146it [01:31,  1.57it/s]Extractor Predicting: 147it [01:32,  1.60it/s]Extractor Predicting: 148it [01:32,  1.57it/s]Extractor Predicting: 149it [01:33,  1.57it/s]Extractor Predicting: 150it [01:33,  1.60it/s]Extractor Predicting: 151it [01:34,  1.60it/s]Extractor Predicting: 152it [01:35,  1.59it/s]Extractor Predicting: 153it [01:35,  1.62it/s]Extractor Predicting: 154it [01:36,  1.64it/s]Extractor Predicting: 155it [01:37,  1.60it/s]Extractor Predicting: 156it [01:37,  1.62it/s]Extractor Predicting: 157it [01:38,  1.59it/s]Extractor Predicting: 158it [01:38,  1.60it/s]Extractor Predicting: 159it [01:39,  1.57it/s]Extractor Predicting: 160it [01:40,  1.60it/s]Extractor Predicting: 161it [01:40,  1.57it/s]Extractor Predicting: 162it [01:41,  1.59it/s]Extractor Predicting: 163it [01:42,  1.61it/s]Extractor Predicting: 164it [01:42,  1.60it/s]Extractor Predicting: 165it [01:43,  1.58it/s]Extractor Predicting: 166it [01:44,  1.54it/s]Extractor Predicting: 167it [01:44,  1.56it/s]Extractor Predicting: 168it [01:45,  1.60it/s]Extractor Predicting: 169it [01:45,  1.58it/s]Extractor Predicting: 170it [01:46,  1.56it/s]Extractor Predicting: 171it [01:47,  1.54it/s]Extractor Predicting: 172it [01:47,  1.58it/s]Extractor Predicting: 173it [01:48,  1.57it/s]Extractor Predicting: 174it [01:49,  1.55it/s]Extractor Predicting: 175it [01:49,  1.55it/s]Extractor Predicting: 176it [01:50,  1.59it/s]Extractor Predicting: 177it [01:51,  1.60it/s]Extractor Predicting: 178it [01:51,  1.62it/s]Extractor Predicting: 179it [01:52,  1.66it/s]Extractor Predicting: 180it [01:52,  1.63it/s]Extractor Predicting: 181it [01:53,  1.55it/s]Extractor Predicting: 182it [01:54,  1.55it/s]Extractor Predicting: 183it [01:54,  1.58it/s]Extractor Predicting: 184it [01:55,  1.56it/s]Extractor Predicting: 185it [01:56,  1.55it/s]Extractor Predicting: 186it [01:56,  1.56it/s]Extractor Predicting: 187it [01:57,  1.58it/s]Extractor Predicting: 188it [01:57,  1.62it/s]Extractor Predicting: 189it [01:58,  1.63it/s]Extractor Predicting: 190it [01:59,  1.61it/s]Extractor Predicting: 191it [01:59,  1.62it/s]Extractor Predicting: 192it [02:00,  1.60it/s]Extractor Predicting: 193it [02:01,  1.64it/s]Extractor Predicting: 194it [02:01,  1.51it/s]Extractor Predicting: 195it [02:02,  1.58it/s]Extractor Predicting: 196it [02:03,  1.55it/s]Extractor Predicting: 197it [02:03,  1.56it/s]Extractor Predicting: 198it [02:04,  1.56it/s]Extractor Predicting: 199it [02:04,  1.63it/s]Extractor Predicting: 200it [02:05,  1.63it/s]Extractor Predicting: 201it [02:06,  1.61it/s]Extractor Predicting: 202it [02:06,  1.64it/s]Extractor Predicting: 203it [02:07,  1.63it/s]Extractor Predicting: 204it [02:07,  1.59it/s]Extractor Predicting: 205it [02:08,  1.59it/s]Extractor Predicting: 206it [02:09,  1.58it/s]Extractor Predicting: 207it [02:09,  1.57it/s]Extractor Predicting: 208it [02:10,  1.57it/s]Extractor Predicting: 209it [02:11,  1.58it/s]Extractor Predicting: 210it [02:11,  1.56it/s]Extractor Predicting: 211it [02:12,  1.55it/s]Extractor Predicting: 212it [02:13,  1.56it/s]Extractor Predicting: 213it [02:13,  1.60it/s]Extractor Predicting: 214it [02:14,  1.59it/s]Extractor Predicting: 215it [02:14,  1.61it/s]Extractor Predicting: 216it [02:15,  1.62it/s]Extractor Predicting: 217it [02:16,  1.64it/s]Extractor Predicting: 218it [02:16,  1.58it/s]Extractor Predicting: 219it [02:17,  1.56it/s]Extractor Predicting: 220it [02:18,  1.54it/s]Extractor Predicting: 221it [02:18,  1.50it/s]Extractor Predicting: 222it [02:19,  1.46it/s]Extractor Predicting: 223it [02:20,  1.53it/s]Extractor Predicting: 224it [02:20,  1.52it/s]Extractor Predicting: 225it [02:21,  1.53it/s]Extractor Predicting: 226it [02:22,  1.51it/s]Extractor Predicting: 227it [02:22,  1.54it/s]Extractor Predicting: 228it [02:23,  1.54it/s]Extractor Predicting: 229it [02:24,  1.58it/s]Extractor Predicting: 230it [02:24,  1.57it/s]Extractor Predicting: 231it [02:25,  1.54it/s]Extractor Predicting: 232it [02:25,  1.56it/s]Extractor Predicting: 233it [02:26,  1.58it/s]Extractor Predicting: 234it [02:27,  1.62it/s]Extractor Predicting: 235it [02:27,  1.62it/s]Extractor Predicting: 236it [02:28,  1.68it/s]Extractor Predicting: 237it [02:28,  1.69it/s]Extractor Predicting: 238it [02:29,  1.69it/s]Extractor Predicting: 239it [02:30,  1.68it/s]Extractor Predicting: 240it [02:30,  1.65it/s]Extractor Predicting: 241it [02:31,  1.61it/s]Extractor Predicting: 242it [02:32,  1.59it/s]Extractor Predicting: 243it [02:32,  1.63it/s]Extractor Predicting: 244it [02:33,  1.64it/s]Extractor Predicting: 245it [02:33,  1.64it/s]Extractor Predicting: 246it [02:34,  1.58it/s]Extractor Predicting: 247it [02:35,  1.58it/s]Extractor Predicting: 248it [02:35,  1.56it/s]Extractor Predicting: 249it [02:36,  1.54it/s]Extractor Predicting: 250it [02:37,  1.52it/s]Extractor Predicting: 251it [02:37,  1.52it/s]Extractor Predicting: 252it [02:38,  1.52it/s]Extractor Predicting: 253it [02:39,  1.52it/s]Extractor Predicting: 254it [02:39,  1.54it/s]Extractor Predicting: 255it [02:40,  1.57it/s]Extractor Predicting: 256it [02:41,  1.52it/s]Extractor Predicting: 257it [02:41,  1.51it/s]Extractor Predicting: 258it [02:42,  1.50it/s]Extractor Predicting: 259it [02:43,  1.52it/s]Extractor Predicting: 260it [02:43,  1.51it/s]Extractor Predicting: 261it [02:44,  1.48it/s]Extractor Predicting: 262it [02:45,  1.50it/s]Extractor Predicting: 263it [02:45,  1.51it/s]Extractor Predicting: 264it [02:46,  1.52it/s]Extractor Predicting: 265it [02:47,  1.51it/s]Extractor Predicting: 266it [02:47,  1.50it/s]Extractor Predicting: 267it [02:48,  1.50it/s]Extractor Predicting: 268it [02:49,  1.51it/s]Extractor Predicting: 269it [02:49,  1.51it/s]Extractor Predicting: 270it [02:50,  1.54it/s]Extractor Predicting: 271it [02:50,  1.53it/s]Extractor Predicting: 272it [02:51,  1.53it/s]Extractor Predicting: 273it [02:52,  1.53it/s]Extractor Predicting: 274it [02:53,  1.50it/s]Extractor Predicting: 275it [02:53,  1.50it/s]Extractor Predicting: 276it [02:54,  1.52it/s]Extractor Predicting: 277it [02:54,  1.54it/s]Extractor Predicting: 278it [02:55,  1.53it/s]Extractor Predicting: 279it [02:56,  1.51it/s]Extractor Predicting: 280it [02:56,  1.57it/s]Extractor Predicting: 281it [02:57,  1.59it/s]Extractor Predicting: 282it [02:58,  1.62it/s]Extractor Predicting: 283it [02:58,  1.59it/s]Extractor Predicting: 284it [02:59,  1.57it/s]Extractor Predicting: 285it [03:00,  1.58it/s]Extractor Predicting: 286it [03:00,  1.63it/s]Extractor Predicting: 287it [03:01,  1.64it/s]Extractor Predicting: 288it [03:01,  1.58it/s]Extractor Predicting: 289it [03:02,  1.58it/s]Extractor Predicting: 290it [03:03,  1.58it/s]Extractor Predicting: 291it [03:03,  1.57it/s]Extractor Predicting: 292it [03:04,  1.62it/s]Extractor Predicting: 293it [03:04,  1.62it/s]Extractor Predicting: 294it [03:05,  1.65it/s]Extractor Predicting: 295it [03:06,  1.70it/s]Extractor Predicting: 296it [03:06,  1.70it/s]Extractor Predicting: 297it [03:07,  1.70it/s]Extractor Predicting: 298it [03:07,  1.61it/s]Extractor Predicting: 299it [03:08,  1.50it/s]Extractor Predicting: 300it [03:09,  1.48it/s]Extractor Predicting: 301it [03:10,  1.48it/s]Extractor Predicting: 302it [03:10,  1.52it/s]Extractor Predicting: 303it [03:11,  1.54it/s]Extractor Predicting: 304it [03:11,  1.54it/s]Extractor Predicting: 305it [03:12,  1.51it/s]Extractor Predicting: 306it [03:13,  1.35it/s]Extractor Predicting: 307it [03:14,  1.39it/s]Extractor Predicting: 308it [03:14,  1.41it/s]Extractor Predicting: 309it [03:15,  1.40it/s]Extractor Predicting: 310it [03:16,  1.42it/s]Extractor Predicting: 311it [03:17,  1.44it/s]Extractor Predicting: 312it [03:17,  1.46it/s]Extractor Predicting: 313it [03:18,  1.48it/s]Extractor Predicting: 314it [03:19,  1.50it/s]Extractor Predicting: 315it [03:19,  1.55it/s]Extractor Predicting: 316it [03:20,  1.52it/s]Extractor Predicting: 317it [03:20,  1.49it/s]Extractor Predicting: 318it [03:21,  1.45it/s]Extractor Predicting: 319it [03:22,  1.46it/s]Extractor Predicting: 320it [03:23,  1.48it/s]Extractor Predicting: 321it [03:23,  1.46it/s]Extractor Predicting: 322it [03:24,  1.45it/s]Extractor Predicting: 323it [03:25,  1.45it/s]Extractor Predicting: 324it [03:25,  1.45it/s]Extractor Predicting: 325it [03:26,  1.45it/s]Extractor Predicting: 326it [03:27,  1.44it/s]Extractor Predicting: 327it [03:27,  1.44it/s]Extractor Predicting: 328it [03:28,  1.45it/s]Extractor Predicting: 329it [03:29,  1.45it/s]Extractor Predicting: 330it [03:29,  1.45it/s]Extractor Predicting: 331it [03:30,  1.44it/s]Extractor Predicting: 332it [03:31,  1.44it/s]Extractor Predicting: 333it [03:32,  1.42it/s]Extractor Predicting: 334it [03:32,  1.44it/s]Extractor Predicting: 335it [03:33,  1.44it/s]Extractor Predicting: 336it [03:34,  1.41it/s]Extractor Predicting: 337it [03:34,  1.42it/s]Extractor Predicting: 338it [03:35,  1.42it/s]Extractor Predicting: 339it [03:36,  1.42it/s]Extractor Predicting: 340it [03:37,  1.44it/s]Extractor Predicting: 341it [03:37,  1.45it/s]Extractor Predicting: 342it [03:38,  1.46it/s]Extractor Predicting: 343it [03:38,  1.48it/s]Extractor Predicting: 344it [03:39,  1.51it/s]Extractor Predicting: 345it [03:40,  1.52it/s]Extractor Predicting: 346it [03:40,  1.64it/s]Extractor Predicting: 346it [03:40,  1.57it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.24493554327808473,
  "recall": 0.016035688449481552,
  "score": 0.030100712911621592,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6093
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6193, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.62it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:08,  1.51it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:10,  1.49it/s]Extractor Predicting: 18it [00:11,  1.44it/s]Extractor Predicting: 19it [00:12,  1.44it/s]Extractor Predicting: 20it [00:13,  1.40it/s]Extractor Predicting: 21it [00:13,  1.42it/s]Extractor Predicting: 22it [00:14,  1.45it/s]Extractor Predicting: 23it [00:15,  1.38it/s]Extractor Predicting: 24it [00:16,  1.39it/s]Extractor Predicting: 25it [00:16,  1.36it/s]Extractor Predicting: 26it [00:17,  1.42it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:19,  1.57it/s]Extractor Predicting: 30it [00:19,  1.64it/s]Extractor Predicting: 31it [00:20,  1.72it/s]Extractor Predicting: 32it [00:20,  1.77it/s]Extractor Predicting: 33it [00:21,  1.81it/s]Extractor Predicting: 34it [00:21,  1.81it/s]Extractor Predicting: 35it [00:22,  1.85it/s]Extractor Predicting: 36it [00:23,  1.83it/s]Extractor Predicting: 37it [00:23,  1.80it/s]Extractor Predicting: 38it [00:24,  1.82it/s]Extractor Predicting: 39it [00:24,  1.81it/s]Extractor Predicting: 40it [00:25,  1.82it/s]Extractor Predicting: 41it [00:25,  1.81it/s]Extractor Predicting: 42it [00:26,  1.78it/s]Extractor Predicting: 43it [00:26,  1.78it/s]Extractor Predicting: 44it [00:27,  1.80it/s]Extractor Predicting: 45it [00:27,  1.85it/s]Extractor Predicting: 46it [00:28,  1.87it/s]Extractor Predicting: 47it [00:29,  1.84it/s]Extractor Predicting: 48it [00:29,  1.82it/s]Extractor Predicting: 49it [00:30,  1.88it/s]Extractor Predicting: 50it [00:30,  1.81it/s]Extractor Predicting: 51it [00:31,  1.77it/s]Extractor Predicting: 52it [00:31,  1.80it/s]Extractor Predicting: 53it [00:32,  1.80it/s]Extractor Predicting: 54it [00:32,  1.81it/s]Extractor Predicting: 55it [00:33,  1.84it/s]Extractor Predicting: 56it [00:34,  1.82it/s]Extractor Predicting: 57it [00:34,  1.86it/s]Extractor Predicting: 58it [00:35,  1.74it/s]Extractor Predicting: 59it [00:35,  1.63it/s]Extractor Predicting: 60it [00:36,  1.54it/s]Extractor Predicting: 61it [00:37,  1.48it/s]Extractor Predicting: 62it [00:38,  1.47it/s]Extractor Predicting: 63it [00:38,  1.45it/s]Extractor Predicting: 64it [00:39,  1.33it/s]Extractor Predicting: 65it [00:40,  1.35it/s]Extractor Predicting: 66it [00:41,  1.37it/s]Extractor Predicting: 67it [00:41,  1.38it/s]Extractor Predicting: 68it [00:42,  1.38it/s]Extractor Predicting: 69it [00:43,  1.38it/s]Extractor Predicting: 70it [00:43,  1.41it/s]Extractor Predicting: 71it [00:44,  1.39it/s]Extractor Predicting: 72it [00:45,  1.40it/s]Extractor Predicting: 73it [00:46,  1.42it/s]Extractor Predicting: 74it [00:46,  1.43it/s]Extractor Predicting: 75it [00:47,  1.46it/s]Extractor Predicting: 76it [00:48,  1.47it/s]Extractor Predicting: 77it [00:48,  1.78it/s]Extractor Predicting: 77it [00:48,  1.59it/s]
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7817679558011049,
  "recall": 0.07041552625031103,
  "score": 0.12919424788860992,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_10_seed_4', 'type': 'filtered', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'filter_data': {'path_pseudo': 'outputs/wrapper/fewrel/unseen_10_seed_4/generator/synthetic.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/filtered.jsonl', 'total_pseudo_per_label': 250, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': True, 'version': 'single', 'rescale_train': False}}
{'num_pseudo': 750, 'num_train': 3000, 'num_pseudo_per_label': 50, 'num_train_per_label': 46}
num of filtered data: 3609 mean pseudo reward: 1.0
fit {'path_train': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/filtered.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 20736
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20836, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/model', pretrained_wv='outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20836, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.395, loss:53864.3878
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 49, avg_time 1.068, loss:2584.1333
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 149, avg_time 1.062, loss:2238.1172
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 98, avg_time 1.059, loss:2109.9903
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 47, avg_time 1.053, loss:2149.1927
>> valid entity prec:0.3140, rec:0.3713, f1:0.3402
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 147, avg_time 2.504, loss:2030.1507
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 96, avg_time 1.060, loss:1884.9567
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 45, avg_time 1.056, loss:1804.8008
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 145, avg_time 1.068, loss:1700.5156
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 94, avg_time 1.055, loss:1623.4783
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4911, rec:0.5425, f1:0.5156
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 43, avg_time 2.453, loss:1479.2752
g_step 1200, step 143, avg_time 1.063, loss:1463.5678
g_step 1300, step 92, avg_time 1.054, loss:1391.7346
g_step 1400, step 41, avg_time 1.058, loss:1328.9365
g_step 1500, step 141, avg_time 1.062, loss:1280.2487
>> valid entity prec:0.5109, rec:0.4491, f1:0.4780
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 90, avg_time 2.441, loss:1261.2615
g_step 1700, step 39, avg_time 1.047, loss:1185.5987
g_step 1800, step 139, avg_time 1.067, loss:1198.3093
g_step 1900, step 88, avg_time 1.056, loss:1140.2290
g_step 2000, step 37, avg_time 1.055, loss:1108.4642
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5212, rec:0.4014, f1:0.4535
>> valid relation prec:0.1212, rec:0.0023, f1:0.0045
>> valid relation with NER prec:0.1212, rec:0.0023, f1:0.0045
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 137, avg_time 2.441, loss:1091.8229
g_step 2200, step 86, avg_time 1.065, loss:1049.9236
g_step 2300, step 35, avg_time 1.045, loss:1010.5635
g_step 2400, step 135, avg_time 1.060, loss:1027.2599
g_step 2500, step 84, avg_time 1.058, loss:965.8770
>> valid entity prec:0.4876, rec:0.4902, f1:0.4889
>> valid relation prec:0.0812, rec:0.0080, f1:0.0146
>> valid relation with NER prec:0.0812, rec:0.0080, f1:0.0146
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 33, avg_time 2.434, loss:946.8357
g_step 2700, step 133, avg_time 1.055, loss:927.2414
g_step 2800, step 82, avg_time 1.057, loss:874.0017
g_step 2900, step 31, avg_time 1.063, loss:889.1943
g_step 3000, step 131, avg_time 1.060, loss:856.3141
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5137, rec:0.4828, f1:0.4978
>> valid relation prec:0.1743, rec:0.0175, f1:0.0317
>> valid relation with NER prec:0.1743, rec:0.0175, f1:0.0317
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.01s/it]Extractor Predicting: 2it [00:07,  3.22s/it]Extractor Predicting: 3it [00:08,  2.04s/it]Extractor Predicting: 4it [00:08,  1.47s/it]Extractor Predicting: 5it [00:09,  1.18s/it]Extractor Predicting: 6it [00:10,  1.02s/it]Extractor Predicting: 7it [00:10,  1.13it/s]Extractor Predicting: 8it [00:11,  1.24it/s]Extractor Predicting: 9it [00:12,  1.33it/s]Extractor Predicting: 10it [00:12,  1.35it/s]Extractor Predicting: 11it [00:13,  1.40it/s]Extractor Predicting: 12it [00:14,  1.47it/s]Extractor Predicting: 13it [00:14,  1.51it/s]Extractor Predicting: 14it [00:15,  1.55it/s]Extractor Predicting: 15it [00:15,  1.54it/s]Extractor Predicting: 16it [00:16,  1.56it/s]Extractor Predicting: 17it [00:17,  1.58it/s]Extractor Predicting: 18it [00:17,  1.58it/s]Extractor Predicting: 19it [00:18,  1.58it/s]Extractor Predicting: 20it [00:19,  1.54it/s]Extractor Predicting: 21it [00:19,  1.53it/s]Extractor Predicting: 22it [00:20,  1.49it/s]Extractor Predicting: 23it [00:21,  1.54it/s]Extractor Predicting: 24it [00:21,  1.56it/s]Extractor Predicting: 25it [00:22,  1.56it/s]Extractor Predicting: 26it [00:22,  1.62it/s]Extractor Predicting: 27it [00:23,  1.61it/s]Extractor Predicting: 28it [00:24,  1.64it/s]Extractor Predicting: 29it [00:24,  1.62it/s]Extractor Predicting: 30it [00:25,  1.57it/s]Extractor Predicting: 31it [00:26,  1.56it/s]Extractor Predicting: 32it [00:26,  1.53it/s]Extractor Predicting: 33it [00:27,  1.49it/s]Extractor Predicting: 34it [00:28,  1.47it/s]Extractor Predicting: 35it [00:28,  1.49it/s]Extractor Predicting: 36it [00:29,  1.47it/s]Extractor Predicting: 37it [00:30,  1.47it/s]Extractor Predicting: 38it [00:30,  1.46it/s]Extractor Predicting: 39it [00:31,  1.48it/s]Extractor Predicting: 40it [00:32,  1.48it/s]Extractor Predicting: 41it [00:32,  1.47it/s]Extractor Predicting: 42it [00:33,  1.47it/s]Extractor Predicting: 43it [00:34,  1.47it/s]Extractor Predicting: 44it [00:34,  1.50it/s]Extractor Predicting: 45it [00:35,  1.52it/s]Extractor Predicting: 46it [00:36,  1.47it/s]Extractor Predicting: 47it [00:36,  1.46it/s]Extractor Predicting: 48it [00:37,  1.46it/s]Extractor Predicting: 49it [00:38,  1.46it/s]Extractor Predicting: 50it [00:39,  1.47it/s]Extractor Predicting: 51it [00:39,  1.48it/s]Extractor Predicting: 52it [00:40,  1.46it/s]Extractor Predicting: 53it [00:41,  1.50it/s]Extractor Predicting: 54it [00:41,  1.48it/s]Extractor Predicting: 55it [00:42,  1.46it/s]Extractor Predicting: 56it [00:43,  1.45it/s]Extractor Predicting: 57it [00:43,  1.43it/s]Extractor Predicting: 58it [00:44,  1.44it/s]Extractor Predicting: 59it [00:45,  1.43it/s]Extractor Predicting: 60it [00:45,  1.43it/s]Extractor Predicting: 61it [00:46,  1.45it/s]Extractor Predicting: 62it [00:47,  1.45it/s]Extractor Predicting: 63it [00:47,  1.48it/s]Extractor Predicting: 64it [00:48,  1.50it/s]Extractor Predicting: 65it [00:49,  1.48it/s]Extractor Predicting: 66it [00:49,  1.50it/s]Extractor Predicting: 67it [00:50,  1.49it/s]Extractor Predicting: 68it [00:51,  1.48it/s]Extractor Predicting: 69it [00:52,  1.47it/s]Extractor Predicting: 70it [00:52,  1.45it/s]Extractor Predicting: 71it [00:53,  1.47it/s]Extractor Predicting: 72it [00:54,  1.48it/s]Extractor Predicting: 73it [00:54,  1.47it/s]Extractor Predicting: 74it [00:55,  1.39it/s]Extractor Predicting: 75it [00:56,  1.43it/s]Extractor Predicting: 76it [00:56,  1.43it/s]Extractor Predicting: 77it [00:57,  1.45it/s]Extractor Predicting: 78it [00:58,  1.44it/s]Extractor Predicting: 79it [00:58,  1.45it/s]Extractor Predicting: 80it [00:59,  1.44it/s]Extractor Predicting: 81it [01:00,  1.49it/s]Extractor Predicting: 82it [01:00,  1.48it/s]Extractor Predicting: 83it [01:01,  1.46it/s]Extractor Predicting: 84it [01:02,  1.47it/s]Extractor Predicting: 85it [01:02,  1.49it/s]Extractor Predicting: 86it [01:03,  1.54it/s]Extractor Predicting: 87it [01:04,  1.58it/s]Extractor Predicting: 88it [01:04,  1.59it/s]Extractor Predicting: 89it [01:05,  1.58it/s]Extractor Predicting: 90it [01:06,  1.60it/s]Extractor Predicting: 91it [01:06,  1.58it/s]Extractor Predicting: 92it [01:07,  1.55it/s]Extractor Predicting: 93it [01:07,  1.59it/s]Extractor Predicting: 94it [01:08,  1.61it/s]Extractor Predicting: 95it [01:09,  1.60it/s]Extractor Predicting: 96it [01:09,  1.61it/s]Extractor Predicting: 97it [01:10,  1.58it/s]Extractor Predicting: 98it [01:11,  1.55it/s]Extractor Predicting: 99it [01:11,  1.53it/s]Extractor Predicting: 100it [01:12,  1.55it/s]Extractor Predicting: 101it [01:13,  1.59it/s]Extractor Predicting: 102it [01:13,  1.57it/s]Extractor Predicting: 103it [01:14,  1.58it/s]Extractor Predicting: 104it [01:14,  1.57it/s]Extractor Predicting: 105it [01:15,  1.60it/s]Extractor Predicting: 106it [01:16,  1.57it/s]Extractor Predicting: 107it [01:16,  1.58it/s]Extractor Predicting: 108it [01:17,  1.57it/s]Extractor Predicting: 109it [01:18,  1.61it/s]Extractor Predicting: 110it [01:18,  1.59it/s]Extractor Predicting: 111it [01:19,  1.58it/s]Extractor Predicting: 112it [01:20,  1.57it/s]Extractor Predicting: 113it [01:20,  1.56it/s]Extractor Predicting: 114it [01:21,  1.52it/s]Extractor Predicting: 115it [01:22,  1.53it/s]Extractor Predicting: 116it [01:22,  1.52it/s]Extractor Predicting: 117it [01:23,  1.51it/s]Extractor Predicting: 118it [01:23,  1.56it/s]Extractor Predicting: 119it [01:24,  1.55it/s]Extractor Predicting: 120it [01:25,  1.56it/s]Extractor Predicting: 121it [01:25,  1.55it/s]Extractor Predicting: 122it [01:26,  1.53it/s]Extractor Predicting: 123it [01:27,  1.53it/s]Extractor Predicting: 124it [01:27,  1.49it/s]Extractor Predicting: 125it [01:28,  1.49it/s]Extractor Predicting: 126it [01:29,  1.50it/s]Extractor Predicting: 127it [01:29,  1.49it/s]Extractor Predicting: 128it [01:30,  1.48it/s]Extractor Predicting: 129it [01:31,  1.44it/s]Extractor Predicting: 130it [01:32,  1.46it/s]Extractor Predicting: 131it [01:32,  1.47it/s]Extractor Predicting: 132it [01:33,  1.50it/s]Extractor Predicting: 133it [01:33,  1.50it/s]Extractor Predicting: 134it [01:34,  1.50it/s]Extractor Predicting: 135it [01:35,  1.50it/s]Extractor Predicting: 136it [01:35,  1.49it/s]Extractor Predicting: 137it [01:36,  1.49it/s]Extractor Predicting: 138it [01:37,  1.49it/s]Extractor Predicting: 139it [01:38,  1.48it/s]Extractor Predicting: 140it [01:38,  1.53it/s]Extractor Predicting: 141it [01:39,  1.51it/s]Extractor Predicting: 142it [01:39,  1.57it/s]Extractor Predicting: 142it [01:39,  1.42it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.24666666666666667,
  "recall": 0.02118522759805325,
  "score": 0.0390192459794358,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.57it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.51it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.49it/s]Extractor Predicting: 30it [00:19,  1.46it/s]Extractor Predicting: 31it [00:20,  1.45it/s]Extractor Predicting: 32it [00:20,  1.45it/s]Extractor Predicting: 33it [00:21,  1.47it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:22,  1.48it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:29,  1.50it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:30,  1.51it/s]Extractor Predicting: 48it [00:31,  1.51it/s]Extractor Predicting: 49it [00:32,  1.51it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:33,  1.51it/s]Extractor Predicting: 52it [00:34,  1.51it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.47it/s]Extractor Predicting: 56it [00:36,  1.49it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:38,  1.54it/s]Extractor Predicting: 59it [00:38,  1.54it/s]Extractor Predicting: 60it [00:39,  1.53it/s]Extractor Predicting: 61it [00:40,  1.53it/s]Extractor Predicting: 62it [00:40,  1.53it/s]Extractor Predicting: 63it [00:41,  1.53it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:42,  1.49it/s]Extractor Predicting: 66it [00:43,  1.48it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:44,  1.55it/s]Extractor Predicting: 69it [00:45,  1.53it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.52it/s]Extractor Predicting: 73it [00:48,  1.47it/s]Extractor Predicting: 74it [00:48,  1.48it/s]Extractor Predicting: 75it [00:49,  1.49it/s]Extractor Predicting: 76it [00:50,  1.49it/s]Extractor Predicting: 77it [00:50,  1.47it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.48it/s]Extractor Predicting: 80it [00:52,  1.48it/s]Extractor Predicting: 81it [00:53,  1.49it/s]Extractor Predicting: 82it [00:54,  1.35it/s]Extractor Predicting: 83it [00:55,  1.39it/s]Extractor Predicting: 84it [00:55,  1.42it/s]Extractor Predicting: 85it [00:56,  1.46it/s]Extractor Predicting: 86it [00:57,  1.45it/s]Extractor Predicting: 87it [00:57,  1.46it/s]Extractor Predicting: 88it [00:58,  1.43it/s]Extractor Predicting: 89it [00:59,  1.45it/s]Extractor Predicting: 90it [00:59,  1.47it/s]Extractor Predicting: 91it [01:00,  1.45it/s]Extractor Predicting: 92it [01:01,  1.46it/s]Extractor Predicting: 93it [01:01,  1.44it/s]Extractor Predicting: 94it [01:02,  1.42it/s]Extractor Predicting: 95it [01:03,  1.43it/s]Extractor Predicting: 96it [01:03,  1.43it/s]Extractor Predicting: 97it [01:04,  1.45it/s]Extractor Predicting: 98it [01:05,  1.44it/s]Extractor Predicting: 99it [01:06,  1.45it/s]Extractor Predicting: 100it [01:06,  1.46it/s]Extractor Predicting: 101it [01:07,  1.45it/s]Extractor Predicting: 102it [01:08,  1.45it/s]Extractor Predicting: 103it [01:08,  1.46it/s]Extractor Predicting: 104it [01:09,  1.46it/s]Extractor Predicting: 105it [01:10,  1.45it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:12,  1.43it/s]Extractor Predicting: 109it [01:12,  1.44it/s]Extractor Predicting: 110it [01:13,  1.42it/s]Extractor Predicting: 111it [01:14,  1.42it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:15,  1.49it/s]Extractor Predicting: 114it [01:16,  1.49it/s]Extractor Predicting: 115it [01:16,  1.49it/s]Extractor Predicting: 116it [01:17,  1.49it/s]Extractor Predicting: 117it [01:18,  1.50it/s]Extractor Predicting: 118it [01:18,  1.51it/s]Extractor Predicting: 119it [01:19,  1.50it/s]Extractor Predicting: 120it [01:20,  1.51it/s]Extractor Predicting: 121it [01:20,  1.51it/s]Extractor Predicting: 122it [01:21,  1.55it/s]Extractor Predicting: 123it [01:22,  1.54it/s]Extractor Predicting: 124it [01:22,  1.54it/s]Extractor Predicting: 125it [01:23,  1.53it/s]Extractor Predicting: 126it [01:24,  1.51it/s]Extractor Predicting: 127it [01:24,  1.51it/s]Extractor Predicting: 128it [01:25,  1.52it/s]Extractor Predicting: 129it [01:26,  1.49it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:27,  1.49it/s]Extractor Predicting: 132it [01:28,  1.50it/s]Extractor Predicting: 133it [01:28,  1.49it/s]Extractor Predicting: 134it [01:29,  1.50it/s]Extractor Predicting: 135it [01:30,  1.53it/s]Extractor Predicting: 136it [01:30,  1.53it/s]Extractor Predicting: 137it [01:31,  1.51it/s]Extractor Predicting: 138it [01:32,  1.51it/s]Extractor Predicting: 139it [01:32,  1.55it/s]Extractor Predicting: 140it [01:33,  1.52it/s]Extractor Predicting: 141it [01:34,  1.50it/s]Extractor Predicting: 142it [01:34,  1.54it/s]Extractor Predicting: 143it [01:35,  1.53it/s]Extractor Predicting: 144it [01:36,  1.51it/s]Extractor Predicting: 145it [01:36,  1.52it/s]Extractor Predicting: 146it [01:37,  1.51it/s]Extractor Predicting: 147it [01:38,  1.48it/s]Extractor Predicting: 148it [01:38,  1.48it/s]Extractor Predicting: 149it [01:39,  1.47it/s]Extractor Predicting: 150it [01:40,  1.50it/s]Extractor Predicting: 151it [01:40,  1.51it/s]Extractor Predicting: 152it [01:41,  1.52it/s]Extractor Predicting: 153it [01:42,  1.52it/s]Extractor Predicting: 154it [01:42,  1.50it/s]Extractor Predicting: 155it [01:43,  1.49it/s]Extractor Predicting: 156it [01:44,  1.48it/s]Extractor Predicting: 157it [01:44,  1.45it/s]Extractor Predicting: 158it [01:45,  1.46it/s]Extractor Predicting: 159it [01:46,  1.46it/s]Extractor Predicting: 160it [01:46,  1.47it/s]Extractor Predicting: 161it [01:47,  1.50it/s]Extractor Predicting: 162it [01:48,  1.50it/s]Extractor Predicting: 163it [01:48,  1.50it/s]Extractor Predicting: 164it [01:49,  1.51it/s]Extractor Predicting: 165it [01:50,  1.48it/s]Extractor Predicting: 166it [01:50,  1.49it/s]Extractor Predicting: 167it [01:51,  1.52it/s]Extractor Predicting: 168it [01:52,  1.50it/s]Extractor Predicting: 169it [01:52,  1.49it/s]Extractor Predicting: 170it [01:53,  1.47it/s]Extractor Predicting: 171it [01:54,  1.44it/s]Extractor Predicting: 172it [01:55,  1.44it/s]Extractor Predicting: 173it [01:55,  1.45it/s]Extractor Predicting: 174it [01:56,  1.41it/s]Extractor Predicting: 175it [01:57,  1.38it/s]Extractor Predicting: 176it [01:57,  1.40it/s]Extractor Predicting: 177it [01:58,  1.41it/s]Extractor Predicting: 178it [01:59,  1.45it/s]Extractor Predicting: 179it [02:00,  1.32it/s]Extractor Predicting: 180it [02:00,  1.40it/s]Extractor Predicting: 181it [02:01,  1.41it/s]Extractor Predicting: 182it [02:02,  1.45it/s]Extractor Predicting: 183it [02:02,  1.46it/s]Extractor Predicting: 184it [02:03,  1.50it/s]Extractor Predicting: 185it [02:04,  1.52it/s]Extractor Predicting: 186it [02:04,  1.53it/s]Extractor Predicting: 187it [02:05,  1.54it/s]Extractor Predicting: 188it [02:05,  1.53it/s]Extractor Predicting: 189it [02:06,  1.54it/s]Extractor Predicting: 190it [02:07,  1.51it/s]Extractor Predicting: 191it [02:08,  1.47it/s]Extractor Predicting: 192it [02:08,  1.50it/s]Extractor Predicting: 193it [02:09,  1.53it/s]Extractor Predicting: 194it [02:09,  1.52it/s]Extractor Predicting: 195it [02:10,  1.52it/s]Extractor Predicting: 196it [02:11,  1.54it/s]Extractor Predicting: 197it [02:11,  1.54it/s]Extractor Predicting: 198it [02:12,  1.51it/s]Extractor Predicting: 199it [02:13,  1.48it/s]Extractor Predicting: 200it [02:13,  1.49it/s]Extractor Predicting: 201it [02:14,  1.50it/s]Extractor Predicting: 202it [02:15,  1.54it/s]Extractor Predicting: 203it [02:15,  1.54it/s]Extractor Predicting: 204it [02:16,  1.53it/s]Extractor Predicting: 205it [02:17,  1.48it/s]Extractor Predicting: 206it [02:17,  1.49it/s]Extractor Predicting: 207it [02:18,  1.51it/s]Extractor Predicting: 208it [02:19,  1.52it/s]Extractor Predicting: 209it [02:19,  1.47it/s]Extractor Predicting: 210it [02:20,  1.46it/s]Extractor Predicting: 211it [02:21,  1.46it/s]Extractor Predicting: 212it [02:21,  1.49it/s]Extractor Predicting: 213it [02:22,  1.50it/s]Extractor Predicting: 214it [02:23,  1.46it/s]Extractor Predicting: 215it [02:24,  1.46it/s]Extractor Predicting: 216it [02:24,  1.49it/s]Extractor Predicting: 217it [02:25,  1.50it/s]Extractor Predicting: 218it [02:26,  1.45it/s]Extractor Predicting: 219it [02:26,  1.46it/s]Extractor Predicting: 220it [02:27,  1.46it/s]Extractor Predicting: 221it [02:28,  1.43it/s]Extractor Predicting: 222it [02:28,  1.44it/s]Extractor Predicting: 223it [02:29,  1.44it/s]Extractor Predicting: 224it [02:30,  1.47it/s]Extractor Predicting: 225it [02:30,  1.47it/s]Extractor Predicting: 226it [02:31,  1.51it/s]Extractor Predicting: 227it [02:32,  1.53it/s]Extractor Predicting: 228it [02:32,  1.51it/s]Extractor Predicting: 229it [02:33,  1.51it/s]Extractor Predicting: 230it [02:34,  1.48it/s]Extractor Predicting: 231it [02:34,  1.47it/s]Extractor Predicting: 232it [02:35,  1.47it/s]Extractor Predicting: 233it [02:36,  1.51it/s]Extractor Predicting: 234it [02:36,  1.50it/s]Extractor Predicting: 235it [02:37,  1.51it/s]Extractor Predicting: 236it [02:38,  1.47it/s]Extractor Predicting: 237it [02:38,  1.47it/s]Extractor Predicting: 238it [02:39,  1.50it/s]Extractor Predicting: 239it [02:40,  1.51it/s]Extractor Predicting: 240it [02:40,  1.50it/s]Extractor Predicting: 241it [02:41,  1.49it/s]Extractor Predicting: 242it [02:42,  1.47it/s]Extractor Predicting: 243it [02:42,  1.44it/s]Extractor Predicting: 244it [02:43,  1.46it/s]Extractor Predicting: 245it [02:44,  1.50it/s]Extractor Predicting: 246it [02:44,  1.48it/s]Extractor Predicting: 247it [02:45,  1.50it/s]Extractor Predicting: 248it [02:46,  1.50it/s]Extractor Predicting: 249it [02:46,  1.50it/s]Extractor Predicting: 250it [02:47,  1.49it/s]Extractor Predicting: 251it [02:48,  1.46it/s]Extractor Predicting: 252it [02:49,  1.34it/s]Extractor Predicting: 253it [02:49,  1.38it/s]Extractor Predicting: 254it [02:50,  1.43it/s]Extractor Predicting: 255it [02:51,  1.44it/s]Extractor Predicting: 256it [02:51,  1.45it/s]Extractor Predicting: 257it [02:52,  1.47it/s]Extractor Predicting: 258it [02:53,  1.47it/s]Extractor Predicting: 259it [02:53,  1.44it/s]Extractor Predicting: 260it [02:54,  1.46it/s]Extractor Predicting: 261it [02:55,  1.48it/s]Extractor Predicting: 262it [02:55,  1.46it/s]Extractor Predicting: 263it [02:56,  1.45it/s]Extractor Predicting: 264it [02:57,  1.45it/s]Extractor Predicting: 265it [02:58,  1.44it/s]Extractor Predicting: 266it [02:58,  1.43it/s]Extractor Predicting: 267it [02:59,  1.41it/s]Extractor Predicting: 268it [03:00,  1.43it/s]Extractor Predicting: 269it [03:00,  1.43it/s]Extractor Predicting: 270it [03:01,  1.42it/s]Extractor Predicting: 271it [03:02,  1.43it/s]Extractor Predicting: 272it [03:02,  1.44it/s]Extractor Predicting: 273it [03:03,  1.43it/s]Extractor Predicting: 274it [03:04,  1.43it/s]Extractor Predicting: 275it [03:05,  1.46it/s]Extractor Predicting: 276it [03:05,  1.46it/s]Extractor Predicting: 277it [03:06,  1.44it/s]Extractor Predicting: 278it [03:07,  1.44it/s]Extractor Predicting: 279it [03:07,  1.44it/s]Extractor Predicting: 280it [03:08,  1.43it/s]Extractor Predicting: 281it [03:09,  1.42it/s]Extractor Predicting: 282it [03:09,  1.44it/s]Extractor Predicting: 283it [03:10,  1.40it/s]Extractor Predicting: 284it [03:11,  1.39it/s]Extractor Predicting: 285it [03:12,  1.38it/s]Extractor Predicting: 286it [03:12,  1.38it/s]Extractor Predicting: 287it [03:13,  1.40it/s]Extractor Predicting: 288it [03:13,  1.83it/s]Extractor Predicting: 288it [03:13,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5330296127562643,
  "recall": 0.033967194077514876,
  "score": 0.06386462882096068,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.35it/s]Extractor Predicting: 3it [00:01,  1.77it/s]Extractor Predicting: 3it [00:01,  1.63it/s]
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_4/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_10_seed_4', 'type': 'filtered', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'filter_data': {'path_pseudo': 'outputs/wrapper/wiki/unseen_10_seed_4/generator/synthetic.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/filtered.jsonl', 'total_pseudo_per_label': 250, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': True, 'version': 'single', 'rescale_train': False}}
{'num_pseudo': 750, 'num_train': 3000, 'num_pseudo_per_label': 50, 'num_train_per_label': 30}
num of filtered data: 3373 mean pseudo reward: 1.0
fit {'path_train': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/filtered.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl'}
train vocab size: 21489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/model', pretrained_wv='outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21589, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.495, loss:54903.8955
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 59, avg_time 1.091, loss:2438.9776
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 18, avg_time 1.106, loss:2147.2274
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 118, avg_time 1.079, loss:2105.6940
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 77, avg_time 1.100, loss:2045.1193
>> valid entity prec:0.4617, rec:0.2685, f1:0.3395
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 36, avg_time 3.000, loss:2016.0647
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 136, avg_time 1.105, loss:1940.8676
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 95, avg_time 1.085, loss:1902.2051
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 54, avg_time 1.116, loss:1715.6333
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 13, avg_time 1.105, loss:1774.2879
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.3500, rec:0.4486, f1:0.3932
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 113, avg_time 2.987, loss:1651.1738
g_step 1200, step 72, avg_time 1.105, loss:1607.5009
g_step 1300, step 31, avg_time 1.065, loss:1492.6814
g_step 1400, step 131, avg_time 1.111, loss:1477.7874
g_step 1500, step 90, avg_time 1.106, loss:1446.8348
>> valid entity prec:0.4636, rec:0.3516, f1:0.3999
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 49, avg_time 2.928, loss:1360.0718
g_step 1700, step 8, avg_time 1.109, loss:1366.2518
g_step 1800, step 108, avg_time 1.092, loss:1289.1355
g_step 1900, step 67, avg_time 1.094, loss:1297.4694
g_step 2000, step 26, avg_time 1.092, loss:1214.7349
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5407, rec:0.2570, f1:0.3484
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 126, avg_time 2.928, loss:1219.8208
g_step 2200, step 85, avg_time 1.090, loss:1167.2004
g_step 2300, step 44, avg_time 1.113, loss:1118.7588
g_step 2400, step 3, avg_time 1.079, loss:1130.7702
g_step 2500, step 103, avg_time 1.102, loss:1108.0400
>> valid entity prec:0.4933, rec:0.3918, f1:0.4367
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 62, avg_time 2.977, loss:1000.6041
g_step 2700, step 21, avg_time 1.085, loss:1042.1361
g_step 2800, step 121, avg_time 1.090, loss:977.8705
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'labels': ['headquarters location', 'licensed to broadcast to', 'member of political party', 'narrative location', 'notable work'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14287
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14387, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:05,  5.54s/it]Extractor Predicting: 2it [00:06,  2.64s/it]Extractor Predicting: 3it [00:07,  2.12s/it]Extractor Predicting: 4it [00:08,  1.54s/it]Extractor Predicting: 5it [00:10,  1.81s/it]Extractor Predicting: 6it [00:11,  1.42s/it]Extractor Predicting: 7it [00:11,  1.17s/it]Extractor Predicting: 8it [00:12,  1.02it/s]Extractor Predicting: 9it [00:13,  1.15it/s]Extractor Predicting: 10it [00:13,  1.25it/s]Extractor Predicting: 11it [00:14,  1.35it/s]Extractor Predicting: 12it [00:15,  1.40it/s]Extractor Predicting: 13it [00:15,  1.46it/s]Extractor Predicting: 14it [00:16,  1.41it/s]Extractor Predicting: 15it [00:17,  1.45it/s]Extractor Predicting: 16it [00:17,  1.47it/s]Extractor Predicting: 17it [00:18,  1.44it/s]Extractor Predicting: 18it [00:19,  1.48it/s]Extractor Predicting: 19it [00:19,  1.53it/s]Extractor Predicting: 20it [00:20,  1.54it/s]Extractor Predicting: 21it [00:20,  1.55it/s]Extractor Predicting: 22it [00:21,  1.53it/s]Extractor Predicting: 23it [00:22,  1.53it/s]Extractor Predicting: 24it [00:22,  1.53it/s]Extractor Predicting: 25it [00:23,  1.53it/s]Extractor Predicting: 26it [00:24,  1.53it/s]Extractor Predicting: 27it [00:24,  1.52it/s]Extractor Predicting: 28it [00:25,  1.56it/s]Extractor Predicting: 29it [00:26,  1.57it/s]Extractor Predicting: 30it [00:26,  1.56it/s]Extractor Predicting: 31it [00:27,  1.56it/s]Extractor Predicting: 32it [00:28,  1.57it/s]Extractor Predicting: 33it [00:28,  1.57it/s]Extractor Predicting: 34it [00:29,  1.57it/s]Extractor Predicting: 35it [00:29,  1.57it/s]Extractor Predicting: 36it [00:30,  1.56it/s]Extractor Predicting: 37it [00:31,  1.57it/s]Extractor Predicting: 38it [00:31,  1.55it/s]Extractor Predicting: 39it [00:32,  1.57it/s]Extractor Predicting: 40it [00:33,  1.54it/s]Extractor Predicting: 41it [00:33,  1.57it/s]Extractor Predicting: 42it [00:34,  1.59it/s]Extractor Predicting: 43it [00:35,  1.58it/s]Extractor Predicting: 44it [00:35,  1.53it/s]Extractor Predicting: 45it [00:36,  1.49it/s]Extractor Predicting: 46it [00:37,  1.50it/s]Extractor Predicting: 47it [00:37,  1.48it/s]Extractor Predicting: 48it [00:38,  1.50it/s]Extractor Predicting: 49it [00:39,  1.50it/s]Extractor Predicting: 50it [00:39,  1.50it/s]Extractor Predicting: 51it [00:40,  1.46it/s]Extractor Predicting: 52it [00:41,  1.46it/s]Extractor Predicting: 53it [00:41,  1.47it/s]Extractor Predicting: 54it [00:42,  1.49it/s]Extractor Predicting: 55it [00:43,  1.48it/s]Extractor Predicting: 56it [00:43,  1.49it/s]Extractor Predicting: 57it [00:44,  1.48it/s]Extractor Predicting: 58it [00:46,  1.06s/it]Extractor Predicting: 59it [00:47,  1.06it/s]Extractor Predicting: 60it [00:47,  1.15it/s]Extractor Predicting: 61it [00:48,  1.26it/s]Extractor Predicting: 62it [00:49,  1.33it/s]Extractor Predicting: 63it [00:49,  1.35it/s]Extractor Predicting: 64it [00:50,  1.40it/s]Extractor Predicting: 65it [00:51,  1.44it/s]Extractor Predicting: 66it [00:51,  1.46it/s]Extractor Predicting: 67it [00:53,  1.21it/s]Extractor Predicting: 68it [00:53,  1.31it/s]Extractor Predicting: 69it [00:54,  1.36it/s]Extractor Predicting: 70it [00:54,  1.39it/s]Extractor Predicting: 71it [00:55,  1.44it/s]Extractor Predicting: 72it [00:56,  1.48it/s]Extractor Predicting: 73it [00:56,  1.52it/s]Extractor Predicting: 74it [00:57,  1.53it/s]Extractor Predicting: 75it [00:58,  1.54it/s]Extractor Predicting: 76it [00:58,  1.56it/s]Extractor Predicting: 77it [00:59,  1.54it/s]Extractor Predicting: 78it [01:00,  1.55it/s]Extractor Predicting: 79it [01:00,  1.54it/s]Extractor Predicting: 80it [01:01,  1.52it/s]Extractor Predicting: 81it [01:02,  1.53it/s]Extractor Predicting: 82it [01:02,  1.54it/s]Extractor Predicting: 83it [01:03,  1.56it/s]Extractor Predicting: 84it [01:03,  1.56it/s]Extractor Predicting: 85it [01:04,  1.57it/s]Extractor Predicting: 86it [01:05,  1.58it/s]Extractor Predicting: 87it [01:05,  1.58it/s]Extractor Predicting: 88it [01:06,  1.56it/s]Extractor Predicting: 89it [01:07,  1.56it/s]Extractor Predicting: 90it [01:07,  1.56it/s]Extractor Predicting: 91it [01:08,  1.57it/s]Extractor Predicting: 92it [01:09,  1.54it/s]Extractor Predicting: 93it [01:09,  1.57it/s]Extractor Predicting: 94it [01:10,  1.57it/s]Extractor Predicting: 95it [01:10,  1.56it/s]Extractor Predicting: 96it [01:11,  1.56it/s]Extractor Predicting: 97it [01:12,  1.56it/s]Extractor Predicting: 98it [01:12,  1.61it/s]Extractor Predicting: 99it [01:13,  1.57it/s]Extractor Predicting: 100it [01:14,  1.56it/s]Extractor Predicting: 101it [01:14,  1.58it/s]Extractor Predicting: 102it [01:15,  1.56it/s]Extractor Predicting: 103it [01:16,  1.54it/s]Extractor Predicting: 104it [01:16,  1.54it/s]Extractor Predicting: 105it [01:17,  1.55it/s]Extractor Predicting: 106it [01:17,  1.57it/s]Extractor Predicting: 107it [01:18,  1.58it/s]Extractor Predicting: 108it [01:19,  1.55it/s]Extractor Predicting: 109it [01:19,  1.55it/s]Extractor Predicting: 110it [01:20,  1.56it/s]Extractor Predicting: 111it [01:21,  1.59it/s]Extractor Predicting: 112it [01:21,  1.59it/s]Extractor Predicting: 113it [01:22,  1.60it/s]Extractor Predicting: 114it [01:23,  1.60it/s]Extractor Predicting: 115it [01:23,  1.59it/s]Extractor Predicting: 116it [01:24,  1.58it/s]Extractor Predicting: 117it [01:24,  1.59it/s]Extractor Predicting: 118it [01:25,  1.60it/s]Extractor Predicting: 119it [01:26,  1.60it/s]Extractor Predicting: 120it [01:26,  1.59it/s]Extractor Predicting: 121it [01:27,  1.56it/s]Extractor Predicting: 122it [01:28,  1.57it/s]Extractor Predicting: 123it [01:28,  1.58it/s]Extractor Predicting: 124it [01:29,  1.43it/s]Extractor Predicting: 125it [01:30,  1.42it/s]Extractor Predicting: 126it [01:30,  1.44it/s]Extractor Predicting: 127it [01:31,  1.47it/s]Extractor Predicting: 128it [01:32,  1.48it/s]Extractor Predicting: 129it [01:32,  1.49it/s]Extractor Predicting: 130it [01:33,  1.53it/s]Extractor Predicting: 131it [01:34,  1.52it/s]Extractor Predicting: 132it [01:34,  1.53it/s]Extractor Predicting: 133it [01:35,  1.48it/s]Extractor Predicting: 134it [01:36,  1.50it/s]Extractor Predicting: 135it [01:36,  1.49it/s]Extractor Predicting: 136it [01:37,  1.50it/s]Extractor Predicting: 137it [01:38,  1.49it/s]Extractor Predicting: 138it [01:38,  1.48it/s]Extractor Predicting: 139it [01:39,  1.49it/s]Extractor Predicting: 140it [01:40,  1.49it/s]Extractor Predicting: 141it [01:40,  1.48it/s]Extractor Predicting: 142it [01:41,  1.50it/s]Extractor Predicting: 143it [01:42,  1.50it/s]Extractor Predicting: 144it [01:42,  1.50it/s]Extractor Predicting: 145it [01:43,  1.51it/s]Extractor Predicting: 146it [01:44,  1.54it/s]Extractor Predicting: 147it [01:44,  1.48it/s]Extractor Predicting: 148it [01:45,  1.48it/s]Extractor Predicting: 149it [01:46,  1.48it/s]Extractor Predicting: 150it [01:46,  1.51it/s]Extractor Predicting: 151it [01:47,  1.53it/s]Extractor Predicting: 152it [01:48,  1.55it/s]Extractor Predicting: 153it [01:48,  1.60it/s]Extractor Predicting: 154it [01:49,  1.57it/s]Extractor Predicting: 155it [01:50,  1.53it/s]Extractor Predicting: 156it [01:50,  1.54it/s]Extractor Predicting: 157it [01:51,  1.56it/s]Extractor Predicting: 158it [01:52,  1.55it/s]Extractor Predicting: 159it [01:52,  1.52it/s]Extractor Predicting: 160it [01:53,  1.55it/s]Extractor Predicting: 161it [01:54,  1.53it/s]Extractor Predicting: 162it [01:54,  1.52it/s]Extractor Predicting: 163it [01:55,  1.52it/s]Extractor Predicting: 164it [01:55,  1.54it/s]Extractor Predicting: 165it [01:56,  1.52it/s]Extractor Predicting: 166it [01:57,  1.53it/s]Extractor Predicting: 167it [01:58,  1.50it/s]Extractor Predicting: 168it [01:58,  1.53it/s]Extractor Predicting: 169it [01:59,  1.51it/s]Extractor Predicting: 170it [01:59,  1.53it/s]Extractor Predicting: 171it [02:00,  1.55it/s]Extractor Predicting: 172it [02:01,  1.53it/s]Extractor Predicting: 173it [02:01,  1.50it/s]Extractor Predicting: 174it [02:02,  1.55it/s]Extractor Predicting: 175it [02:03,  1.53it/s]Extractor Predicting: 176it [02:03,  1.53it/s]Extractor Predicting: 177it [02:04,  1.48it/s]Extractor Predicting: 178it [02:05,  1.46it/s]Extractor Predicting: 179it [02:05,  1.48it/s]Extractor Predicting: 180it [02:06,  1.46it/s]Extractor Predicting: 181it [02:07,  1.46it/s]Extractor Predicting: 182it [02:07,  1.50it/s]Extractor Predicting: 183it [02:08,  1.47it/s]Extractor Predicting: 184it [02:09,  1.46it/s]Extractor Predicting: 185it [02:10,  1.43it/s]Extractor Predicting: 186it [02:10,  1.45it/s]Extractor Predicting: 187it [02:11,  1.44it/s]Extractor Predicting: 188it [02:12,  1.45it/s]Extractor Predicting: 189it [02:12,  1.47it/s]Extractor Predicting: 190it [02:13,  1.45it/s]Extractor Predicting: 191it [02:14,  1.46it/s]Extractor Predicting: 192it [02:14,  1.47it/s]Extractor Predicting: 193it [02:15,  1.51it/s]Extractor Predicting: 194it [02:16,  1.51it/s]Extractor Predicting: 195it [02:16,  1.91it/s]Extractor Predicting: 195it [02:16,  1.43it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21244
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21344, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.49it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.52it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.56it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.57it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:22,  1.51it/s]Extractor Predicting: 36it [00:23,  1.44it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.50it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.54it/s]Extractor Predicting: 41it [00:26,  1.50it/s]Extractor Predicting: 42it [00:27,  1.49it/s]Extractor Predicting: 43it [00:28,  1.47it/s]Extractor Predicting: 44it [00:29,  1.50it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:30,  1.57it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.54it/s]Extractor Predicting: 49it [00:32,  1.59it/s]Extractor Predicting: 50it [00:32,  1.59it/s]Extractor Predicting: 51it [00:33,  1.57it/s]Extractor Predicting: 52it [00:33,  1.60it/s]Extractor Predicting: 53it [00:34,  1.65it/s]Extractor Predicting: 54it [00:35,  1.66it/s]Extractor Predicting: 55it [00:35,  1.65it/s]Extractor Predicting: 56it [00:36,  1.60it/s]Extractor Predicting: 57it [00:37,  1.60it/s]Extractor Predicting: 58it [00:37,  1.66it/s]Extractor Predicting: 59it [00:38,  1.65it/s]Extractor Predicting: 60it [00:38,  1.65it/s]Extractor Predicting: 61it [00:39,  1.58it/s]Extractor Predicting: 62it [00:40,  1.62it/s]Extractor Predicting: 63it [00:40,  1.63it/s]Extractor Predicting: 64it [00:41,  1.59it/s]Extractor Predicting: 65it [00:41,  1.62it/s]Extractor Predicting: 66it [00:42,  1.61it/s]Extractor Predicting: 67it [00:43,  1.65it/s]Extractor Predicting: 68it [00:43,  1.65it/s]Extractor Predicting: 69it [00:44,  1.67it/s]Extractor Predicting: 70it [00:44,  1.65it/s]Extractor Predicting: 71it [00:45,  1.61it/s]Extractor Predicting: 72it [00:46,  1.60it/s]Extractor Predicting: 73it [00:46,  1.60it/s]Extractor Predicting: 74it [00:47,  1.59it/s]Extractor Predicting: 75it [00:48,  1.58it/s]Extractor Predicting: 76it [00:48,  1.54it/s]Extractor Predicting: 77it [00:49,  1.55it/s]Extractor Predicting: 78it [00:50,  1.56it/s]Extractor Predicting: 79it [00:50,  1.61it/s]Extractor Predicting: 80it [00:51,  1.46it/s]Extractor Predicting: 81it [00:52,  1.53it/s]Extractor Predicting: 82it [00:52,  1.56it/s]Extractor Predicting: 83it [00:53,  1.58it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:54,  1.57it/s]Extractor Predicting: 86it [00:55,  1.62it/s]Extractor Predicting: 87it [00:55,  1.63it/s]Extractor Predicting: 88it [00:56,  1.69it/s]Extractor Predicting: 89it [00:56,  1.68it/s]Extractor Predicting: 90it [00:57,  1.68it/s]Extractor Predicting: 91it [00:58,  1.76it/s]Extractor Predicting: 92it [00:58,  1.72it/s]Extractor Predicting: 93it [00:59,  1.72it/s]Extractor Predicting: 94it [00:59,  1.72it/s]Extractor Predicting: 95it [01:00,  1.53it/s]Extractor Predicting: 96it [01:01,  1.54it/s]Extractor Predicting: 97it [01:01,  1.53it/s]Extractor Predicting: 98it [01:02,  1.59it/s]Extractor Predicting: 99it [01:03,  1.62it/s]Extractor Predicting: 100it [01:03,  1.62it/s]Extractor Predicting: 101it [01:04,  1.64it/s]Extractor Predicting: 102it [01:04,  1.63it/s]Extractor Predicting: 103it [01:05,  1.64it/s]Extractor Predicting: 104it [01:06,  1.64it/s]Extractor Predicting: 105it [01:06,  1.69it/s]Extractor Predicting: 106it [01:07,  1.68it/s]Extractor Predicting: 107it [01:07,  1.70it/s]Extractor Predicting: 108it [01:08,  1.67it/s]Extractor Predicting: 109it [01:09,  1.72it/s]Extractor Predicting: 110it [01:09,  1.66it/s]Extractor Predicting: 111it [01:10,  1.69it/s]Extractor Predicting: 112it [01:10,  1.64it/s]Extractor Predicting: 113it [01:11,  1.63it/s]Extractor Predicting: 114it [01:12,  1.64it/s]Extractor Predicting: 115it [01:12,  1.62it/s]Extractor Predicting: 116it [01:13,  1.62it/s]Extractor Predicting: 117it [01:14,  1.61it/s]Extractor Predicting: 118it [01:14,  1.59it/s]Extractor Predicting: 119it [01:15,  1.59it/s]Extractor Predicting: 120it [01:15,  1.60it/s]Extractor Predicting: 121it [01:16,  1.64it/s]Extractor Predicting: 122it [01:17,  1.61it/s]Extractor Predicting: 123it [01:18,  1.23it/s]Extractor Predicting: 124it [01:19,  1.31it/s]Extractor Predicting: 125it [01:19,  1.37it/s]Extractor Predicting: 126it [01:20,  1.44it/s]Extractor Predicting: 127it [01:20,  1.50it/s]Extractor Predicting: 128it [01:21,  1.51it/s]Extractor Predicting: 129it [01:22,  1.54it/s]Extractor Predicting: 130it [01:22,  1.54it/s]Extractor Predicting: 131it [01:23,  1.52it/s]Extractor Predicting: 132it [01:24,  1.54it/s]Extractor Predicting: 133it [01:24,  1.55it/s]Extractor Predicting: 134it [01:25,  1.56it/s]Extractor Predicting: 135it [01:25,  1.60it/s]Extractor Predicting: 136it [01:26,  1.59it/s]Extractor Predicting: 137it [01:27,  1.63it/s]Extractor Predicting: 138it [01:27,  1.60it/s]Extractor Predicting: 139it [01:28,  1.64it/s]Extractor Predicting: 140it [01:29,  1.64it/s]Extractor Predicting: 141it [01:29,  1.60it/s]Extractor Predicting: 142it [01:30,  1.64it/s]Extractor Predicting: 143it [01:30,  1.62it/s]Extractor Predicting: 144it [01:31,  1.56it/s]Extractor Predicting: 145it [01:32,  1.52it/s]Extractor Predicting: 146it [01:32,  1.54it/s]Extractor Predicting: 147it [01:33,  1.58it/s]Extractor Predicting: 148it [01:34,  1.54it/s]Extractor Predicting: 149it [01:34,  1.54it/s]Extractor Predicting: 150it [01:35,  1.58it/s]Extractor Predicting: 151it [01:36,  1.58it/s]Extractor Predicting: 152it [01:36,  1.57it/s]Extractor Predicting: 153it [01:37,  1.60it/s]Extractor Predicting: 154it [01:37,  1.62it/s]Extractor Predicting: 155it [01:38,  1.58it/s]Extractor Predicting: 156it [01:39,  1.60it/s]Extractor Predicting: 157it [01:39,  1.57it/s]Extractor Predicting: 158it [01:40,  1.58it/s]Extractor Predicting: 159it [01:41,  1.55it/s]Extractor Predicting: 160it [01:41,  1.57it/s]Extractor Predicting: 161it [01:42,  1.55it/s]Extractor Predicting: 162it [01:43,  1.57it/s]Extractor Predicting: 163it [01:43,  1.58it/s]Extractor Predicting: 164it [01:44,  1.57it/s]Extractor Predicting: 165it [01:44,  1.55it/s]Extractor Predicting: 166it [01:45,  1.53it/s]Extractor Predicting: 167it [01:46,  1.54it/s]Extractor Predicting: 168it [01:46,  1.58it/s]Extractor Predicting: 169it [01:47,  1.56it/s]Extractor Predicting: 170it [01:48,  1.53it/s]Extractor Predicting: 171it [01:48,  1.50it/s]Extractor Predicting: 172it [01:49,  1.55it/s]Extractor Predicting: 173it [01:50,  1.55it/s]Extractor Predicting: 174it [01:50,  1.53it/s]Extractor Predicting: 175it [01:51,  1.53it/s]Extractor Predicting: 176it [01:52,  1.57it/s]Extractor Predicting: 177it [01:52,  1.58it/s]Extractor Predicting: 178it [01:53,  1.60it/s]Extractor Predicting: 179it [01:53,  1.64it/s]Extractor Predicting: 180it [01:54,  1.61it/s]Extractor Predicting: 181it [01:55,  1.53it/s]Extractor Predicting: 182it [01:55,  1.52it/s]Extractor Predicting: 183it [01:56,  1.55it/s]Extractor Predicting: 184it [01:57,  1.53it/s]Extractor Predicting: 185it [01:57,  1.52it/s]Extractor Predicting: 186it [01:58,  1.53it/s]Extractor Predicting: 187it [01:59,  1.56it/s]Extractor Predicting: 188it [01:59,  1.60it/s]Extractor Predicting: 189it [02:00,  1.62it/s]Extractor Predicting: 190it [02:01,  1.58it/s]Extractor Predicting: 191it [02:01,  1.59it/s]Extractor Predicting: 192it [02:02,  1.58it/s]Extractor Predicting: 193it [02:02,  1.61it/s]Extractor Predicting: 194it [02:03,  1.67it/s]Extractor Predicting: 195it [02:03,  1.69it/s]Extractor Predicting: 196it [02:04,  1.63it/s]Extractor Predicting: 197it [02:05,  1.61it/s]Extractor Predicting: 198it [02:05,  1.58it/s]Extractor Predicting: 199it [02:06,  1.64it/s]Extractor Predicting: 200it [02:07,  1.64it/s]Extractor Predicting: 201it [02:07,  1.61it/s]Extractor Predicting: 202it [02:08,  1.63it/s]Extractor Predicting: 203it [02:08,  1.63it/s]Extractor Predicting: 204it [02:09,  1.58it/s]Extractor Predicting: 205it [02:10,  1.58it/s]Extractor Predicting: 206it [02:10,  1.56it/s]Extractor Predicting: 207it [02:11,  1.40it/s]Extractor Predicting: 208it [02:12,  1.45it/s]Extractor Predicting: 209it [02:13,  1.49it/s]Extractor Predicting: 210it [02:13,  1.48it/s]Extractor Predicting: 211it [02:14,  1.49it/s]Extractor Predicting: 212it [02:15,  1.51it/s]Extractor Predicting: 213it [02:15,  1.55it/s]Extractor Predicting: 214it [02:16,  1.55it/s]Extractor Predicting: 215it [02:16,  1.58it/s]Extractor Predicting: 216it [02:17,  1.59it/s]Extractor Predicting: 217it [02:18,  1.61it/s]Extractor Predicting: 218it [02:18,  1.54it/s]Extractor Predicting: 219it [02:19,  1.53it/s]Extractor Predicting: 220it [02:20,  1.51it/s]Extractor Predicting: 221it [02:20,  1.47it/s]Extractor Predicting: 222it [02:21,  1.44it/s]Extractor Predicting: 223it [02:22,  1.50it/s]Extractor Predicting: 224it [02:22,  1.50it/s]Extractor Predicting: 225it [02:23,  1.50it/s]Extractor Predicting: 226it [02:24,  1.49it/s]Extractor Predicting: 227it [02:24,  1.52it/s]Extractor Predicting: 228it [02:25,  1.52it/s]Extractor Predicting: 229it [02:26,  1.55it/s]Extractor Predicting: 230it [02:26,  1.55it/s]Extractor Predicting: 231it [02:27,  1.52it/s]Extractor Predicting: 232it [02:28,  1.54it/s]Extractor Predicting: 233it [02:28,  1.56it/s]Extractor Predicting: 234it [02:29,  1.60it/s]Extractor Predicting: 235it [02:29,  1.61it/s]Extractor Predicting: 236it [02:30,  1.67it/s]Extractor Predicting: 237it [02:31,  1.68it/s]Extractor Predicting: 238it [02:31,  1.68it/s]Extractor Predicting: 239it [02:32,  1.67it/s]Extractor Predicting: 240it [02:32,  1.64it/s]Extractor Predicting: 241it [02:33,  1.60it/s]Extractor Predicting: 242it [02:34,  1.58it/s]Extractor Predicting: 243it [02:34,  1.61it/s]Extractor Predicting: 244it [02:35,  1.62it/s]Extractor Predicting: 245it [02:36,  1.62it/s]Extractor Predicting: 246it [02:36,  1.54it/s]Extractor Predicting: 247it [02:37,  1.55it/s]Extractor Predicting: 248it [02:38,  1.53it/s]Extractor Predicting: 249it [02:38,  1.51it/s]Extractor Predicting: 250it [02:39,  1.50it/s]Extractor Predicting: 251it [02:40,  1.50it/s]Extractor Predicting: 252it [02:40,  1.50it/s]Extractor Predicting: 253it [02:41,  1.51it/s]Extractor Predicting: 254it [02:42,  1.53it/s]Extractor Predicting: 255it [02:42,  1.56it/s]Extractor Predicting: 256it [02:43,  1.51it/s]Extractor Predicting: 257it [02:44,  1.50it/s]Extractor Predicting: 258it [02:44,  1.49it/s]Extractor Predicting: 259it [02:45,  1.51it/s]Extractor Predicting: 260it [02:46,  1.50it/s]Extractor Predicting: 261it [02:46,  1.46it/s]Extractor Predicting: 262it [02:47,  1.47it/s]Extractor Predicting: 263it [02:48,  1.49it/s]Extractor Predicting: 264it [02:48,  1.51it/s]Extractor Predicting: 265it [02:49,  1.49it/s]Extractor Predicting: 266it [02:50,  1.48it/s]Extractor Predicting: 267it [02:50,  1.49it/s]Extractor Predicting: 268it [02:51,  1.50it/s]Extractor Predicting: 269it [02:52,  1.51it/s]Extractor Predicting: 270it [02:52,  1.53it/s]Extractor Predicting: 271it [02:53,  1.53it/s]Extractor Predicting: 272it [02:54,  1.52it/s]Extractor Predicting: 273it [02:54,  1.52it/s]Extractor Predicting: 274it [02:55,  1.49it/s]Extractor Predicting: 275it [02:56,  1.48it/s]Extractor Predicting: 276it [02:56,  1.51it/s]Extractor Predicting: 277it [02:57,  1.52it/s]Extractor Predicting: 278it [02:58,  1.51it/s]Extractor Predicting: 279it [02:58,  1.51it/s]Extractor Predicting: 280it [02:59,  1.56it/s]Extractor Predicting: 281it [02:59,  1.58it/s]Extractor Predicting: 282it [03:00,  1.61it/s]Extractor Predicting: 283it [03:01,  1.57it/s]Extractor Predicting: 284it [03:01,  1.55it/s]Extractor Predicting: 285it [03:02,  1.56it/s]Extractor Predicting: 286it [03:03,  1.62it/s]Extractor Predicting: 287it [03:03,  1.63it/s]Extractor Predicting: 288it [03:04,  1.57it/s]Extractor Predicting: 289it [03:05,  1.57it/s]Extractor Predicting: 290it [03:05,  1.57it/s]Extractor Predicting: 291it [03:06,  1.55it/s]Extractor Predicting: 292it [03:06,  1.60it/s]Extractor Predicting: 293it [03:07,  1.61it/s]Extractor Predicting: 294it [03:08,  1.63it/s]Extractor Predicting: 295it [03:08,  1.68it/s]Extractor Predicting: 296it [03:09,  1.68it/s]Extractor Predicting: 297it [03:09,  1.68it/s]Extractor Predicting: 298it [03:10,  1.58it/s]Extractor Predicting: 299it [03:11,  1.47it/s]Extractor Predicting: 300it [03:12,  1.45it/s]Extractor Predicting: 301it [03:12,  1.45it/s]Extractor Predicting: 302it [03:13,  1.49it/s]Extractor Predicting: 303it [03:14,  1.52it/s]Extractor Predicting: 304it [03:14,  1.51it/s]Extractor Predicting: 305it [03:15,  1.49it/s]Extractor Predicting: 306it [03:16,  1.49it/s]Extractor Predicting: 307it [03:16,  1.48it/s]Extractor Predicting: 308it [03:17,  1.48it/s]Extractor Predicting: 309it [03:18,  1.44it/s]Extractor Predicting: 310it [03:19,  1.30it/s]Extractor Predicting: 311it [03:19,  1.34it/s]Extractor Predicting: 312it [03:20,  1.39it/s]Extractor Predicting: 313it [03:21,  1.42it/s]Extractor Predicting: 314it [03:21,  1.45it/s]Extractor Predicting: 315it [03:22,  1.50it/s]Extractor Predicting: 316it [03:23,  1.48it/s]Extractor Predicting: 317it [03:23,  1.46it/s]Extractor Predicting: 318it [03:24,  1.42it/s]Extractor Predicting: 319it [03:25,  1.44it/s]Extractor Predicting: 320it [03:25,  1.45it/s]Extractor Predicting: 321it [03:26,  1.44it/s]Extractor Predicting: 322it [03:27,  1.43it/s]Extractor Predicting: 323it [03:28,  1.43it/s]Extractor Predicting: 324it [03:28,  1.43it/s]Extractor Predicting: 325it [03:29,  1.43it/s]Extractor Predicting: 326it [03:30,  1.42it/s]Extractor Predicting: 327it [03:30,  1.41it/s]Extractor Predicting: 328it [03:31,  1.42it/s]Extractor Predicting: 329it [03:32,  1.42it/s]Extractor Predicting: 330it [03:32,  1.43it/s]Extractor Predicting: 331it [03:33,  1.40it/s]Extractor Predicting: 332it [03:34,  1.41it/s]Extractor Predicting: 333it [03:35,  1.38it/s]Extractor Predicting: 334it [03:35,  1.41it/s]Extractor Predicting: 335it [03:36,  1.40it/s]Extractor Predicting: 336it [03:37,  1.36it/s]Extractor Predicting: 337it [03:38,  1.38it/s]Extractor Predicting: 338it [03:38,  1.38it/s]Extractor Predicting: 339it [03:39,  1.39it/s]Extractor Predicting: 340it [03:40,  1.41it/s]Extractor Predicting: 341it [03:40,  1.43it/s]Extractor Predicting: 342it [03:41,  1.44it/s]Extractor Predicting: 343it [03:42,  1.47it/s]Extractor Predicting: 344it [03:42,  1.50it/s]Extractor Predicting: 345it [03:43,  1.50it/s]Extractor Predicting: 346it [03:43,  1.62it/s]Extractor Predicting: 346it [03:43,  1.55it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.42857142857142855,
  "recall": 0.0003617072582589824,
  "score": 0.0007228044813877847,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6093
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6193, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.44it/s]Extractor Predicting: 19it [00:12,  1.43it/s]Extractor Predicting: 20it [00:13,  1.39it/s]Extractor Predicting: 21it [00:13,  1.42it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:15,  1.37it/s]Extractor Predicting: 24it [00:16,  1.38it/s]Extractor Predicting: 25it [00:16,  1.42it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:18,  1.49it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.59it/s]Extractor Predicting: 30it [00:19,  1.65it/s]Extractor Predicting: 31it [00:20,  1.74it/s]Extractor Predicting: 32it [00:20,  1.78it/s]Extractor Predicting: 33it [00:21,  1.83it/s]Extractor Predicting: 34it [00:21,  1.82it/s]Extractor Predicting: 35it [00:22,  1.86it/s]Extractor Predicting: 36it [00:23,  1.84it/s]Extractor Predicting: 37it [00:23,  1.81it/s]Extractor Predicting: 38it [00:24,  1.83it/s]Extractor Predicting: 39it [00:24,  1.82it/s]Extractor Predicting: 40it [00:25,  1.83it/s]Extractor Predicting: 41it [00:25,  1.82it/s]Extractor Predicting: 42it [00:26,  1.80it/s]Extractor Predicting: 43it [00:26,  1.80it/s]Extractor Predicting: 44it [00:27,  1.82it/s]Extractor Predicting: 45it [00:27,  1.87it/s]Extractor Predicting: 46it [00:28,  1.88it/s]Extractor Predicting: 47it [00:29,  1.86it/s]Extractor Predicting: 48it [00:29,  1.84it/s]Extractor Predicting: 49it [00:30,  1.89it/s]Extractor Predicting: 50it [00:30,  1.83it/s]Extractor Predicting: 51it [00:31,  1.78it/s]Extractor Predicting: 52it [00:31,  1.81it/s]Extractor Predicting: 53it [00:32,  1.81it/s]Extractor Predicting: 54it [00:32,  1.81it/s]Extractor Predicting: 55it [00:33,  1.84it/s]Extractor Predicting: 56it [00:33,  1.83it/s]Extractor Predicting: 57it [00:34,  1.86it/s]Extractor Predicting: 58it [00:35,  1.74it/s]Extractor Predicting: 59it [00:35,  1.63it/s]Extractor Predicting: 60it [00:36,  1.54it/s]Extractor Predicting: 61it [00:37,  1.49it/s]Extractor Predicting: 62it [00:37,  1.47it/s]Extractor Predicting: 63it [00:38,  1.46it/s]Extractor Predicting: 64it [00:39,  1.45it/s]Extractor Predicting: 65it [00:40,  1.44it/s]Extractor Predicting: 66it [00:40,  1.44it/s]Extractor Predicting: 67it [00:41,  1.43it/s]Extractor Predicting: 68it [00:42,  1.41it/s]Extractor Predicting: 69it [00:42,  1.40it/s]Extractor Predicting: 70it [00:43,  1.42it/s]Extractor Predicting: 71it [00:44,  1.40it/s]Extractor Predicting: 72it [00:45,  1.41it/s]Extractor Predicting: 73it [00:45,  1.42it/s]Extractor Predicting: 74it [00:46,  1.44it/s]Extractor Predicting: 75it [00:47,  1.46it/s]Extractor Predicting: 76it [00:47,  1.47it/s]Extractor Predicting: 77it [00:48,  1.78it/s]Extractor Predicting: 77it [00:48,  1.60it/s]
{
  "path_pred": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_4/extractor/results_multi_is_eval_False.json"
}
